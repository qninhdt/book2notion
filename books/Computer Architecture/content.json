{
  "chapters": [
    {
      "name": "Basic Concepts and Computer Evolution",
      "sections": [
        {
          "name": "Organization and Architecture",
          "content": "In describing computers, a distinction is often made between\n   *computer architecture*\n   and\n   *computer organization*\n   . Although it is difficult to give precise definitions for these terms, a consensus exists about the general areas covered by each. For example, see [VRAN80], [SIEW82], and [BELL78a]; an interesting alternative view is presented in [REDD76].\n\n\n**Computer architecture**\n   refers to those attributes of a system visible to a programmer or, put another way, those attributes that have a direct impact on the logical execution of a program. A term that is often used interchangeably with computer architecture is\n   **instruction set architecture (ISA)**\n   . The ISA defines instruction formats, instruction opcodes, registers, instruction and data memory; the effect of executed instructions on the registers and memory; and an algorithm for controlling instruction execution.\n   **Computer organization**\n   refers to the operational units and their interconnections that realize the architectural specifications. Examples of architectural attributes include the instruction set, the number of bits used to represent various data types (e.g., numbers, characters), I/O mechanisms, and techniques for addressing memory. Organizational attributes include those hardware details transparent to the programmer, such as control signals; interfaces between the computer and peripherals; and the memory technology used.\n\n\nFor example, it is an architectural design issue whether a computer will have a multiply instruction. It is an organizational issue whether that instruction will be implemented by a special multiply unit or by a mechanism that makes repeated use of the add unit of the system. The organizational decision may be based on the anticipated frequency of use of the multiply instruction, the relative speed of the two approaches, and the cost and physical size of a special multiply unit.\n\n\nHistorically, and still today, the distinction between architecture and organization has been an important one. Many computer manufacturers offer a family of computer models, all with the same architecture but with differences in organization. Consequently, the different models in the family have different price and performance characteristics. Furthermore, a particular architecture may span many years and encompass a number of different computer models, its organization changing with changing technology. A prominent example of both these phenomena is the IBM System/370 architecture. This architecture was first introduced in 1970 and\n\n\nincluded a number of models. The customer with modest requirements could buy a cheaper, slower model and, if demand increased, later upgrade to a more expensive, faster model without having to abandon software that had already been developed. Over the years, IBM has introduced many new models with improved technology to replace older models, offering the customer greater speed, lower cost, or both. These newer models retained the same architecture so that the customer's software investment was protected. Remarkably, the System/370 architecture, with a few enhancements, has survived to this day as the architecture of IBM's mainframe product line.\n\n\nIn a class of computers called microcomputers, the relationship between architecture and organization is very close. Changes in technology not only influence organization but also result in the introduction of more powerful and more complex architectures. Generally, there is less of a requirement for generation-to-generation compatibility for these smaller machines. Thus, there is more interplay between organizational and architectural design decisions. An intriguing example of this is the reduced instruction set computer (RISC), which we examine in Chapter 15.\n\n\nThis book examines both computer organization and computer architecture. The emphasis is perhaps more on the side of organization. However, because a computer organization must be designed to implement a particular architectural specification, a thorough treatment of organization requires a detailed examination of architecture as well."
        },
        {
          "name": "Structure and Function",
          "content": "A computer is a complex system; contemporary computers contain millions of elementary electronic components. How, then, can one clearly describe them? The key is to recognize the hierarchical nature of most complex systems, including the computer [SIMO96]. A hierarchical system is a set of interrelated subsystems, each of the latter, in turn, hierarchical in structure until we reach some lowest level of elementary subsystem.\n\n\nThe hierarchical nature of complex systems is essential to both their design and their description. The designer need only deal with a particular level of the system at a time. At each level, the system consists of a set of components and their interrelationships. The behavior at each level depends only on a simplified, abstracted characterization of the system at the next lower level. At each level, the designer is concerned with structure and function:\n\n\n  * ■\n    **Structure:**\n    The way in which the components are interrelated.\n  * ■\n    **Function:**\n    The operation of each individual component as part of the structure.\n\n\nIn terms of description, we have two choices: starting at the bottom and building up to a complete description, or beginning with a top view and decomposing the system into its subparts. Evidence from a number of fields suggests that the top-down approach is the clearest and most effective [WEIN75].\n\n\nThe approach taken in this book follows from this viewpoint. The computer system will be described from the top down. We begin with the major components of a computer, describing their structure and function, and proceed to successively\n\n\nlower layers of the hierarchy. The remainder of this section provides a very brief overview of this plan of attack.\n\n\n\n\n**Function**\n\n\nBoth the structure and functioning of a computer are, in essence, simple. In general terms, there are only four basic functions that a computer can perform:\n\n\n  * ■\n    **Data processing:**\n    Data may take a wide variety of forms, and the range of processing requirements is broad. However, we shall see that there are only a few fundamental methods or types of data processing.\n  * ■\n    **Data storage:**\n    Even if the computer is processing data on the fly (i.e., data come in and get processed, and the results go out immediately), the computer must temporarily store at least those pieces of data that are being worked on at any given moment. Thus, there is at least a short-term data storage function. Equally important, the computer performs a long-term data storage function. Files of data are stored on the computer for subsequent retrieval and update.\n  * ■\n    **Data movement:**\n    The computer's operating environment consists of devices that serve as either sources or destinations of data. When data are received from or delivered to a device that is directly connected to the computer, the process is known as\n    *input-output (I/O)*\n    , and the device is referred to as a\n    *peripheral*\n    . When data are moved over longer distances, to or from a remote device, the process is known as\n    *data communications*\n    .\n  * ■\n    **Control:**\n    Within the computer, a control unit manages the computer's resources and orchestrates the performance of its functional parts in response to instructions.\n\n\nThe preceding discussion may seem absurdly generalized. It is certainly possible, even at a top level of computer structure, to differentiate a variety of functions, but to quote [SIEW82]:\n\n\nThere is remarkably little shaping of computer structure to fit the function to be performed. At the root of this lies the general-purpose nature of computers, in which all the functional specialization occurs at the time of programming and not at the time of design.\n\n\n\n\n**Structure**\n\n\nWe now look in a general way at the internal structure of a computer. We begin with a traditional computer with a single processor that employs a microprogrammed control unit, then examine a typical multicore structure.\n\n\n**SIMPLE SINGLE-PROCESSOR COMPUTER**\n   Figure 1.1 provides a hierarchical view of the internal structure of a traditional single-processor computer. There are four main structural components:\n\n\n  * ■\n    **Central processing unit (CPU):**\n    Controls the operation of the computer and performs its data processing functions; often simply referred to as\n    **processor**\n    .\n  * ■\n    **Main memory:**\n    Stores data.\n\n\n\n\n![Figure 1.1: The Computer: Top-Level Structure. This diagram illustrates the hierarchical structure of a computer. At the top level, a large circle labeled 'COMPUTER' contains three overlapping circles: 'I/O', 'Main memory', and 'CPU'. A dashed line connects the 'CPU' circle to a second, larger circle labeled 'CPU'. This second circle contains three overlapping circles: 'Registers', 'ALU', and 'Control unit'. A dashed line connects the 'Control unit' circle to a third, large circle labeled 'CONTROL UNIT'. This third circle contains three overlapping circles: 'Sequencing logic', 'Control unit registers and decoders', and 'Control memory'.](images/image_0001.jpeg)\n\n\nFigure 1.1: The Computer: Top-Level Structure. This diagram illustrates the hierarchical structure of a computer. At the top level, a large circle labeled 'COMPUTER' contains three overlapping circles: 'I/O', 'Main memory', and 'CPU'. A dashed line connects the 'CPU' circle to a second, larger circle labeled 'CPU'. This second circle contains three overlapping circles: 'Registers', 'ALU', and 'Control unit'. A dashed line connects the 'Control unit' circle to a third, large circle labeled 'CONTROL UNIT'. This third circle contains three overlapping circles: 'Sequencing logic', 'Control unit registers and decoders', and 'Control memory'.\n\n\n**Figure 1.1**\n   The Computer: Top-Level Structure\n\n\n  * ■\n    **I/O:**\n    Moves data between the computer and its external environment.\n  * ■\n    **System interconnection:**\n    Some mechanism that provides for communication among CPU, main memory, and I/O. A common example of system interconnection is by means of a\n    **system bus**\n    , consisting of a number of conducting wires to which all the other components attach.\n\n\nThere may be one or more of each of the aforementioned components. Traditionally, there has been just a single processor. In recent years, there has been increasing use of multiple processors in a single computer. Some design issues relating to multiple processors crop up and are discussed as the text proceeds; Part Five focuses on such computers.\n\n\nEach of these components will be examined in some detail in Part Two. However, for our purposes, the most interesting and in some ways the most complex component is the CPU. Its major structural components are as follows:\n\n\n  * ■\n    **Control unit:**\n    Controls the operation of the CPU and hence the computer.\n  * ■\n    **Arithmetic and logic unit (ALU):**\n    Performs the computer's data processing functions.\n  * ■\n    **Registers:**\n    Provides storage internal to the CPU.\n  * ■\n    **CPU interconnection:**\n    Some mechanism that provides for communication among the control unit, ALU, and registers.\n\n\nPart Three covers these components, where we will see that complexity is added by the use of parallel and pipelined organizational techniques. Finally, there are several approaches to the implementation of the control unit; one common approach is a\n   *microprogrammed*\n   implementation. In essence, a microprogrammed control unit operates by executing microinstructions that define the functionality of the control unit. With this approach, the structure of the control unit can be depicted, as in Figure 1.1. This structure is examined in Part Four.\n\n\n***MULTICORE COMPUTER STRUCTURE***\n   As was mentioned, contemporary computers generally have multiple processors. When these processors all reside on a single chip, the term\n   *multicore computer*\n   is used, and each processing unit (consisting of a control unit, ALU, registers, and perhaps cache) is called a\n   *core*\n   . To clarify the terminology, this text will use the following definitions.\n\n\n  * ■\n    **Central processing unit (CPU):**\n    That portion of a computer that fetches and executes instructions. It consists of an ALU, a control unit, and registers. In a system with a single processing unit, it is often simply referred to as a\n    *processor*\n    .\n  * ■\n    **Core:**\n    An individual processing unit on a processor chip. A core may be equivalent in functionality to a CPU on a single-CPU system. Other specialized processing units, such as one optimized for vector and matrix operations, are also referred to as cores.\n  * ■\n    **Processor:**\n    A physical piece of silicon containing one or more cores. The processor is the computer component that interprets and executes instructions. If a processor contains multiple cores, it is referred to as a\n    **multicore processor**\n    .\n\n\nAfter about a decade of discussion, there is broad industry consensus on this usage.\n\n\nAnother prominent feature of contemporary computers is the use of multiple layers of memory, called\n   *cache memory*\n   , between the processor and main memory. Chapter 4 is devoted to the topic of cache memory. For our purposes in this section, we simply note that a cache memory is smaller and faster than main memory and is used to speed up memory access, by placing in the cache data from main memory, that is likely to be used in the near future. A greater performance improvement may be obtained by using multiple levels of cache, with level 1 (L1) closest to the core and additional levels (L2, L3, and so on) progressively farther from the core. In this scheme, level\n   \n    n\n   \n   is smaller and faster than level\n   \n    n + 1\n   \n   .\n\n\nFigure 1.2 is a simplified view of the principal components of a typical multicore computer. Most computers, including embedded computers in smartphones and tablets, plus personal computers, laptops, and workstations, are housed on a motherboard. Before describing this arrangement, we need to define some terms. A\n   **printed circuit board (PCB)**\n   is a rigid, flat board that holds and interconnects chips and other electronic components. The board is made of layers, typically two to ten, that interconnect components via copper pathways that are etched into the board. The main printed circuit board in a computer is called a system board or\n   **motherboard**\n   , while smaller ones that plug into the slots in the main board are called expansion boards.\n\n\nThe most prominent elements on the motherboard are the chips. A\n   **chip**\n   is a single piece of semiconducting material, typically silicon, upon which electronic circuits and logic gates are fabricated. The resulting product is referred to as an\n   **integrated circuit**\n   .\n\n\n\n\n![Figure 1.2: Simplified View of Major Elements of a Multicore Computer. The diagram shows three nested boxes. The outermost box is the MOTHERBOARD, containing Main memory chips (5), I/O chips (4), and a Processor chip (1). The Processor chip is expanded into a PROCESSOR CHIP box, which contains 4 Cores and 2 L3 cache blocks. One of the Cores is further expanded into a CORE box, which contains Instruction logic, Arithmetic and logic unit (ALU), Load/store logic, L1 I-cache, L1 data cache, L2 instruction cache, and L2 data cache.](images/image_0002.jpeg)\n\n\nThe diagram illustrates the hierarchical structure of a multicore computer's hardware components:\n\n\n  * **MOTHERBOARD**\n     (outermost box):\n       * Main memory chips (5)\n  * I/O chips (4)\n  * Processor chip (1)\n  * **PROCESSOR CHIP**\n     (middle box, connected to the Processor chip on the motherboard):\n       * 4 Cores (arranged in a 2x2 grid)\n  * 2 L3 cache blocks (one below each column of cores)\n  * **CORE**\n     (innermost box, connected to one of the cores on the processor chip):\n       * Instruction logic\n  * Arithmetic and logic unit (ALU)\n  * Load/store logic\n  * L1 I-cache\n  * L1 data cache\n  * L2 instruction cache\n  * L2 data cache\n\n\nFigure 1.2: Simplified View of Major Elements of a Multicore Computer. The diagram shows three nested boxes. The outermost box is the MOTHERBOARD, containing Main memory chips (5), I/O chips (4), and a Processor chip (1). The Processor chip is expanded into a PROCESSOR CHIP box, which contains 4 Cores and 2 L3 cache blocks. One of the Cores is further expanded into a CORE box, which contains Instruction logic, Arithmetic and logic unit (ALU), Load/store logic, L1 I-cache, L1 data cache, L2 instruction cache, and L2 data cache.\n\n\n**Figure 1.2**\n   Simplified View of Major Elements of a Multicore Computer\n\n\nThe motherboard contains a slot or socket for the processor chip, which typically contains multiple individual cores, in what is known as a\n   *multicore processor*\n   . There are also slots for memory chips, I/O controller chips, and other key computer components. For desktop computers, expansion slots enable the inclusion of more components on expansion boards. Thus, a modern motherboard connects only a few individual chip components, with each chip containing from a few thousand up to hundreds of millions of transistors.\n\n\nFigure 1.2 shows a processor chip that contains eight cores and an L3 cache. Not shown is the logic required to control operations between the cores and the cache and between the cores and the external circuitry on the motherboard. The figure indicates that the L3 cache occupies two distinct portions of the chip surface. However, typically, all cores have access to the entire L3 cache via the aforementioned control circuits. The processor chip shown in Figure 1.2 does not represent any specific product, but provides a general idea of how such chips are laid out.\n\n\nNext, we zoom in on the structure of a single core, which occupies a portion of the processor chip. In general terms, the functional elements of a core are:\n\n\n  * ■\n    **Instruction logic:**\n    This includes the tasks involved in fetching instructions, and decoding each instruction to determine the instruction operation and the memory locations of any operands.\n  * ■\n    **Arithmetic and logic unit (ALU):**\n    Performs the operation specified by an instruction.\n  * ■\n    **Load/store logic:**\n    Manages the transfer of data to and from main memory via cache.\n\n\nThe core also contains an L1 cache, split between an instruction cache (I-cache) that is used for the transfer of instructions to and from main memory, and an L1 data cache, for the transfer of operands and results. Typically, today's processor chips also include an L2 cache as part of the core. In many cases, this cache is also split between instruction and data caches, although a combined, single L2 cache is also used.\n\n\nKeep in mind that this representation of the layout of the core is only intended to give a general idea of internal core structure. In a given product, the functional elements may not be laid out as the three distinct elements shown in Figure 1.2, especially if some or all of these functions are implemented as part of a microprogrammed control unit.\n\n\n**EXAMPLES**\n   It will be instructive to look at some real-world examples that illustrate the hierarchical structure of computers. Figure 1.3 is a photograph of the motherboard for a computer built around two Intel Quad-Core Xeon processor chips. Many of the elements labeled on the photograph are discussed subsequently in this book. Here, we mention the most important, in addition to the processor sockets:\n\n\n  * ■ PCI-Express slots for a high-end display adapter and for additional peripherals (Section 3.6 describes PCIe).\n  * ■ Ethernet controller and Ethernet ports for network connections.\n  * ■ USB sockets for peripheral devices.\n\n\n\n\n![Figure 1.3: Motherboard with Two Intel Quad-Core Xeon Processors. The image shows a top-down view of a server motherboard. Two large black cooling fans are positioned over the central processing units (CPUs). Various components are labeled with lines pointing to their locations: '2x Quad-Core Intel® Xeon® Processors with Integrated Memory Controllers' points to the CPU sockets; 'Six Channel DDR3-1333 Memory Interfaces Up to 48GB' points to the memory slots; 'Intel® 3420 Chipset' points to a chip near the CPU; 'Serial ATA/300 (SATA) Interfaces' points to the SATA ports; '2x USB 2.0 Internal' and '2x USB 2.0 External' point to the USB headers and ports; 'VGA Video Output' points to the video connector; 'BIOS' points to the chip on the right; '2x Ethernet Ports 10/100/1000Base-T' points to the network ports; 'Ethernet Controller' points to the network chip; 'Power & Backplane I/O Connector C' points to the power connector; 'PCI Express® Connector B' points to a PCIe slot; 'PCI Express® Connector A' points to another PCIe slot; and 'Clock' points to a clock source component.](images/image_0003.jpeg)\n\n\nFigure 1.3: Motherboard with Two Intel Quad-Core Xeon Processors. The image shows a top-down view of a server motherboard. Two large black cooling fans are positioned over the central processing units (CPUs). Various components are labeled with lines pointing to their locations: '2x Quad-Core Intel® Xeon® Processors with Integrated Memory Controllers' points to the CPU sockets; 'Six Channel DDR3-1333 Memory Interfaces Up to 48GB' points to the memory slots; 'Intel® 3420 Chipset' points to a chip near the CPU; 'Serial ATA/300 (SATA) Interfaces' points to the SATA ports; '2x USB 2.0 Internal' and '2x USB 2.0 External' point to the USB headers and ports; 'VGA Video Output' points to the video connector; 'BIOS' points to the chip on the right; '2x Ethernet Ports 10/100/1000Base-T' points to the network ports; 'Ethernet Controller' points to the network chip; 'Power & Backplane I/O Connector C' points to the power connector; 'PCI Express® Connector B' points to a PCIe slot; 'PCI Express® Connector A' points to another PCIe slot; and 'Clock' points to a clock source component.\n\n\n**Figure 1.3**\n   Motherboard with Two Intel Quad-Core Xeon Processors\n\n\nSource: Chassis Plans,\n   www.chassis-plans.com\n\n\n  * ■ Serial ATA (SATA) sockets for connection to disk memory (Section 7.7 discusses Ethernet, USB, and SATA).\n  * ■ Interfaces for DDR (double data rate) main memory chips (Section 5.3 discusses DDR).\n  * ■ Intel 3420 chipset is an I/O controller for direct memory access operations between peripheral devices and main memory (Section 7.5 discusses DDR).\n\n\nFollowing our top-down strategy, as illustrated in Figures 1.1 and 1.2, we can now zoom in and look at the internal structure of a processor chip. For variety, we look at an IBM chip instead of the Intel processor chip. Figure 1.4 is a photograph of the processor chip for the IBM zEnterprise EC12 mainframe computer. This chip has 2.75 billion transistors. The superimposed labels indicate how the silicon real estate of the chip is allocated. We see that this chip has six cores, or processors. In addition, there are two large areas labeled L3 cache, which are shared by all six processors. The L3 control logic controls traffic between the L3 cache and the cores and between the L3 cache and the external environment. Additionally, there is storage control (SC) logic between the cores and the L3 cache. The memory controller (MC) function controls access to memory external to the chip. The GX I/O bus controls the interface to the channel adapters accessing the I/O.\n\n\nGoing down one level deeper, we examine the internal structure of a single core, as shown in the photograph of Figure 1.5. Keep in mind that this is a portion of the silicon surface area making up a single-processor chip. The main sub-areas within this core area are the following:\n\n\n  * ■\n    **ISU (instruction sequence unit)**\n    : Determines the sequence in which instructions are executed in what is referred to as a superscalar architecture (Chapter 16).\n  * ■\n    **IFU (instruction fetch unit)**\n    : Logic for fetching instructions.\n\n\n\n\n![Figure 1.4: zEnterprise EC12 Processor Unit (PU) chip diagram. This is a top-down view of a silicon die. It features a central 'L3 Cache Control' block surrounded by six 'CORE' blocks arranged in a 2x3 grid. Each core is connected to an 'SC i/o' (System Controller I/O) block. The die also includes 'G X i/o' blocks on the left and right sides, and 'M C i/o' blocks on the top and bottom edges. The entire chip is labeled 'zEnterprise EC12'.](images/image_0004.jpeg)\n\n\nFigure 1.4: zEnterprise EC12 Processor Unit (PU) chip diagram. This is a top-down view of a silicon die. It features a central 'L3 Cache Control' block surrounded by six 'CORE' blocks arranged in a 2x3 grid. Each core is connected to an 'SC i/o' (System Controller I/O) block. The die also includes 'G X i/o' blocks on the left and right sides, and 'M C i/o' blocks on the top and bottom edges. The entire chip is labeled 'zEnterprise EC12'.\n\n\n**Figure 1.4**\n   zEnterprise EC12 Processor Unit (PU) chip diagram\n\n\nSource: IBM zEnterprise EC12 Technical Guide, December 2013, SG24-8049-01. IBM, Reprinted by Permission\n\n\n\n\n![Figure 1.5: zEnterprise EC12 Core layout. This diagram shows the internal block diagram of a single core. At the top is the 'IFU' (Instruction Fetch Unit). Below it is the 'IDU' (Instruction Decode Unit). To the right of the IFU is the 'ISU' (Instruction Storage Unit), which contains the 'FXU' (Fixed-Point Unit) and 'BFU' (Binary Floating-Point Unit). Below the IDU is the 'I-cache' (Instruction Cache). To the left of the IDU is the 'XU' (Translation Unit). Below the XU is the 'Instr. L2' (Instruction L2 Cache). To the right of the XU is the 'L2 Control' block. Below the L2 Control block is the 'COP' (Control and Operations Processor). To the right of the L2 Control block is the 'LSU' (Load-Store Unit), which contains the 'Data-L2' (Data L2 Cache). To the right of the LSU is the 'DFU' (Decimal Floating-Point Unit). To the right of the DFU is the 'RU' (Recovery Unit).](images/image_0005.jpeg)\n\n\nFigure 1.5: zEnterprise EC12 Core layout. This diagram shows the internal block diagram of a single core. At the top is the 'IFU' (Instruction Fetch Unit). Below it is the 'IDU' (Instruction Decode Unit). To the right of the IFU is the 'ISU' (Instruction Storage Unit), which contains the 'FXU' (Fixed-Point Unit) and 'BFU' (Binary Floating-Point Unit). Below the IDU is the 'I-cache' (Instruction Cache). To the left of the IDU is the 'XU' (Translation Unit). Below the XU is the 'Instr. L2' (Instruction L2 Cache). To the right of the XU is the 'L2 Control' block. Below the L2 Control block is the 'COP' (Control and Operations Processor). To the right of the L2 Control block is the 'LSU' (Load-Store Unit), which contains the 'Data-L2' (Data L2 Cache). To the right of the LSU is the 'DFU' (Decimal Floating-Point Unit). To the right of the DFU is the 'RU' (Recovery Unit).\n\n\n**Figure 1.5**\n   zEnterprise EC12 Core layout\n\n\nSource: IBM zEnterprise EC12 Technical Guide, December 2013, SG24-8049-01. IBM, Reprinted by Permission\n\n\n  * ■\n    **IDU (instruction decode unit):**\n    The IDU is fed from the IFU buffers, and is responsible for the parsing and decoding of all z/Architecture operation codes.\n  * ■\n    **LSU (load-store unit):**\n    The LSU contains the 96-kB L1 data cache,\n    \n     1\n    \n    and manages data traffic between the L2 data cache and the functional execution units. It is responsible for handling all types of operand accesses of all lengths, modes, and formats as defined in the z/Architecture.\n  * ■\n    **XU (translation unit):**\n    This unit translates logical addresses from instructions into physical addresses in main memory. The XU also contains a translation lookaside buffer (TLB) used to speed up memory access. TLBs are discussed in Chapter 8.\n  * ■\n    **FXU (fixed-point unit):**\n    The FXU executes fixed-point arithmetic operations.\n  * ■\n    **BFU (binary floating-point unit):**\n    The BFU handles all binary and hexadecimal floating-point operations, as well as fixed-point multiplication operations.\n  * ■\n    **DFU (decimal floating-point unit):**\n    The DFU handles both fixed-point and floating-point operations on numbers that are stored as decimal digits.\n  * ■\n    **RU (recovery unit):**\n    The RU keeps a copy of the complete state of the system that includes all registers, collects hardware fault signals, and manages the hardware recovery actions.\n\n\n1\n   \n   kB = kilobyte = 2048 bytes. Numerical prefixes are explained in a document under the “Other Useful” tab at ComputerScienceStudent.com.\n\n\n  * ■\n    **COP (dedicated co-processor):**\n    The COP is responsible for data compression and encryption functions for each core.\n  * ■\n    **I-cache:**\n    This is a 64-kB L1 instruction cache, allowing the IFU to prefetch instructions before they are needed.\n  * ■\n    **L2 control:**\n    This is the control logic that manages the traffic through the two L2 caches.\n  * ■\n    **Data-L2:**\n    A 1-MB L2 data cache for all memory traffic other than instructions.\n  * ■\n    **Instr-L2:**\n    A 1-MB L2 instruction cache.\n\n\nAs we progress through the book, the concepts introduced in this section will become clearer."
        },
        {
          "name": "A Brief History of Computers",
          "content": "In this section, we provide a brief overview of the history of the development of computers. This history is interesting in itself, but more importantly, provides a basic introduction to many important concepts that we deal with throughout the book.\n\n\n\n\n**The First Generation: Vacuum Tubes**\n\n\nThe first generation of computers used vacuum tubes for digital logic elements and memory. A number of research and then commercial computers were built using vacuum tubes. For our purposes, it will be instructive to examine perhaps the most famous first-generation computer, known as the IAS computer.\n\n\nA fundamental design approach first implemented in the IAS computer is known as the\n   *stored-program concept*\n   . This idea is usually attributed to the mathematician John von Neumann. Alan Turing developed the idea at about the same time. The first publication of the idea was in a 1945 proposal by von Neumann for a new computer, the EDVAC (Electronic Discrete Variable Computer).\n   \n    3\n\n\nIn 1946, von Neumann and his colleagues began the design of a new stored-program computer, referred to as the IAS computer, at the Princeton Institute for Advanced Studies. The IAS computer, although not completed until 1952, is the prototype of all subsequent general-purpose computers.\n   \n    4\n\n\nFigure 1.6 shows the structure of the IAS computer (compare with Figure 1.1). It consists of\n\n\n  * ■ A\n    **main memory**\n    , which stores both data and instructions\n    \n     5\n  * ■ An\n    **arithmetic and logic unit (ALU)**\n    capable of operating on binary data\n\n\n2\n   \n   This book's Companion Web site (\n   WilliamStallings.com/ComputerOrganization\n   ) contains several links to sites that provide photographs of many of the devices and components discussed in this section.\n\n\n3\n   \n   The 1945 report on EDVAC is available at\n   box.com/COA10e\n   .\n\n\n4\n   \n   A 1954 report [GOLD54] describes the implemented IAS machine and lists the final instruction set. It is available at\n   box.com/COA10e\n   .\n\n\n5\n   \n   In this book, unless otherwise noted, the term\n   *instruction*\n   refers to a machine instruction that is directly interpreted and executed by the processor, in contrast to a statement in a high-level language, such as Ada or C++, which must first be compiled into a series of machine instructions before being executed.\n\n\n\n\n![Diagram of the IAS Structure showing the Central processing unit (CPU), Main memory (M), and Input-output equipment (I, O).](images/image_0006.jpeg)\n\n\nThe diagram illustrates the IAS Structure, which is divided into three main components: the Central processing unit (CPU), Main memory (M), and Input-output equipment (I, O).\n\n\n**Central processing unit (CPU):**\n    This is a dashed box containing two sub-units: the Arithmetic-logic unit (CA) and the Program control unit (CC).\n\n\n  * **Arithmetic-logic unit (CA):**\n     Contains the Accumulator register (AC), Multiply-quotient register (MQ), Arithmetic-logic circuits, and Memory buffer register (MBR). AC and MQ are connected to the Arithmetic-logic circuits. MBR is connected to both the Arithmetic-logic circuits and the Program control unit (CC).\n  * **Program control unit (CC):**\n     Contains the Program counter (PC), Instruction buffer register (IBR), Memory address register (MAR), Instruction register (IR), and Control circuits. PC and MAR are connected to each other. PC and IBR are connected to the Control circuits. IBR and IR are connected to each other. The Control circuits send Control signals to the IR and the Main memory (M).\n\n\n**Main memory (M):**\n    A vertical stack of memory cells labeled M(0) through M(4095). It receives Addresses from the MAR and Instructions and data from the CPU and I/O equipment.\n\n\n**Input-output equipment (I, O):**\n    A vertical block that exchanges Instructions and data with the CPU and Main memory (M).\n\n\n**Legend:**\n\n\n  * AC: Accumulator register\n  * MQ: multiply-quotient register\n  * MBR: memory buffer register\n  * IBR: instruction buffer register\n  * PC: program counter\n  * MAR: memory address register\n  * IR: instruction register\n\n\nDiagram of the IAS Structure showing the Central processing unit (CPU), Main memory (M), and Input-output equipment (I, O).\n\n\n**Figure 1.6**\n   IAS Structure\n\n\n  * ■ A\n    **control unit**\n    , which interprets the instructions in memory and causes them to be executed\n  * ■\n    **Input–output (I/O)**\n    equipment operated by the control unit\n\n\nThis structure was outlined in von Neumann's earlier proposal, which is worth quoting in part at this point [VONN45]:\n\n\n2.2\n   **First:**\n   Since the device is primarily a computer, it will have to perform the elementary operations of arithmetic most frequently. These are addition, subtraction, multiplication, and division. It is therefore reasonable that it should contain specialized organs for just these operations.\n\n\nIt must be observed, however, that while this principle as such is probably sound, the specific way in which it is realized requires close scrutiny. At any rate a\n   *central arithmetical*\n   part of the device will probably have to exist, and this constitutes the\n   *first specific part: CA*\n   .\n\n\n2.3\n   **Second:**\n   The logical control of the device, that is, the proper sequencing of its operations, can be most efficiently carried out by a central control organ. If the device is to be\n   *elastic*\n   , that is, as nearly as possible\n   *all purpose*\n   , then a distinction must be made between the specific instructions given for and defining a particular problem, and the general control organs that see to it that these instructions—no matter what they are—are carried out. The former must be stored in some way; the latter are represented by definite operating parts of the device. By the\n   *central control*\n   we mean this latter function only, and the organs that perform it form the\n   *second specific part: CC*\n   .\n\n\n2.4\n   **Third:**\n   Any device that is to carry out long and complicated sequences of operations (specifically of calculations) must have a considerable memory . . .\n\n\nThe instructions which govern a complicated problem may constitute considerable material, particularly so if the code is circumstantial (which it is in most arrangements). This material must be remembered.\n\n\nAt any rate, the total\n   *memory*\n   constitutes the\n   *third specific part of the device: M*\n   .\n\n\n2.6 The three specific parts CA, CC (together C), and M correspond to the\n   *associative*\n   neurons in the human nervous system. It remains to discuss the equivalents of the\n   *sensory*\n   or\n   *aferent*\n   and the\n   *motor*\n   or\n   *efferent*\n   neurons. These are the\n   *input*\n   and\n   *output*\n   organs of the device.\n\n\nThe device must be endowed with the ability to maintain input and output (sensory and motor) contact with some specific medium of this type. The medium will be called the\n   *outside recording medium of the device: R*\n   .\n\n\n2.7\n   **Fourth:**\n   The device must have organs to transfer information from R into its specific parts C and M. These organs form its\n   *input*\n   , the\n   *fourth specific part: I*\n   . It will be seen that it is best to make all transfers from R (by I) into M and never directly from C.\n\n\n2.8\n   **Fifth:**\n   The device must have organs to transfer from its specific parts C and M into R. These organs form its\n   *output*\n   , the\n   *fifth specific part: O*\n   . It will be seen that it is again best to make all transfers from M (by O) into R, and never directly from C.\n\n\nWith rare exceptions, all of today's computers have this same general structure and function and are thus referred to as\n   *von Neumann machines*\n   . Thus, it is worthwhile at this point to describe briefly the operation of the IAS computer [BURK46, GOLD54]. Following [HAYE98], the terminology and notation of von Neumann\n\n\nare changed in the following to conform more closely to modern usage; the examples accompanying this discussion are based on that latter text.\n\n\nThe memory of the IAS consists of 4,096 storage locations, called\n   *words*\n   , of 40 binary digits (bits) each.\n   \n    6\n   \n   Both data and instructions are stored there. Numbers are represented in binary form, and each instruction is a binary code. Figure 1.7 illustrates these formats. Each number is represented by a sign bit and a 39-bit value. A word may alternatively contain two 20-bit instructions, with each instruction consisting of an 8-bit operation code (opcode) specifying the operation to be performed and a 12-bit address designating one of the words in memory (numbered from 0 to 999).\n\n\nThe control unit operates the IAS by fetching instructions from memory and executing them one at a time. We explain these operations with reference to Figure 1.6. This figure reveals that both the control unit and the ALU contain storage locations, called\n   *registers*\n   , defined as follows:\n\n\n  * ■\n    **Memory buffer register (MBR):**\n    Contains a word to be stored in memory or sent to the I/O unit, or is used to receive a word from memory or from the I/O unit.\n  * ■\n    **Memory address register (MAR):**\n    Specifies the address in memory of the word to be written from or read into the MBR.\n  * ■\n    **Instruction register (IR):**\n    Contains the 8-bit opcode instruction being executed.\n  * ■\n    **Instruction buffer register (IBR):**\n    Employed to hold temporarily the right-hand instruction from a word in memory.\n  * ■\n    **Program counter (PC):**\n    Contains the address of the next instruction pair to be fetched from memory.\n  * ■\n    **Accumulator (AC) and multiplier quotient (MQ):**\n    Employed to hold temporarily operands and results of ALU operations. For example, the result\n\n\n\n\n![Figure 1.7 IAS Memory Formats. (a) Number word: A 40-bit word with a sign bit (0) and a 39-bit value (39). (b) Instruction word: A 40-bit word containing two 20-bit instructions. The first instruction has an 8-bit opcode (0) and a 12-bit address (8). The second instruction has an 8-bit opcode (20) and a 12-bit address (28). The word value is 39.](images/image_0007.jpeg)\n\n\nFigure 1.7 illustrates the IAS Memory Formats. (a) Number word: A 40-bit word consisting of a sign bit (0) and a 39-bit value (39). (b) Instruction word: A 40-bit word containing two 20-bit instructions. The first instruction (left) consists of an 8-bit opcode (0) and a 12-bit address (8). The second instruction (right) consists of an 8-bit opcode (20) and a 12-bit address (28). The word value is 39.\n\n\nFigure 1.7 IAS Memory Formats. (a) Number word: A 40-bit word with a sign bit (0) and a 39-bit value (39). (b) Instruction word: A 40-bit word containing two 20-bit instructions. The first instruction has an 8-bit opcode (0) and a 12-bit address (8). The second instruction has an 8-bit opcode (20) and a 12-bit address (28). The word value is 39.\n\n\n**Figure 1.7**\n   IAS Memory Formats\n\n\n6\n   \n   There is no universal definition of the term\n   *word*\n   . In general, a word is an ordered set of bytes or bits that is the normal unit in which information may be stored, transmitted, or operated on within a given computer. Typically, if a processor has a fixed-length instruction set, then the instruction length equals the word length.\n\n\nof multiplying two 40-bit numbers is an 80-bit number; the most significant 40 bits are stored in the AC and the least significant in the MQ.\n\n\nThe IAS operates by repetitively performing an\n   *instruction cycle*\n   , as shown in Figure 1.8. Each instruction cycle consists of two subcycles. During the\n   *fetch cycle*\n   , the opcode of the next instruction is loaded into the IR and the address portion is loaded into the MAR. This instruction may be taken from the IBR, or it can be obtained from memory by loading a word into the MBR, and then down to the IBR, IR, and MAR.\n\n\nWhy the indirection? These operations are controlled by electronic circuitry and result in the use of data paths. To simplify the electronics, there is only one register that is used to specify the address in memory for a read or write and only one register used for the source or destination.\n\n\n\n\n![Partial Flowchart of IAS Operation](images/image_0008.jpeg)\n\n\nThe flowchart illustrates the IAS instruction cycle, divided into a Fetch cycle and an Execution cycle.\n\n\n**Fetch cycle:**\n\n\n  * **Start**\n     (Oval) leads to a decision:\n     **Is next instruction in IBR?**\n     (Diamond).\n  * If\n     **Yes**\n     :\n     **No memory access required**\n     (Text) leads to\n     **IR ← IBR (0:7)**\n     and\n     **MAR ← IBR (8:19)**\n     (Process boxes).\n  * If\n     **No**\n     :\n     **MAR ← PC**\n     (Process box) leads to\n     **MBR ← M(MAR)**\n     (Process box).\n  * From\n     **MBR ← M(MAR)**\n     , a decision\n     **Left instruction required?**\n     (Diamond) is made:\n       * If\n       **Yes**\n       :\n       **IBR ← MBR (20:39)**\n       ,\n       **IR ← MBR (0:7)**\n       , and\n       **MAR ← MBR (8:19)**\n       (Process box).\n  * If\n       **No**\n       :\n       **IR ← MBR (20:27)**\n       and\n       **MAR ← MBR (28:39)**\n       (Process box).\n  * Both paths lead to\n     **PC ← PC + 1**\n     (Process box).\n\n\n**Execution cycle:**\n\n\n  * A dashed line separates the Fetch cycle from the Execution cycle.\n  * From\n     **PC ← PC + 1**\n     , the flow enters the Execution cycle with the label\n     **Decode instruction in IR**\n     .\n  * The flow splits based on the IR:\n       * Path 1:\n       **AC ← M(X)**\n       (Process box) leads to\n       **MBR ← M(MAR)**\n       (Process box), then\n       **AC ← MBR**\n       (Process box).\n  * Path 2:\n       **Go to M(X, 0:19)**\n       (Text) leads to\n       **PC ← MAR**\n       (Process box).\n  * Path 3:\n       **If AC > 0 then go to M(X, 0:19)**\n       (Text) leads to a decision\n       **Is AC > 0?**\n       (Diamond).\n  * From\n     **Is AC > 0?**\n     :\n       * If\n       **Yes**\n       :\n       **AC ← AC + M(X)**\n       (Process box) leads to\n       **MBR ← M(MAR)**\n       (Process box), then\n       **AC ← AC + MBR**\n       (Process box).\n  * If\n       **No**\n       : The flow returns to the Fetch cycle.\n\n\n**Legend:**\n\n\n  * **M(X) = contents of memory location whose address is X**\n  * **(i:j) = bits i through j**\n\n\nPartial Flowchart of IAS Operation\n\n\n**Figure 1.8**\n   Partial Flowchart of IAS Operation\n\n\nOnce the opcode is in the IR, the\n   *execute cycle*\n   is performed. Control circuitry interprets the opcode and executes the instruction by sending out the appropriate control signals to cause data to be moved or an operation to be performed by the ALU.\n\n\nThe IAS computer had a total of 21 instructions, which are listed in Table 1.1. These can be grouped as follows:\n\n\n  * ■\n    **Data transfer:**\n    Move data between memory and ALU registers or between two ALU registers.\n  * ■\n    **Unconditional branch:**\n    Normally, the control unit executes instructions in sequence from memory. This sequence can be changed by a branch instruction, which facilitates repetitive operations.\n\n\n**Table 1.1**\n   The IAS Instruction Set\n\n\n\nInstruction Type | Opcode | Symbolic Representation | Description\nData transfer | 00001010 | LOAD MQ | Transfer contents of register MQ to the accumulator AC\n00001001 | LOAD MQ,M(X) | Transfer contents of memory location X to MQ\n00100001 | STOR M(X) | Transfer contents of accumulator to memory location X\n00000001 | LOAD M(X) | Transfer M(X) to the accumulator\n00000010 | LOAD\n      \n       -M(X) | Transfer\n      \n       -M(X)\n      \n      to the accumulator\n00000011 | LOAD\n      \n       |M(X)| | Transfer absolute value of M(X) to the accumulator\n00000100 | LOAD\n      \n       -|M(X)| | Transfer\n      \n       -|M(X)|\n      \n      to the accumulator\nUnconditional branch | 00001101 | JUMP M(X,0:19) | Take next instruction from left half of M(X)\n00001110 | JUMP M(X,20:39) | Take next instruction from right half of M(X)\nConditional branch | 00001111 | JUMP + M(X,0:19) | If number in the accumulator is nonnegative, take next instruction from left half of M(X)\n00010000 | JUMP + M(X,20:39) | If number in the accumulator is nonnegative, take next instruction from right half of M(X)\nArithmetic | 00000101 | ADD M(X) | Add M(X) to AC; put the result in AC\n00000111 | ADD\n      \n       |M(X)| | Add\n      \n       |M(X)|\n      \n      to AC; put the result in AC\n00000110 | SUB M(X) | Subtract M(X) from AC; put the result in AC\n00001000 | SUB\n      \n       |M(X)| | Subtract\n      \n       |M(X)|\n      \n      from AC; put the remainder in AC\n00001011 | MUL M(X) | Multiply M(X) by MQ; put most significant bits of result in AC, put least significant bits in MQ\n00001100 | DIV M(X) | Divide AC by M(X); put the quotient in MQ and the remainder in AC\n00010100 | LSH | Multiply accumulator by 2; that is, shift left one bit position\n00010101 | RSH | Divide accumulator by 2; that is, shift right one position\nAddress modify | 00010010 | STOR M(X,8:19) | Replace left address field at M(X) by 12 rightmost bits of AC\n00010011 | STOR M(X,28:39) | Replace right address field at M(X) by 12 rightmost bits of AC\n\n\n  * ■\n    **Conditional branch:**\n    The branch can be made dependent on a condition, thus allowing decision points.\n  * ■\n    **Arithmetic:**\n    Operations performed by the ALU.\n  * ■\n    **Address modify:**\n    Permits addresses to be computed in the ALU and then inserted into instructions stored in memory. This allows a program considerable addressing flexibility.\n\n\nTable 1.1 presents instructions (excluding I/O instructions) in a symbolic, easy-to-read form. In binary form, each instruction must conform to the format of Figure 1.7b. The opcode portion (first 8 bits) specifies which of the 21 instructions is to be executed. The address portion (remaining 12 bits) specifies which of the 4,096 memory locations is to be involved in the execution of the instruction.\n\n\nFigure 1.8 shows several examples of instruction execution by the control unit. Note that each operation requires several steps, some of which are quite elaborate. The multiplication operation requires 39 suboperations, one for each bit position except that of the sign bit.\n\n\n\n\n**The Second Generation: Transistors**\n\n\nThe first major change in the electronic computer came with the replacement of the vacuum tube by the transistor. The transistor, which is smaller, cheaper, and generates less heat than a vacuum tube, can be used in the same way as a vacuum tube to construct computers. Unlike the vacuum tube, which requires wires, metal plates, a glass capsule, and a vacuum, the transistor is a\n   *solid-state device*\n   , made from silicon.\n\n\nThe transistor was invented at Bell Labs in 1947 and by the 1950s had launched an electronic revolution. It was not until the late 1950s, however, that fully transistorized computers were commercially available. The use of the transistor defines the\n   *second generation*\n   of computers. It has become widely accepted to classify computers into generations based on the fundamental hardware technology employed (Table 1.2). Each new generation is characterized by greater processing performance, larger memory capacity, and smaller size than the previous one.\n\n\nBut there are other changes as well. The second generation saw the introduction of more complex arithmetic and logic units and control units, the use of high-level programming languages, and the provision of\n   *system software*\n   with the\n\n\n**Table 1.2**\n   Computer Generations\n\n\n\nGeneration | Approximate Dates | Technology | Typical Speed (operations per second)\n1 | 1946–1957 | Vacuum tube | 40,000\n2 | 1957–1964 | Transistor | 200,000\n3 | 1965–1971 | Small- and medium-scale integration | 1,000,000\n4 | 1972–1977 | Large scale integration | 10,000,000\n5 | 1978–1991 | Very large scale integration | 100,000,000\n6 | 1991– | Ultra large scale integration | >1,000,000,000\n\n\ncomputer. In broad terms, system software provided the ability to load programs, move data to peripherals, and libraries to perform common computations, similar to what modern operating systems, such as Windows and Linux, do.\n\n\nIt will be useful to examine an important member of the second generation: the IBM 7094 [BELL71]. From the introduction of the 700 series in 1952 to the introduction of the last member of the 7000 series in 1964, this IBM product line underwent an evolution that is typical of computer products. Successive members of the product line showed increased performance, increased capacity, and/or lower cost.\n\n\nThe size of main memory, in multiples of\n   \n    2^{10}\n   \n   36-bit words, grew from 2k (\n   \n    1k = 2^{10}\n   \n   ) to 32k words,\n   \n    7\n   \n   while the time to access one word of memory, the\n   *memory cycle time*\n   , fell from 30\n   \n    \\mu\n   \n   s to 1.4\n   \n    \\mu\n   \n   s. The number of opcodes grew from a modest 24 to 185.\n\n\nAlso, over the lifetime of this series of computers, the relative speed of the CPU increased by a factor of 50. Speed improvements are achieved by improved electronics (e.g., a transistor implementation is faster than a vacuum tube implementation) and more complex circuitry. For example, the IBM 7094 includes an Instruction Backup Register, used to buffer the next instruction. The control unit fetches two adjacent words from memory for an instruction fetch. Except for the occurrence of a branching instruction, which is relatively infrequent (perhaps 10 to 15%), this means that the control unit has to access memory for an instruction on only half the instruction cycles. This prefetching significantly reduces the average instruction cycle time.\n\n\nFigure 1.9 shows a large (many peripherals) configuration for an IBM 7094, which is representative of second-generation computers. Several differences from the IAS computer are worth noting. The most important of these is the use of\n   *data channels*\n   . A data channel is an independent I/O module with its own processor and instruction set. In a computer system with such devices, the CPU does not execute detailed I/O instructions. Such instructions are stored in a main memory to be executed by a special-purpose processor in the data channel itself. The CPU initiates an I/O transfer by sending a control signal to the data channel, instructing it to execute a sequence of instructions in memory. The data channel performs its task independently of the CPU and signals the CPU when the operation is complete. This arrangement relieves the CPU of a considerable processing burden.\n\n\nAnother new feature is the\n   *multiplexor*\n   , which is the central termination point for data channels, the CPU, and memory. The multiplexor schedules access to the memory from the CPU and data channels, allowing these devices to act independently.\n\n\n\n\n**The Third Generation: Integrated Circuits**\n\n\nA single, self-contained transistor is called a\n   *discrete component*\n   . Throughout the 1950s and early 1960s, electronic equipment was composed largely of discrete components—transistors, resistors, capacitors, and so on. Discrete components were manufactured separately, packaged in their own containers, and soldered or wired\n\n\n7\n   \n   A discussion of the uses of numerical prefixes, such as kilo and giga, is contained in a supporting document at the Computer Science Student Resource Site at\n   ComputerScienceStudent.com\n   .\n\n\n\n\n![Diagram of an IBM 7094 computer configuration showing internal components and peripheral devices.](images/image_0009.jpeg)\n\n\nThe diagram illustrates the architecture of an IBM 7094 computer. It is divided into two main sections by a vertical dashed line: the internal computer components on the left and the peripheral devices on the right.\n\n\n**Internal Components (Left):**\n\n\n  * **CPU**\n     (Central Processing Unit)\n  * **Multi-plexor**\n  * **Memory**\n\n\nConnections within the internal section:\n\n\n  * A vertical double-headed arrow connects the\n     **CPU**\n     and the\n     **Multi-plexor**\n     .\n  * A vertical double-headed arrow connects the\n     **Multi-plexor**\n     and the\n     **Memory**\n     .\n\n\n**Peripheral Devices (Right):**\n\n\n  * **Mag tape units**\n  * **Card punch**\n  * **Line printer**\n  * **Card reader**\n  * **Drum**\n  * **Disk**\n     (two units)\n  * **Hyper-tapes**\n  * **Teleprocessing equipment**\n\n\n**Connections between Internal Components and Peripheral Devices:**\n\n\n  * The\n     **CPU**\n     is connected to a\n     **Data channel**\n     (indicated by a horizontal arrow from CPU to the Data channel).\n  * The\n     **Multi-plexor**\n     is connected to three\n     **Data channel**\n     blocks (indicated by horizontal arrows from Multi-plexor to each Data channel).\n  * The\n     **Memory**\n     is connected to a\n     **Data channel**\n     (indicated by a horizontal arrow from Memory to the Data channel).\n  * Each\n     **Data channel**\n     block is connected to multiple peripheral devices (indicated by horizontal arrows from each Data channel to the devices):\n\n\n  * The top\n     **Data channel**\n     connects to\n     **Mag tape units**\n     ,\n     **Card punch**\n     ,\n     **Line printer**\n     , and\n     **Card reader**\n     .\n  * The middle\n     **Data channel**\n     connects to\n     **Drum**\n     and\n     **Disk**\n     .\n  * The bottom\n     **Data channel**\n     connects to\n     **Disk**\n     ,\n     **Hyper-tapes**\n     , and\n     **Teleprocessing equipment**\n     .\n\n\nDiagram of an IBM 7094 computer configuration showing internal components and peripheral devices.\n\n\n**Figure 1.9**\n   An IBM 7094 Configuration\n\n\ntogether onto Masonite-like circuit boards, which were then installed in computers, oscilloscopes, and other electronic equipment. Whenever an electronic device called for a transistor, a little tube of metal containing a pinhead-sized piece of silicon had to be soldered to a circuit board. The entire manufacturing process, from transistor to circuit board, was expensive and cumbersome.\n\n\nThese facts of life were beginning to create problems in the computer industry. Early second-generation computers contained about 10,000 transistors. This figure grew to the hundreds of thousands, making the manufacture of newer, more powerful machines increasingly difficult.\n\n\nIn 1958 came the achievement that revolutionized electronics and started the era of microelectronics: the invention of the integrated circuit. It is the integrated circuit that defines the third generation of computers. In this section, we provide a brief introduction to the technology of integrated circuits. Then we look at perhaps the two most important members of the third generation, both of which were introduced at the beginning of that era: the IBM System/360 and the DEC PDP-8.\n\n\n***MICROELECTRONICS***\n   Microelectronics means, literally, “small electronics.” Since the beginnings of digital electronics and the computer industry, there has been a persistent and consistent trend toward the reduction in size of digital electronic circuits. Before examining the implications and benefits of this trend, we need to say something about the nature of digital electronics. A more detailed discussion is found in Chapter 11.\n\n\nThe basic elements of a digital computer, as we know, must perform data storage, movement, processing, and control functions. Only two fundamental types of components are required (Figure 1.10): gates and memory cells. A\n   **gate**\n   is a device that implements a simple Boolean or logical function. For example, an AND gate with inputs\n   \n    A\n   \n   and\n   \n    B\n   \n   and output\n   \n    C\n   \n   implements the expression IF\n   \n    A\n   \n   AND\n   \n    B\n   \n   ARE TRUE THEN\n   \n    C\n   \n   IS TRUE. Such devices are called gates because they control data flow in much the same way that canal gates control the flow of water. The\n   **memory cell**\n   is a device that can store 1 bit of data; that is, the device can be in one of two stable states at any time. By interconnecting large numbers of these fundamental devices, we can construct a computer. We can relate this to our four basic functions as follows:\n\n\n  * ■\n    **Data storage:**\n    Provided by memory cells.\n  * ■\n    **Data processing:**\n    Provided by gates.\n  * ■\n    **Data movement:**\n    The paths among components are used to move data from memory to memory and from memory through gates to memory.\n  * ■\n    **Control:**\n    The paths among components can carry control signals. For example, a gate will have one or two data inputs plus a control signal input that activates the gate. When the control signal is ON, the gate performs its function on the data inputs and produces a data output. Conversely, when the control signal is OFF, the output line is null, such as the one produced by a high impedance state. Similarly, the memory cell will store the bit that is on its input lead when the WRITE control signal is ON and will place the bit that is in the cell on its output lead when the READ control signal is ON.\n\n\nThus, a computer consists of gates, memory cells, and interconnections among these elements. The gates and memory cells are, in turn, constructed of simple electronic components, such as transistors and capacitors.\n\n\nThe integrated circuit exploits the fact that such components as transistors, resistors, and conductors can be fabricated from a semiconductor such as silicon. It is merely an extension of the solid-state art to fabricate an entire circuit in a tiny piece of silicon rather than assemble discrete components made from separate pieces of silicon into the same circuit. Many transistors can be produced at the same time on a single wafer of silicon. Equally important, these transistors can be connected with a process of metallization to form circuits.\n\n\n\n\n![Diagram illustrating two fundamental computer elements: (a) Gate and (b) Memory cell.](images/image_0010.jpeg)\n\n\nThe diagram consists of two parts, (a) and (b), each showing a block diagram of a fundamental computer element.\n\n\n**(a) Gate:**\n    A rectangular block labeled \"Boolean logic function\". It has three input lines on the left, labeled \"Input\" with dots indicating multiple lines. It has one output line on the right, labeled \"Output\". Below the block, there is a separate input line labeled \"Activate signal\" with an arrow pointing up into the bottom of the block.\n\n\n**(b) Memory cell:**\n    A rectangular block labeled \"Binary storage cell\". It has one input line on the left, labeled \"Input\". It has one output line on the right, labeled \"Output\". Below the block, there are two control lines: \"Read\" and \"Write\", both with arrows pointing up into the bottom of the block.\n\n\nDiagram illustrating two fundamental computer elements: (a) Gate and (b) Memory cell.\n\n\n**Figure 1.10**\n   Fundamental Computer Elements\n\n\nFigure 1.11 depicts the key concepts in an integrated circuit. A thin\n   *wafer*\n   of silicon is divided into a matrix of small areas, each a few millimeters square. The identical circuit pattern is fabricated in each area, and the wafer is broken up into\n   *chips*\n   . Each chip consists of many gates and/or memory cells plus a number of input and output attachment points. This chip is then packaged in housing that protects it and provides pins for attachment to devices beyond the chip. A number of these packages can then be interconnected on a printed circuit board to produce larger and more complex circuits.\n\n\nInitially, only a few gates or memory cells could be reliably manufactured and packaged together. These early integrated circuits are referred to as\n   **small-scale integration (SSI)**\n   . As time went on, it became possible to pack more and more components on the same chip. This growth in density is illustrated in Figure 1.12; it is one of the most remarkable technological trends ever recorded.\n   \n    8\n   \n   This figure reflects the famous Moore’s law, which was propounded by Gordon Moore, cofounder of Intel, in 1965 [MOOR65]. Moore observed that the number of transistors that could be put on a single chip was doubling every year, and correctly predicted that this pace would continue into the near future. To the surprise of many, including Moore, the pace continued year after year and decade after decade. The pace slowed to a doubling every 18 months in the 1970s but has sustained that rate ever since.\n\n\nThe consequences of Moore’s law are profound:\n\n\n  * 1. The cost of a chip has remained virtually unchanged during this period of rapid growth in density. This means that the cost of computer logic and memory circuitry has fallen at a dramatic rate.\n\n\n\n\n![Diagram illustrating the relationship among Wafer, Chip, and Gate. A large circle labeled 'Wafer' contains a grid of small squares. One square is highlighted and labeled 'Chip'. The 'Chip' is shown as a square with a grid of smaller squares, one of which is highlighted and labeled 'Gate'. Below the 'Chip' is a smaller square labeled 'Packaged chip' with pins extending from its bottom edge.](images/image_0011.jpeg)\n\n\nDiagram illustrating the relationship among Wafer, Chip, and Gate. A large circle labeled 'Wafer' contains a grid of small squares. One square is highlighted and labeled 'Chip'. The 'Chip' is shown as a square with a grid of smaller squares, one of which is highlighted and labeled 'Gate'. Below the 'Chip' is a smaller square labeled 'Packaged chip' with pins extending from its bottom edge.\n\n\n**Figure 1.11**\n   Relationship among Wafer, Chip, and Gate\n\n\n8\n   \n   Note that the vertical axis uses a log scale. A basic review of log scales is in the math refresher document at the Computer Science Student Resource Site at\n   ComputerScienceStudent.com\n   .\n\n\n  * 2. Because logic and memory elements are placed closer together on more densely packed chips, the electrical path length is shortened, increasing operating speed.\n  * 3. The computer becomes smaller, making it more convenient to place in a variety of environments.\n  * 4. There is a reduction in power requirements.\n  * 5. The interconnections on the integrated circuit are much more reliable than solder connections. With more circuitry on each chip, there are fewer interchip connections.\n\n\n**IBM SYSTEM/360**\n   By 1964, IBM had a firm grip on the computer market with its 7000 series of machines. In that year, IBM announced the System/360, a new family of computer products. Although the announcement itself was no surprise, it contained some unpleasant news for current IBM customers: the 360 product line was incompatible with older IBM machines. Thus, the transition to the 360 would be difficult for the current customer base, but IBM felt this was necessary to break out of some of the constraints of the 7000 architecture and to produce a system capable of evolving with the new integrated circuit technology [PADE81, GIFF87]. The strategy paid off both financially and technically. The 360 was the success of the decade and cemented IBM as the overwhelmingly dominant computer vendor, with a market share above 70%. And, with some modifications and extensions, the architecture of the 360 remains to this day the architecture of IBM's mainframe\n   \n    9\n   \n   computers. Examples using this architecture can be found throughout this text.\n\n\nThe System/360 was the industry's first planned family of computers. The family covered a wide range of performance and cost. The models were compatible in the\n\n\n\n\n![Figure 1.12: Growth in Transistor Count on Integrated Circuits. A line graph showing the exponential growth of transistor counts from 1947 to 2011. The x-axis shows years from 1947 to 2011, with labels at 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 2000, 05, and 11. The y-axis is logarithmic, with labels at 1, 10, 100, 1,000, 10,000, 100,000, 1 m, 10 m, 100 m, 1 bn, 10 bn, and 100 bn. The graph shows a steady upward trend, with a significant acceleration after 1970. Three diagonal labels are present: 'First working transistor' (1947), 'Invention of integrated circuit' (1959), and 'Moore's law promulgated' (1970).](images/image_0012.jpeg)\n\n\nYear | Transistor Count (approximate)\n1947 | 1\n1950 | 10\n1955 | 100\n1960 | 1,000\n1965 | 10,000\n1970 | 100,000\n1975 | 1,000,000\n1980 | 10,000,000\n1985 | 100,000,000\n1990 | 1,000,000,000\n1995 | 10,000,000,000\n2000 | 100,000,000,000\n2005 | 1,000,000,000,000\n2011 | 10,000,000,000,000\n\n\nFigure 1.12: Growth in Transistor Count on Integrated Circuits. A line graph showing the exponential growth of transistor counts from 1947 to 2011. The x-axis shows years from 1947 to 2011, with labels at 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 2000, 05, and 11. The y-axis is logarithmic, with labels at 1, 10, 100, 1,000, 10,000, 100,000, 1 m, 10 m, 100 m, 1 bn, 10 bn, and 100 bn. The graph shows a steady upward trend, with a significant acceleration after 1970. Three diagonal labels are present: 'First working transistor' (1947), 'Invention of integrated circuit' (1959), and 'Moore's law promulgated' (1970).\n\n\n**Figure 1.12**\n   Growth in Transistor Count on Integrated Circuits\n\n\n9\n   \n   The term\n   *mainframe*\n   is used for the larger, most powerful computers other than supercomputers. Typical characteristics of a mainframe are that it supports a large database, has elaborate I/O hardware, and is used in a central data processing facility.\n\n\nsense that a program written for one model should be capable of being executed by another model in the series, with only a difference in the time it takes to execute.\n\n\nThe concept of a family of compatible computers was both novel and extremely successful. A customer with modest requirements and a budget to match could start with the relatively inexpensive Model 30. Later, if the customer's needs grew, it was possible to upgrade to a faster machine with more memory without sacrificing the investment in already-developed software. The characteristics of a family are as follows:\n\n\n  * ■\n    **Similar or identical instruction set:**\n    In many cases, the exact same set of machine instructions is supported on all members of the family. Thus, a program that executes on one machine will also execute on any other. In some cases, the lower end of the family has an instruction set that is a subset of that of the top end of the family. This means that programs can move up but not down.\n  * ■\n    **Similar or identical operating system:**\n    The same basic operating system is available for all family members. In some cases, additional features are added to the higher-end members.\n  * ■\n    **Increasing speed:**\n    The rate of instruction execution increases in going from lower to higher family members.\n  * ■\n    **Increasing number of I/O ports:**\n    The number of I/O ports increases in going from lower to higher family members.\n  * ■\n    **Increasing memory size:**\n    The size of main memory increases in going from lower to higher family members.\n  * ■\n    **Increasing cost:**\n    At a given point in time, the cost of a system increases in going from lower to higher family members.\n\n\nHow could such a family concept be implemented? Differences were achieved based on three factors: basic speed, size, and degree of simultaneity [STEV64]. For example, greater speed in the execution of a given instruction could be gained by the use of more complex circuitry in the ALU, allowing suboperations to be carried out in parallel. Another way of increasing speed was to increase the width of the data path between main memory and the CPU. On the Model 30, only 1 byte (8 bits) could be fetched from main memory at a time, whereas 8 bytes could be fetched at a time on the Model 75.\n\n\nThe System/360 not only dictated the future course of IBM but also had a profound impact on the entire industry. Many of its features have become standard on other large computers.\n\n\n**DEC PDP-8**\n   In the same year that IBM shipped its first System/360, another momentous first shipment occurred: PDP-8 from Digital Equipment Corporation (DEC). At a time when the average computer required an air-conditioned room, the PDP-8 (dubbed a minicomputer by the industry, after the miniskirt of the day) was small enough that it could be placed on top of a lab bench or be built into other equipment. It could not do everything the mainframe could, but at $16,000, it was cheap enough for each lab technician to have one. In contrast, the System/360 series of mainframe computers introduced just a few months before cost hundreds of thousands of dollars.\n\n\nThe low cost and small size of the PDP-8 enabled another manufacturer to purchase a PDP-8 and integrate it into a total system for resale. These other manufacturers came to be known as\n   **original equipment manufacturers (OEMs)**\n   , and the OEM market became and remains a major segment of the computer marketplace.\n\n\nIn contrast to the central-switched architecture (Figure 1.9) used by IBM on its 700/7000 and 360 systems, later models of the PDP-8 used a structure that became virtually universal for microcomputers: the bus structure. This is illustrated in Figure 1.13. The PDP-8 bus, called the Omnibus, consists of 96 separate signal paths, used to carry control, address, and data signals. Because all system components share a common set of signal paths, their use can be controlled by the CPU. This architecture is highly flexible, allowing modules to be plugged into the bus to create various configurations. It is only in recent years that the bus structure has given way to a structure known as point-to-point interconnect, described in Chapter 3.\n\n\n\n\n**Later Generations**\n\n\nBeyond the third generation there is less general agreement on defining generations of computers. Table 1.2 suggests that there have been a number of later generations, based on advances in integrated circuit technology. With the introduction of\n   **large-scale integration (LSI)**\n   , more than 1,000 components can be placed on a single integrated circuit chip. Very-large-scale integration (VLSI) achieved more than 10,000 components per chip, while current ultra-large-scale integration (ULSI) chips can contain more than one billion components.\n\n\nWith the rapid pace of technology, the high rate of introduction of new products, and the importance of software and communications as well as hardware, the classification by generation becomes less clear and less meaningful. In this section, we mention two of the most important of developments in later generations.\n\n\n**SEMICONDUCTOR MEMORY**\n   The first application of integrated circuit technology to computers was the construction of the processor (the control unit and the arithmetic and logic unit) out of integrated circuit chips. But it was also found that this same technology could be used to construct memories.\n\n\nIn the 1950s and 1960s, most computer memory was constructed from tiny rings of ferromagnetic material, each about a sixteenth of an inch in diameter. These rings were strung up on grids of fine wires suspended on small screens inside the computer. Magnetized one way, a ring (called a\n   *core*\n   ) represented a one; magnetized the other way, it stood for a zero. Magnetic-core memory was rather fast; it took as little as a millionth of a second to read a bit stored in memory. But it was\n\n\n\n\n![Diagram of the PDP-8 Bus Structure showing various modules connected to a central Omnibus bus.](images/image_0013.jpeg)\n\n\nThe diagram illustrates the PDP-8 Bus Structure. A central horizontal bar represents the 'Omnibus'. Above this bar, several vertical lines represent different system modules. From left to right, these are: 'Console controller', 'CPU', 'Main memory', 'I/O module', an ellipsis '...', and another 'I/O module'. Each module is represented by a rectangular box with its name inside, and a vertical line connects the bottom of the box to the Omnibus bar.\n\n\nDiagram of the PDP-8 Bus Structure showing various modules connected to a central Omnibus bus.\n\n\nFigure 1.13 PDP-8 Bus Structure\n\n\nexpensive and bulky, and used destructive readout: The simple act of reading a core erased the data stored in it. It was therefore necessary to install circuits to restore the data as soon as it had been extracted.\n\n\nThen, in 1970, Fairchild produced the first relatively capacious semiconductor memory. This chip, about the size of a single core, could hold 256 bits of memory. It was nondestructive and much faster than core. It took only 70 billionths of a second to read a bit. However, the cost per bit was higher than for that of core.\n\n\nIn 1974, a seminal event occurred: The price per bit of semiconductor memory dropped below the price per bit of core memory. Following this, there has been a continuing and rapid decline in memory cost accompanied by a corresponding increase in physical memory density. This has led the way to smaller, faster machines with memory sizes of larger and more expensive machines from just a few years earlier. Developments in memory technology, together with developments in processor technology to be discussed next, changed the nature of computers in less than a decade. Although bulky, expensive computers remain a part of the landscape, the computer has also been brought out to the “end user,” with office machines and personal computers.\n\n\nSince 1970, semiconductor memory has been through 13 generations: 1k, 4k, 16k, 64k, 256k, 1M, 4M, 16M, 64M, 256M, 1G, 4G, and, as of this writing, 8 Gb on a single chip (\n   \n    1\\text{k} = 2^{10}\n   \n   ,\n   \n    1\\text{M} = 2^{20}\n   \n   ,\n   \n    1\\text{G} = 2^{30}\n   \n   ). Each generation has provided increased storage density, accompanied by declining cost per bit and declining access time. Densities are projected to reach 16 Gb by 2018 and 32 Gb by 2023 [ITRS14].\n\n\n***MICROPROCESSORS***\n   Just as the density of elements on memory chips has continued to rise, so has the density of elements on processor chips. As time went on, more and more elements were placed on each chip, so that fewer and fewer chips were needed to construct a single computer processor.\n\n\nA breakthrough was achieved in 1971, when Intel developed its 4004. The 4004 was the first chip to contain\n   *all*\n   of the components of a CPU on a single chip: The microprocessor was born.\n\n\nThe 4004 can add two 4-bit numbers and can multiply only by repeated addition. By today’s standards, the 4004 is hopelessly primitive, but it marked the beginning of a continuing evolution of microprocessor capability and power.\n\n\nThis evolution can be seen most easily in the number of bits that the processor deals with at a time. There is no clear-cut measure of this, but perhaps the best measure is the data bus width: the number of bits of data that can be brought into or sent out of the processor at a time. Another measure is the number of bits in the accumulator or in the set of general-purpose registers. Often, these measures coincide, but not always. For example, a number of microprocessors were developed that operate on 16-bit numbers in registers but can only read and write 8 bits at a time.\n\n\nThe next major step in the evolution of the microprocessor was the introduction in 1972 of the Intel 8008. This was the first 8-bit microprocessor and was almost twice as complex as the 4004.\n\n\nNeither of these steps was to have the impact of the next major event: the introduction in 1974 of the Intel 8080. This was the first general-purpose microprocessor. Whereas the 4004 and the 8008 had been designed for specific applications, the 8080 was designed to be the CPU of a general-purpose microcomputer. Like the\n\n\n8008, the 8080 is an 8-bit microprocessor. The 8080, however, is faster, has a richer instruction set, and has a large addressing capability.\n\n\nAbout the same time, 16-bit microprocessors began to be developed. However, it was not until the end of the 1970s that powerful, general-purpose 16-bit microprocessors appeared. One of these was the 8086. The next step in this trend occurred in 1981, when both Bell Labs and Hewlett-Packard developed 32-bit, single-chip microprocessors. Intel introduced its own 32-bit microprocessor, the 80386, in 1985 (Table 1.3).\n\n\n**Table 1.3**\n   Evolution of Intel Microprocessors (page 1 of 2)\n\n\n\n\n**(a) 1970s Processors**\n\n\n\n | 4004 | 8008 | 8080 | 8086 | 8088\nIntroduced | 1971 | 1972 | 1974 | 1978 | 1979\nClock speeds | 108 kHz | 108 kHz | 2 MHz | 5 MHz, 8 MHz, 10 MHz | 5 MHz, 8 MHz\nBus width | 4 bits | 8 bits | 8 bits | 16 bits | 8 bits\nNumber of transistors | 2,300 | 3,500 | 6,000 | 29,000 | 29,000\nFeature size (\n      \n       \\mu\\text{m}\n      \n      ) | 10 | 8 | 6 | 3 | 6\nAddressable memory | 640 bytes | 16 KB | 64 KB | 1 MB | 1 MB\n\n\n\n\n**(b) 1980s Processors**\n\n\n\n | 80286 | 386TM DX | 386TM SX | 486TM DX CPU\nIntroduced | 1982 | 1985 | 1988 | 1989\nClock speeds | 6–12.5 MHz | 16–33 MHz | 16–33 MHz | 25–50 MHz\nBus width | 16 bits | 32 bits | 16 bits | 32 bits\nNumber of transistors | 134,000 | 275,000 | 275,000 | 1.2 million\nFeature size (\n      \n       \\mu\\text{m}\n      \n      ) | 1.5 | 1 | 1 | 0.8–1\nAddressable memory | 16 MB | 4 GB | 16 MB | 4 GB\nVirtual memory | 1 GB | 64 TB | 64 TB | 64 TB\nCache | — | — | — | 8 kB\n\n\n\n\n**(c) 1990s Processors**\n\n\n\n | 486TM SX | Pentium | Pentium Pro | Pentium II\nIntroduced | 1991 | 1993 | 1995 | 1997\nClock speeds | 16–33 MHz | 60–166 MHz, | 150–200 MHz | 200–300 MHz\nBus width | 32 bits | 32 bits | 64 bits | 64 bits\nNumber of transistors | 1.185 million | 3.1 million | 5.5 million | 7.5 million\nFeature size (\n      \n       \\mu\\text{m}\n      \n      ) | 1 | 0.8 | 0.6 | 0.35\nAddressable memory | 4 GB | 4 GB | 64 GB | 64 GB\nVirtual memory | 64 TB | 64 TB | 64 TB | 64 TB\nCache | 8 kB | 8 kB | 512 kB L1 and\n      \n      1 MB L2 | 512 kB L2\n\n\n\n\n**(d) Recent Processors**\n\n\n\n | Pentium III | Pentium 4 | Core 2 Duo | Core i7 EE 4960X\nIntroduced | 1999 | 2000 | 2006 | 2013\nClock speeds | 450–660 MHz | 1.3–1.8 GHz | 1.06–1.2 GHz | 4 GHz\nBus width | 64 bits | 64 bits | 64 bits | 64 bits\nNumber of transistors | 9.5 million | 42 million | 167 million | 1.86 billion\nFeature size (nm) | 250 | 180 | 65 | 22\nAddressable memory | 64 GB | 64 GB | 64 GB | 64 GB\nVirtual memory | 64 TB | 64 TB | 64 TB | 64 TB\nCache | 512 kB L2 | 256 kB L2 | 2 MB L2 | 1.5 MB L2/15 MB L3\nNumber of cores | 1 | 1 | 2 | 6"
        },
        {
          "name": "The Evolution of the Intel x86 Architecture",
          "content": "Throughout this book, we rely on many concrete examples of computer design and implementation to illustrate concepts and to illuminate trade-offs. Numerous systems, both contemporary and historical, provide examples of important computer architecture design features. But the book relies principally on examples from two processor families: the Intel x86 and the ARM architectures. The current x86 offerings represent the results of decades of design effort on\n   **complex instruction set computers (CISCs)**\n   . The x86 incorporates the sophisticated design principles once found only on mainframes and supercomputers and serves as an excellent example of CISC design. An alternative approach to processor design is the\n   **reduced instruction set computer (RISC)**\n   . The ARM architecture is used in a wide variety of embedded systems and is one of the most powerful and best-designed RISC-based systems on the market. In this section and the next, we provide a brief overview of these two systems.\n\n\nIn terms of market share, Intel has ranked as the number one maker of microprocessors for non-embedded systems for decades, a position it seems unlikely to yield. The evolution of its flagship microprocessor product serves as a good indicator of the evolution of computer technology in general.\n\n\nTable 1.3 shows that evolution. Interestingly, as microprocessors have grown faster and much more complex, Intel has actually picked up the pace. Intel used to develop microprocessors one after another, every four years. But Intel hopes to keep rivals at bay by trimming a year or two off this development time, and has done so with the most recent x86 generations.\n   \n    10\n\n\n10\n   \n   Intel refers to this as the\n   *tick-tock model*\n   . Using this model, Intel has successfully delivered next-generation silicon technology as well as new processor microarchitecture on alternating years for the past several years. See\n   http://www.intel.com/content/www/us/en/silicon-innovations/intel-tick-tock-model-general.html\n   .\n\n\nIt is worthwhile to list some of the highlights of the evolution of the Intel product line:\n\n\n  * ■\n    **8080:**\n    The world's first general-purpose microprocessor. This was an 8-bit machine, with an 8-bit data path to memory. The 8080 was used in the first personal computer, the Altair.\n  * ■\n    **8086:**\n    A far more powerful, 16-bit machine. In addition to a wider data path and larger registers, the 8086 sported an instruction cache, or queue, that prefetches a few instructions before they are executed. A variant of this processor, the 8088, was used in IBM's first personal computer, securing the success of Intel. The 8086 is the first appearance of the x86 architecture.\n  * ■\n    **80286:**\n    This extension of the 8086 enabled addressing a 16-MB memory instead of just 1 MB.\n  * ■\n    **80386:**\n    Intel's first 32-bit machine, and a major overhaul of the product. With a 32-bit architecture, the 80386 rivaled the complexity and power of minicomputers and mainframes introduced just a few years earlier. This was the first Intel processor to support multitasking, meaning it could run multiple programs at the same time.\n  * ■\n    **80486:**\n    The 80486 introduced the use of much more sophisticated and powerful cache technology and sophisticated instruction pipelining. The 80486 also offered a built-in math coprocessor, offloading complex math operations from the main CPU.\n  * ■\n    **Pentium:**\n    With the Pentium, Intel introduced the use of superscalar techniques, which allow multiple instructions to execute in parallel.\n  * ■\n    **Pentium Pro:**\n    The Pentium Pro continued the move into superscalar organization begun with the Pentium, with aggressive use of register renaming, branch prediction, data flow analysis, and speculative execution.\n  * ■\n    **Pentium II:**\n    The Pentium II incorporated Intel MMX technology, which is designed specifically to process video, audio, and graphics data efficiently.\n  * ■\n    **Pentium III:**\n    The Pentium III incorporates additional floating-point instructions: The Streaming SIMD Extensions (SSE) instruction set extension added 70 new instructions designed to increase performance when exactly the same operations are to be performed on multiple data objects. Typical applications are digital signal processing and graphics processing.\n  * ■\n    **Pentium 4:**\n    The Pentium 4 includes additional floating-point and other enhancements for multimedia.\n    \n     11\n  * ■\n    **Core:**\n    This is the first Intel x86 microprocessor with a dual core, referring to the implementation of two cores on a single chip.\n  * ■\n    **Core 2:**\n    The Core 2 extends the Core architecture to 64 bits. The Core 2 Quad provides four cores on a single chip. More recent Core offerings have up to 10 cores per chip. An important addition to the architecture was the Advanced Vector Extensions instruction set that provided a set of 256-bit, and then 512-bit, instructions for efficient processing of vector data.\n\n\n11\n   \n   With the Pentium 4, Intel switched from Roman numerals to Arabic numerals for model numbers.\n\n\nAlmost 40 years after its introduction in 1978, the x86 architecture continues to dominate the processor market outside of embedded systems. Although the organization and technology of the x86 machines have changed dramatically over the decades, the instruction set architecture has evolved to remain backward compatible with earlier versions. Thus, any program written on an older version of the x86 architecture can execute on newer versions. All changes to the instruction set architecture have involved additions to the instruction set, with no subtractions. The rate of change has been the addition of roughly one instruction per month added to the architecture [ANTH08], so that there are now thousands of instructions in the instruction set.\n\n\nThe x86 provides an excellent illustration of the advances in computer hardware over the past 35 years. The 1978 8086 was introduced with a clock speed of 5 MHz and had 29,000 transistors. A six-core Core i7 EE 4960X introduced in 2013 operates at 4 GHz, a speedup of a factor of 800, and has 1.86 billion transistors, about 64,000 times as many as the 8086. Yet the Core i7 EE 4960X is in only a slightly larger package than the 8086 and has a comparable cost."
        },
        {
          "name": "Embedded Systems",
          "content": "The term\n   *embedded system*\n   refers to the use of electronics and software within a product, as opposed to a general-purpose computer, such as a laptop or desktop system. Millions of computers are sold every year, including laptops, personal computers, workstations, servers, mainframes, and supercomputers. In contrast, billions of computer systems are produced each year that are embedded within larger devices. Today, many, perhaps most, devices that use electric power have an embedded computing system. It is likely that in the near future virtually all such devices will have embedded computing systems.\n\n\nTypes of devices with embedded systems are almost too numerous to list. Examples include cell phones, digital cameras, video cameras, calculators, microwave ovens, home security systems, washing machines, lighting systems, thermostats, printers, various automotive systems (e.g., transmission control, cruise control, fuel injection, anti-lock brakes, and suspension systems), tennis rackets, toothbrushes, and numerous types of sensors and actuators in automated systems.\n\n\nOften, embedded systems are tightly coupled to their environment. This can give rise to real-time constraints imposed by the need to interact with the environment. Constraints, such as required speeds of motion, required precision of measurement, and required time durations, dictate the timing of software operations. If multiple activities must be managed simultaneously, this imposes more complex real-time constraints.\n\n\nFigure 1.14 shows in general terms an embedded system organization. In addition to the processor and memory, there are a number of elements that differ from the typical desktop or laptop computer:\n\n\n  * ■ There may be a variety of interfaces that enable the system to measure, manipulate, and otherwise interact with the external environment. Embedded systems often interact (sense, manipulate, and communicate) with external world through sensors and actuators and hence are typically reactive systems; a\n\n\n\n\n![Figure 1.14: Possible Organization of an Embedded System. A central 'Processor' block is connected to 'Human interface', 'A/D conversion', 'D/A Conversion', 'Diagnostic port', 'Memory', and 'Custom logic'. 'Sensors' feed into 'A/D conversion', and 'Actuators/indicators' feed into 'D/A Conversion'. 'Memory' and 'Custom logic' are interconnected with bidirectional arrows.](images/image_0014.jpeg)\n\n\ngraph TD\n    HI[Human interface] <--> P[Processor]\n    P <--> AD[A/D conversion]\n    P <--> DA[D/A Conversion]\n    P <--> DP[Diagnostic port]\n    P <--> M[Memory]\n    P <--> CL[Custom logic]\n    S[Sensors] <--> AD\n    AI[Actuators/indicators] <--> DA\n    M <--> CL\n  \nFigure 1.14: Possible Organization of an Embedded System. A central 'Processor' block is connected to 'Human interface', 'A/D conversion', 'D/A Conversion', 'Diagnostic port', 'Memory', and 'Custom logic'. 'Sensors' feed into 'A/D conversion', and 'Actuators/indicators' feed into 'D/A Conversion'. 'Memory' and 'Custom logic' are interconnected with bidirectional arrows.\n\n\n**Figure 1.14**\n   Possible Organization of an Embedded System\n\n\nreactive system is in continual interaction with the environment and executes at a pace determined by that environment.\n\n\n  * ■ The human interface may be as simple as a flashing light or as complicated as real-time robotic vision. In many cases, there is no human interface.\n  * ■ The diagnostic port may be used for diagnosing the system that is being controlled—not just for diagnosing the computer.\n  * ■ Special-purpose field programmable (FPGA), application-specific (ASIC), or even nondigital hardware may be used to increase performance or reliability.\n  * ■ Software often has a fixed function and is specific to the application.\n  * ■ Efficiency is of paramount importance for embedded systems. They are optimized for energy, code size, execution time, weight and dimensions, and cost.\n\n\nThere are several noteworthy areas of similarity to general-purpose computer systems as well:\n\n\n  * ■ Even with nominally fixed function software, the ability to field upgrade to fix bugs, to improve security, and to add functionality, has become very important for embedded systems, and not just in consumer devices.\n  * ■ One comparatively recent development has been of embedded system platforms that support a wide variety of apps. Good examples of this are smart-phones and audio/visual devices, such as smart TVs.\n\n\n\n\n**The Internet of Things**\n\n\nIt is worthwhile to separately callout one of the major drivers in the proliferation of embedded systems. The\n   **Internet of things (IoT)**\n   is a term that refers to the expanding\n\n\ninterconnection of smart devices, ranging from appliances to tiny sensors. A dominant theme is the embedding of short-range mobile transceivers into a wide array of gadgets and everyday items, enabling new forms of communication between people and things, and between things themselves. The Internet now supports the interconnection of billions of industrial and personal objects, usually through cloud systems. The objects deliver sensor information, act on their environment, and, in some cases, modify themselves, to create overall management of a larger system, like a factory or city.\n\n\nThe IoT is primarily driven by deeply embedded devices (defined below). These devices are low-bandwidth, low-repetition data-capture, and low-bandwidth data-usage appliances that communicate with each other and provide data via user interfaces. Embedded appliances, such as high-resolution video security cameras, video VoIP phones, and a handful of others, require high-bandwidth streaming capabilities. Yet countless products simply require packets of data to be intermittently delivered.\n\n\nWith reference to the end systems supported, the Internet has gone through roughly four generations of deployment culminating in the IoT:\n\n\n  * 1.\n    **Information technology (IT):**\n    PCs, servers, routers, firewalls, and so on, bought as IT devices by enterprise IT people and primarily using wired connectivity.\n  * 2.\n    **Operational technology (OT):**\n    Machines/appliances with embedded IT built by non-IT companies, such as medical machinery, SCADA (supervisory control and data acquisition), process control, and kiosks, bought as appliances by enterprise OT people and primarily using wired connectivity.\n  * 3.\n    **Personal technology:**\n    Smartphones, tablets, and eBook readers bought as IT devices by consumers (employees) exclusively using wireless connectivity and often multiple forms of wireless connectivity.\n  * 4.\n    **Sensor/actuator technology:**\n    Single-purpose devices bought by consumers, IT, and OT people exclusively using wireless connectivity, generally of a single form, as part of larger systems.\n\n\nIt is the fourth generation that is usually thought of as the IoT, and it is marked by the use of billions of embedded devices.\n\n\n\n\n**Embedded Operating Systems**\n\n\nThere are two general approaches to developing an embedded operating system (OS). The first approach is to take an existing OS and adapt it for the embedded application. For example, there are embedded versions of Linux, Windows, and Mac, as well as other commercial and proprietary operating systems specialized for embedded systems. The other approach is to design and implement an OS intended solely for embedded use. An example of the latter is TinyOS, widely used in wireless sensor networks. This topic is explored in depth in [STAL15].\n\n\n\n\n**Application Processors versus Dedicated Processors**\n\n\nIn this subsection, and the next two, we briefly introduce some terms commonly found in the literature on embedded systems.\n   **Application processors**\n   are defined\n\n\nby the processor’s ability to execute complex operating systems, such as Linux, Android, and Chrome. Thus, the application processor is general-purpose in nature. A good example of the use of an embedded application processor is the smartphone. The embedded system is designed to support numerous apps and perform a wide variety of functions.\n\n\nMost embedded systems employ a\n   **dedicated processor**\n   , which, as the name implies, is dedicated to one or a small number of specific tasks required by the host device. Because such an embedded system is dedicated to a specific task or tasks, the processor and associated components can be engineered to reduce size and cost.\n\n\n\n\n**Microprocessors versus Microcontrollers**\n\n\nAs we have seen, early\n   **microprocessor**\n   chips included registers, an ALU, and some sort of control unit or instruction processing logic. As transistor density increased, it became possible to increase the complexity of the instruction set architecture, and ultimately to add memory and more than one processor. Contemporary microprocessor chips, as shown in Figure 1.2, include multiple cores and a substantial amount of cache memory.\n\n\nA\n   **microcontroller**\n   chip makes a substantially different use of the logic space available. Figure 1.15 shows in general terms the elements typically found on a microcontroller chip. As shown, a microcontroller is a single chip that contains the processor, non-volatile memory for the program (ROM), volatile memory for input and output (RAM), a clock, and an I/O control unit. The processor portion of the microcontroller has a much lower silicon area than other microprocessors and much higher energy efficiency. We examine microcontroller organization in more detail in Section 1.6.\n\n\nAlso called a “computer on a chip,” billions of microcontroller units are embedded each year in myriad products from toys to appliances to automobiles. For example, a single vehicle can use 70 or more microcontrollers. Typically, especially for the smaller, less expensive microcontrollers, they are used as dedicated processors for specific tasks. For example, microcontrollers are heavily utilized in automation processes. By providing simple reactions to input, they can control machinery, turn fans on and off, open and close valves, and so forth. They are integral parts of modern industrial technology and are among the most inexpensive ways to produce machinery that can handle extremely complex functionalities.\n\n\nMicrocontrollers come in a range of physical sizes and processing power. Processors range from 4-bit to 32-bit architectures. Microcontrollers tend to be much slower than microprocessors, typically operating in the MHz range rather than the GHz speeds of microprocessors. Another typical feature of a microcontroller is that it does not provide for human interaction. The microcontroller is programmed for a specific task, embedded in its device, and executes as and when required.\n\n\n\n\n**Embedded versus Deeply Embedded Systems**\n\n\nWe have, in this section, defined the concept of an embedded system. A subset of embedded systems, and a quite numerous subset, is referred to as\n   **deeply embedded systems**\n   . Although this term is widely used in the technical and commercial\n\n\n\n\n![Diagram of a typical microcontroller chip showing internal components and their connections.](images/image_0015.jpeg)\n\n\nThe diagram illustrates the internal architecture of a typical microcontroller chip, enclosed within a dashed boundary. At the top is a green box labeled\n    **Processor**\n    . Below it is a central vertical line labeled\n    **System bus**\n    . To the left of the bus are four functional blocks:\n    **A/D converter**\n    ,\n    **D/A converter**\n    ,\n    **Serial I/O ports**\n    , and\n    **Parallel I/O ports**\n    . To the right of the bus are four memory and control blocks:\n    **RAM**\n    ,\n    **ROM**\n    ,\n    **EEPROM**\n    , and\n    **TIMER**\n    . The\n    **Processor**\n    is connected to the\n    **System bus**\n    . The\n    **A/D converter**\n    is connected to the\n    **System bus**\n    and to an external input labeled\n    **Analog data acquisition**\n    . The\n    **D/A converter**\n    is connected to the\n    **System bus**\n    and to an external output labeled\n    **Analog data transmission**\n    . The\n    **Serial I/O ports**\n    are connected to the\n    **System bus**\n    and to an external interface labeled\n    **Send/receive data**\n    . The\n    **Parallel I/O ports**\n    are connected to the\n    **System bus**\n    and to an external interface labeled\n    **Peripheral interfaces**\n    . The\n    **RAM**\n    and\n    **ROM**\n    blocks are grouped under a bracket labeled\n    **Temporary data**\n    and\n    **Program and data**\n    respectively. The\n    **EEPROM**\n    block is grouped under a bracket labeled\n    **Permanent data**\n    . The\n    **TIMER**\n    block is grouped under a bracket labeled\n    **Timing functions**\n    .\n\n\nDiagram of a typical microcontroller chip showing internal components and their connections.\n\n\n**Figure 1.15**\n   Typical Microcontroller Chip Elements\n\n\nliterature, you will search the Internet in vain (or at least I did) for a straightforward definition. Generally, we can say that a deeply embedded system has a processor whose behavior is difficult to observe both by the programmer and the user. A deeply embedded system uses a microcontroller rather than a microprocessor, is not programmable once the program logic for the device has been burned into ROM (read-only memory), and has no interaction with a user.\n\n\nDeeply embedded systems are dedicated, single-purpose devices that detect something in the environment, perform a basic level of processing, and then do something with the results. Deeply embedded systems often have wireless capability and appear in networked configurations, such as networks of sensors deployed over a large area (e.g., factory, agricultural field). The Internet of things depends heavily on deeply embedded systems. Typically, deeply embedded systems have extreme resource constraints in terms of memory, processor size, time, and power consumption."
        },
        {
          "name": "Arm Architecture",
          "content": "The ARM architecture refers to a processor architecture that has evolved from RISC design principles and is used in embedded systems. Chapter 15 examines RISC design principles in detail. In this section, we give a brief overview of the ARM architecture.\n\n\n\n\n**ARM Evolution**\n\n\nARM is a family of RISC-based microprocessors and microcontrollers designed by ARM Holdings, Cambridge, England. The company doesn't make processors but instead designs microprocessor and multicore architectures and licenses them to manufacturers. Specifically, ARM Holdings has two types of licensable products: processors and processor architectures. For processors, the customer buys the rights to use ARM-supplied design in their own chips. For a processor architecture, the customer buys the rights to design their own processor compliant with ARM's architecture.\n\n\nARM chips are high-speed processors that are known for their small die size and low power requirements. They are widely used in smartphones and other handheld devices, including game systems, as well as a large variety of consumer products. ARM chips are the processors in Apple's popular iPod and iPhone devices, and are used in virtually all Android smartphones as well. ARM is probably the most widely used embedded processor architecture and indeed the most widely used processor architecture of any kind in the world [VANC14].\n\n\nThe origins of ARM technology can be traced back to the British-based Acorn Computers company. In the early 1980s, Acorn was awarded a contract by the British Broadcasting Corporation (BBC) to develop a new microcomputer architecture for the BBC Computer Literacy Project. The success of this contract enabled Acorn to go on to develop the first commercial RISC processor, the Acorn RISC Machine (ARM). The first version, ARM1, became operational in 1985 and was used for internal research and development as well as being used as a coprocessor in the BBC machine.\n\n\nIn this early stage, Acorn used the company VLSI Technology to do the actual fabrication of the processor chips. VLSI was licensed to market the chip on its own and had some success in getting other companies to use the ARM in their products, particularly as an embedded processor.\n\n\nThe ARM design matched a growing commercial need for a high-performance, low-power-consumption, small-size, and low-cost processor for embedded applications. But further development was beyond the scope of Acorn's capabilities. Accordingly, a new company was organized, with Acorn, VLSI, and Apple Computer as founding partners, known as ARM Ltd. The Acorn RISC Machine became Advanced RISC Machines.\n   \n    12\n\n\n\n\n**Instruction Set Architecture**\n\n\nThe ARM instruction set is highly regular, designed for efficient implementation of the processor and efficient execution. All instructions are 32 bits long and follow a regular format. This makes the ARM ISA suitable for implementation over a wide range of products.\n\n\nAugmenting the basic ARM ISA is the Thumb instruction set, which is a re-encoded subset of the ARM instruction set. Thumb is designed to increase the performance of ARM implementations that use a 16-bit or narrower memory data bus,\n\n\n12\n   \n   The company dropped the designation\n   *Advanced RISC Machines*\n   in the late 1990s. It is now simply known as the ARM architecture.\n\n\nand to allow better code density than provided by the ARM instruction set. The Thumb instruction set contains a subset of the ARM 32-bit instruction set recoded into 16-bit instructions. The current defined version is Thumb-2.\n\n\nThe ARM and Thumb-2 ISAs are discussed in Chapters 12 and 13.\n\n\n\n\n**ARM Products**\n\n\nARM Holdings licenses a number of specialized microprocessors and related technologies, but the bulk of their product line is the Cortex family of microprocessor architectures. There are three Cortex architectures, conveniently labeled with the initials A, R, and M.\n\n\n**CORTEX-A/CORTEX-A50**\n   The Cortex-A and Cortex-A50 are application processors, intended for mobile devices such as smartphones and eBook readers, as well as consumer devices such as digital TV and home gateways (e.g., DSL and cable Internet modems). These processors run at higher clock frequency (over 1 GHz), and support a memory management unit (MMU), which is required for full feature OSs such as Linux, Android, MS Windows, and mobile OSs. An MMU is a hardware module that supports virtual memory and paging by translating virtual addresses into physical addresses; this topic is explored in Chapter 8.\n\n\nThe two architectures use both the ARM and Thumb-2 instruction sets; the principal difference is that the Cortex-A is a 32-bit machine, and the Cortex-A50 is a 64-bit machine.\n\n\n**CORTEX-R**\n   The Cortex-R is designed to support real-time applications, in which the timing of events needs to be controlled with rapid response to events. They can run at a fairly high clock frequency (e.g., 200MHz to 800MHz) and have very low response latency. The Cortex-R includes enhancements both to the instruction set and to the processor organization to support deeply embedded real-time devices. Most of these processors do not have MMU; the limited data requirements and the limited number of simultaneous processes eliminates the need for elaborate hardware and software support for virtual memory. The Cortex-R does have a Memory Protection Unit (MPU), cache, and other memory features designed for industrial applications. An MPU is a hardware module that prohibits one program in memory from accidentally accessing memory assigned to another active program. Using various methods, a protective boundary is created around the program, and instructions within the program are prohibited from referencing data outside of that boundary.\n\n\nExamples of embedded systems that would use the Cortex-R are automotive braking systems, mass storage controllers, and networking and printing devices.\n\n\n**CORTEX-M**\n   Cortex-M series processors have been developed primarily for the microcontroller domain where the need for fast, highly deterministic interrupt management is coupled with the desire for extremely low gate count and lowest possible power consumption. As with the Cortex-R series, the Cortex-M architecture has an MPU but no MMU. The Cortex-M uses only the Thumb-2 instruction set. The market for the Cortex-M includes IoT devices, wireless sensor/actuator networks used in factories and other enterprises, automotive body electronics, and so on.\n\n\nThere are currently four versions of the Cortex-M series:\n\n\n  * ■\n    **Cortex-M0:**\n    Designed for 8- and 16-bit applications, this model emphasizes low cost, ultra low power, and simplicity. It is optimized for small silicon die size (starting from 12k gates) and use in the lowest cost chips.\n  * ■\n    **Cortex-M0+:**\n    An enhanced version of the M0 that is more energy efficient.\n  * ■\n    **Cortex-M3:**\n    Designed for 16- and 32-bit applications, this model emphasizes performance and energy efficiency. It also has comprehensive debug and trace features to enable software developers to develop their applications quickly.\n  * ■\n    **Cortex-M4:**\n    This model provides all the features of the Cortex-M3, with additional instructions to support digital signal processing tasks.\n\n\nIn this text, we will primarily use the ARM Cortex-M3 as our example embedded system processor. It is the best suited of all ARM models for general-purpose microcontroller use. The Cortex-M3 is used by a variety of manufacturers of microcontroller products. Initial microcontroller devices from lead partners already combine the Cortex-M3 processor with flash, SRAM, and multiple peripherals to provide a competitive offering at the price of just $1.\n\n\nFigure 1.16 provides a block diagram of the EFM32 microcontroller from Silicon Labs. The figure also shows detail of the Cortex-M3 processor and core components. We examine each level in turn.\n\n\nThe\n   **Cortex-M3 core**\n   makes use of separate buses for instructions and data. This arrangement is sometimes referred to as a Harvard architecture, in contrast with the von Neumann architecture, which uses the same signal buses and memory for both instructions and data. By being able to read both an instruction and data from memory at the same time, the Cortex-M3 processor can perform many operations in parallel, speeding application execution. The core contains a decoder for Thumb instructions, an advanced ALU with support for hardware multiply and divide, control logic, and interfaces to the other components of the processor. In particular, there is an interface to the nested vector interrupt controller (NVIC) and the embedded trace macrocell (ETM) module.\n\n\nThe core is part of a module called the\n   **Cortex-M3 processor**\n   . This term is somewhat misleading, because typically in the literature, the terms core and processor are viewed as equivalent. In addition to the core, the processor includes the following elements:\n\n\n  * ■\n    **NVIC:**\n    Provides configurable interrupt handling abilities to the processor. It facilitates low-latency exception and interrupt handling, and controls power management.\n  * ■\n    **ETM:**\n    An optional debug component that enables reconstruction of program execution. The ETM is designed to be a high-speed, low-power debug tool that only supports instruction trace.\n  * ■\n    **Debug access port (DAP):**\n    This provides an interface for external debug access to the processor.\n  * ■\n    **Debug logic:**\n    Basic debug functionality includes processor halt, single-step, processor core register access, unlimited software breakpoints, and full system memory access.\n\n\n\n\n![Block diagram of a typical Microcontroller Chip based on Cortex-M3 architecture.](images/image_0016.jpeg)\n\n\nThe diagram illustrates the architecture of a typical Microcontroller Chip based on the Cortex-M3 core. It is organized into three main levels: the top-level chip, the Cortex-M3 Core, and the Cortex-M3 Processor.\n\n\n**Microcontroller Chip (Top Level):**\n\n\n  * **Security:**\n     Hardware AES.\n  * **Analog Interfaces:**\n     A/D converter, D/A converter.\n  * **Timers & Triggers:**\n     Periph bus int, Timer/counter, Low energy, Real time ctr, Pulse counter, Watch-dog tmr.\n  * **Parallel I/O Ports:**\n     Pin reset, General purpose I/O, External Interrupts.\n  * **Serial Interfaces:**\n     USART, USB, UART, Low-energy UART.\n  * **Peripheral bus:**\n     Connects the top-level blocks.\n  * **32-bit bus:**\n     Connects the peripheral bus to the Core and memory blocks.\n  * **Energy management:**\n     Voltage regulator, Voltage comparator, Power-on reset, Brown-out detector.\n  * **Clock management:**\n     High frequency RC oscillator, High freq crystal oscillator, Low frequency RC oscillator, Low freq crystal oscillator.\n  * **Core and memory:**\n     Flash memory 64 kB, SRAM memory 64 kB, Debug interface, DMA controller, Memory protection unit, Cortex-M3 processor.\n\n\n**Cortex-M3 Core (Bottom Left):**\n\n\n  * NVIC interface\n  * ETM interface\n  * 32-bit ALU (Hardware divider, 32-bit multiplier)\n  * Control logic\n  * Thumb decode\n  * Instruction interface\n  * Data interface\n\n\n**Cortex-M3 Processor (Bottom Right):**\n\n\n  * ICode interface\n  * SRAM & peripheral I/F\n  * Bus matrix\n  * Debug logic\n  * DAP\n  * Memory protection unit\n  * NVIC\n  * ARM core\n  * ETM\n\n\nDashed lines indicate the hierarchical relationship between the Cortex-M3 Core and the Cortex-M3 Processor, which together form the Core and memory block of the Microcontroller Chip.\n\n\nBlock diagram of a typical Microcontroller Chip based on Cortex-M3 architecture.\n\n\n**Figure 1.16**\n   Typical Microcontroller Chip Based on Cortex-M3\n\n\n  * ■\n    **I Code interface:**\n    Fetches instructions from the code memory space.\n  * ■\n    **SRAM & peripheral interface:**\n    Read/write interface to data memory and peripheral devices.\n  * ■\n    **Bus matrix:**\n    Connects the core and debug interfaces to external buses on the microcontroller.\n  * ■\n    **Memory protection unit:**\n    Protects critical data used by the operating system from user applications, separating processing tasks by disallowing access to each other's data, disabling access to memory regions, allowing memory regions to be defined as read-only, and detecting unexpected memory accesses that could potentially break the system.\n\n\nThe upper part of Figure 1.16 shows the block diagram of a typical microcontroller built with the Cortex-M3, in this case the EFM32 microcontroller. This microcontroller is marketed for use in a wide variety of devices, including energy, gas, and water metering; alarm and security systems; industrial automation devices; home automation devices; smart accessories; and health and fitness devices. The silicon chip consists of 10 main areas:\n   \n    13\n\n\n  * ■\n    **Core and memory:**\n    This region includes the Cortex-M3 processor, static RAM (SRAM) data memory,\n    \n     14\n    \n    and flash memory\n    \n     15\n    \n    for storing program instructions and nonvarying application data. Flash memory is nonvolatile (data is not lost when power is shut off) and so is ideal for this purpose. The SRAM stores variable data. This area also includes a debug interface, which makes it easy to reprogram and update the system in the field.\n  * ■\n    **Parallel I/O ports:**\n    Configurable for a variety of parallel I/O schemes.\n  * ■\n    **Serial interfaces:**\n    Supports various serial I/O schemes.\n  * ■\n    **Analog interfaces:**\n    Analog-to-digital and digital-to-analog logic to support sensors and actuators.\n  * ■\n    **Timers and triggers:**\n    Keeps track of timing and counts events, generates output waveforms, and triggers timed actions in other peripherals.\n  * ■\n    **Clock management:**\n    Controls the clocks and oscillators on the chip. Multiple clocks and oscillators are used to minimize power consumption and provide short startup times.\n  * ■\n    **Energy management:**\n    Manages the various low-energy modes of operation of the processor and peripherals to provide real-time management of the energy needs so as to minimize energy consumption.\n  * ■\n    **Security:**\n    The chip includes a hardware implementation of the Advanced Encryption Standard (AES).\n\n\n13\n   \n   This discussion does not go into details about all of the individual modules; for the interested reader, an in-depth discussion is provided in the document EFM32G200.pdf, available at\n   box.com/COA10e\n   .\n\n\n14\n   \n   Static RAM (SRAM) is a form of random-access memory used for cache memory; see Chapter 5.\n\n\n15\n   \n   Flash memory is a versatile form of memory used both in microcontrollers and as external memory; it is discussed in Chapter 6.\n\n\n  * ■\n    **32-bit bus:**\n    Connects all of the components on the chip.\n  * ■\n    **Peripheral bus:**\n    A network which lets the different peripheral module communicate directly with each other without involving the processor. This supports timing-critical operation and reduces software overhead.\n\n\nComparing Figure 1.16 with Figure 1.2, you will see many similarities and the same general hierarchical structure. Note, however, that the top level of a microcontroller computer system is a single chip, whereas for a multicore computer, the top level is a motherboard containing a number of chips. Another noteworthy difference is that there is no cache, neither in the Cortex-M3 processor nor in the microcontroller as a whole, which plays an important role if the code or data resides in external memory. Though the number of cycles to read the instruction or data varies depending on cache hit or miss, the cache greatly improves the performance when external memory is used. Such overhead is not needed for a microcontroller."
        },
        {
          "name": "Cloud Computing",
          "content": "Although the general concepts for cloud computing go back to the 1950s, cloud computing services first became available in the early 2000s, particularly targeted at large enterprises. Since then, cloud computing has spread to small and medium size businesses, and most recently to consumers. Apple's iCloud was launched in 2012 and had 20 million users within a week of launch. Evernote, the cloud-based notetaking and archiving service, launched in 2008, approached 100 million users in less than 6 years. In this section, we provide a brief overview. Cloud computing is examined in more detail in Chapter 17.\n\n\n\n\n**Basic Concepts**\n\n\nThere is an increasingly prominent trend in many organizations to move a substantial portion or even all information technology (IT) operations to an Internet-connected infrastructure known as enterprise cloud computing. At the same time, individual users of PCs and mobile devices are relying more and more on cloud computing services to backup data, sync devices, and share, using personal cloud computing. NIST defines cloud computing, in NIST SP-800-145 (\n   *The NIST Definition of Cloud Computing*\n   ), as follows:\n\n\n**Cloud computing:**\n   A model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.\n\n\nBasically, with cloud computing, you get economies of scale, professional network management, and professional security management. These features can be attractive to companies large and small, government agencies, and individual PC and mobile users. The individual or company only needs to pay for the storage\n\n\ncapacity and services they need. The user, be it company or individual, doesn't have the hassle of setting up a database system, acquiring the hardware they need, doing maintenance, and backing up the data—all these are part of the cloud service.\n\n\nIn theory, another big advantage of using cloud computing to store your data and share it with others is that the cloud provider takes care of security. Alas, the customer is not always protected. There have been a number of security failures among cloud providers. Evernote made headlines in early 2013 when it told all of its users to reset their passwords after an intrusion was discovered.\n\n\n**Cloud networking**\n   refers to the networks and network management functionality that must be in place to enable cloud computing. Most cloud computing solutions rely on the Internet, but that is only a piece of the networking infrastructure. One example of cloud networking is the provisioning of high-performance and/or high-reliability networking between the provider and subscriber. In this case, some or all of the traffic between an enterprise and the cloud bypasses the Internet and uses dedicated private network facilities owned or leased by the cloud service provider. More generally, cloud networking refers to the collection of network capabilities required to access a cloud, including making use of specialized services over the Internet, linking enterprise data centers to a cloud, and using firewalls and other network security devices at critical points to enforce access security policies.\n\n\nWe can think of\n   **cloud storage**\n   as a subset of cloud computing. In essence, cloud storage consists of database storage and database applications hosted remotely on cloud servers. Cloud storage enables small businesses and individual users to take advantage of data storage that scales with their needs and to take advantage of a variety of database applications without having to buy, maintain, and manage the storage assets.\n\n\n\n\n**Cloud Services**\n\n\nThe essential purpose of cloud computing is to provide for the convenient rental of computing resources. A cloud service provider (CSP) maintains computing and data storage resources that are available over the Internet or private networks. Customers can rent a portion of these resources as needed. Virtually all cloud service is provided using one of three models (Figure 1.17): SaaS, PaaS, and IaaS, which we examine in this section.\n\n\n**SOFTWARE AS A SERVICE (SAAS)**\n   As the name implies, a SaaS cloud provides service to customers in the form of software, specifically application software, running on and accessible in the cloud. SaaS follows the familiar model of Web services, in this case applied to cloud resources. SaaS enables the customer to use the cloud provider's applications running on the provider's cloud infrastructure. The applications are accessible from various client devices through a simple interface such as a Web browser. Instead of obtaining desktop and server licenses for software products it uses, an enterprise obtains the same functions from the cloud service. SaaS saves the complexity of software installation, maintenance, upgrades, and patches. Examples of services at this level are Gmail, Google's e-mail service, and Salesforce.com, which help firms keep track of their customers.\n\n\nCommon subscribers to SaaS are organizations that want to provide their employees with access to typical office productivity software, such as document\n\n\n\n\n![Diagram comparing Traditional IT architecture, Infrastructure as a service (IaaS), Platform as a service (PaaS), and Software as a service (SaaS) models. The diagram shows a stack of components from Applications down to Networking. In Traditional IT, all components are managed by the client. In IaaS, the client manages Applications, Application Framework, Compilers, Run-time environment, Databases, and Operating system, while the CSP manages Virtual machine, Server hardware, Storage, and Networking. In PaaS, the client manages Applications and Application Framework, while the CSP manages Compilers, Run-time environment, Databases, Operating system, Virtual machine, Server hardware, Storage, and Networking. In SaaS, the client only manages Applications, while the CSP manages everything else. A large double-headed arrow at the bottom indicates the spectrum from more complex and customizable (Traditional IT) to less complex and less customizable (SaaS).](images/image_0017.jpeg)\n\n\n**Traditional IT architecture**\n\n\n**Infrastructure as a service (IaaS)**\n\n\n**Platform as a service (PaaS)**\n\n\n**Software as a service (SaaS)**\n\n\n**More complex**\n\n\n**More upfront cost**\n\n\n**Less scalable**\n\n\n**More customizable**\n\n\n**Less complex**\n\n\n**Lower upfront cost**\n\n\n**More scalable**\n\n\n**Less customizable**\n\n\nDiagram comparing Traditional IT architecture, Infrastructure as a service (IaaS), Platform as a service (PaaS), and Software as a service (SaaS) models. The diagram shows a stack of components from Applications down to Networking. In Traditional IT, all components are managed by the client. In IaaS, the client manages Applications, Application Framework, Compilers, Run-time environment, Databases, and Operating system, while the CSP manages Virtual machine, Server hardware, Storage, and Networking. In PaaS, the client manages Applications and Application Framework, while the CSP manages Compilers, Run-time environment, Databases, Operating system, Virtual machine, Server hardware, Storage, and Networking. In SaaS, the client only manages Applications, while the CSP manages everything else. A large double-headed arrow at the bottom indicates the spectrum from more complex and customizable (Traditional IT) to less complex and less customizable (SaaS).\n\n\nIT = information technology\n   \n\n   CSP = cloud service provider\n\n\n**Figure 1.17**\n   Alternative Information Technology Architectures\n\n\nmanagement and email. Individuals also commonly use the SaaS model to acquire cloud resources. Typically, subscribers use specific applications on demand. The cloud provider also usually offers data-related features such as automatic backup and data sharing between subscribers.\n\n\n**PLATFORM AS A SERVICE (PAAS)**\n   A PaaS cloud provides service to customers in the form of a platform on which the customer's applications can run. PaaS enables the customer to deploy onto the cloud infrastructure containing customer-created or acquired applications. A PaaS cloud provides useful software building blocks, plus a number of development tools, such as programming languages, run-time environments, and other tools that assist in deploying new applications. In effect, PaaS is an operating system in the cloud. PaaS is useful for an organization that wants to develop new or tailored applications while paying for the needed computing resources only as needed and only for as long as needed. Google App Engine and the Salesforce1 Platform from Salesforce.com are examples of PaaS.\n\n\n**INFRASTRUCTURE AS A SERVICE (IaaS)**\n   With IaaS, the customer has access to the underlying cloud infrastructure. IaaS provides virtual machines and other abstracted hardware and operating systems, which may be controlled through a service application programming interface (API). IaaS offers the customer processing, storage, networks, and other fundamental computing resources so that the customer is able to deploy and run arbitrary software, which can include operating systems and applications. IaaS enables customers to combine basic computing services, such as number crunching and data storage, to build highly adaptable computer systems. Examples of IaaS are Amazon Elastic Compute Cloud (Amazon EC2) and Windows Azure."
        }
      ]
    },
    {
      "name": "Performance Issues",
      "sections": [
        {
          "name": "Designing for Performance",
          "content": "Year by year, the cost of computer systems continues to drop dramatically, while the performance and capacity of those systems continue to rise equally dramatically. Today's laptops have the computing power of an IBM mainframe from 10 or 15 years ago. Thus, we have virtually \"free\" computer power. Processors are so inexpensive that we now have microprocessors we throw away. The digital pregnancy test is an example (used once and then thrown away). And this continuing technological revolution has enabled the development of applications of astounding complexity and power. For example, desktop applications that require the great power of today's microprocessor-based systems include\n\n\n  * ■ Image processing\n  * ■ Three-dimensional rendering\n  * ■ Speech recognition\n  * ■ Videoconferencing\n  * ■ Multimedia authoring\n  * ■ Voice and video annotation of files\n  * ■ Simulation modeling\n\n\nWorkstation systems now support highly sophisticated engineering and scientific applications and have the capacity to support image and video applications. In addition, businesses are relying on increasingly powerful servers to handle transaction and database processing and to support massive client/server networks that have replaced the huge mainframe computer centers of yesteryear. As well, cloud service\n\n\nproviders use massive high-performance banks of servers to satisfy high-volume, high-transaction-rate applications for a broad spectrum of clients.\n\n\nWhat is fascinating about all this from the perspective of computer organization and architecture is that, on the one hand, the basic building blocks for today's computer miracles are virtually the same as those of the IAS computer from over 50 years ago, while on the other hand, the techniques for squeezing the maximum performance out of the materials at hand have become increasingly sophisticated.\n\n\nThis observation serves as a guiding principle for the presentation in this book. As we progress through the various elements and components of a computer, two objectives are pursued. First, the book explains the fundamental functionality in each area under consideration, and second, the book explores those techniques required to achieve maximum performance. In the remainder of this section, we highlight some of the driving factors behind the need to design for performance.\n\n\n\n\n**Microprocessor Speed**\n\n\nWhat gives Intel x86 processors or IBM mainframe computers such mind-boggling power is the relentless pursuit of speed by processor chip manufacturers. The evolution of these machines continues to bear out Moore's law, described in Chapter 1. So long as this law holds, chipmakers can unleash a new generation of chips every three years—with four times as many transistors. In memory chips, this has quadrupled the capacity of\n   **dynamic random-access memory (DRAM)**\n   , still the basic technology for computer main memory, every three years. In microprocessors, the addition of new circuits, and the speed boost that comes from reducing the distances between them, has improved performance four- or fivefold every three years or so since Intel launched its x86 family in 1978.\n\n\nBut the raw speed of the microprocessor will not achieve its potential unless it is fed a constant stream of work to do in the form of computer instructions. Anything that gets in the way of that smooth flow undermines the power of the processor. Accordingly, while the chipmakers have been busy learning how to fabricate chips of greater and greater density, the processor designers must come up with ever more elaborate techniques for feeding the monster. Among the techniques built into contemporary processors are the following:\n\n\n  * ■\n    **Pipelining:**\n    The execution of an instruction involves multiple stages of operation, including fetching the instruction, decoding the opcode, fetching operands, performing a calculation, and so on. Pipelining enables a processor to work simultaneously on multiple instructions by performing a different phase for each of the multiple instructions at the same time. The processor overlaps operations by moving data or instructions into a conceptual pipe with all stages of the pipe processing simultaneously. For example, while one instruction is being executed, the computer is decoding the next instruction. This is the same principle as seen in an assembly line.\n  * ■\n    **Branch prediction:**\n    The processor looks ahead in the instruction code fetched from memory and predicts which branches, or groups of instructions, are likely to be processed next. If the processor guesses right most of the time, it can prefetch the correct instructions and buffer them so that the processor is kept busy. The more sophisticated examples of this strategy predict not just\n\n\nthe next branch but multiple branches ahead. Thus, branch prediction potentially increases the amount of work available for the processor to execute.\n\n\n  * ■\n    **Superscalar execution:**\n    This is the ability to issue more than one instruction in every processor clock cycle. In effect, multiple parallel pipelines are used.\n  * ■\n    **Data flow analysis:**\n    The processor analyzes which instructions are dependent on each other's results, or data, to create an optimized schedule of instructions. In fact, instructions are scheduled to be executed when ready, independent of the original program order. This prevents unnecessary delay.\n  * ■\n    **Speculative execution:**\n    Using branch prediction and data flow analysis, some processors speculatively execute instructions ahead of their actual appearance in the program execution, holding the results in temporary locations. This enables the processor to keep its execution engines as busy as possible by executing instructions that are likely to be needed.\n\n\nThese and other sophisticated techniques are made necessary by the sheer power of the processor. Collectively they make it possible to execute many instructions per processor cycle, rather than to take many cycles per instruction.\n\n\n\n\n**Performance Balance**\n\n\nWhile processor power has raced ahead at breakneck speed, other critical components of the computer have not kept up. The result is a need to look for performance balance: an adjustment/tuning of the organization and architecture to compensate for the mismatch among the capabilities of the various components.\n\n\nThe problem created by such mismatches is particularly critical at the interface between processor and main memory. While processor speed has grown rapidly, the speed with which data can be transferred between main memory and the processor has lagged badly. The interface between processor and main memory is the most crucial pathway in the entire computer because it is responsible for carrying a constant flow of program instructions and data between memory chips and the processor. If memory or the pathway fails to keep pace with the processor's insistent demands, the processor stalls in a wait state, and valuable processing time is lost.\n\n\nA system architect can attack this problem in a number of ways, all of which are reflected in contemporary computer designs. Consider the following examples:\n\n\n  * ■ Increase the number of bits that are retrieved at one time by making DRAMs “wider” rather than “deeper” and by using wide bus data paths.\n  * ■ Change the DRAM interface to make it more efficient by including a cache\n    \n     1\n    \n    or other buffering scheme on the DRAM chip.\n  * ■ Reduce the frequency of memory access by incorporating increasingly complex and efficient cache structures between the processor and main memory. This includes the incorporation of one or more caches on the processor chip as well as on an off-chip cache close to the processor chip.\n\n\n1\n   \n   A cache is a relatively small fast memory interposed between a larger, slower memory and the logic that accesses the larger memory. The cache holds recently accessed data and is designed to speed up subsequent access to the same data. Caches are discussed in Chapter 4.\n\n\n  * ■ Increase the interconnect bandwidth between processors and memory by using higher-speed buses and a hierarchy of buses to buffer and structure data flow.\n\n\nAnother area of design focus is the handling of I/O devices. As computers become faster and more capable, more sophisticated applications are developed that support the use of peripherals with intensive I/O demands. Figure 2.1 gives some examples of typical peripheral devices in use on personal computers and workstations. These devices create tremendous data throughput demands. While the current generation of processors can handle the data pumped out by these devices, there remains the problem of getting that data moved between processor and peripheral. Strategies here include caching and buffering schemes plus the use of higher-speed interconnection buses and more elaborate interconnection structures. In addition, the use of multiple-processor configurations can aid in satisfying I/O demands.\n\n\nThe key in all this is balance. Designers constantly strive to balance the throughput and processing demands of the processor components, main memory, I/O devices, and the interconnection structures. This design must constantly be rethought to cope with two constantly evolving factors:\n\n\n  * ■ The rate at which performance is changing in the various technology areas (processor, buses, memory, peripherals) differs greatly from one type of element to another.\n  * ■ New applications and new peripheral devices constantly change the nature of the demand on the system in terms of typical instruction profile and the data access patterns.\n\n\n\n\n![Figure 2.1: Typical I/O Device Data Rates. A horizontal bar chart showing data rates for various I/O devices on a logarithmic scale from 10^1 to 10^11 bps.](images/image_0018.jpeg)\n\n\nA horizontal bar chart titled 'Typical I/O Device Data Rates'. The y-axis lists nine devices: Ethernet modem (max speed), Graphics display, Wi-Fi modem (max speed), Hard disk, Optical disc, Laser printer, Scanner, Mouse, and Keyboard. The x-axis represents the 'Data Rate (bps)' on a logarithmic scale with major ticks at\n    \n     10^1\n    \n    ,\n    \n     10^2\n    \n    ,\n    \n     10^3\n    \n    ,\n    \n     10^4\n    \n    ,\n    \n     10^5\n    \n    ,\n    \n     10^6\n    \n    ,\n    \n     10^7\n    \n    ,\n    \n     10^8\n    \n    ,\n    \n     10^9\n    \n    ,\n    \n     10^{10}\n    \n    , and\n    \n     10^{11}\n    \n    . The bars are teal-colored. The Ethernet modem has the highest data rate, reaching approximately\n    \n     10^{11}\n    \n    bps. The Graphics display and Wi-Fi modem follow, both around\n    \n     10^{10}\n    \n    bps. The Hard disk is around\n    \n     10^9\n    \n    bps, the Optical disc around\n    \n     10^8\n    \n    bps, the Laser printer around\n    \n     10^7\n    \n    bps, the Scanner around\n    \n     10^6\n    \n    bps, the Mouse around\n    \n     10^2\n    \n    bps, and the Keyboard around\n    \n     10^1\n    \n    bps.\n\n\n\nDevice | Approximate Data Rate (bps)\nEthernet modem (max speed) | 10^{11}\nGraphics display | 10^{10}\nWi-Fi modem (max speed) | 10^{10}\nHard disk | 10^9\nOptical disc | 10^8\nLaser printer | 10^7\nScanner | 10^6\nMouse | 10^2\nKeyboard | 10^1\n\n\nFigure 2.1: Typical I/O Device Data Rates. A horizontal bar chart showing data rates for various I/O devices on a logarithmic scale from 10^1 to 10^11 bps.\n\n\n**Figure 2.1**\n   Typical I/O Device Data Rates\n\n\nThus, computer design is a constantly evolving art form. This book attempts to present the fundamentals on which this art form is based and to present a survey of the current state of that art.\n\n\n\n\n**Improvements in Chip Organization and Architecture**\n\n\nAs designers wrestle with the challenge of balancing processor performance with that of main memory and other computer components, the need to increase processor speed remains. There are three approaches to achieving increased processor speed:\n\n\n  * ■ Increase the hardware speed of the processor. This increase is fundamentally due to shrinking the size of the logic gates on the processor chip, so that more gates can be packed together more tightly and to increasing the clock rate. With gates closer together, the propagation time for signals is significantly reduced, enabling a speeding up of the processor. An increase in clock rate means that individual operations are executed more rapidly.\n  * ■ Increase the size and speed of caches that are interposed between the processor and main memory. In particular, by dedicating a portion of the processor chip itself to the cache, cache access times drop significantly.\n  * ■ Make changes to the processor organization and architecture that increase the effective speed of instruction execution. Typically, this involves using parallelism in one form or another.\n\n\nTraditionally, the dominant factor in performance gains has been in increases in clock speed due and logic density. However, as clock speed and logic density increase, a number of obstacles become more significant [INTE04]:\n\n\n  * ■\n    **Power:**\n    As the density of logic and the clock speed on a chip increase, so does the power density (Watts/cm\n    \n     2\n    \n    ). The difficulty of dissipating the heat generated on high-density, high-speed chips is becoming a serious design issue [GIBB04, BORK03].\n  * ■\n    **RC delay:**\n    The speed at which electrons can flow on a chip between transistors is limited by the resistance and capacitance of the metal wires connecting them; specifically, delay increases as the RC product increases. As components on the chip decrease in size, the wire interconnects become thinner, increasing resistance. Also, the wires are closer together, increasing capacitance.\n  * ■\n    **Memory latency and throughput:**\n    Memory access speed (latency) and transfer speed (throughput) lag processor speeds, as previously discussed.\n\n\nThus, there will be more emphasis on organization and architectural approaches to improving performance. These techniques are discussed in later chapters of the text.\n\n\nBeginning in the late 1980s, and continuing for about 15 years, two main strategies have been used to increase performance beyond what can be achieved simply by increasing clock speed. First, there has been an increase in cache capacity. There are now typically two or three levels of cache between the processor and main memory. As chip density has increased, more of the cache memory has been incorporated on the chip, enabling faster cache access. For example, the original Pentium\n\n\nchip devoted about 10% of on-chip area to a cache. Contemporary chips devote over half of the chip area to caches. And, typically, about three-quarters of the other half is for pipeline-related control and buffering.\n\n\nSecond, the instruction execution logic within a processor has become increasingly complex to enable parallel execution of instructions within the processor. Two noteworthy design approaches have been pipelining and superscalar. A pipeline works much as an assembly line in a manufacturing plant enabling different stages of execution of different instructions to occur at the same time along the pipeline. A superscalar approach in essence allows multiple pipelines within a single processor, so that instructions that do not depend on one another can be executed in parallel.\n\n\nBy the mid to late 90s, both of these approaches were reaching a point of diminishing returns. The internal organization of contemporary processors is exceedingly complex and is able to squeeze a great deal of parallelism out of the instruction stream. It seems likely that further significant increases in this direction will be relatively modest [GIBB04]. With three levels of cache on the processor chip, each level providing substantial capacity, it also seems that the benefits from the cache are reaching a limit.\n\n\nHowever, simply relying on increasing clock rate for increased performance runs into the power dissipation problem already referred to. The faster the clock rate, the greater the amount of power to be dissipated, and some fundamental physical limits are being reached.\n\n\nFigure 2.2 illustrates the concepts we have been discussing.\n   \n    2\n   \n   The top line shows that, as per Moore’s Law, the number of transistors on a single chip continues to\n\n\n\n\n![Figure 2.2: Processor Trends. A log-linear plot showing the growth of Transistors (Thousands), Frequency (MHz), Power (W), and Cores from 1970 to 2010. The y-axis is logarithmic, ranging from 0.1 to 10^7. The x-axis is linear, ranging from 1970 to 2010. Transistors (diamonds) show the steepest growth, followed by Frequency (squares), Power (triangles), and Cores (circles).](images/image_0019.jpeg)\n\n\nDetailed description of Figure 2.2: The graph plots four metrics against time from 1970 to 2010. The y-axis is logarithmic, with major ticks at 0.1, 1, 10, 10^2, 10^3, 10^4, 10^5, 10^6, and 10^7. The x-axis is linear, with major ticks every 5 years. The legend identifies four series: Transistors (Thousands) represented by dark blue diamonds, Frequency (MHz) represented by dark blue squares, Power (W) represented by dark blue triangles, and Cores represented by light blue circles. The Transistors series shows a steady upward trend, starting around 10^3 in 1970 and reaching approximately 10^6.5 in 2010. The Frequency series starts around 1 MHz in 1970 and reaches about 3 GHz in 2010. The Power series starts around 1 W in 1970 and reaches about 100 W in 2010. The Cores series starts around 1 in 1970 and reaches about 10 in 2010.\n\n\n\nYear | Transistors (Thousands) | Frequency (MHz) | Power (W) | Cores\n1970 | 1000 | 1 | 1 | 1\n1975 | 10000 | 10 | 10 | 1\n1980 | 100000 | 100 | 100 | 1\n1985 | 1000000 | 1000 | 1000 | 1\n1990 | 10000000 | 10000 | 10000 | 1\n1995 | 100000000 | 100000 | 100000 | 1\n2000 | 1000000000 | 1000000 | 1000000 | 1\n2005 | 10000000000 | 10000000 | 10000000 | 10\n2010 | 100000000000 | 100000000 | 100000000 | 100\n\n\nFigure 2.2: Processor Trends. A log-linear plot showing the growth of Transistors (Thousands), Frequency (MHz), Power (W), and Cores from 1970 to 2010. The y-axis is logarithmic, ranging from 0.1 to 10^7. The x-axis is linear, ranging from 1970 to 2010. Transistors (diamonds) show the steepest growth, followed by Frequency (squares), Power (triangles), and Cores (circles).\n\n\n**Figure 2.2**\n   Processor Trends\n\n\n2\n   \n   I am grateful to Professor Kathy Yelick of UC Berkeley, who provided this graph.\n\n\ngrow exponentially.\n   \n    3\n   \n   Meanwhile, the clock speed has leveled off, in order to prevent a further rise in power. To continue increasing performance, designers have had to find ways of exploiting the growing number of transistors other than simply building a more complex processor. The response in recent years has been the development of the multicore computer chip."
        },
        {
          "name": "Multicore, Mics, and GPGPUs",
          "content": "With all of the difficulties cited in the preceding section in mind, designers have turned to a fundamentally new approach to improving performance: placing multiple processors on the same chip, with a large shared cache. The use of multiple processors on the same chip, also referred to as multiple cores, or\n   **multicore**\n   , provides the potential to increase performance without increasing the clock rate. Studies indicate that, within a processor, the increase in performance is roughly proportional to the square root of the increase in complexity [BORK03]. But if the software can support the effective use of multiple processors, then doubling the number of processors almost doubles performance. Thus, the strategy is to use two simpler processors on the chip rather than one more complex processor.\n\n\nIn addition, with two processors, larger caches are justified. This is important because the power consumption of memory logic on a chip is much less than that of processing logic.\n\n\nAs the logic density on chips continues to rise, the trend for both more cores and more cache on a single chip continues. Two-core chips were quickly followed by four-core chips, then 8, then 16, and so on. As the caches became larger, it made performance sense to create two and then three levels of cache on a chip, with initially, the first-level cache dedicated to an individual processor and levels two and three being shared by all the processors. It is now common for the second-level cache to also be private to each core.\n\n\nChip manufacturers are now in the process of making a huge leap forward in the number of cores per chip, with more than 50 cores per chip. The leap in performance as well as the challenges in developing software to exploit such a large number of cores has led to the introduction of a new term:\n   **many integrated core (MIC)**\n   .\n\n\nThe multicore and MIC strategy involves a homogeneous collection of general-purpose processors on a single chip. At the same time, chip manufacturers are pursuing another design option: a chip with multiple general-purpose processors plus\n   **graphics processing units (GPUs)**\n   and specialized cores for video processing and other tasks. In broad terms, a GPU is a core designed to perform parallel operations on graphics data. Traditionally found on a plug-in graphics card (display adapter), it is used to encode and render 2D and 3D graphics as well as process video.\n\n\nSince GPUs perform parallel operations on multiple sets of data, they are increasingly being used as vector processors for a variety of applications that require repetitive computations. This blurs the line between the GPU and the CPU\n\n\n3\n   \n   The observant reader will note that the transistor count values in this figure are significantly less than those of Figure 1.12. That latter figure shows the transistor count for a form of main memory known as DRAM (discussed in Chapter 5), which supports higher transistor density than processor chips.\n\n\n[AROR12, FATA08, PROP11]. When a broad range of applications are supported by such a processor, the term\n   **general-purpose computing on GPUs (GPGPU)**\n   is used.\n\n\nWe explore design characteristics of multicore computers in Chapter 18 and GPGPUs in Chapter 19."
        },
        {
          "name": "Two Laws that Provide Insight: Ahmdahl’s Law and Little’s Law",
          "content": "In this section, we look at two equations, called “laws.” The two laws are unrelated but both provide insight into the performance of parallel systems and multicore systems.\n\n\n\n\n**Amdahl's Law**\n\n\nComputer system designers look for ways to improve system performance by advances in technology or change in design. Examples include the use of parallel processors, the use of a memory cache hierarchy, and speedup in memory access time and I/O transfer rate due to technology improvements. In all of these cases, it is important to note that a speedup in one aspect of the technology or design does not result in a corresponding improvement in performance. This limitation is succinctly expressed by Amdahl's law.\n\n\nAmdahl's law was first proposed by Gene Amdahl in 1967 ([AMDA67], [AMDA13]) and deals with the potential speedup of a program using multiple processors compared to a single processor. Consider a program running on a single processor such that a fraction\n   \n    (1 - f)\n   \n   of the execution time involves code that is inherently sequential, and a fraction\n   \n    f\n   \n   that involves code that is infinitely parallelizable with no scheduling overhead. Let\n   \n    T\n   \n   be the total execution time of the program using a single processor. Then the speedup using a parallel processor with\n   \n    N\n   \n   processors that fully exploits the parallel portion of the program is as follows:\n\n\n\\begin{aligned} \\text{Speedup} &= \\frac{\\text{Time to execute program on a single processor}}{\\text{Time to execute program on } N \\text{ parallel processors}} \\\\ &= \\frac{T(1 - f) + Tf}{T(1 - f) + \\frac{Tf}{N}} = \\frac{1}{(1 - f) + \\frac{f}{N}} \\end{aligned}\n\n\nThis equation is illustrated in Figures 2.3 and 2.4. Two important conclusions can be drawn:\n\n\n  * 1. When\n    \n     f\n    \n    is small, the use of parallel processors has little effect.\n  * 2. As\n    \n     N\n    \n    approaches infinity, speedup is bound by\n    \n     1/(1 - f)\n    \n    , so that there are diminishing returns for using more processors.\n\n\nThese conclusions are too pessimistic, an assertion first put forward in [GUST88]. For example, a server can maintain multiple threads or multiple tasks to handle multiple clients and execute the threads or tasks in parallel up to the limit of the number of processors. Many database applications involve computations on massive amounts of data that can be split up into multiple parallel tasks.\n\n\n\n\n![Figure 2.3: Illustration of Amdahl's Law. The diagram shows a horizontal timeline of total execution time T. The timeline is divided into two segments: (1-f)T and fT. Below this, a solid horizontal bar represents the parallelizable portion of the task, which takes fT time. To the left of this bar is a dashed vertical line, and to the right is a solid vertical line. Below the solid bar, a second timeline shows the execution time after parallelization. This timeline is divided into (1-f)T and fT/N. The total execution time is shown as (1-f)(1-1/N)T.](images/image_0020.jpeg)\n\n\nFigure 2.3: Illustration of Amdahl's Law. The diagram shows a horizontal timeline of total execution time T. The timeline is divided into two segments: (1-f)T and fT. Below this, a solid horizontal bar represents the parallelizable portion of the task, which takes fT time. To the left of this bar is a dashed vertical line, and to the right is a solid vertical line. Below the solid bar, a second timeline shows the execution time after parallelization. This timeline is divided into (1-f)T and fT/N. The total execution time is shown as (1-f)(1-1/N)T.\n\n\n**Figure 2.3**\n   Illustration of Amdahl's Law\n\n\nNevertheless, Amdahl's law illustrates the problems facing industry in the development of multicore machines with an ever-growing number of cores: The software that runs on such machines must be adapted to a highly parallel execution environment to exploit the power of parallel processing.\n\n\nAmdahl's law can be generalized to evaluate any design or technical improvement in a computer system. Consider any enhancement to a feature of a system that results in a speedup. The speedup can be expressed as\n\n\n\\text{Speedup} = \\frac{\\text{Performance after enhancement}}{\\text{Performance before enhancement}} = \\frac{\\text{Execution time before enhancement}}{\\text{Execution time after enhancement}} \\quad (2.1)\n\n\n\n\n![Figure 2.4: A graph showing Speedup versus Number of Processors for different values of f (fraction of code that is sequential). The x-axis is logarithmic, ranging from 1 to 1000 processors. The y-axis is linear, ranging from 0 to 20 speedup. Four curves are shown: f=0.95 (solid line, highest speedup), f=0.90 (solid line), f=0.75 (dashed line), and f=0.5 (dashed line, lowest speedup). All curves start at (1, 1) and increase as the number of processors increases, eventually leveling off.](images/image_0021.jpeg)\n\n\nFigure 2.4: A graph showing Speedup versus Number of Processors for different values of f (fraction of code that is sequential). The x-axis is logarithmic, ranging from 1 to 1000 processors. The y-axis is linear, ranging from 0 to 20 speedup. Four curves are shown: f=0.95 (solid line, highest speedup), f=0.90 (solid line), f=0.75 (dashed line), and f=0.5 (dashed line, lowest speedup). All curves start at (1, 1) and increase as the number of processors increases, eventually leveling off.\n\n\n**Figure 2.4**\n   Amdahl's Law for Multiprocessors\n\n\nSuppose that a feature of the system is used during execution a fraction of the time\n   \n    f\n   \n   , before enhancement, and that the speedup of that feature after enhancement is\n   \n    SU_f\n   \n   . Then the overall speedup of the system is\n\n\n\\text{Speedup} = \\frac{1}{(1 - f) + \\frac{f}{SU_f}}\n\n\n**EXAMPLE 2.1**\n   Suppose that a task makes extensive use of floating-point operations, with 40% of the time consumed by floating-point operations. With a new hardware design, the floating-point module is sped up by a factor of\n   \n    K\n   \n   . Then the overall speedup is as follows:\n\n\n\\text{Speedup} = \\frac{1}{0.6 + \\frac{0.4}{K}}\n\n\nThus, independent of\n   \n    K\n   \n   , the maximum speedup is 1.67.\n\n\n\n\n**Little's Law**\n\n\nA fundamental and simple relation with broad applications is Little's Law [LITT61, LITT11].\n   \n    4\n   \n   We can apply it to almost any system that is statistically in steady state, and in which there is no leakage. Specifically, we have a steady state system to which items arrive at an average rate of\n   \n    \\lambda\n   \n   items per unit time. The items stay in the system an average of\n   \n    W\n   \n   units of time. Finally, there is an average of\n   \n    L\n   \n   units in the system at any one time. Little's Law relates these three variables as\n   \n    L = \\lambda W\n   \n   .\n\n\nUsing queuing theory terminology, Little's Law applies to a queuing system. The central element of the system is a server, which provides some service to items. Items from some population of items arrive at the system to be served. If the server is idle, an item is served immediately. Otherwise, an arriving item joins a waiting line, or queue. There can be a single queue for a single server, a single queue for multiple servers, or multiples queues, one for each of multiple servers. When a server has completed serving an item, the item departs. If there are items waiting in the queue, one is immediately dispatched to the server. The server in this model can represent anything that performs some function or service for a collection of items. Examples: A processor provides service to processes; a transmission line provides a transmission service to packets or frames of data; and an I/O device provides a read or write service for I/O requests.\n\n\nTo understand Little's formula, consider the following argument, which focuses on the experience of a single item. When the item arrives, it will find on\n\n\n4\n   \n   The second reference is a retrospective article on his law that Little wrote 50 years after his original paper. That must be unique in the history of the technical literature, although Amdahl comes close, with a 46-year gap between [AMDA67] and [AMDA13].\n\n\naverage\n   \n    L\n   \n   items ahead of it, one being serviced and the rest in the queue. When the item leaves the system after being serviced, it will leave behind on average the same number of items in the system, namely\n   \n    L\n   \n   , because\n   \n    L\n   \n   is defined as the average number of items waiting. Further, the average time that the item was in the system was\n   \n    W\n   \n   . Since items arrive at a rate of\n   \n    \\lambda\n   \n   , we can reason that in the time\n   \n    W\n   \n   , a total of\n   \n    \\lambda W\n   \n   items must have arrived. Thus\n   \n    w = \\lambda W\n   \n   .\n\n\nTo summarize, under steady state conditions, the average number of items in a queuing system equals the average rate at which items arrive multiplied by the average time that an item spends in the system. This relationship requires very few assumptions. We do not need to know what the service time distribution is, what the distribution of arrival times is, or the order or priority in which items are served. Because of its simplicity and generality, Little's Law is extremely useful and has experienced somewhat of a revival due to the interest in performance problems related to multicore computers.\n\n\nA very simple example, from [LITT11], illustrates how Little's Law might be applied. Consider a multicore system, with each core supporting multiple threads of execution. At some level, the cores share a common memory. The cores share a common main memory and typically share a common cache memory as well. In any case, when a thread is executing, it may arrive at a point at which it must retrieve a piece of data from the common memory. The thread stops and sends out a request for that data. All such stopped threads are in a queue. If the system is being used as a server, an analyst can determine the demand on the system in terms of the rate of user requests, and then translate that into the rate of requests for data from the threads generated to respond to an individual user request. For this purpose, each user request is broken down into subtasks that are implemented as threads. We then have\n   \n    \\lambda\n   \n   = the average rate of total thread processing required after all members' requests have been broken down into whatever detailed subtasks are required. Define\n   \n    L\n   \n   as the average number of stopped threads waiting during some relevant time. Then\n   \n    W\n   \n   = average response time. This simple model can serve as a guide to designers as to whether user requirements are being met and, if not, provide a quantitative measure of the amount of improvement needed."
        },
        {
          "name": "Basic Measures of Computer Performance",
          "content": "In evaluating processor hardware and setting requirements for new systems, performance is one of the key parameters to consider, along with cost, size, security, reliability, and, in some cases, power consumption.\n\n\nIt is difficult to make meaningful performance comparisons among different processors, even among processors in the same family. Raw speed is far less important than how a processor performs when executing a given application. Unfortunately, application performance depends not just on the raw speed of the processor but also on the instruction set, choice of implementation language, efficiency of the compiler, and skill of the programming done to implement the application.\n\n\nIn this section, we look at some traditional measures of processor speed. In the next section, we examine benchmarking, which is the most common approach to assessing processor and computer system performance. The following section discusses how to average results from multiple tests.\n\n\n\n\n**Clock Speed**\n\n\nOperations performed by a processor, such as fetching an instruction, decoding the instruction, performing an arithmetic operation, and so on, are governed by a system clock. Typically, all operations begin with the pulse of the clock. Thus, at the most fundamental level, the speed of a processor is dictated by the pulse frequency produced by the clock, measured in cycles per second, or Hertz (Hz).\n\n\nTypically, clock signals are generated by a quartz crystal, which generates a constant sine wave while power is applied. This wave is converted into a digital voltage pulse stream that is provided in a constant flow to the processor circuitry (Figure 2.5). For example, a 1-GHz processor receives 1 billion pulses per second. The rate of pulses is known as the\n   **clock rate**\n   , or\n   **clock speed**\n   . One increment, or pulse, of the clock is referred to as a\n   **clock cycle**\n   , or a\n   **clock tick**\n   . The time between pulses is the\n   **cycle time**\n   .\n\n\nThe clock rate is not arbitrary, but must be appropriate for the physical layout of the processor. Actions in the processor require signals to be sent from one processor element to another. When a signal is placed on a line inside the processor, it takes some finite amount of time for the voltage levels to settle down so that an accurate value (logical 1 or 0) is available. Furthermore, depending on the physical layout of the processor circuits, some signals may change more rapidly than others. Thus, operations must be synchronized and paced so that the proper electrical signal (voltage) values are available for each operation.\n\n\nThe execution of an instruction involves a number of discrete steps, such as fetching the instruction from memory, decoding the various portions of the instruction, loading and storing data, and performing arithmetic and logical operations. Thus, most instructions on most processors require multiple clock cycles to complete. Some instructions may take only a few cycles, while others require dozens. In addition, when pipelining is used, multiple instructions are being executed simultaneously. Thus, a straight comparison of clock speeds on different processors does not tell the whole story about performance.\n\n\n\n\n![Diagram of a system clock generation process. A quartz crystal is shown on the left, connected by a wavy line to a block labeled 'analog to digital conversion'. This block is then connected by a square-wave line to the right.](images/image_0022.jpeg)\n\n\nThe diagram illustrates the system clock generation process. It shows a 'quartz crystal' block on the left, which is connected by a wavy line to an 'analog to digital conversion' block. The 'analog to digital conversion' block then outputs a square-wave signal, represented by a line with sharp edges, extending to the right.\n\n\nDiagram of a system clock generation process. A quartz crystal is shown on the left, connected by a wavy line to a block labeled 'analog to digital conversion'. This block is then connected by a square-wave line to the right.\n\n\nFrom Computer Desktop Encyclopedia\n   \n\n   1998, The Computer Language Co.\n\n\n**Figure 2.5**\n   System Clock\n\n\n\n\n**Instruction Execution Rate**\n\n\nA processor is driven by a clock with a constant frequency\n   \n    f\n   \n   or, equivalently, a constant cycle time\n   \n    \\tau\n   \n   , where\n   \n    \\tau = 1/f\n   \n   . Define the instruction count,\n   \n    I_c\n   \n   , for a program as the number of machine instructions executed for that program until it runs to completion or for some defined time interval. Note that this is the number of instruction executions, not the number of instructions in the object code of the program. An important parameter is the average cycles per instruction (\n   \n    CPI\n   \n   ) for a program. If all instructions required the same number of clock cycles, then\n   \n    CPI\n   \n   would be a constant value for a processor. However, on any given processor, the number of clock cycles required varies for different types of instructions, such as load, store, branch, and so on. Let\n   \n    CPI_i\n   \n   be the number of cycles required for instruction type\n   \n    i\n   \n   , and\n   \n    I_i\n   \n   be the number of executed instructions of type\n   \n    i\n   \n   for a given program. Then we can calculate an overall\n   \n    CPI\n   \n   as follows:\n\n\nCPI = \\frac{\\sum_{i=1}^{n} (CPI_i \\times I_i)}{I_c} \\quad (2.2)\n\n\nThe processor time\n   \n    T\n   \n   needed to execute a given program can be expressed as\n\n\nT = I_c \\times CPI \\times \\tau\n\n\nWe can refine this formulation by recognizing that during the execution of an instruction, part of the work is done by the processor, and part of the time a word is being transferred to or from memory. In this latter case, the time to transfer depends on the memory cycle time, which may be greater than the processor cycle time. We can rewrite the preceding equation as\n\n\nT = I_c \\times [p + (m \\times k)] \\times \\tau\n\n\nwhere\n   \n    p\n   \n   is the number of processor cycles needed to decode and execute the instruction,\n   \n    m\n   \n   is the number of memory references needed, and\n   \n    k\n   \n   is the ratio between memory cycle time and processor cycle time. The five performance factors in the preceding equation (\n   \n    I_c, p, m, k, \\tau\n   \n   ) are influenced by four system attributes: the design of the instruction set (known as\n   *instruction set architecture*\n   ); compiler technology (how effective the compiler is in producing an efficient machine language program from a high-level language program); processor implementation; and cache and memory hierarchy. Table 2.1 is a matrix in which one dimension shows the five performance factors and the other dimension shows the four system attributes. An X in a cell indicates a system attribute that affects a performance factor.\n\n\n**Table 2.1**\n   Performance Factors and System Attributes\n\n\n\n | I_c | p | m | k | \\tau\nInstruction set architecture | X | X |  |  | \nCompiler technology | X | X | X |  | \nProcessor implementation |  | X |  |  | X\nCache and memory hierarchy |  |  |  | X | X\n\n\nA common measure of performance for a processor is the rate at which instructions are executed, expressed as millions of instructions per second (MIPS), referred to as the\n   **MIPS rate**\n   . We can express the MIPS rate in terms of the clock rate and\n   \n    CPI\n   \n   as follows:\n\n\n\\text{MIPS rate} = \\frac{I_c}{T \\times 10^6} = \\frac{f}{CPI \\times 10^6} \\quad (2.3)\n\n\n**EXAMPLE 2.2**\n   Consider the execution of a program that results in the execution of 2 million instructions on a 400-MHz processor. The program consists of four major types of instructions. The instruction mix and the\n   \n    CPI\n   \n   for each instruction type are given below, based on the result of a program trace experiment:\n\n\n\nInstruction Type | CPI | Instruction Mix (%)\nArithmetic and logic | 1 | 60\nLoad/store with cache hit | 2 | 18\nBranch | 4 | 12\nMemory reference with cache miss | 8 | 10\n\n\nThe average\n   \n    CPI\n   \n   when the program is executed on a uniprocessor with the above trace results is\n   \n    CPI = 0.6 + (2 \\times 0.18) + (4 \\times 0.12) + (8 \\times 0.1) = 2.24\n   \n   . The corresponding MIPS rate is\n   \n    (400 \\times 10^6)/(2.24 \\times 10^6) \\approx 178\n   \n   .\n\n\nAnother common performance measure deals only with floating-point instructions. These are common in many scientific and game applications. Floating-point performance is expressed as millions of floating-point operations per second (MFLOPS), defined as follows:\n\n\n\\text{MFLOPS rate} = \\frac{\\text{Number of executed floating-point operations in a program}}{\\text{Execution time} \\times 10^6}"
        },
        {
          "name": "Calculating the Mean",
          "content": "In evaluating some aspect of computer system performance, it is often the case that a single number, such as execution time or memory consumed, is used to characterize performance and to compare systems. Clearly, a single number can provide only a very simplified view of a system's capability. Nevertheless, and especially in the field of benchmarking, single numbers are typically used for performance comparison [SMIT88].\n\n\nAs is discussed in Section 2.6, the use of benchmarks to compare systems involves calculating the mean value of a set of data points related to execution time. It turns out that there are multiple alternative algorithms that can be used for calculating a mean value, and this has been the source of some controversy in\n\n\nthe benchmarking field. In this section, we define these alternative algorithms and comment on some of their properties. This prepares us for a discussion in the next section of mean calculation in benchmarking.\n\n\nThe three common formulas used for calculating a mean are arithmetic, geometric, and harmonic. Given a set of\n   \n    n\n   \n   real numbers\n   \n    (x_1, x_2, \\dots, x_n)\n   \n   , the three means are defined as follows:\n\n\n\n\n**Arithmetic mean**\n\n\nAM = \\frac{x_1 + \\dots + x_n}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\quad (2.4)\n\n\n\n\n**Geometric mean**\n\n\nGM = \\sqrt[n]{x_1 \\times \\dots \\times x_n} = \\left( \\prod_{i=1}^{n} x_i \\right)^{1/n} = e \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\ln(x_i) \\right) \\quad (2.5)\n\n\n\n\n**Harmonic mean**\n\n\nHM = \\frac{n}{\\left( \\frac{1}{x_1} \\right) + \\dots + \\left( \\frac{1}{x_n} \\right)} = \\frac{n}{\\sum_{i=1}^{n} \\left( \\frac{1}{x_i} \\right)} \\quad x_i > 0 \\quad (2.6)\n\n\nIt can be shown that the following inequality holds:\n\n\nAM \\le GM \\le HM\n\n\nThe values are equal only if\n   \n    x_1 = x_2 = \\dots = x_n\n   \n   .\n\n\nWe can get a useful insight into these alternative calculations by defining the functional mean. Let\n   \n    f(x)\n   \n   be a continuous monotonic function defined in the interval\n   \n    0 \\le y < \\infty\n   \n   . The functional mean with respect to the function\n   \n    f(x)\n   \n   for\n   \n    n\n   \n   positive real numbers\n   \n    (x_1, x_2, \\dots, x_n)\n   \n   is defined as\n\n\n\\mathbf{Functional\\ mean} \\quad FM = f^{-1} \\left( \\frac{f(x_1) + \\dots + f(x_n)}{n} \\right) = f^{-1} \\left( \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\right)\n\n\nwhere\n   \n    f^{-1}(x)\n   \n   is the inverse of\n   \n    f(x)\n   \n   . The mean values defined in Equations (2.1) through (2.3) are special cases of the functional mean, as follows:\n\n\n  * ■ AM is the FM with respect to\n    \n     f(x) = x\n  * ■ GM is the FM with respect to\n    \n     f(x) = \\ln x\n  * ■ HM is the FM with respect to\n    \n     f(x) = 1/x\n\n\n**EXAMPLE 2.3**\n   Figure 2.6 illustrates the three means applied to various data sets, each of which has eleven data points and a maximum data point value of 11. The median value is also included in the chart. Perhaps what stands out the most in this figure is that the HM has a tendency to produce a misleading result when the data is skewed to larger values or when there is a small-value outlier.\n\n\n\n\n![Figure 2.6: Comparison of Means on Various Data Sets. A horizontal bar chart showing MD, AM, GM, and HM for seven data sets (a) through (g). The x-axis ranges from 0 to 11. MD is always 11. AM varies from 11 to 1. GM varies from 11 to 1. HM varies from 11 to 1. The data sets are: (a) Constant (11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11); (b) Clustered around a central value (3, 5, 6, 6, 7, 7, 8, 9, 11); (c) Uniform distribution (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11); (d) Large-number bias (1, 4, 4, 7, 7, 9, 9, 10, 10, 11, 11); (e) Small-number bias (1, 1, 2, 2, 3, 3, 5, 5, 8, 8, 11); (f) Upper outlier (11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1); (g) Lower outlier (1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11).](images/image_0023.jpeg)\n\n\nSet | MD | AM | GM | HM\n(a) Constant | 11 | 11 | 11 | 11\n(b) Clustered | 11 | 7 | 6.5 | 6.2\n(c) Uniform | 11 | 6 | 5 | 3.8\n(d) Large-number bias | 11 | 9 | 6.5 | 4.5\n(e) Small-number bias | 11 | 3 | 3.5 | 2.5\n(f) Upper outlier | 11 | 1 | 1 | 1\n(g) Lower outlier | 11 | 10 | 9 | 6\n\n\nFigure 2.6: Comparison of Means on Various Data Sets. A horizontal bar chart showing MD, AM, GM, and HM for seven data sets (a) through (g). The x-axis ranges from 0 to 11. MD is always 11. AM varies from 11 to 1. GM varies from 11 to 1. HM varies from 11 to 1. The data sets are: (a) Constant (11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11); (b) Clustered around a central value (3, 5, 6, 6, 7, 7, 8, 9, 11); (c) Uniform distribution (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11); (d) Large-number bias (1, 4, 4, 7, 7, 9, 9, 10, 10, 11, 11); (e) Small-number bias (1, 1, 2, 2, 3, 3, 5, 5, 8, 8, 11); (f) Upper outlier (11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1); (g) Lower outlier (1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11).\n\n\n  * (a) Constant (11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11)\n    \n\n    (b) Clustered around a central value (3, 5, 6, 6, 7, 7, 8, 9, 11)\n    \n\n    (c) Uniform distribution (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\n    \n\n    (d) Large-number bias (1, 4, 4, 7, 7, 9, 9, 10, 10, 11, 11)\n    \n\n    (e) Small-number bias (1, 1, 2, 2, 3, 3, 5, 5, 8, 8, 11)\n    \n\n    (f) Upper outlier (11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n    \n\n    (g) Lower outlier (1, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11)\n\n\nMD = median\n   \n\n   AM = arithmetic mean\n   \n\n   GM = geometric mean\n   \n\n   HM = harmonic mean\n\n\n**Figure 2.6**\n   Comparison of Means on Various Data Sets (each set has a maximum data point value of 11)\n\n\nLet us now consider which of these means are appropriate for a given performance measure. As a preface to these remarks, it should be noted that a number of papers ([CITR06], [FLEM86], [GILA95], [JACO95], [JOHN04], [MASH04], [SMIT88]) and books ([HENN12], [HWAN93], [JAIN91], [LILJ00]) over the years have argued the pros and cons of the three means for performance analysis and come to conflicting conclusions. To simplify a complex controversy, we just note that the conclusions reached depend very much on the examples chosen and the way in which the objectives are stated.\n\n\n\n\n**Arithmetic Mean**\n\n\nAn AM is an appropriate measure if the sum of all the measurements is a meaningful and interesting value. The AM is a good candidate for comparing the execution time performance of several systems. For example, suppose we were interested in using a system for large-scale simulation studies and wanted to evaluate several alternative products. On each system we could run the simulation multiple times with different input values for each run, and then take the average execution time across all runs. The use of multiple runs with different inputs should ensure that the results are not heavily biased by some unusual feature of a given input set. The AM of all the runs is a good measure of the system's performance on simulations, and a good number to use for system comparison.\n\n\nThe AM used for a time-based variable (e.g., seconds), such as program execution time, has the important property that it is directly proportional to the total time. So, if the total time doubles, the mean value doubles.\n\n\n\n\n**Harmonic Mean**\n\n\nFor some situations, a system's execution rate may be viewed as a more useful measure of the value of the system. This could be either the instruction execution rate, measured in MIPS or MFLOPS, or a program execution rate, which measures the rate at which a given type of program can be executed. Consider how we wish the calculated mean to behave. It makes no sense to say that we would like the mean rate to be proportional to the total rate, where the total rate is defined as the sum of the individual rates. The sum of the rates would be a meaningless statistic. Rather, we would like the mean to be inversely proportional to the total execution time. For example, if the total time to execute all the benchmark programs in a suite of programs is twice as much for system C as for system D, we would want the mean value of the execution rate to be half as much for system C as for system D.\n\n\nLet us look at a basic example and first examine how the AM performs. Suppose we have a set of\n   \n    n\n   \n   benchmark programs and record the execution times of each program on a given system as\n   \n    t_1, t_2, \\dots, t_n\n   \n   . For simplicity, let us assume that each program executes the same number of operations\n   \n    Z\n   \n   ; we could weight the individual programs and calculate accordingly but this would not change the conclusion of our argument. The execution rate for each individual program is\n   \n    R_i = Z/t_i\n   \n   . We use the AM to calculate the average execution rate.\n\n\nAM = \\frac{1}{n} \\sum_{i=1}^{n} R_i = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{Z}{t_i} = \\frac{Z}{n} \\sum_{i=1}^{n} \\frac{1}{t_i}\n\n\nWe see that the AM execution rate is proportional to the sum of the inverse execution times, which is not the same as being inversely proportional to the sum of the execution times. Thus, the AM does not have the desired property.\n\n\nThe HM yields the following result.\n\n\nHM = \\frac{n}{\\sum_{i=1}^{n} \\left( \\frac{1}{R_i} \\right)} = \\frac{n}{\\sum_{i=1}^{n} \\left( \\frac{1}{Z/t_i} \\right)} = \\frac{nZ}{\\sum_{i=1}^{n} t_i}\n\n\nThe HM is inversely proportional to the total execution time, which is the desired property.\n\n\n**EXAMPLE 2.4**\n   A simple numerical example will illustrate the difference between the two means in calculating a mean value of the rates, shown in Table 2.2. The table compares the performance of three computers on the execution of two programs. For simplicity, we assume that the execution of each program results in the execution of\n   \n    10^8\n   \n   floating-point operations. The left half of the table shows the execution times for each computer running each program, the total execution time, and the AM of the execution times. Computer A executes in less total time than B, which executes in less total time than C, and this is reflected accurately in the AM.\n\n\nThe right half of the table provides a comparison in terms of rates, expressed in MFLOPS. The rate calculation is straightforward. For example, program 1 executes 100 million floating-point operations. Computer A takes 2 seconds to execute the program for a MFLOPS rate of\n   \n    100/2 = 50\n   \n   . Next, consider the AM of the rates. The greatest value is for computer A, which suggests that A is the fastest computer. In terms of total execution time, A has the minimum time, so it is the fastest computer of the three. But the AM of rates shows B as slower than C, whereas in fact B is faster than C. Looking at the HM values, we see that they correctly reflect the speed ordering of the computers. This confirms that the HM is preferred when calculating rates.\n\n\nThe reader may wonder why go through all this effort. If we want to compare execution times, we could simply compare the total execution times of the three systems. If we want to compare rates, we could simply take the inverse of the total execution time, as shown in the table. There are two reasons for doing the individual calculations rather than only looking at the aggregate numbers:\n\n\n**Table 2.2**\n   A Comparison of Arithmetic and Harmonic Means for Rates\n\n\n\n | Computer A time (secs) | Computer B time (secs) | Computer C time (secs) | Computer A rate (MFLOPS) | Computer B rate (MFLOPS) | Computer C rate (MFLOPS)\nProgram 1 (\n      \n       10^8\n      \n      FP ops) | 2.0 | 1.0 | 0.75 | 50 | 100 | 133.33\nProgram 2 (\n      \n       10^8\n      \n      FP ops) | 0.75 | 2.0 | 4.0 | 133.33 | 50 | 25\nTotal execution time | 2.75 | 3.0 | 4.75 | — | — | —\nArithmetic mean of times | 1.38 | 1.5 | 2.38 | — | — | —\nInverse of total execution time (1/sec) | 0.36 | 0.33 | 0.21 | — | — | —\nArithmetic mean of rates | — | — | — | 91.67 | 75.00 | 79.17\nHarmonic mean of rates | — | — | — | 72.72 | 66.67 | 42.11\n\n\n  * 1. A customer or researcher may be interested not only in the overall average performance but also performance against different types of benchmark programs, such as business applications, scientific modeling, multimedia applications, and systems programs. Thus, a breakdown by type of benchmark is needed as well as a total.\n  * 2. Usually, the different programs used for evaluation are weighted differently. In Table 2.2, it is assumed that the two test programs execute the same number of operations. If that is not the case, we may want to weight accordingly. Or different programs could be weighted differently to reflect importance or priority.\n\n\nLet us see what the result is if test programs are weighted proportional to the number of operations. Following the preceding notation, each program\n   \n    i\n   \n   executes\n   \n    Z_i\n   \n   instructions in a time\n   \n    t_i\n   \n   . Each rate is weighted by the instructions count. The weighted HM is therefore:\n\n\nWHM = \\frac{1}{\\sum_{i=1}^{n} \\left( \\left( \\frac{Z_i}{\\sum_{j=1}^{n} Z_j} \\right) \\left( \\frac{1}{R_i} \\right) \\right)} = \\frac{n}{\\sum_{i=1}^{n} \\left( \\left( \\frac{Z_i}{\\sum_{j=1}^{n} Z_j} \\right) \\left( \\frac{t_i}{Z_i} \\right) \\right)} = \\frac{\\sum_{j=1}^{n} Z_j}{\\sum_{i=1}^{n} t_i} \\quad (2.7)\n\n\nWe see that the weighted HM is the quotient of the sum of the operation count divided by the sum of the execution times.\n\n\n\n\n**Geometric Mean**\n\n\nLooking at the equations for the three types of means, it is easier to get an intuitive sense of the behavior of the AM and the HM than that of the GM. Several observations, from [FEIT15], may be helpful in this regard. First, we note that with respect to changes in values, the GM gives equal weight to all of the values in the data set. For example, suppose the set of data values to be averaged includes a few large values and more small values. Here, the AM is dominated by the large values. A change of 10% in the largest value will have a noticeable effect, while a change in the smallest value by the same factor will have a negligible effect. In contrast, a change in value by 10% of any of the data values results in the same change in the GM:\n   \n    \\sqrt[n]{1.1}\n   \n   .\n\n\n**EXAMPLE 2.5**\n   This point is illustrated by data set (e) in Figure 2.6. Here are the effects of increasing either the maximum or the minimum value in the data set by 10%:\n\n\n\n | Geometric Mean | Arithmetic Mean\nOriginal value | 3.37 | 4.45\nIncrease max value from 11 to 12.1 (+10%) | 3.40 (+ 0.87%) | 4.55 (+ 2.24%)\nIncrease min value from 1 to 1.1 (+10%) | 3.40 (+ 0.87%) | 4.46 (+ 0.20%)\n\n\nA second observation is that for the GM of a ratio, the GM of the ratios equals the ratio of the GMs:\n\n\nGM = \\left( \\prod_{i=1}^{n} \\frac{Z_i}{t_i} \\right)^{1/n} = \\frac{\\left( \\prod_{i=1}^{n} Z_i \\right)^{1/n}}{\\left( \\prod_{i=1}^{n} t_i \\right)^{1/n}} \\quad (2.8)\n\n\nCompare this with Equation 2.4.\n\n\nFor use with execution times, as opposed to rates, one drawback of the GM is that it may be non-monotonic relative to the more intuitive AM. In other words there may be cases where the AM of one data set is larger than that of another set, but the GM is smaller.\n\n\n**EXAMPLE 2.6**\n   In Figure 2.6, the AM for data set d is larger than the AM for data set c, but the opposite is true for the GM.\n\n\n\n | Data set c | Data set d\nArithmetic mean | 7.00 | 7.55\nGeometric mean | 6.68 | 6.42\n\n\nOne property of the GM that has made it appealing for benchmark analysis is that it provides consistent results when measuring the relative performance of machines. This is in fact what benchmarks are primarily used for: to compare one machine with another in terms of performance metrics. The results, as we have seen, are expressed in terms of values that are normalized to a reference machine.\n\n\n**EXAMPLE 2.7**\n   A simple example will illustrate the way in which the GM exhibits consistency for normalized results. In Table 2.3, we use the same performance results as were used in Table 2.2. In Table 2.3a, all results are normalized to Computer A, and the means are calculated on the normalized values. Based on total execution time, A is faster than B, which is faster than C. Both the AMs and GMs of the normalized times reflect this. In Table 2.3b, the systems are now normalized to B. Again the GMs correctly reflect the relative speeds of the three computers, but now the AM produces a different ordering.\n\n\nSadly, consistency does not always produce correct results. In Table 2.4, some of the execution times are altered. Once again, the AM reports conflicting results for the two normalizations. The GM reports consistent results, but the result is that B is faster than A and C, which are equal.\n\n\nIt is examples like this that have fueled the “benchmark means wars” in the citations listed earlier. It is safe to say that no single number can provide all the information that one needs for comparing performance across systems. However,\n\n\n**Table 2.3**\n\n\n**(a) Results normalized to Computer A**\n\n\n\n | Computer A time | Computer B time | Computer C time\nProgram 1 | 2.0 (1.0) | 1.0 (0.5) | 0.75 (0.38)\nProgram 2 | 0.75 (1.0) | 2.0 (2.67) | 4.0 (5.33)\nTotal execution time | 2.75 | 3.0 | 4.75\nArithmetic mean of normalized times | 1.00 | 1.58 | 2.85\nGeometric mean of normalized times | 1.00 | 1.15 | 1.41\n\n\n\n\n**(b) Results normalized to Computer B**\n\n\n\n | Computer A time | Computer B time | Computer C time\nProgram 1 | 2.0 (2.0) | 1.0 (1.0) | 0.75 (0.75)\nProgram 2 | 0.75 (0.38) | 2.0 (1.0) | 4.0 (2.0)\nTotal execution time | 2.75 | 3.0 | 4.75\nArithmetic mean of normalized times | 1.19 | 1.00 | 1.38\nGeometric mean of normalized times | 0.87 | 1.00 | 1.22\n\n\n**Table 2.4**\n\n\n**(a) Results normalized to Computer A**\n\n\n\n | Computer A time | Computer B time | Computer C time\nProgram 1 | 2.0 (1.0) | 1.0 (0.5) | 0.20 (0.1)\nProgram 2 | 0.4 (1.0) | 2.0 (5.0) | 4.0 (10.0)\nTotal execution time | 2.4 | 3.00 | 4.2\nArithmetic mean of normalized times | 1.00 | 2.75 | 5.05\nGeometric mean of normalized times | 1.00 | 1.58 | 1.00\n\n\n\n\n**(b) Results normalized to Computer B**\n\n\n\n | Computer A time | Computer B time | Computer C time\nProgram 1 | 2.0 (2.0) | 1.0 (1.0) | 0.20 (0.2)\nProgram 2 | 0.4 (0.2) | 2.0 (1.0) | 4.0 (2.0)\nTotal execution time | 2.4 | 3.00 | 4.2\nArithmetic mean of normalized times | 1.10 | 1.00 | 1.10\nGeometric mean of normalized times | 0.63 | 1.00 | 0.63\n\n\ndespite the conflicting opinions in the literature, SPEC has chosen to use the GM, for several reasons:\n\n\n  * 1. As mentioned, the GM gives consistent results regardless of which system is used as a reference. Because benchmarking is primarily a comparison analysis, this is an important feature.\n  * 2. As documented in [MCMA93], and confirmed in subsequent analyses by SPEC analysts [MASH04], the GM is less biased by outliers than the HM or AM.\n  * 3. [MASH04] demonstrates that distributions of performance ratios are better modeled by lognormal distributions than by normal ones, because of the generally skewed distribution of the normalized numbers. This is confirmed in [CITR06]. And, as shown in Equation (2.5), the GM can be described as the back-transformed average of a lognormal distribution."
        },
        {
          "name": "Benchmarks and Spec",
          "content": "**Benchmark Principles**\n\n\nMeasures such as MIPS and MFLOPS have proven inadequate to evaluating the performance of processors. Because of differences in instruction sets, the instruction execution rate is not a valid means of comparing the performance of different architectures.\n\n\n**EXAMPLE 2.8**\n   Consider this high-level language statement:\n\n\nA = B + C   /* assume all quantities in main memory */\nWith a traditional instruction set architecture, referred to as a complex instruction set computer (CISC), this instruction can be compiled into one processor instruction:\n\n\nadd   mem(B), mem(C), mem(A)\nOn a typical RISC machine, the compilation would look something like this:\n\n\nload  mem(B), reg(1);\nload  mem(C), reg(2);\nadd   reg(1), reg(2), reg(3);\nstore reg(3), mem(A)\nBecause of the nature of the RISC architecture (discussed in Chapter 15), both machines may execute the original high-level language instruction in about the same time. If this example is representative of the two machines, then if the CISC machine is rated at 1 MIPS, the RISC machine would be rated at 4 MIPS. But both do the same amount of high-level language work in the same amount of time.\n\n\nAnother consideration is that the performance of a given processor on a given program may not be useful in determining how that processor will perform on a very different type of application. Accordingly, beginning in the late 1980s and early 1990s, industry and academic interest shifted to measuring the performance of\n\n\nsystems using a set of benchmark programs. The same set of programs can be run on different machines and the execution times compared. Benchmarks provide guidance to customers trying to decide which system to buy, and can be useful to vendors and designers in determining how to design systems to meet benchmark goals.\n\n\n[WEIC90] lists the following as desirable characteristics of a benchmark program:\n\n\n  * 1. It is written in a high-level language, making it portable across different machines.\n  * 2. It is representative of a particular kind of programming domain or paradigm, such as systems programming, numerical programming, or commercial programming.\n  * 3. It can be measured easily.\n  * 4. It has wide distribution.\n\n\n\n\n**SPEC Benchmarks**\n\n\nThe common need in industry and academic and research communities for generally accepted computer performance measurements has led to the development of standardized benchmark suites. A benchmark suite is a collection of programs, defined in a high-level language, that together attempt to provide a representative test of a computer in a particular application or system programming area. The best known such collection of benchmark suites is defined and maintained by the Standard Performance Evaluation Corporation (SPEC), an industry consortium. This organization defines several benchmark suites aimed at evaluating computer systems. SPEC performance measurements are widely used for comparison and research purposes.\n\n\nThe best known of the SPEC benchmark suites is SPEC CPU2006. This is the industry standard suite for processor-intensive applications. That is, SPEC CPU2006 is appropriate for measuring performance for applications that spend most of their time doing computation rather than I/O.\n\n\nOther SPEC suites include the following:\n\n\n  * ■\n    **SPECviewperf**\n    : Standard for measuring 3D graphics performance based on professional applications.\n  * ■\n    **SPECwpc**\n    : benchmark to measure all key aspects of workstation performance based on diverse professional applications, including media and entertainment, product development, life sciences, financial services, and energy.\n  * ■\n    **SPECjvm2008**\n    : Intended to evaluate performance of the combined hardware and software aspects of the Java Virtual Machine (JVM) client platform.\n  * ■\n    **SPECjbb2013 (Java Business Benchmark)**\n    : A benchmark for evaluating server-side Java-based electronic commerce applications.\n  * ■\n    **SPECsfs2008**\n    : Designed to evaluate the speed and request-handling capabilities of file servers.\n  * ■\n    **SPECvirt_sc2013**\n    : Performance evaluation of datacenter servers used in virtualized server consolidation. Measures the end-to-end performance of all system components including the hardware, virtualization platform, and the virtualized guest operating system and application software. The benchmark supports hardware virtualization, operating system virtualization, and hardware partitioning schemes.\n\n\nThe CPU2006 suite is based on existing applications that have already been ported to a wide variety of platforms by SPEC industry members. In order to make the benchmark results reliable and realistic, the CPU2006 benchmarks are drawn from real-life applications, rather than using artificial loop programs or synthetic benchmarks. The suite consists of 12 integer benchmarks written in C and C++, and 17 floating-point benchmarks written in C, C++, and Fortran (Tables 2.5 and 2.6). The suite contains over 3 million lines of code. This is the fifth generation of\n\n\n**Table 2.5**\n   SPEC CPU2006 Integer Benchmarks\n\n\n\nBenchmark | Reference time (hours) | Instr count (billion) | Language | Application Area | Brief Description\n400.perlbench | 2.71 | 2378 | C | Programming Language | PERL programming language interpreter, applied to a set of three programs.\n401.bzip2 | 2.68 | 2472 | C | Compression | General-purpose data compression with most work done in memory, rather than doing I/O.\n403.gcc | 2.24 | 1064 | C | C Compiler | Based on gcc Version 3.2, generates code for Opteron.\n429.mcf | 2.53 | 327 | C | Combinatorial Optimization | Vehicle scheduling algorithm.\n445.gobmk | 2.91 | 1603 | C | Artificial Intelligence | Plays the game of Go, a simply described but deeply complex game.\n456.hmmer | 2.59 | 3363 | C | Search Gene Sequence | Protein sequence analysis using profile-hidden Markov models.\n458.sjeng | 3.36 | 2383 | C | Artificial Intelligence | A highly ranked chess program that also plays several chess variants.\n462.libquantum | 5.76 | 3555 | C | Physics / Quantum Computing | Simulates a quantum computer, running Shor's polynomial-time factorization algorithm.\n464.h264ref | 6.15 | 3731 | C | Video Compression | H.264/AVC (Advanced Video Coding) video compression.\n471.omnetpp | 1.74 | 687 | C++ | Discrete Event Simulation | Uses the OMNet++ discrete event simulator to model a large Ethernet campus network.\n473.astar | 1.95 | 1200 | C++ | Path-finding Algorithms | Pathfinding library for 2D maps.\n483.xalancbmk | 1.92 | 1184 | C++ | XML Processing | A modified version of Xalan-C++, which transforms XML documents to other document types.\n\n\n**Table 2.6**\n\nBenchmark | Reference time (hours) | Instr count (billion) | Language | Application Area | Brief Description\n410.bwaves | 3.78 | 1176 | Fortran | Fluid Dynamics | Computes 3D transonic transient laminar viscous flow.\n416.gamess | 5.44 | 5189 | Fortran | Quantum Chemistry | Quantum chemical computations.\n433.milc | 2.55 | 937 | C | Physics / Quantum Chromodynamics | Simulates behavior of quarks and gluons.\n434.zeusmp | 2.53 | 1566 | Fortran | Physics / CFD | Computational fluid dynamics simulation of astrophysical phenomena.\n435.gromacs | 1.98 | 1958 | C, Fortran | Biochemistry / Molecular Dynamics | Simulates Newtonian equations of motion for hundreds to millions of particles.\n436.cactusADM | 3.32 | 1376 | C, Fortran | Physics / General Relativity | Solves the Einstein evolution equations.\n437.leslie3d | 2.61 | 1273 | Fortran | Fluid Dynamics | Models fuel injection flows.\n444.namd | 2.23 | 2483 | C++ | Biology / Molecular Dynamics | Simulates large biomolecular systems.\n447.dealII | 3.18 | 2323 | C++ | Finite Element Analysis | Program library targeted at adaptive finite elements and error estimation.\n450.soplex | 2.32 | 703 | C++ | Linear Programming, Optimization | Test cases include railroad planning and military airlift models.\n453.povray | 1.48 | 940 | C++ | Image Ray-Tracing | 3D image rendering.\n454.calcuix | 2.29 | 3,04 | C, Fortran | Structural Mechanics | Finite element code for linear and nonlinear 3D structural applications.\n459.GemsFDTD | 2.95 | 1320 | Fortran | Computational Electromagnetics | Solves the Maxwell equations in 3D.\n465.tonto | 2.73 | 2392 | Fortran | Quantum Chemistry | Quantum chemistry package, adapted for crystallographic tasks.\n470.lbm | 3.82 | 1500 | C | Fluid Dynamics | Simulates incompressible fluids in 3D.\n481.wrf | 3.10 | 1684 | C, Fortran | Weather | Weather forecasting model.\n482.sphinx3 | 5.41 | 2472 | C | Speech Recognition | Speech recognition software.\n\n\nprocessor-intensive suites from SPEC, replacing SPEC CPU2000, SPEC CPU95, SPEC CPU92, and SPEC CPU89 [HENN07].\n\n\nTo better understand published results of a system using CPU2006, we define the following terms used in the SPEC documentation:\n\n\n  * ■\n    **Benchmark:**\n    A program written in a high-level language that can be compiled and executed on any computer that implements the compiler.\n  * ■\n    **System under test:**\n    This is the system to be evaluated.\n  * ■\n    **Reference machine:**\n    This is a system used by SPEC to establish a baseline performance for all benchmarks. Each benchmark is run and measured on this machine to establish a reference time for that benchmark. A system under test is evaluated by running the CPU2006 benchmarks and comparing the results for running the same programs on the reference machine.\n  * ■\n    **Base metric:**\n    These are required for all reported results and have strict guidelines for compilation. In essence, the standard compiler with more or less default settings should be used on each system under test to achieve comparable results.\n  * ■\n    **Peak metric:**\n    This enables users to attempt to optimize system performance by optimizing the compiler output. For example, different compiler options may be used on each benchmark, and feedback-directed optimization is allowed.\n  * ■\n    **Speed metric:**\n    This is simply a measurement of the time it takes to execute a compiled benchmark. The speed metric is used for comparing the ability of a computer to complete single tasks.\n  * ■\n    **Rate metric:**\n    This is a measurement of how many tasks a computer can accomplish in a certain amount of time; this is called a\n    **throughput**\n    , capacity, or rate measure. The rate metric allows the system under test to execute simultaneous tasks to take advantage of multiple processors.\n\n\nSPEC uses a historical Sun system, the “Ultra Enterprise 2,” which was introduced in 1997, as the reference machine. The reference machine uses a 296-MHz UltraSPARC II processor. It takes about 12 days to do a rule-conforming run of the base metrics for CINT2006 and CFP2006 on the CPU2006 reference machine. Tables 2.5 and 2.6 show the amount of time to run each benchmark using the reference machine. The tables also show the dynamic instruction counts on the reference machine, as reported in [PHAN07]. These values are the actual number of instructions executed during the run of each program.\n\n\nWe now consider the specific calculations that are done to assess a system. We consider the integer benchmarks; the same procedures are used to create a floating-point benchmark value. For the integer benchmarks, there are 12 programs in the test suite. Calculation is a three-step process (Figure 2.7):\n\n\n  * 1. The first step in evaluating a system under test is to compile and run each program on the system three times. For each program, the runtime is measured and the median value is selected. The reason to use three runs and take the median value is to account for variations in execution time that are not intrinsic to the program, such as disk access time variations, and OS kernel execution variations from one run to another.\n\n\n\n\n![SPEC Evaluation Flowchart](images/image_0024.jpeg)\n\n\ngraph TD\n    Start([Start]) --> GetNext[Get next program]\n    GetNext --> RunThree[Run program three times]\n    RunThree --> SelectMedian[Select median value]\n    SelectMedian --> Ratio[Ratio(prog) = Tref(prog)/TSUT(prog)]\n    Ratio --> MorePrograms{More programs?}\n    MorePrograms -- Yes --> GetNext\n    MorePrograms -- No --> ComputeMean[Compute geometric mean of all ratios]\n    ComputeMean --> End([End])\n  \nThe flowchart illustrates the SPEC evaluation process. It begins with a 'Start' node, followed by 'Get next program', 'Run program three times', and 'Select median value'. The next step is to calculate the ratio:\n    \n     \\text{Ratio}(\\text{prog}) = T_{\\text{ref}}(\\text{prog})/T_{\\text{SUT}}(\\text{prog})\n    \n    . A decision is then made: 'More programs?'. If 'Yes', the process loops back to 'Get next program'. If 'No', the process proceeds to 'Compute geometric mean of all ratios' and finally to 'End'.\n\n\nSPEC Evaluation Flowchart\n\n\n**Figure 2.7**\n   SPEC Evaluation Flowchart\n\n\n  * 2. Next, each of the 12 results is normalized by calculating the runtime ratio of the reference run time to the system run time. The ratio is calculated as follows:\n\n\nr_i = \\frac{T_{ref_i}}{T_{sut_i}} \\quad (2.9)\n\n\nwhere\n   \n    T_{ref_i}\n   \n   is the execution time of benchmark program\n   \n    i\n   \n   on the reference system and\n   \n    T_{sut_i}\n   \n   is the execution time of benchmark program\n   \n    i\n   \n   on the system under test. Thus, ratios are higher for faster machines.\n\n\n  * 3. Finally, the geometric mean of the 12 runtime ratios is calculated to yield the overall metric:\n\n\nr_G = \\left( \\prod_{i=1}^{12} r_i \\right)^{1/12}\n\n\nFor the integer benchmarks, four separate metrics can be calculated:\n\n\n  * ■\n    **SPECint2006:**\n    The geometric mean of 12 normalized ratios when the benchmarks are compiled with peak tuning.\n  * ■\n    **SPECint_base2006:**\n    The geometric mean of 12 normalized ratios when the benchmarks are compiled with base tuning.\n  * ■\n    **SPECint_rate2006:**\n    The geometric mean of 12 normalized throughput ratios when the benchmarks are compiled with peak tuning.\n  * ■\n    **SPECint_rate_base2006:**\n    The geometric mean of 12 normalized throughput ratios when the benchmarks are compiled with base tuning.\n\n\n**EXAMPLE 2.9**\n   The results for the Sun Blade 1000 are shown in Table 2.7a. One of the SPEC CPU2006 integer benchmark is 464.h264ref. This is a reference implementation of H.264/AVC (Advanced Video Coding), the latest state-of-the-art video compression standard. The Sun Blade 1000 executes this program in a median time of 5,259 seconds. The reference implementation requires 22,130 seconds. The ratio is calculated as:\n   \n    22,130/5,259 = 4.21\n   \n   . The speed metric is calculated by taking the twelfth root of the product of the ratios:\n\n\n(3.18 \\times 2.96 \\times 2.98 \\times 3.91 \\times 3.17 \\times 3.61 \\times 3.51 \\times 2.01 \\times 4.21 \\times 2.43 \\times 2.75 \\times 3.42)^{1/12} = 3.12\n\n\nThe rate metrics take into account a system with multiple processors. To test a machine, a number of copies\n   \n    N\n   \n   is selected—usually this is equal to the number of processors or the number of simultaneous threads of execution on the test system. Each individual test program's rate is determined by taking the median of three runs. Each run consists of\n   \n    N\n   \n   copies of the program running simultaneously on the test system. The execution time is the time it takes for all the copies to finish (i.e., the time from when the first copy starts until the last copy finishes). The rate metric for that program is calculated by the following formula:\n\n\nrate_i = N \\times \\frac{Tref_i}{Tsut_i}\n\n\nThe rate score for the system under test is determined from a geometric mean of rates for each program in the test suite.\n\n\n**EXAMPLE 2.10**\n   The results for the Sun Blade X6250 are shown in Table 2.7b. This system has two processor chips, with two cores per chip, for a total of four cores. To get the rate metric, each benchmark program is executed simultaneously on all four cores, with the execution time being the time from the start of all four copies to the end of the slowest run. The speed ratio is calculated as before, and the rate value is simply four times the speed ratio. The final rate metric is found by taking the geometric mean of the rate values:\n\n\n(78.63 \\times 62.97 \\times 60.87 \\times 77.29 \\times 65.87 \\times 83.68 \\times 76.70 \\times 134.98 \\times 106.65 \\times 40.39 \\times 48.41 \\times 65.40)^{1/12} = 71.59\n\n\n**Table 2.7**\n   Some SPEC CINT2006 Results\n\n\n**(a) Sun Blade 1000**\n\n\n\nBenchmark | Execution time (secs) | Execution time (secs) | Execution time (secs) | Reference time (secs) | Ratio\n400.perlbench | 3077 | 3076 | 3080 | 9770 | 3.18\n401.bzip2 | 3260 | 3263 | 3260 | 9650 | 2.96\n403.gcc | 2711 | 2701 | 2702 | 8050 | 2.98\n429.mcf | 2356 | 2331 | 2301 | 9120 | 3.91\n445.gobmk | 3319 | 3310 | 3308 | 10,490 | 3.17\n456.hmmer | 2586 | 2587 | 2601 | 9330 | 3.61\n\n\n(Continued)\n\n\n**Table 2.7**\n\n\n**(a) Sun Blade 1000**\n\n\n\nBenchmark | Execution time (secs) | Execution time (secs) | Execution time (secs) | Reference time (secs) | Ratio\n458.sjeng | 3452 | 3449 | 3449 | 12,100 | 3.51\n462.libquantum | 10,318 | 10,319 | 10,273 | 20,720 | 2.01\n464.h264ref | 5246 | 5290 | 5259 | 22,130 | 4.21\n471.omnetpp | 2565 | 2572 | 2582 | 6250 | 2.43\n473.astar | 2522 | 2554 | 2565 | 7020 | 2.75\n483.xalancbmk | 2014 | 2018 | 2018 | 6900 | 3.42\n\n\n\n\n**(b) Sun Blade X6250**\n\n\n\nBenchmark | Execution time (secs) | Execution time (secs) | Execution time (secs) | Reference time (secs) | Ratio | Rate\n400.perlbmch | 497 | 497 | 497 | 9770 | 19.66 | 78.63\n401.bzip2 | 613 | 614 | 613 | 9650 | 15.74 | 62.97\n403.gcc | 529 | 529 | 529 | 8050 | 15.22 | 60.87\n429.mcf | 472 | 472 | 473 | 9120 | 19.32 | 77.29\n445.gobmk | 637 | 637 | 637 | 10,490 | 16.47 | 65.87\n456.hmmer | 446 | 446 | 446 | 9330 | 20.92 | 83.68\n458.sjeng | 631 | 632 | 630 | 12,100 | 19.18 | 76.70\n462.libquantum | 614 | 614 | 614 | 20,720 | 33.75 | 134.98\n464.h264ref | 830 | 830 | 830 | 22,130 | 26.66 | 106.65\n471.omnetpp | 619 | 620 | 619 | 6250 | 10.10 | 40.39\n473.astar | 580 | 580 | 580 | 7020 | 12.10 | 48.41\n483.xalancbmk | 422 | 422 | 422 | 6900 | 16.35 | 65.40"
        }
      ]
    },
    {
      "name": "A Top-Level View of Computer Function and Interconnection",
      "sections": [
        {
          "name": "Computer Components",
          "content": "As discussed in Chapter 1, virtually all contemporary computer designs are based on concepts developed by John von Neumann at the Institute for Advanced Studies, Princeton. Such a design is referred to as the\n   *von Neumann architecture*\n   and is based on three key concepts:\n\n\n  * ■ Data and instructions are stored in a single read-write memory.\n  * ■ The contents of this memory are addressable by location, without regard to the type of data contained there.\n\n\n  * ■ Execution occurs in a sequential fashion (unless explicitly modified) from one instruction to the next.\n\n\nThe reasoning behind these concepts was discussed in Chapter 2 but is worth summarizing here. There is a small set of basic logic components that can be combined in various ways to store binary data and perform arithmetic and logical operations on that data. If there is a particular computation to be performed, a configuration of logic components designed specifically for that computation could be constructed. We can think of the process of connecting the various components in the desired configuration as a form of programming. The resulting “program” is in the form of hardware and is termed a\n   *hardwired program*\n   .\n\n\nNow consider this alternative. Suppose we construct a general-purpose configuration of arithmetic and logic functions. This set of hardware will perform various functions on data depending on control signals applied to the hardware. In the original case of customized hardware, the system accepts data and produces results (Figure 3.1a). With general-purpose hardware, the system accepts data and control signals and produces results. Thus, instead of rewiring the hardware for each new program, the programmer merely needs to supply a new set of control signals.\n\n\nHow shall control signals be supplied? The answer is simple but subtle. The entire program is actually a sequence of steps. At each step, some arithmetic or logical operation is performed on some data. For each step, a new set of control signals is needed. Let us provide a unique code for each possible set of control signals,\n\n\n\n\n![Figure 3.1: Hardware and Software Approaches. (a) Programming in hardware: A single block labeled 'Sequence of arithmetic and logic functions' receives 'Data' and produces 'Results'. (b) Programming in software: An 'Instruction interpreter' block receives 'Instruction codes' and sends 'Control signals' to a 'General-purpose arithmetic and logic functions' block, which also receives 'Data' and produces 'Results'.](images/image_0025.jpeg)\n\n\nFigure 3.1 illustrates two approaches to programming a computer system:\n\n\n**(a) Programming in hardware:**\n    A single block labeled \"Sequence of arithmetic and logic functions\" receives input labeled \"Data\" and produces output labeled \"Results\".\n\n\n**(b) Programming in software:**\n    This approach involves two main components. An \"Instruction interpreter\" block receives input labeled \"Instruction codes\". This block sends \"Control signals\" to a second block labeled \"General-purpose arithmetic and logic functions\". The \"General-purpose arithmetic and logic functions\" block also receives input labeled \"Data\" and produces output labeled \"Results\".\n\n\nFigure 3.1: Hardware and Software Approaches. (a) Programming in hardware: A single block labeled 'Sequence of arithmetic and logic functions' receives 'Data' and produces 'Results'. (b) Programming in software: An 'Instruction interpreter' block receives 'Instruction codes' and sends 'Control signals' to a 'General-purpose arithmetic and logic functions' block, which also receives 'Data' and produces 'Results'.\n\n\n**Figure 3.1**\n   Hardware and Software Approaches\n\n\nand let us add to the general-purpose hardware a segment that can accept a code and generate control signals (Figure 3.1b).\n\n\nProgramming is now much easier. Instead of rewiring the hardware for each new program, all we need to do is provide a new sequence of codes. Each code is, in effect, an instruction, and part of the hardware interprets each instruction and generates control signals. To distinguish this new method of programming, a sequence of codes or instructions is called\n   *software*\n   .\n\n\nFigure 3.1b indicates two major components of the system: an instruction interpreter and a module of general-purpose arithmetic and logic functions. These two constitute the CPU. Several other components are needed to yield a functioning computer. Data and instructions must be put into the system. For this we need some sort of input module. This module contains basic components for accepting data and instructions in some form and converting them into an internal form of signals usable by the system. A means of reporting results is needed, and this is in the form of an output module. Taken together, these are referred to as\n   *I/O components*\n   .\n\n\nOne more component is needed. An input device will bring instructions and data in sequentially. But a program is not invariably executed sequentially; it may jump around (e.g., the IAS jump instruction). Similarly, operations on data may require access to more than just one element at a time in a predetermined sequence. Thus, there must be a place to temporarily store both instructions and data. That module is called\n   *memory*\n   , or\n   *main memory*\n   , to distinguish it from external storage or peripheral devices. Von Neumann pointed out that the same memory could be used to store both instructions and data.\n\n\nFigure 3.2 illustrates these top-level components and suggests the interactions among them. The CPU exchanges data with memory. For this purpose, it typically makes use of two internal (to the CPU) registers: a\n   **memory address register (MAR)**\n   , which specifies the address in memory for the next read or write, and a\n   **memory buffer register (MBR)**\n   , which contains the data to be written into memory or receives the data read from memory. Similarly, an I/O address register (I/OAR) specifies a particular I/O device. An I/O buffer register (I/OBR) is used for the exchange of data between an I/O module and the CPU.\n\n\nA memory module consists of a set of locations, defined by sequentially numbered addresses. Each location contains a binary number that can be interpreted as either an instruction or data. An I/O module transfers data from external devices to CPU and memory, and vice versa. It contains internal buffers for temporarily holding these data until they can be sent on.\n\n\nHaving looked briefly at these major components, we now turn to an overview of how these components function together to execute programs."
        },
        {
          "name": "Computer Function",
          "content": "The basic function performed by a computer is execution of a program, which consists of a set of instructions stored in memory. The processor does the actual work by executing instructions specified in the program. This section provides an overview of\n\n\n\n\n![Figure 3.2: Computer Components: Top-Level View. The diagram shows three main components: CPU, Main memory, and I/O Module, interconnected by a System bus. The CPU contains registers (PC, MAR, IR, MBR, I/O AR, I/O BR) and an Execution unit. Main memory is organized into blocks labeled 0, 1, 2, ..., n-2, n-1, containing instructions and data. The I/O Module contains Buffers. The System bus connects the CPU and Main memory, and the I/O Module is also connected to the bus.](images/image_0026.jpeg)\n\n\nPC = Program counter\n    \n\n    IR = Instruction register\n    \n\n    MAR = Memory address register\n    \n\n    MBR = Memory buffer register\n    \n\n    I/O AR = Input/output address register\n    \n\n    I/O BR = Input/output buffer register\n\n\nFigure 3.2: Computer Components: Top-Level View. The diagram shows three main components: CPU, Main memory, and I/O Module, interconnected by a System bus. The CPU contains registers (PC, MAR, IR, MBR, I/O AR, I/O BR) and an Execution unit. Main memory is organized into blocks labeled 0, 1, 2, ..., n-2, n-1, containing instructions and data. The I/O Module contains Buffers. The System bus connects the CPU and Main memory, and the I/O Module is also connected to the bus.\n\n\n**Figure 3.2**\n   Computer Components: Top-Level View\n\n\nthe key elements of program execution. In its simplest form, instruction processing consists of two steps: The processor reads (\n   *fetches*\n   ) instructions from memory one at a time and executes each instruction. Program execution consists of repeating the process of instruction fetch and instruction execution. The instruction execution may involve several operations and depends on the nature of the instruction (see, for example, the lower portion of Figure 2.4).\n\n\nThe processing required for a single instruction is called an\n   **instruction cycle**\n   . Using the simplified two-step description given previously, the instruction cycle is depicted in Figure 3.3. The two steps are referred to as the\n   **fetch cycle**\n   and the\n   **execute cycle**\n   . Program execution halts only if the machine is turned off, some sort of unrecoverable error occurs, or a program instruction that halts the computer is encountered.\n\n\n\n\n**Instruction Fetch and Execute**\n\n\nAt the beginning of each instruction cycle, the processor fetches an instruction from memory. In a typical processor, a register called the program counter (PC) holds the address of the instruction to be fetched next. Unless told otherwise, the processor\n\n\n\n\n![Flowchart of the Basic Instruction Cycle. It starts with a rounded rectangle labeled 'START'. An arrow points to a rectangle labeled 'Fetch next instruction'. Above this arrow is the label 'Fetch cycle'. An arrow points from 'Fetch next instruction' to a rectangle labeled 'Execute instruction'. Above this arrow is the label 'Execute cycle'. An arrow points from 'Execute instruction' to a rounded rectangle labeled 'HALT'.](images/image_0027.jpeg)\n\n\ngraph LR\n    START([START]) --> Fetch[Fetch next instruction]\n    Fetch -- \"Fetch cycle\" --> Execute[Execute instruction]\n    Execute -- \"Execute cycle\" --> HALT([HALT])\n  \nFlowchart of the Basic Instruction Cycle. It starts with a rounded rectangle labeled 'START'. An arrow points to a rectangle labeled 'Fetch next instruction'. Above this arrow is the label 'Fetch cycle'. An arrow points from 'Fetch next instruction' to a rectangle labeled 'Execute instruction'. Above this arrow is the label 'Execute cycle'. An arrow points from 'Execute instruction' to a rounded rectangle labeled 'HALT'.\n\n\n**Figure 3.3**\n   Basic Instruction Cycle\n\n\nalways increments the PC after each instruction fetch so that it will fetch the next instruction in sequence (i.e., the instruction located at the next higher memory address). So, for example, consider a computer in which each instruction occupies one 16-bit word of memory. Assume that the program counter is set to memory location 300, where the location address refers to a 16-bit word. The processor will next fetch the instruction at location 300. On succeeding instruction cycles, it will fetch instructions from locations 301, 302, 303, and so on. This sequence may be altered, as explained presently.\n\n\nThe fetched instruction is loaded into a register in the processor known as the instruction register (IR). The instruction contains bits that specify the action the processor is to take. The processor interprets the instruction and performs the required action. In general, these actions fall into four categories:\n\n\n  * ■\n    **Processor-memory:**\n    Data may be transferred from processor to memory or from memory to processor.\n  * ■\n    **Processor-I/O:**\n    Data may be transferred to or from a peripheral device by transferring between the processor and an I/O module.\n  * ■\n    **Data processing:**\n    The processor may perform some arithmetic or logic operation on data.\n  * ■\n    **Control:**\n    An instruction may specify that the sequence of execution be altered. For example, the processor may fetch an instruction from location 149, which specifies that the next instruction be from location 182. The processor will remember this fact by setting the program counter to 182. Thus, on the next fetch cycle, the instruction will be fetched from location 182 rather than 150.\n\n\nAn instruction's execution may involve a combination of these actions.\n\n\nConsider a simple example using a hypothetical machine that includes the characteristics listed in Figure 3.4. The processor contains a single data register, called an accumulator (AC). Both instructions and data are 16 bits long. Thus, it is convenient to organize memory using 16-bit words. The instruction format provides 4 bits for the opcode, so that there can be as many as\n   \n    2^4 = 16\n   \n   different opcodes, and up to\n   \n    2^{12} = 4096\n   \n   (4K) words of memory can be directly addressed.\n\n\nFigure 3.5 illustrates a partial program execution, showing the relevant portions of memory and processor registers.\n   \n    1\n   \n   The program fragment shown adds the contents of the memory word at address 940 to the contents of the memory word at\n\n\n1\n   \n   Hexadecimal notation is used, in which each digit represents 4 bits. This is the most convenient notation for representing the contents of memory and registers when the word length is a multiple of 4. See Chapter 9 for a basic refresher on number systems (decimal, binary, hexadecimal).\n\n\n\n0 | 3 4 | 15\nOpcode | Address\n\n\n(a) Instruction format\n\n\n\n0 | 1 | 15\n | Magnitude\n\n\n(b) Integer format\n\n\nProgram counter (PC) = Address of instruction\n   \n\n   Instruction register (IR) = Instruction being executed\n   \n\n   Accumulator (AC) = Temporary storage\n\n\n(c) Internal CPU registers\n\n\n0001 = Load AC from memory\n   \n\n   0010 = Store AC to memory\n   \n\n   0101 = Add to AC from memory\n\n\n(d) Partial list of opcodes\n\n\n**Figure 3.4**\n\n\n![](images/image_0028.jpeg)\n\n\nMemory | CPU registers | Memory | CPU registers\n300 1 9 4 0\n       \n       301 5 9 4 1\n       \n       302 2 9 4 1\n       \n       •\n       \n       940 0 0 0 3\n       \n       941 0 0 0 2 | 3 0 0 PC\n       \n       1 9 4 0 AC\n       \n       IR | 300 1 9 4 0\n       \n       301 5 9 4 1\n       \n       302 2 9 4 1\n       \n       •\n       \n       940 0 0 0 3\n       \n       941 0 0 0 2 | 3 0 1 PC\n       \n       0 0 0 3 AC\n       \n       1 9 4 0 IR\nStep 1 |  | Step 2 | \n300 1 9 4 0\n       \n       301 5 9 4 1\n       \n       302 2 9 4 1\n       \n       •\n       \n       940 0 0 0 3\n       \n       941 0 0 0 2 | 3 0 1 PC\n       \n       0 0 0 3 AC\n       \n       5 9 4 1 IR | 300 1 9 4 0\n       \n       301 5 9 4 1\n       \n       302 2 9 4 1\n       \n       •\n       \n       940 0 0 0 3\n       \n       941 0 0 0 2 | 3 0 2 PC\n       \n       0 0 0 5 AC\n       \n       5 9 4 1 IR\nStep 3 |  | Step 4 | 3 + 2 = 5\n300 1 9 4 0\n       \n       301 5 9 4 1\n       \n       302 2 9 4 1\n       \n       •\n       \n       940 0 0 0 3\n       \n       941 0 0 0 2 | 3 0 2 PC\n       \n       0 0 0 5 AC\n       \n       2 9 4 1 IR | 300 1 9 4 0\n       \n       301 5 9 4 1\n       \n       302 2 9 4 1\n       \n       •\n       \n       940 0 0 0 3\n       \n       941 0 0 0 5 | 3 0 3 PC\n       \n       0 0 0 5 AC\n       \n       2 9 4 1 IR\nStep 5 |  | Step 6 |\n\n\n**Figure 3.5**\naddress 941 and stores the result in the latter location. Three instructions, which can be described as three fetch and three execute cycles, are required:\n\n\n  * 1. The PC contains 300, the address of the first instruction. This instruction (the value 1940 in hexadecimal) is loaded into the instruction register IR, and the PC is incremented. Note that this process involves the use of a memory address register and a memory buffer register. For simplicity, these intermediate registers are ignored.\n  * 2. The first 4 bits (first hexadecimal digit) in the IR indicate that the AC is to be loaded. The remaining 12 bits (three hexadecimal digits) specify the address (940) from which data are to be loaded.\n  * 3. The next instruction (5941) is fetched from location 301, and the PC is incremented.\n  * 4. The old contents of the AC and the contents of location 941 are added, and the result is stored in the AC.\n  * 5. The next instruction (2941) is fetched from location 302, and the PC is incremented.\n  * 6. The contents of the AC are stored in location 941.\n\n\nIn this example, three instruction cycles, each consisting of a fetch cycle and an execute cycle, are needed to add the contents of location 940 to the contents of 941. With a more complex set of instructions, fewer cycles would be needed. Some older processors, for example, included instructions that contain more than one memory address. Thus, the execution cycle for a particular instruction on such processors could involve more than one reference to memory. Also, instead of memory references, an instruction may specify an I/O operation.\n\n\nFor example, the PDP-11 processor includes an instruction, expressed symbolically as ADD B,A, that stores the sum of the contents of memory locations B and A into memory location A. A single instruction cycle with the following steps occurs:\n\n\n  * ■ Fetch the ADD instruction.\n  * ■ Read the contents of memory location A into the processor.\n  * ■ Read the contents of memory location B into the processor. In order that the contents of A are not lost, the processor must have at least two registers for storing memory values, rather than a single accumulator.\n  * ■ Add the two values.\n  * ■ Write the result from the processor to memory location A.\n\n\nThus, the execution cycle for a particular instruction may involve more than one reference to memory. Also, instead of memory references, an instruction may specify an I/O operation. With these additional considerations in mind, Figure 3.6 provides a more detailed look at the basic instruction cycle of Figure 3.3. The figure is in the form of a state diagram. For any given instruction cycle, some states may be null and others may be visited more than once. The states can be described as follows:\n\n\n  * ■\n    **Instruction address calculation (iac):**\n    Determine the address of the next instruction to be executed. Usually, this involves adding a fixed number to\n\n\n\n\n![Instruction Cycle State Diagram](images/image_0029.jpeg)\n\n\nThe diagram illustrates the Instruction Cycle State Diagram, showing a sequence of states represented by circles. The states are arranged in two rows. The top row contains 'Instruction fetch', 'Operand fetch', and 'Operand store'. The bottom row contains 'Instruction address calculation', 'Instruction operation decoding', 'Operand address calculation', 'Data operation', and 'Operand address calculation'. Arrows indicate the flow between states. A long feedback arrow at the bottom loops from 'Operand store' back to 'Instruction address calculation'. Labels on the arrows include 'Multiple operands' (from Operand fetch to Data operation), 'Multiple results' (from Operand store to Operand address calculation), 'Instruction complete, fetch next instruction' (from Operand store to Instruction address calculation), and 'Return for string or vector data' (from Operand store to Operand address calculation).\n\n\nInstruction Cycle State Diagram\n\n\n**Figure 3.6**\n   Instruction Cycle State Diagram\n\n\nthe address of the previous instruction. For example, if each instruction is 16 bits long and memory is organized into 16-bit words, then add 1 to the previous address. If, instead, memory is organized as individually addressable 8-bit bytes, then add 2 to the previous address.\n\n\n  * ■\n    **Instruction fetch (if):**\n    Read instruction from its memory location into the processor.\n  * ■\n    **Instruction operation decoding (iod):**\n    Analyze instruction to determine type of operation to be performed and operand(s) to be used.\n  * ■\n    **Operand address calculation (oac):**\n    If the operation involves reference to an operand in memory or available via I/O, then determine the address of the operand.\n  * ■\n    **Operand fetch (of):**\n    Fetch the operand from memory or read it in from I/O.\n  * ■\n    **Data operation (do):**\n    Perform the operation indicated in the instruction.\n  * ■\n    **Operand store (os):**\n    Write the result into memory or out to I/O.\n\n\nStates in the upper part of Figure 3.6 involve an exchange between the processor and either memory or an I/O module. States in the lower part of the diagram involve only internal processor operations. The oac state appears twice, because an instruction may involve a read, a write, or both. However, the action performed during that state is fundamentally the same in both cases, and so only a single state identifier is needed.\n\n\nAlso note that the diagram allows for multiple operands and multiple results, because some instructions on some machines require this. For example, the PDP-11 instruction ADD A,B results in the following sequence of states: iac, if, iod, oac, of, oac, of, do, oac, os.\n\n\nFinally, on some machines, a single instruction can specify an operation to be performed on a vector (one-dimensional array) of numbers or a string (one-dimensional\n\n\narray) of characters. As Figure 3.6 indicates, this would involve repetitive operand fetch and/or store operations.\n\n\n\n\n**Interrupts**\n\n\nVirtually all computers provide a mechanism by which other modules (I/O, memory) may\n   **interrupt**\n   the normal processing of the processor. Table 3.1 lists the most common classes of interrupts. The specific nature of these interrupts is examined later in this book, especially in Chapters 7 and 14. However, we need to introduce the concept now to understand more clearly the nature of the instruction cycle and the implications of interrupts on the interconnection structure. The reader need not be concerned at this stage about the details of the generation and processing of interrupts, but only focus on the communication between modules that results from interrupts.\n\n\nInterrupts are provided primarily as a way to improve processing efficiency. For example, most external devices are much slower than the processor. Suppose that the processor is transferring data to a printer using the instruction cycle scheme of Figure 3.3. After each write operation, the processor must pause and remain idle until the printer catches up. The length of this pause may be on the order of many hundreds or even thousands of instruction cycles that do not involve memory. Clearly, this is a very wasteful use of the processor.\n\n\nFigure 3.7a illustrates this state of affairs. The user program performs a series of WRITE calls interleaved with processing. Code segments 1, 2, and 3 refer to sequences of instructions that do not involve I/O. The WRITE calls are to an I/O program that is a system utility and that will perform the actual I/O operation. The I/O program consists of three sections:\n\n\n  * ■ A sequence of instructions, labeled 4 in the figure, to prepare for the actual I/O operation. This may include copying the data to be output into a special buffer and preparing the parameters for a device command.\n  * ■ The actual I/O command. Without the use of interrupts, once this command is issued, the program must wait for the I/O device to perform the requested function (or periodically poll the device). The program might wait by simply repeatedly performing a test operation to determine if the I/O operation is done.\n  * ■ A sequence of instructions, labeled 5 in the figure, to complete the operation. This may include setting a flag indicating the success or failure of the operation.\n\n\n**Table 3.1**\n   Classes of Interrupts\n\n\n\nProgram | Generated by some condition that occurs as a result of an instruction execution, such as arithmetic overflow, division by zero, attempt to execute an illegal machine instruction, or reference outside a user's allowed memory space.\nTimer | Generated by a timer within the processor. This allows the operating system to perform certain functions on a regular basis.\nI/O | Generated by an I/O controller, to signal normal completion of an operation, request service from the processor, or to signal a variety of error conditions.\nHardware Failure | Generated by a failure such as power failure or memory parity error.\n\n\n\n\n![Figure 3.7: Program Flow of Control without and with Interrupts. The diagram consists of three panels: (a) No interrupts, (b) Interrupts; short I/O wait, and (c) Interrupts; long I/O wait. Each panel shows the flow of control between a User Program (steps 1, 2, 3) and an I/O Program (steps 4, 5). Solid arrows represent normal flow, while dashed arrows represent interrupt handling. Panel (b) includes an Interrupt Handler (step 5) and shows an 'X' mark on the User Program's steps 2b and 3b, indicating interrupt occurrences.](images/image_0030.jpeg)\n\n\n(a) No interrupts\n\n\n(b) Interrupts; short I/O wait\n\n\n(c) Interrupts; long I/O wait\n\n\nX = interrupt occurs during course of execution of user program\n\n\nFigure 3.7: Program Flow of Control without and with Interrupts. The diagram consists of three panels: (a) No interrupts, (b) Interrupts; short I/O wait, and (c) Interrupts; long I/O wait. Each panel shows the flow of control between a User Program (steps 1, 2, 3) and an I/O Program (steps 4, 5). Solid arrows represent normal flow, while dashed arrows represent interrupt handling. Panel (b) includes an Interrupt Handler (step 5) and shows an 'X' mark on the User Program's steps 2b and 3b, indicating interrupt occurrences.\n\n\n**Figure 3.7**\n   Program Flow of Control without and with Interrupts\n\n\nBecause the I/O operation may take a relatively long time to complete, the I/O program is hung up waiting for the operation to complete; hence, the user program is stopped at the point of the WRITE call for some considerable period of time.\n\n\n**INTERRUPTS AND THE INSTRUCTION CYCLE**\n   With interrupts, the processor can be engaged in executing other instructions while an I/O operation is in progress. Consider the flow of control in Figure 3.7b. As before, the user program reaches a point at which it makes a system call in the form of a WRITE call. The I/O program that is invoked in this case consists only of the preparation code and the actual I/O command. After these few instructions have been executed, control returns to the user program. Meanwhile, the external device is busy accepting data from computer memory and printing it. This I/O operation is conducted concurrently with the execution of instructions in the user program.\n\n\nWhen the external device becomes ready to be serviced—that is, when it is ready to accept more data from the processor—the I/O module for that external device sends an\n   *interrupt request*\n   signal to the processor. The processor responds by suspending operation of the current program, branching off to a program to service that particular I/O device, known as an\n   **interrupt handler**\n   , and resuming the original execution after the device is serviced. The points at which such interrupts occur are indicated by an asterisk in Figure 3.7b.\n\n\nLet us try to clarify what is happening in Figure 3.7. We have a user program that contains two WRITE commands. There is a segment of code at the beginning, then one WRITE command, then a second segment of code, then a second WRITE command, then a third and final segment of code. The WRITE command invokes the I/O program provided by the OS. Similarly, the I/O program consists of a segment of code, followed by an I/O command, followed by another segment of code. The I/O command invokes a hardware I/O operation.\n\n\n\n\n![](images/image_0031.jpeg)\n\n\n**USER PROGRAM**\n\n\n\n⟨statement⟩ | } | Code segment 1 |  | I/O PROGRAM | } | Code segment 4\n⟨statement⟩ | } | : | ⟨statement⟩ | } | :\n⟨statement⟩ | } | : | ⟨statement⟩ | } | :\nWRITE |  |  | I/O command |  | \n⟨statement⟩ | } | Code segment 2 |  | ⟨statement⟩ | } | Code segment 5\n⟨statement⟩ | } | : | ⟨statement⟩ | } | :\n⟨statement⟩ | } | : | ⟨statement⟩ | } | :\nWRITE |  |  | ⟨statement⟩ |  | \n⟨statement⟩ | } | Code segment 3 |  | ⟨statement⟩ | } | \n⟨statement⟩ | } | : | ⟨statement⟩ | } | :\n⟨statement⟩ | } | : | ⟨statement⟩ | } | :\n\n\n\n\n![Diagram illustrating the Transfer of Control via Interrupts. A vertical stack of boxes represents a User program with instructions labeled 1, 2, ..., i, i+1, ..., M. An arrow labeled 'Interrupt occurs here' points to the boundary between instructions i and i+1. A line from this point leads to an Interrupt handler box, which contains instructions labeled ... . A return arrow points from the bottom of the Interrupt handler back to the boundary between instructions i and i+1 in the User program.](images/image_0032.jpeg)\n\n\nDiagram illustrating the Transfer of Control via Interrupts. A vertical stack of boxes represents a User program with instructions labeled 1, 2, ..., i, i+1, ..., M. An arrow labeled 'Interrupt occurs here' points to the boundary between instructions i and i+1. A line from this point leads to an Interrupt handler box, which contains instructions labeled ... . A return arrow points from the bottom of the Interrupt handler back to the boundary between instructions i and i+1 in the User program.\n\n\n**Figure 3.8**\n   Transfer of Control via Interrupts\n\n\nFrom the point of view of the user program, an interrupt is just that: an interruption of the normal sequence of execution. When the interrupt processing is completed, execution resumes (Figure 3.8). Thus, the user program does not have to contain any special code to accommodate interrupts; the processor and the operating system are responsible for suspending the user program and then resuming it at the same point.\n\n\nTo accommodate interrupts, an\n   *interrupt cycle*\n   is added to the instruction cycle, as shown in Figure 3.9. In the interrupt cycle, the processor checks to see if any interrupts have occurred, indicated by the presence of an interrupt signal. If no interrupts are pending, the processor proceeds to the fetch cycle and fetches the next instruction of the current program. If an interrupt is pending, the processor does the following:\n\n\n  * ■ It suspends execution of the current program being executed and saves its context. This means saving the address of the next instruction to be executed\n\n\n\n\n![Flowchart of the Instruction Cycle with Interrupts. The cycle consists of three main stages: Fetch cycle, Execute cycle, and Interrupt cycle. It starts with a START oval. The Fetch cycle contains a 'Fetch next instruction' box. The Execute cycle contains an 'Execute instruction' box. The Interrupt cycle contains a 'Check for interrupt; process interrupt' box. Transitions are labeled: 'Interrupts disabled' from Fetch to Execute, 'Interrupts enabled' from Execute to Interrupt, and a return path from Interrupt to Fetch. A HALT oval is at the bottom, reachable from the Execute cycle.](images/image_0033.jpeg)\n\n\nFlowchart of the Instruction Cycle with Interrupts. The cycle consists of three main stages: Fetch cycle, Execute cycle, and Interrupt cycle. It starts with a START oval. The Fetch cycle contains a 'Fetch next instruction' box. The Execute cycle contains an 'Execute instruction' box. The Interrupt cycle contains a 'Check for interrupt; process interrupt' box. Transitions are labeled: 'Interrupts disabled' from Fetch to Execute, 'Interrupts enabled' from Execute to Interrupt, and a return path from Interrupt to Fetch. A HALT oval is at the bottom, reachable from the Execute cycle.\n\n\n**Figure 3.9**\n   Instruction Cycle with Interrupts\n\n\n(current contents of the program counter) and any other data relevant to the processor's current activity.\n\n\n  * ■ It sets the program counter to the starting address of an\n    *interrupt handler*\n    routine.\n\n\nThe processor now proceeds to the fetch cycle and fetches the first instruction in the interrupt handler program, which will service the interrupt. The interrupt handler program is generally part of the operating system. Typically, this program determines the nature of the interrupt and performs whatever actions are needed. In the example we have been using, the handler determines which I/O module generated the interrupt and may branch to a program that will write more data out to that I/O module. When the interrupt handler routine is completed, the processor can resume execution of the user program at the point of interruption.\n\n\nIt is clear that there is some overhead involved in this process. Extra instructions must be executed (in the interrupt handler) to determine the nature of the interrupt and to decide on the appropriate action. Nevertheless, because of the relatively large amount of time that would be wasted by simply waiting on an I/O operation, the processor can be employed much more efficiently with the use of interrupts.\n\n\nTo appreciate the gain in efficiency, consider Figure 3.10, which is a timing diagram based on the flow of control in Figures 3.7a and 3.7b. In this figure, user program code segments are shaded green, and I/O program code segments are\n\n\n\n\n![Timing diagram comparing program execution with and without interrupts for short I/O waits.](images/image_0034.jpeg)\n\n\nThe diagram illustrates the execution of a program with and without interrupts, showing the impact of short I/O waits.\n\n\n**(a) Without interrupts:**\n    The timeline shows the processor executing user program segments (green) and I/O operations (black). The sequence is: 1 (user), 4 (user), I/O operation (processor waits), 5 (user), 2 (user), 4 (user), I/O operation (processor waits), 5 (user), 3 (user).\n\n\n**(b) With interrupts:**\n    The timeline shows the processor executing user program segments (green) and I/O operations (black). The sequence is: 1 (user), 4 (user), I/O operation concurrent with processor executing (2a), 5 (user), 2 (user), 4 (user), I/O operation concurrent with processor executing (3a), 5 (user), 3 (user). This demonstrates that the processor can continue executing user code while an I/O operation is in progress, improving efficiency.\n\n\nTiming diagram comparing program execution with and without interrupts for short I/O waits.\n\n\n**Figure 3.10**\n   Program Timing: Short I/O Wait\n\n\nshaded gray. Figure 3.10a shows the case in which interrupts are not used. The processor must wait while an I/O operation is performed.\n\n\nFigures 3.7b and 3.10b assume that the time required for the I/O operation is relatively short: less than the time to complete the execution of instructions between write operations in the user program. In this case, the segment of code labeled code segment 2 is interrupted. A portion of the code (2a) executes (while the I/O operation is performed) and then the interrupt occurs (upon the completion of the I/O operation). After the interrupt is serviced, execution resumes with the remainder of code segment 2 (2b).\n\n\nThe more typical case, especially for a slow device such as a printer, is that the I/O operation will take much more time than executing a sequence of user instructions. Figure 3.7c indicates this state of affairs. In this case, the user program reaches the second WRITE call before the I/O operation spawned by the first call is complete. The result is that the user program is hung up at that point. When the preceding I/O operation is completed, this new WRITE call may be processed, and a new I/O operation may be started. Figure 3.11 shows the timing for this situation with\n\n\n\n\n![Figure 3.11: Program Timing: Long I/O Wait. The diagram compares two execution timelines. Timeline (a) 'Without interrupts' shows a processor executing code segments 1, 4, 5, 2, 4, 5, 3 sequentially, with a long I/O operation (black bar) between segments 2 and 4, causing the processor to wait. Timeline (b) 'With interrupts' shows the processor executing segments 1, 4, 2, 5, 4, 3, 5, where the I/O operations are concurrent with processor execution, allowing the processor to continue working while the I/O completes.](images/image_0035.jpeg)\n\n\nFigure 3.11 consists of two vertical timelines labeled (a) and (b). A vertical arrow on the left labeled 'Time' points downwards.\n\n\n**Timeline (a) Without interrupts:**\n    The sequence of code segments is 1 (light green), 4 (light gray), 5 (light gray), 2 (light green), 4 (light gray), 5 (light gray), 3 (light green). A long black horizontal bar between segments 2 and 4 is labeled 'I/O operation; processor waits'. A double-headed vertical arrow points to this bar.\n\n\n**Timeline (b) With interrupts:**\n    The sequence of code segments is 1 (light green), 4 (light gray), 2 (light green), 5 (light gray), 4 (light gray), 3 (light green), 5 (light gray). Two black horizontal bars are present: one between segments 2 and 4, and another between segments 4 and 5. Both are labeled 'I/O operation concurrent with processor executing; then processor waits'. Double-headed vertical arrows point to these bars.\n\n\nFigure 3.11: Program Timing: Long I/O Wait. The diagram compares two execution timelines. Timeline (a) 'Without interrupts' shows a processor executing code segments 1, 4, 5, 2, 4, 5, 3 sequentially, with a long I/O operation (black bar) between segments 2 and 4, causing the processor to wait. Timeline (b) 'With interrupts' shows the processor executing segments 1, 4, 2, 5, 4, 3, 5, where the I/O operations are concurrent with processor execution, allowing the processor to continue working while the I/O completes.\n\n\n**Figure 3.11**\n   Program Timing: Long I/O Wait\n\n\nand without the use of interrupts. We can see that there is still a gain in efficiency because part of the time during which the I/O operation is under way overlaps with the execution of user instructions.\n\n\nFigure 3.12 shows a revised instruction cycle state diagram that includes interrupt cycle processing.\n\n\n***MULTIPLE INTERRUPTS***\n   The discussion so far has focused only on the occurrence of a single interrupt. Suppose, however, that multiple interrupts can occur. For example, a program may be receiving data from a communications line and printing results. The printer will generate an interrupt every time it completes a print operation. The communication line controller will generate an interrupt every time a unit of data arrives. The unit could either be a single character or a block, depending on the nature of the communications discipline. In any case, it is possible for a communications interrupt to occur while a printer interrupt is being processed.\n\n\nTwo approaches can be taken to dealing with multiple interrupts. The first is to disable interrupts while an interrupt is being processed. A\n   **disabled interrupt**\n   simply means that the processor can and will ignore that interrupt request signal. If an interrupt occurs during this time, it generally remains pending and will be checked by the processor after the processor has enabled interrupts. Thus, when a user program is executing and an interrupt occurs, interrupts are disabled immediately. After the interrupt handler routine completes, interrupts are enabled before resuming the user program, and the processor checks to see if additional interrupts have occurred. This approach is nice and simple, as interrupts are handled in strict sequential order (Figure 3.13a).\n\n\nThe drawback to the preceding approach is that it does not take into account relative priority or time-critical needs. For example, when input arrives from the communications line, it may need to be absorbed rapidly to make room for more input. If the first batch of input has not been processed before the second batch arrives, data may be lost.\n\n\nA second approach is to define priorities for interrupts and to allow an interrupt of higher priority to cause a lower-priority interrupt handler to be itself interrupted (Figure 3.13b). As an example of this second approach, consider a system with three I/O devices: a printer, a disk, and a communications line, with increasing priorities of 2, 4, and 5, respectively. Figure 3.14 illustrates a possible sequence. A user program begins at\n   \n    t = 0\n   \n   . At\n   \n    t = 10\n   \n   , a printer interrupt occurs; user information is placed on the system stack and execution continues at the printer\n   **interrupt service routine (ISR)**\n   . While this routine is still executing, at\n   \n    t = 15\n   \n   , a communications interrupt occurs. Because the communications line has higher priority than the printer, the interrupt is honored. The printer ISR is interrupted, its state is pushed onto the stack, and execution continues at the communications ISR. While this routine is executing, a disk interrupt occurs (\n   \n    t = 20\n   \n   ). Because this interrupt is of lower priority, it is simply held, and the communications ISR runs to completion.\n\n\nWhen the communications ISR is complete (\n   \n    t = 25\n   \n   ), the previous processor state is restored, which is the execution of the printer ISR. However, before even a single instruction in that routine can be executed, the processor honors the higher-priority disk interrupt and control transfers to the disk ISR. Only when that\n\n\n\n\n![Diagram of the Pentium Pro pipeline showing the flow of instructions and operands through various stages.](images/image_0036.jpeg)\n\n\nThe diagram illustrates the Pentium Pro pipeline, showing the flow of instructions and operands through various stages. The stages are represented by circles, and the flow is indicated by arrows.\n\n\n  * **Instruction fetch**\n      (top left) is connected to\n      **Instruction address calculation**\n      (bottom left) by a downward arrow.\n  * **Instruction address calculation**\n      is connected to\n      **Instruction operation decoding**\n      (middle left) by a rightward arrow.\n  * **Instruction operation decoding**\n      is connected to\n      **Operand address calculation**\n      (middle center) by a rightward arrow.\n  * **Operand address calculation**\n      is connected to\n      **Data operation**\n      (middle right) by a rightward arrow.\n  * **Data operation**\n      is connected to\n      **Operand address calculation**\n      (far right) by a rightward arrow.\n  * **Operand address calculation**\n      (far right) is connected to\n      **Interrupt check**\n      (far right) by a rightward arrow.\n  * **Interrupt check**\n      is connected to\n      **Interrupt**\n      (far right) by a rightward arrow.\n  * **Interrupt**\n      is connected to\n      **Instruction address calculation**\n      (bottom left) by a downward arrow, labeled \"Instruction complete, fetch next instruction\".\n  * **Operand fetch**\n      (top center) is connected to\n      **Operand address calculation**\n      (middle center) by a downward arrow, labeled \"Multiple operands\".\n  * **Operand fetch**\n      is connected to\n      **Data operation**\n      (middle right) by a downward arrow.\n  * **Operand store**\n      (top right) is connected to\n      **Operand address calculation**\n      (far right) by a downward arrow, labeled \"Multiple results\".\n  * **Operand store**\n      is connected to\n      **Interrupt check**\n      (far right) by a downward arrow.\n  * **Data operation**\n      is connected to\n      **Operand address calculation**\n      (far right) by a downward arrow, labeled \"Return for string or vector data\".\n  * **Interrupt check**\n      is connected to\n      **Operand address calculation**\n      (bottom left) by a downward arrow, labeled \"No interrupt\".\n\n\nDiagram of the Pentium Pro pipeline showing the flow of instructions and operands through various stages.\n\n\n**Figure 3.12**\n    Instruction Cycle State Diagram, with Interrupts\n\n\n\n\n![Diagram (a) Sequential interrupt processing](images/image_0037.jpeg)\n\n\nDiagram (a) illustrates sequential interrupt processing. It shows three vertical bars representing execution contexts: a grey bar for the 'User program' and two light blue bars for 'Interrupt handler X' and 'Interrupt handler Y'. Each bar contains a vertical dashed line representing the execution flow. Arrows indicate the transfer of control: an arrow from the user program to handler X, an arrow from handler X to handler Y, and an arrow from handler Y back to the user program. This sequence shows that handler Y can only begin execution after handler X has completed.\n\n\nDiagram (a) Sequential interrupt processing\n\n\n(a) Sequential interrupt processing\n\n\n\n\n![Diagram (b) Nested interrupt processing](images/image_0038.jpeg)\n\n\nDiagram (b) illustrates nested interrupt processing. It shows the same three vertical bars as in (a). Arrows indicate the transfer of control: an arrow from the user program to handler X, an arrow from handler X to handler Y, and an arrow from handler Y back to the user program. In this model, handler Y can begin execution while handler X is still active, representing a nested interrupt scenario.\n\n\nDiagram (b) Nested interrupt processing\n\n\n(b) Nested interrupt processing\n\n\n**Figure 3.13**\n   Transfer of Control with Multiple Interrupts\n\n\n\n\n![Figure 3.14: Example Time Sequence of Multiple Interrupts. The diagram shows four vertical bars representing different execution contexts: 'User program' (dark gray), 'Printer interrupt service routine' (light gray), 'Communication interrupt service routine' (light gray), and 'Disk interrupt service routine' (light gray). The 'User program' bar has a label '-t = 0' at the top. Arrows indicate the flow of control over time (t): an arrow from the user program to the printer ISR at t = 10; an arrow from the printer ISR to the communication ISR at t = 15; an arrow from the communication ISR to the disk ISR at t = 25; an arrow from the disk ISR back to the printer ISR at t = 35; and an arrow from the printer ISR back to the user program at t = 40.](images/image_0039.jpeg)\n\n\nFigure 3.14: Example Time Sequence of Multiple Interrupts. The diagram shows four vertical bars representing different execution contexts: 'User program' (dark gray), 'Printer interrupt service routine' (light gray), 'Communication interrupt service routine' (light gray), and 'Disk interrupt service routine' (light gray). The 'User program' bar has a label '-t = 0' at the top. Arrows indicate the flow of control over time (t): an arrow from the user program to the printer ISR at t = 10; an arrow from the printer ISR to the communication ISR at t = 15; an arrow from the communication ISR to the disk ISR at t = 25; an arrow from the disk ISR back to the printer ISR at t = 35; and an arrow from the printer ISR back to the user program at t = 40.\n\n\n**Figure 3.14**\n   Example Time Sequence of Multiple Interrupts\n\n\nroutine is complete (\n   \n    t = 35\n   \n   ) is the printer ISR resumed. When that routine completes (\n   \n    t = 40\n   \n   ), control finally returns to the user program.\n\n\n\n\n**I/O Function**\n\n\nThus far, we have discussed the operation of the computer as controlled by the processor, and we have looked primarily at the interaction of processor and memory. The discussion has only alluded to the role of the I/O component. This role is discussed in detail in Chapter 7, but a brief summary is in order here.\n\n\nAn I/O module (e.g., a disk controller) can exchange data directly with the processor. Just as the processor can initiate a read or write with memory, designating the address of a specific location, the processor can also read data from or write data to an I/O module. In this latter case, the processor identifies a specific device that is controlled by a particular I/O module. Thus, an instruction sequence similar in form to that of Figure 3.5 could occur, with I/O instructions rather than memory-referencing instructions.\n\n\nIn some cases, it is desirable to allow I/O exchanges to occur directly with memory. In such a case, the processor grants to an I/O module the authority to read from or write to memory, so that the I/O-memory transfer can occur without tying up the processor. During such a transfer, the I/O module issues read or write commands to memory, relieving the processor of responsibility for the exchange. This operation is known as direct memory access (DMA) and is examined in Chapter 7."
        },
        {
          "name": "Interconnection Structures",
          "content": "A computer consists of a set of components or modules of three basic types (processor, memory, I/O) that communicate with each other. In effect, a computer is a network of basic modules. Thus, there must be paths for connecting the modules.\n\n\nThe collection of paths connecting the various modules is called the\n   *interconnection structure*\n   . The design of this structure will depend on the exchanges that must be made among modules.\n\n\nFigure 3.15 suggests the types of exchanges that are needed by indicating the major forms of input and output for each module type\n   \n    2\n   \n   :\n\n\n  * ■\n    **Memory:**\n    Typically, a memory module will consist of\n    \n     N\n    \n    words of equal length. Each word is assigned a unique numerical address (\n    \n     0, 1, \\dots, N-1\n    \n    ). A word of data can be read from or written into the memory. The nature of the operation\n\n\n\n\n![Diagram of Computer Modules showing Memory, I/O module, and CPU with their respective input and output signals.](images/image_0040.jpeg)\n\n\nThe diagram illustrates the interconnection structure for three computer modules: Memory, I/O module, and CPU.\n\n\n  * **Memory Module:**\n     Contains\n     \n      N\n     \n     words, indexed from 0 to\n     \n      N-1\n     \n     . It has four input lines (Read, Write, Address, Data) and one output line (Data).\n  * **I/O Module:**\n     Contains\n     \n      M\n     \n     ports. It has four input lines (Read, Write, Address, Data) and four output lines (Internal data, External data, Interrupt signals, Data).\n  * **CPU Module:**\n     Has three input lines (Instructions, Data, Interrupt signals) and three output lines (Address, Control signals, Data).\n\n\nDiagram of Computer Modules showing Memory, I/O module, and CPU with their respective input and output signals.\n\n\n**Figure 3.15**\n   Computer Modules\n\n\n2\n   \n   The wide arrows represent multiple signal lines carrying multiple bits of information in parallel. Each narrow arrow represents a single signal line.\n\n\nis indicated by read and write control signals. The location for the operation is specified by an address.\n\n\n  * ■\n    **I/O module:**\n    From an internal (to the computer system) point of view, I/O is functionally similar to memory. There are two operations; read and write. Further, an I/O module may control more than one external device. We can refer to each of the interfaces to an external device as a\n    *port*\n    and give each a unique address (e.g., 0, 1, . . . ,\n    \n     M-1\n    \n    ). In addition, there are external data paths for the input and output of data with an external device. Finally, an I/O module may be able to send interrupt signals to the processor.\n  * ■\n    **Processor:**\n    The processor reads in instructions and data, writes out data after processing, and uses control signals to control the overall operation of the system. It also receives interrupt signals.\n\n\nThe preceding list defines the data to be exchanged. The interconnection structure must support the following types of transfers:\n\n\n  * ■\n    **Memory to processor:**\n    The processor reads an instruction or a unit of data from memory.\n  * ■\n    **Processor to memory:**\n    The processor writes a unit of data to memory.\n  * ■\n    **I/O to processor:**\n    The processor reads data from an I/O device via an I/O module.\n  * ■\n    **Processor to I/O:**\n    The processor sends data to the I/O device.\n  * ■\n    **I/O to or from memory:**\n    For these two cases, an I/O module is allowed to exchange data directly with memory, without going through the processor, using direct memory access.\n\n\nOver the years, a number of interconnection structures have been tried. By far the most common are (1) the\n   **bus**\n   and various multiple-bus structures, and (2) point-to-point interconnection structures with packetized data transfer. We devote the remainder of this chapter for a discussion of these structures."
        },
        {
          "name": "Bus Interconnection",
          "content": "The bus was the dominant means of computer system component interconnection for decades. For general-purpose computers, it has gradually given way to various point-to-point interconnection structures, which now dominate computer system design. However, bus structures are still commonly used for embedded systems, particularly microcontrollers. In this section, we give a brief overview of bus structure. Appendix C provides more detail.\n\n\nA bus is a communication pathway connecting two or more devices. A key characteristic of a bus is that it is a shared transmission medium. Multiple devices connect to the bus, and a signal transmitted by any one device is available for reception by all other devices attached to the bus. If two devices transmit during the same time period, their signals will overlap and become garbled. Thus, only one device at a time can successfully transmit.\n\n\nTypically, a bus consists of multiple communication pathways, or lines. Each line is capable of transmitting signals representing binary 1 and binary 0. Over time, a sequence of binary digits can be transmitted across a single line. Taken together, several lines of a bus can be used to transmit binary digits simultaneously (in parallel). For example, an 8-bit unit of data can be transmitted over eight bus lines.\n\n\nComputer systems contain a number of different buses that provide pathways between components at various levels of the computer system hierarchy. A bus that connects major computer components (processor, memory, I/O) is called a\n   **system bus**\n   . The most common computer interconnection structures are based on the use of one or more system buses.\n\n\nA system bus consists, typically, of from about fifty to hundreds of separate lines. Each line is assigned a particular meaning or function. Although there are many different bus designs, on any bus the lines can be classified into three functional groups (Figure 3.16): data, address, and control lines. In addition, there may be power distribution lines that supply power to the attached modules.\n\n\nThe\n   **data lines**\n   provide a path for moving data among system modules. These lines, collectively, are called the\n   **data bus**\n   . The data bus may consist of 32, 64, 128, or even more separate lines, the number of lines being referred to as the\n   *width*\n   of the data bus. Because each line can carry only one bit at a time, the number of lines determines how many bits can be transferred at a time. The width of the data bus is a key factor in determining overall system performance. For example, if the data bus is 32 bits wide and each instruction is 64 bits long, then the processor must access the memory module twice during each instruction cycle.\n\n\nThe\n   **address lines**\n   are used to designate the source or destination of the data on the data bus. For example, if the processor wishes to read a word (8, 16, or 32 bits) of data from memory, it puts the address of the desired word on the address lines. Clearly, the width of the\n   **address bus**\n   determines the maximum possible memory capacity of the system. Furthermore, the address lines are generally also used to address I/O ports. Typically, the higher-order bits are used to select a particular module on the bus, and the lower-order bits select a memory location or I/O port within the module. For example, on an 8-bit address bus, address 01111111 and below might reference locations in a memory module (module 0) with 128 words of memory, and address 10000000 and above refer to devices attached to an I/O module (module 1).\n\n\nThe\n   **control lines**\n   are used to control the access to and the use of the data and address lines. Because the data and address lines are shared by all components,\n\n\n\n\n![Diagram of a Bus Interconnection Scheme showing a central bus with three types of lines: Control lines, Address lines, and Data lines, connecting a CPU, Memory, and I/O modules.](images/image_0041.jpeg)\n\n\nThe diagram illustrates a bus interconnection scheme. A central horizontal bus is shown, with three types of lines branching off to the left: 'Control lines' (top), 'Address lines' (middle), and 'Data lines' (bottom). These lines connect to three types of modules: a 'CPU' module at the far left, and two groups of 'Memory' and 'I/O' modules. The 'Memory' and 'I/O' groups are separated by an ellipsis, indicating multiple modules of each type. Each module is represented by a rectangular block with vertical lines representing its internal pins or connections to the bus. A large bracket on the right side of the bus is labeled 'Bus'.\n\n\nDiagram of a Bus Interconnection Scheme showing a central bus with three types of lines: Control lines, Address lines, and Data lines, connecting a CPU, Memory, and I/O modules.\n\n\n**Figure 3.16**\n   Bus Interconnection Scheme\n\n\nthere must be a means of controlling their use. Control signals transmit both command and timing information among system modules. Timing signals indicate the validity of data and address information. Command signals specify operations to be performed. Typical control lines include:\n\n\n  * ■\n    **Memory write:**\n    causes data on the bus to be written into the addressed location.\n  * ■\n    **Memory read:**\n    causes data from the addressed location to be placed on the bus.\n  * ■\n    **I/O write:**\n    causes data on the bus to be output to the addressed I/O port.\n  * ■\n    **I/O read:**\n    causes data from the addressed I/O port to be placed on the bus.\n  * ■\n    **Transfer ACK:**\n    indicates that data have been accepted from or placed on the bus.\n  * ■\n    **Bus request:**\n    indicates that a module needs to gain control of the bus.\n  * ■\n    **Bus grant:**\n    indicates that a requesting module has been granted control of the bus.\n  * ■\n    **Interrupt request:**\n    indicates that an interrupt is pending.\n  * ■\n    **Interrupt ACK:**\n    acknowledges that the pending interrupt has been recognized.\n  * ■\n    **Clock:**\n    is used to synchronize operations.\n  * ■\n    **Reset:**\n    initializes all modules.\n\n\nThe operation of the bus is as follows. If one module wishes to send data to another, it must do two things: (1) obtain the use of the bus, and (2) transfer data via the bus. If one module wishes to request data from another module, it must (1) obtain the use of the bus, and (2) transfer a request to the other module over the appropriate control and address lines. It must then wait for that second module to send the data."
        },
        {
          "name": "Point-to-Point Interconnect",
          "content": "The shared bus architecture was the standard approach to interconnection between the processor and other components (memory, I/O, and so on) for decades. But contemporary systems increasingly rely on point-to-point interconnection rather than shared buses.\n\n\nThe principal reason driving the change from bus to point-to-point interconnect was the electrical constraints encountered with increasing the frequency of wide synchronous buses. At higher and higher data rates, it becomes increasingly difficult to perform the synchronization and arbitration functions in a timely fashion. Further, with the advent of multicore chips, with multiple processors and significant memory on a single chip, it was found that the use of a conventional shared bus on the same chip magnified the difficulties of increasing bus data rate and reducing bus latency to keep up with the processors. Compared to the shared bus, the point-to-point interconnect has lower latency, higher data rate, and better scalability.\n\n\nIn this section, we look at an important and representative example of the point-to-point interconnect approach: Intel's\n   **QuickPath Interconnect (QPI)**\n   , which was introduced in 2008.\n\n\nThe following are significant characteristics of QPI and other point-to-point interconnect schemes:\n\n\n  * ■\n    **Multiple direct connections:**\n    Multiple components within the system enjoy direct pairwise connections to other components. This eliminates the need for arbitration found in shared transmission systems.\n  * ■\n    **Layered protocol architecture:**\n    As found in network environments, such as TCP/IP-based data networks, these processor-level interconnects use a layered protocol architecture, rather than the simple use of control signals found in shared bus arrangements.\n  * ■\n    **Packetized data transfer:**\n    Data are not sent as a raw bit stream. Rather, data are sent as a sequence of packets, each of which includes control headers and error control codes.\n\n\nFigure 3.17 illustrates a typical use of QPI on a multicore computer. The QPI links (indicated by the green arrow pairs in the figure) form a switching fabric that enables data to move throughout the network. Direct QPI connections can be established between each pair of core processors. If core A in Figure 3.17 needs to access the memory controller in core D, it sends its request through either cores B or C, which must in turn forward that request on to the memory controller in core D. Similarly, larger systems with eight or more processors can be built using processors with three links and routing traffic through intermediate processors.\n\n\nIn addition, QPI is used to connect to an I/O module, called an I/O hub (IOH). The IOH acts as a switch directing traffic to and from I/O devices. Typically in newer\n\n\n\n\n![Diagram of a Multicore Configuration Using QPI. The diagram shows four cores (A, B, C, D) arranged in a square. Each core is connected to its immediate neighbors (top, bottom, left, right) by green double-headed arrows representing QPI links. Each core also has a direct QPI link to every other core, forming a fully connected mesh. Each core is connected to a DRAM block (labeled 'DRAM') by a blue double-headed arrow representing a Memory bus. Each core is also connected to an I/O Hub (labeled 'I/O Hub') by a red double-headed arrow representing PCI Express. Each I/O Hub is connected to an I/O device (labeled 'I/O device') by a red double-headed arrow representing PCI Express. A legend at the bottom identifies the link types: green double-headed arrows for QPI, red double-headed arrows for PCI Express, and blue double-headed arrows for Memory bus.](images/image_0042.jpeg)\n\n\nDiagram of a Multicore Configuration Using QPI. The diagram shows four cores (A, B, C, D) arranged in a square. Each core is connected to its immediate neighbors (top, bottom, left, right) by green double-headed arrows representing QPI links. Each core also has a direct QPI link to every other core, forming a fully connected mesh. Each core is connected to a DRAM block (labeled 'DRAM') by a blue double-headed arrow representing a Memory bus. Each core is also connected to an I/O Hub (labeled 'I/O Hub') by a red double-headed arrow representing PCI Express. Each I/O Hub is connected to an I/O device (labeled 'I/O device') by a red double-headed arrow representing PCI Express. A legend at the bottom identifies the link types: green double-headed arrows for QPI, red double-headed arrows for PCI Express, and blue double-headed arrows for Memory bus.\n\n\n**Figure 3.17**\n   Multicore Configuration Using QPI\n\n\n\n\n![Diagram illustrating the QPI Layers architecture. Two vertical stacks of four layers each are shown. The layers, from top to bottom, are Protocol, Routing, Link, and Physical. Horizontal arrows between the stacks indicate data flow: 'Packets' at the Protocol layer, 'Flits' at the Link layer, and 'Phits' at the Physical layer.](images/image_0043.jpeg)\n\n\nDiagram illustrating the QPI Layers architecture. Two vertical stacks of four layers each are shown. The layers, from top to bottom, are Protocol, Routing, Link, and Physical. Horizontal arrows between the stacks indicate data flow: 'Packets' at the Protocol layer, 'Flits' at the Link layer, and 'Phits' at the Physical layer.\n\n\nFigure 3.18 QPI Layers\n\n\nsystems, the link from the IOH to the I/O device controller uses an interconnect technology called PCI Express (PCIe), described later in this chapter. The IOH translates between the QPI protocols and formats and the PCIe protocols and formats. A core also links to a main memory module (typically the memory uses dynamic access random memory (DRAM) technology) using a dedicated memory bus.\n\n\nQPI is defined as a four-layer protocol architecture,\n   \n    3\n   \n   encompassing the following layers (Figure 3.18):\n\n\n  * ■\n    **Physical:**\n    Consists of the actual wires carrying the signals, as well as circuitry and logic to support ancillary features required in the transmission and receipt of the 1s and 0s. The unit of transfer at the Physical layer is 20 bits, which is called a\n    **Phit**\n    (physical unit).\n  * ■\n    **Link:**\n    Responsible for reliable transmission and flow control. The Link layer's unit of transfer is an 80-bit\n    **Flit**\n    (flow control unit).\n  * ■\n    **Routing:**\n    Provides the framework for directing packets through the fabric.\n  * ■\n    **Protocol:**\n    The high-level set of rules for exchanging\n    **packets**\n    of data between devices. A packet is comprised of an integral number of Flits.\n\n\n\n\n**QPI Physical Layer**\n\n\nFigure 3.19 shows the physical architecture of a QPI port. The QPI port consists of 84 individual links grouped as follows. Each data path consists of a pair of wires that transmits data one bit at a time; the pair is referred to as a\n   **lane**\n   . There are 20 data lanes in each direction (transmit and receive), plus a clock lane in each direction. Thus, QPI is capable of transmitting 20 bits in parallel in each direction. The 20-bit unit is referred to as a\n   *phit*\n   . Typical signaling speeds of the link in current products calls for operation at 6.4 GT/s (transfers per second). At 20 bits per transfer, that adds up to 16 GB/s, and since QPI links involve dedicated bidirectional pairs, the total capacity is 32 GB/s.\n\n\n3\n   \n   The reader unfamiliar with the concept of a protocol architecture will find a brief overview in Appendix D.\n\n\n\n\n![Diagram of the Physical Interface of the Intel QPI Interconnect between Component A and Component B.](images/image_0044.jpeg)\n\n\nThe diagram illustrates the physical interface of the Intel QPI Interconnect between two components, Component A and Component B. Each component contains an Intel QuickPath Interconnect Port. The ports are divided into four quadrants of 5 lanes each. Component A's port has Transmission Lanes on the left and Reception Lanes on the right. Component B's port has Reception Lanes on the left and Transmission Lanes on the right. The lanes are connected between the corresponding quadrants of the two components. Clock signals are indicated by vertical lines labeled 'Fwd Clk' and 'Rev Clk' on the left and right sides of each component's port.\n\n\nDiagram of the Physical Interface of the Intel QPI Interconnect between Component A and Component B.\n\n\n**Figure 3.19**\n   Physical Interface of the Intel QPI Interconnect\n\n\nThe lanes in each direction are grouped into four quadrants of 5 lanes each. In some applications, the link can also operate at half or quarter widths in order to reduce power consumption or work around failures.\n\n\nThe form of transmission on each lane is known as\n   **differential signaling**\n   , or\n   **balanced transmission**\n   . With balanced transmission, signals are transmitted as a current that travels down one conductor and returns on the other. The binary value depends on the voltage difference. Typically, one line has a positive voltage value and the other line has zero voltage, and one line is associated with binary 1 and one line is associated with binary 0. Specifically, the technique used by QPI is known as\n   *low-voltage differential signaling*\n   (LVDS). In a typical implementation, the transmitter injects a small current into one wire or the other, depending on the logic level to be sent. The current passes through a resistor at the receiving end, and then returns in the opposite direction along the other wire. The receiver senses the polarity of the voltage across the resistor to determine the logic level.\n\n\nAnother function performed by the physical layer is that it manages the translation between 80-bit flits and 20-bit phits using a technique known as\n   **multilane distribution**\n   . The flits can be considered as a bit stream that is distributed across the data lanes in a round-robin fashion (first bit to first lane, second bit to second lane, etc.), as illustrated in Figure 3.20. This approach enables QPI to achieve very high data rates by implementing the physical link between two ports as multiple parallel channels.\n\n\n\n\n**QPI Link Layer**\n\n\nThe QPI link layer performs two key functions: flow control and error control. These functions are performed as part of the QPI link layer protocol, and operate on the level of the flit (flow control unit). Each flit consists of a 72-bit message payload and\n\n\n\n\n![Diagram illustrating QPI Multilane Distribution. A central horizontal sequence of flits is labeled 'bit stream of flits'. The flits are numbered from left to right as #2n+1, #2n, ..., #n+2, #n+1, #n, ..., #2, #1. Arrows from the central stream point to three parallel lanes on the right, labeled QPI lane 0, QPI lane 1, and QPI lane 19. Each lane contains a sequence of flits: lane 0 has #2n+1, #n+1, #1; lane 1 has #2n+2, #n+2, #2; and lane 19 has #3n, #2n, #n. Vertical dots between the lanes indicate multiple other lanes.](images/image_0045.jpeg)\n\n\nDiagram illustrating QPI Multilane Distribution. A central horizontal sequence of flits is labeled 'bit stream of flits'. The flits are numbered from left to right as #2n+1, #2n, ..., #n+2, #n+1, #n, ..., #2, #1. Arrows from the central stream point to three parallel lanes on the right, labeled QPI lane 0, QPI lane 1, and QPI lane 19. Each lane contains a sequence of flits: lane 0 has #2n+1, #n+1, #1; lane 1 has #2n+2, #n+2, #2; and lane 19 has #3n, #2n, #n. Vertical dots between the lanes indicate multiple other lanes.\n\n\n**Figure 3.20**\n   QPI Multilane Distribution\n\n\nan 8-bit error control code called a cyclic redundancy check (CRC). We discuss error control codes in Chapter 5.\n\n\nA flit payload may consist of data or message information. The data flits transfer the actual bits of data between cores or between a core and an IOH. The message flits are used for such functions as flow control, error control, and cache coherence. We discuss cache coherence in Chapters 5 and 17.\n\n\nThe\n   **flow control function**\n   is needed to ensure that a sending QPI entity does not overwhelm a receiving QPI entity by sending data faster than the receiver can process the data and clear buffers for more incoming data. To control the flow of data, QPI makes use of a credit scheme. During initialization, a sender is given a set number of credits to send flits to a receiver. Whenever a flit is sent to the receiver, the sender decrements its credit counters by one credit. Whenever a buffer is freed at the receiver, a credit is returned to the sender for that buffer. Thus, the receiver controls that pace at which data is transmitted over a QPI link.\n\n\nOccasionally, a bit transmitted at the physical layer is changed during transmission, due to noise or some other phenomenon. The\n   **error control function**\n   at the link layer detects and recovers from such bit errors, and so isolates higher layers from experiencing bit errors. The procedure works as follows for a flow of data from system A to system B:\n\n\n  * 1. As mentioned, each 80-bit flit includes an 8-bit CRC field. The CRC is a function of the value of the remaining 72 bits. On transmission, A calculates a CRC value for each flit and inserts that value into the flit.\n  * 2. When a flit is received, B calculates a CRC value for the 72-bit payload and compares this value with the value of the incoming CRC value in the flit. If the two CRC values do not match, an error has been detected.\n  * 3. When B detects an error, it sends a request to A to retransmit the flit that is in error. However, because A may have had sufficient credit to send a stream of flits, so that additional flits have been transmitted after the flit in error and\n\n\nbefore A receives the request to retransmit. Therefore, the request is for A to back up and retransmit the damaged flit plus all subsequent flits.\n\n\n\n\n**QPI Routing Layer**\n\n\nThe routing layer is used to determine the course that a packet will traverse across the available system interconnects. Routing tables are defined by firmware and describe the possible paths that a packet can follow. In small configurations, such as a two-socket platform, the routing options are limited and the routing tables quite simple. For larger systems, the routing table options are more complex, giving the flexibility of routing and rerouting traffic depending on how (1) devices are populated in the platform, (2) system resources are partitioned, and (3) reliability events result in mapping around a failing resource.\n\n\n\n\n**QPI Protocol Layer**\n\n\nIn this layer, the packet is defined as the unit of transfer. The packet contents definition is standardized with some flexibility allowed to meet differing market segment requirements. One key function performed at this level is a cache coherency protocol, which deals with making sure that main memory values held in multiple caches are consistent. A typical data packet payload is a block of data being sent to or from a cache."
        },
        {
          "name": "PCI Express",
          "content": "The\n   **peripheral component interconnect (PCI)**\n   is a popular high-bandwidth, processor-independent bus that can function as a mezzanine or peripheral bus. Compared with other common bus specifications, PCI delivers better system performance for high-speed I/O subsystems (e.g., graphic display adapters, network interface controllers, and disk controllers).\n\n\nIntel began work on PCI in 1990 for its Pentium-based systems. Intel soon released all the patents to the public domain and promoted the creation of an industry association, the PCI Special Interest Group (SIG), to develop further and maintain the compatibility of the PCI specifications. The result is that PCI has been widely adopted and is finding increasing use in personal computer, workstation, and server systems. Because the specification is in the public domain and is supported by a broad cross-section of the microprocessor and peripheral industry, PCI products built by different vendors are compatible.\n\n\nAs with the system bus discussed in the preceding sections, the bus-based PCI scheme has not been able to keep pace with the data rate demands of attached devices. Accordingly, a new version, known as\n   **PCI Express (PCIe)**\n   has been developed. PCIe, as with QPI, is a point-to-point interconnect scheme intended to replace bus-based schemes such as PCI.\n\n\nA key requirement for PCIe is high capacity to support the needs of higher data rate I/O devices, such as Gigabit Ethernet. Another requirement deals with the need to support time-dependent data streams. Applications such as video-on-demand and audio redistribution are putting real-time constraints on servers too. Many communications applications and embedded PC control systems also process data in real-time. Today's platforms must also deal with multiple concurrent\n\n\ntransfers at ever-increasing data rates. It is no longer acceptable to treat all data as equal—it is more important, for example, to process streaming data first since late real-time data is as useless as no data. Data needs to be tagged so that an I/O system can prioritize its flow throughout the platform.\n\n\n\n\n**PCI Physical and Logical Architecture**\n\n\nFigure 3.21 shows a typical configuration that supports the use of PCIe. A\n   **root complex**\n   device, also referred to as a\n   *chipset*\n   or a\n   *host bridge*\n   , connects the processor and memory subsystem to the PCI Express switch fabric comprising one or more PCIe and PCIe switch devices. The root complex acts as a buffering device, to deal with difference in data rates between I/O controllers and memory and processor components. The root complex also translates between PCIe transaction formats and the processor and memory signal and control requirements. The chipset will typically support multiple PCIe ports, some of which attach directly to a PCIe device, and one or more that attach to a switch that manages multiple PCIe streams. PCIe links from the chipset may attach to the following kinds of devices that implement PCIe:\n\n\n  * ■\n    **Switch:**\n    The switch manages multiple PCIe streams.\n  * ■\n    **PCIe endpoint:**\n    An I/O device or controller that implements PCIe, such as a Gigabit ethernet switch, a graphics or video controller, disk interface, or a communications controller.\n\n\n\n\n![Diagram of a typical PCIe configuration showing a root complex (Chipset) connected to various components and a switch fabric.](images/image_0046.jpeg)\n\n\nThe diagram illustrates a typical PCIe configuration. At the top, two 'Core' blocks are connected to a central 'Chipset' block. The 'Chipset' is connected to several peripheral devices: 'Gigabit ethernet', 'PCIe-PCI bridge', and two 'Memory' blocks. The 'Chipset' is also connected to a central 'Switch' block via a PCIe link. The 'Switch' block is an octagon with four ports, each connected to a 'PCIe endpoint' block. The connections are labeled with 'PCIe'.\n\n\nDiagram of a typical PCIe configuration showing a root complex (Chipset) connected to various components and a switch fabric.\n\n\n**Figure 3.21**\n   Typical Configuration Using PCIe\n\n\n  * ■\n    **Legacy endpoint:**\n    Legacy endpoint category is intended for existing designs that have been migrated to PCI Express, and it allows legacy behaviors such as use of I/O space and locked transactions. PCI Express endpoints are not permitted to require the use of I/O space at runtime and must not use locked transactions. By distinguishing these categories, it is possible for a system designer to restrict or eliminate legacy behaviors that have negative impacts on system performance and robustness.\n  * ■\n    **PCIe/PCI bridge:**\n    Allows older PCI devices to be connected to PCIe-based systems.\n\n\nAs with QPI, PCIe interactions are defined using a protocol architecture. The PCIe protocol architecture encompasses the following layers (Figure 3.22):\n\n\n  * ■\n    **Physical:**\n    Consists of the actual wires carrying the signals, as well as circuitry and logic to support ancillary features required in the transmission and receipt of the 1s and 0s.\n  * ■\n    **Data link:**\n    Is responsible for reliable transmission and flow control. Data packets generated and consumed by the DLL are called Data Link Layer Packets (DLLPs).\n  * ■\n    **Transaction:**\n    Generates and consumes data packets used to implement load/store data transfer mechanisms and also manages the flow control of those packets between the two components on a link. Data packets generated and consumed by the TL are called Transaction Layer Packets (TLPs).\n\n\nAbove the TL are software layers that generate read and write requests that are transported by the transaction layer to the I/O devices using a packet-based transaction protocol.\n\n\n\n\n**PCIe Physical Layer**\n\n\nSimilar to QPI, PCIe is a point-to-point architecture. Each PCIe port consists of a number of bidirectional lanes (note that in QPI, the lane refers to transfer in one direction only). Transfer in each direction in a lane is by means of differential signaling over a pair of wires. A PCI port can provide 1, 4, 6, 16, or 32 lanes. In what follows, we refer to the PCIe 3.0 specification, introduced in late 2010.\n\n\nAs with QPI, PCIe uses a multilane distribution technique. Figure 3.23 shows an example for a PCIe port consisting of four lanes. Data are distributed to the four\n\n\n\n\n![Diagram of PCIe Protocol Layers showing two endpoints (left and right) each with Transaction, Data link, and Physical layers. Bidirectional arrows indicate communication between corresponding layers: Transaction layer packets (TLPs) between Transaction layers, Data link layer packets (DLLPs) between Data link layers, and Physical layer communication between Physical layers.](images/image_0047.jpeg)\n\n\nThe diagram illustrates the PCIe protocol layers across two endpoints. Each endpoint is represented by a vertical stack of three layers: Transaction (top), Data link (middle), and Physical (bottom). Bidirectional arrows connect the corresponding layers of the two endpoints: Transaction layer packets (TLPs) between the Transaction layers, Data link layer packets (DLLPs) between the Data link layers, and a direct Physical layer connection between the Physical layers.\n\n\nDiagram of PCIe Protocol Layers showing two endpoints (left and right) each with Transaction, Data link, and Physical layers. Bidirectional arrows indicate communication between corresponding layers: Transaction layer packets (TLPs) between Transaction layers, Data link layer packets (DLLPs) between Data link layers, and Physical layer communication between Physical layers.\n\n\n**Figure 3.22**\n   PCIe Protocol Layers\n\n\n\n\n![Diagram illustrating PCIe Multilane Distribution. A byte stream of bytes B7 through B0 is distributed across four PCIe lanes. The distribution is interleaved: Lane 0 gets B4 and B0, Lane 1 gets B5 and B1, Lane 2 gets B6 and B2, and Lane 3 gets B7 and B3. Each lane then performs a 128b/130b encoding.](images/image_0048.jpeg)\n\n\nThe diagram illustrates the distribution of a byte stream across multiple PCIe lanes. A horizontal sequence of bytes, labeled\n    \n     B7, B6, B5, B4, B3, B2, B1, B0\n    \n    , is shown on the left. A bracket above this sequence is labeled \"byte stream\". Arrows indicate the distribution of these bytes to four separate PCIe lanes on the right. Each lane consists of a pair of bytes followed by a hexagonal encoding block labeled \"128b/130b\", which then leads to the \"PCIe lane\" output.\n\n\n\nPCIe Lane | Bytes (from left to right) | Encoding\nlane 0 | B4, B0 | 128b/130b\nlane 1 | B5, B1 | 128b/130b\nlane 2 | B6, B2 | 128b/130b\nlane 3 | B7, B3 | 128b/130b\n\n\nDiagram illustrating PCIe Multilane Distribution. A byte stream of bytes B7 through B0 is distributed across four PCIe lanes. The distribution is interleaved: Lane 0 gets B4 and B0, Lane 1 gets B5 and B1, Lane 2 gets B6 and B2, and Lane 3 gets B7 and B3. Each lane then performs a 128b/130b encoding.\n\n\n**Figure 3.23**\n   PCIe Multilane Distribution\n\n\nlanes 1 byte at a time using a simple round-robin scheme. At each physical lane, data are buffered and processed 16 bytes (128 bits) at a time. Each block of 128 bits is encoded into a unique 130-bit codeword for transmission; this is referred to as 128b/130b encoding. Thus, the effective data rate of an individual lane is reduced by a factor of 128/130.\n\n\nTo understand the rationale for the 128b/130b encoding, note that unlike QPI, PCIe does not use its clock line to synchronize the bit stream. That is, the clock line is not used to determine the start and end point of each incoming bit; it is used for other signaling purposes only. However, it is necessary for the receiver to be synchronized with the transmitter, so that the receiver knows when each bit begins and ends. If there is any drift between the clocks used for bit transmission and reception of the transmitter and receiver, errors may occur. To compensate for the possibility of drift, PCIe relies on the receiver synchronizing with the transmitter based on the transmitted signal. As with QPI, PCIe uses differential signaling over a pair of wires. Synchronization can be achieved by the receiver looking for transitions in the data and synchronizing its clock to the transition. However, consider that with a long string of 1s or 0s using differential signaling, the output is a constant voltage over a long period of time. Under these circumstances, any drift between the clocks of transmitter and receiver will result in loss of synchronization between the two.\n\n\nA common approach, and the one used in PCIe 3.0, to overcoming the problem of a long string of bits of one value is scrambling. Scrambling, which does not increase the number of bits to be transmitted, is a mapping technique that tends to make the data appear more random. The scrambling tends to spread out the number of transitions so that they appear at the receiver more uniformly spaced, which is good for synchronization. Also, other transmission properties, such as spectral properties, are enhanced if the data are more nearly of a random nature rather than constant or repetitive. For more discussion of scrambling, see Appendix E.\n\n\nAnother technique that can aid in synchronization is encoding, in which additional bits are inserted into the bit stream to force transitions. For PCIe 3.0, each group of 128 bits of input is mapped into a 130-bit block by adding a 2-bit block sync header. The value of the header is 10 for a data block and 01 for what is called an\n   *ordered set block*\n   , which refers to a link-level information block.\n\n\nFigure 3.24 illustrates the use of scrambling and encoding. Data to be transmitted are fed into a scrambler. The scrambled output is then fed into a 128b/130b encoder, which buffers 128 bits and then maps the 128-bit block into a 130-bit block. This block then passes through a parallel-to-serial converter and transmitted one bit at a time using differential signaling.\n\n\nAt the receiver, a clock is synchronized to the incoming data to recover the bit stream. This then passes through a serial-to-parallel converter to produce a stream of 130-bit blocks. Each block is passed through a 128b/130b decoder to recover the original scrambled bit pattern, which is then descrambled to produce the original bit stream.\n\n\nUsing these techniques, a data rate of 16 GB/s can be achieved. One final detail to mention; each transmission of a block of data over a PCI link begins and ends with an 8-bit framing sequence intended to give the receiver time to synchronize with the incoming physical layer bit stream.\n\n\n\n\n![Figure 3.24: PCIe Transmit and Receive Block Diagrams. (a) Transmitter: 8b input to Scrambler, 8b to 128b/130b Encoding, 130b to Parallel to serial, 1b to Transmitter differential driver, output D+ D-. (b) Receiver: D+ D- to Differential receiver, 1b to Data recovery circuit, 1b to Serial to parallel, 130b to 128b/130b decoding, 128b to Descrambler, output 8b. Clock recovery circuit is connected to the Data recovery circuit.](images/image_0049.jpeg)\n\n\nThe diagram illustrates the PCIe transmit and receive paths. The transmitter (a) takes 8-bit data, scrambles it, encodes it into 130 bits, converts it to a serial signal, and then drives it onto the D+ and D- differential lines. The receiver (b) captures these signals, recovers the clock, and then descrambles the 128-bit data to produce the final 8-bit output.\n\n\nFigure 3.24: PCIe Transmit and Receive Block Diagrams. (a) Transmitter: 8b input to Scrambler, 8b to 128b/130b Encoding, 130b to Parallel to serial, 1b to Transmitter differential driver, output D+ D-. (b) Receiver: D+ D- to Differential receiver, 1b to Data recovery circuit, 1b to Serial to parallel, 130b to 128b/130b decoding, 128b to Descrambler, output 8b. Clock recovery circuit is connected to the Data recovery circuit.\n\n\n**Figure 3.24**\n   PCIe Transmit and Receive Block Diagrams\n\n\n\n\n**PCIe Transaction Layer**\n\n\nThe transaction layer (TL) receives read and write requests from the software above the TL and creates request packets for transmission to a destination via the link layer. Most transactions use a\n   *split transaction*\n   technique, which works in the following fashion. A request packet is sent out by a source PCIe device, which then waits for a response, called a\n   *completion*\n   packet. The completion following a request is initiated by the completer only when it has the data and/or status ready for delivery. Each packet has a unique identifier that enables completion packets to be directed to the correct originator. With the split transaction technique, the completion is separated in time from the request, in contrast to a typical bus operation in which both sides of a transaction must be available to seize and use the bus. Between the request and the completion, other PCIe traffic may use the link.\n\n\nTL messages and some write transactions are\n   *posted transactions*\n   , meaning that no response is expected.\n\n\nThe TL packet format supports 32-bit memory addressing and extended 64-bit memory addressing. Packets also have attributes such as “no-snoop,”\n\n\n“relaxed ordering,” and “priority,” which may be used to optimally route these packets through the I/O subsystem.\n\n\n**ADDRESS SPACES AND TRANSACTION TYPES**\n   The TL supports four address spaces:\n\n\n  * ■\n    **Memory:**\n    The memory space includes system main memory. It also includes PCIe I/O devices. Certain ranges of memory addresses map into I/O devices.\n  * ■\n    **I/O:**\n    This address space is used for legacy PCI devices, with reserved memory address ranges used to address legacy I/O devices.\n  * ■\n    **Configuration:**\n    This address space enables the TL to read/write configuration registers associated with I/O devices.\n  * ■\n    **Message:**\n    This address space is for control signals related to interrupts, error handling, and power management.\n\n\nTable 3.2 shows the transaction types provided by the TL. For memory, I/O, and configuration address spaces, there are read and write transactions. In the case of memory transactions, there is also a read lock request function. Locked operations occur as a result of device drivers requesting atomic access to registers on a PCIe device. A device driver, for example, can atomically read, modify, and then write to a device register. To accomplish this, the device driver causes the processor to execute an instruction or set of instructions. The root complex converts these processor instructions into a sequence of PCIe transactions, which perform individual read and write requests for the device driver. If these transactions must be executed atomically, the root complex locks the PCIe link while executing the transactions. This locking prevents transactions that are not part of the sequence from occurring. This sequence of transactions is called a locked operation. The particular set\n\n\n**Table 3.2**\n   PCIe TLP Transaction Types\n\n\n\nAddress Space | TLP Type | Purpose\nMemory | Memory Read Request | Transfer data to or from a location in the system memory map.\nMemory Read Lock Request\nMemory Write Request\nI/O | I/O Read Request | Transfer data to or from a location in the system memory map for legacy devices.\nI/O Write Request\nConfiguration | Config Type 0 Read Request | Transfer data to or from a location in the configuration space of a PCIe device.\nConfig Type 0 Write Request\nConfig Type 1 Read Request\nConfig Type 1 Write Request\nMessage | Message Request | Provides in-band messaging and event reporting.\nMessage Request with Data\nMemory, I/O, Configuration | Completion | Returned for certain requests.\nCompletion with Data\nCompletion Locked\nCompletion Locked with Data\n\n\nof processor instructions that can cause a locked operation to occur depends on the system chip set and processor architecture.\n\n\nTo maintain compatibility with PCI, PCIe supports both Type 0 and Type 1 configuration cycles. A Type 1 cycle propagates downstream until it reaches the bridge interface hosting the bus (link) that the target device resides on. The configuration transaction is converted on the destination link from Type 1 to Type 0 by the bridge.\n\n\nFinally, completion messages are used with split transactions for memory, I/O, and configuration transactions.\n\n\n**TLP PACKET ASSEMBLY**\n   PCIe transactions are conveyed using transaction layer packets, which are illustrated in Figure 3.25a. A TLP originates in the transaction layer of the sending device and terminates at the transaction layer of the receiving device.\n\n\n\n\n![Figure 3.25 PCIe Protocol Data Unit Format. (a) Transaction Layer Packet: A vertical stack of fields. From top to bottom: STP framing (1 octet), Sequence number (2 octets), Header (12 or 16 octets), Data (0 to 4096 octets), ECRC (0 or 4 octets), LCRC (4 octets), and STP framing (1 octet). Brackets on the right indicate 'Created by Transaction Layer' for the Header and Data sections, and 'Appended by Data Link Layer' for the STP framing sections. (b) Data Link Layer Packet: A vertical stack of fields. From top to bottom: Start (1 octet), DLLP (4 octets), CRC (2 octets), and End (1 octet). Brackets on the right indicate 'Created by DLL' for the DLLP and CRC sections, and 'Appended by PL' for the Start and End sections.](images/image_0050.jpeg)\n\n\n(a) Transaction Layer Packet\n\n\n(b) Data Link Layer Packet\n\n\nFigure 3.25 PCIe Protocol Data Unit Format. (a) Transaction Layer Packet: A vertical stack of fields. From top to bottom: STP framing (1 octet), Sequence number (2 octets), Header (12 or 16 octets), Data (0 to 4096 octets), ECRC (0 or 4 octets), LCRC (4 octets), and STP framing (1 octet). Brackets on the right indicate 'Created by Transaction Layer' for the Header and Data sections, and 'Appended by Data Link Layer' for the STP framing sections. (b) Data Link Layer Packet: A vertical stack of fields. From top to bottom: Start (1 octet), DLLP (4 octets), CRC (2 octets), and End (1 octet). Brackets on the right indicate 'Created by DLL' for the DLLP and CRC sections, and 'Appended by PL' for the Start and End sections.\n\n\n**Figure 3.25**\n   PCIe Protocol Data Unit Format\n\n\nUpper layer software sends to the TL the information needed for the TL to create the core of the TLP, which consists of the following fields:\n\n\n  * ■\n    **Header:**\n    The header describes the type of packet and includes information needed by the receiver to process the packet, including any needed routing information. The internal header format is discussed subsequently.\n  * ■\n    **Data:**\n    A data field of up to 4096 bytes may be included in the TLP. Some TLPs do not contain a data field.\n  * ■\n    **ECRC:**\n    An optional end-to-end CRC field enables the destination TL layer to check for errors in the header and data portions of the TLP.\n\n\n\n\n**PCIe Data Link Layer**\n\n\nThe purpose of the PCIe data link layer is to ensure reliable delivery of packets across the PCIe link. The DLL participates in the formation of TLPs and also transmits DLLPs.\n\n\n**DATA LINK LAYER PACKETS**\n   Data link layer packets originate at the data link layer of a transmitting device and terminate at the DLL of the device on the other end of the link. Figure 3.25b shows the format of a DLLP. There are three important groups of DLLPs used in managing a link: flow control packets, power management packets, and TLP ACK and NAK packets. Power management packets are used in managing power platform budgeting. Flow control packets regulate the rate at which TLPs and DLLPs can be transmitted across a link. The ACK and NAK packets are used in TLP processing, discussed in the following paragraphs.\n\n\n**TRANSACTION LAYER PACKET PROCESSING**\n   The DLL adds two fields to the core of the TLP created by the TL (Figure 3.25a): a 16-bit sequence number and a 32-bit link-layer CRC (LCRC). Whereas the core fields created at the TL are only used at the destination TL, the two fields added by the DLL are processed at each intermediate node on the way from source to destination.\n\n\nWhen a TLP arrives at a device, the DLL strips off the sequence number and LCRC fields and checks the LCRC. There are two possibilities:\n\n\n  * 1. If no errors are detected, the core portion of the TLP is handed up to the local transaction layer. If this receiving device is the intended destination, then the TL processes the TLP. Otherwise, the TL determines a route for the TLP and passes it back down to the DLL for transmission over the next link on the way to the destination.\n  * 2. If an error is detected, the DLL schedules an NAK DLL packet to return back to the remote transmitter. The TLP is eliminated.\n\n\nWhen the DLL transmits a TLP, it retains a copy of the TLP. If it receives an NAK for the TLP with this sequence number, it retransmits the TLP. When it receives an ACK, it discards the buffered TLP."
        }
      ]
    },
    {
      "name": "Cache Memory",
      "sections": [
        {
          "name": "Computer Memory System Overview",
          "content": "**Characteristics of Memory Systems**\n\n\nThe complex subject of computer memory is made more manageable if we classify memory systems according to their key characteristics. The most important of these are listed in Table 4.1.\n\n\nThe term\n   **location**\n   in Table 4.1 refers to whether memory is internal or external to the computer. Internal memory is often equated with main memory, but there are other forms of internal memory. The processor requires its own local memory, in the form of registers (e.g., see Figure 2.3). Further, as we will see, the control unit portion of the processor may also require its own internal memory. We will defer discussion of these latter two types of internal memory to later chapters. Cache is another form of internal memory. External memory consists of peripheral storage devices, such as disk and tape, that are accessible to the processor via I/O controllers.\n\n\nAn obvious characteristic of memory is its\n   **capacity**\n   . For internal memory, this is typically expressed in terms of bytes (1 byte = 8 bits) or words. Common word lengths are 8, 16, and 32 bits. External memory capacity is typically expressed in terms of bytes.\n\n\n**Table 4.1**\n\nLocation | Performance\nInternal (e.g., processor registers, cache, main memory) | Access time\nExternal (e.g., optical disks, magnetic disks, tapes) | Cycle time\n | Transfer rate\nCapacity | Physical Type\nNumber of words | Semiconductor\nNumber of bytes | Magnetic\n | Optical\n | Magneto-optical\nUnit of Transfer | Physical Characteristics\nWord | Volatile/nonvolatile\nBlock | Erasable/nonerasable\nAccess Method | Organization\nSequential | Memory modules\nDirect | \nRandom | \nAssociative | \n\n\nA related concept is the\n   **unit of transfer**\n   . For internal memory, the unit of transfer is equal to the number of electrical lines into and out of the memory module. This may be equal to the word length, but is often larger, such as 64, 128, or 256 bits. To clarify this point, consider three related concepts for internal memory:\n\n\n  * ■\n    **Word:**\n    The “natural” unit of organization of memory. The size of a word is typically equal to the number of bits used to represent an integer and to the instruction length. Unfortunately, there are many exceptions. For example, the CRAY C90 (an older model CRAY supercomputer) has a 64-bit word length but uses a 46-bit integer representation. The Intel x86 architecture has a wide variety of instruction lengths, expressed as multiples of bytes, and a word size of 32 bits.\n  * ■\n    **Addressable units:**\n    In some systems, the addressable unit is the word. However, many systems allow addressing at the byte level. In any case, the relationship between the length in bits\n    \n     A\n    \n    of an address and the number\n    \n     N\n    \n    of addressable units is\n    \n     2^A = N\n    \n    .\n  * ■\n    **Unit of transfer:**\n    For main memory, this is the number of bits read out of or written into memory at a time. The unit of transfer need not equal a word or an addressable unit. For external memory, data are often transferred in much larger units than a word, and these are referred to as blocks.\n\n\nAnother distinction among memory types is the\n   **method of accessing**\n   units of data. These include the following:\n\n\n  * ■\n    **Sequential access:**\n    Memory is organized into units of data, called records. Access must be made in a specific linear sequence. Stored addressing information is used to separate records and assist in the retrieval process. A shared read–write mechanism is used, and this must be moved from its current location to the desired location, passing and rejecting each intermediate record. Thus, the time to access an arbitrary record is highly variable. Tape units, discussed in Chapter 6, are sequential access.\n  * ■\n    **Direct access:**\n    As with sequential access, direct access involves a shared read–write mechanism. However, individual blocks or records have a unique\n\n\naddress based on physical location. Access is accomplished by direct access to reach a general vicinity plus sequential searching, counting, or waiting to reach the final location. Again, access time is variable. Disk units, discussed in Chapter 6, are direct access.\n\n\n  * ■\n    **Random access:**\n    Each addressable location in memory has a unique, physically wired-in addressing mechanism. The time to access a given location is independent of the sequence of prior accesses and is constant. Thus, any location can be selected at random and directly addressed and accessed. Main memory and some cache systems are random access.\n  * ■\n    **Associative:**\n    This is a random access type of memory that enables one to make a comparison of desired bit locations within a word for a specified match, and to do this for all words simultaneously. Thus, a word is retrieved based on a portion of its contents rather than its address. As with ordinary random-access memory, each location has its own addressing mechanism, and retrieval time is constant independent of location or prior access patterns. Cache memories may employ associative access.\n\n\nFrom a user's point of view, the two most important characteristics of memory are capacity and\n   **performance**\n   . Three performance parameters are used:\n\n\n  * ■\n    **Access time (latency):**\n    For random-access memory, this is the time it takes to perform a read or write operation, that is, the time from the instant that an address is presented to the memory to the instant that data have been stored or made available for use. For non-random-access memory, access time is the time it takes to position the read-write mechanism at the desired location.\n  * ■\n    **Memory cycle time:**\n    This concept is primarily applied to random-access memory and consists of the access time plus any additional time required before a second access can commence. This additional time may be required for transients to die out on signal lines or to regenerate data if they are read destructively. Note that memory cycle time is concerned with the system bus, not the processor.\n  * ■\n    **Transfer rate:**\n    This is the rate at which data can be transferred into or out of a memory unit. For random-access memory, it is equal to\n    \n     1/(\\text{cycle time})\n    \n    . For non-random-access memory, the following relationship holds:\n\n\nT_n = T_A + \\frac{n}{R} \\quad (4.1)\n\n\nwhere\n\n\nT_n\n   \n   = Average time to read or write\n   \n    n\n   \n   bits\n\n\nT_A\n   \n   = Average access time\n\n\nn\n   \n   = Number of bits\n\n\nR\n   \n   = Transfer rate, in bits per second (bps)\n\n\nA variety of\n   **physical types**\n   of memory have been employed. The most common today are semiconductor memory, magnetic surface memory, used for disk and tape, and optical and magneto-optical.\n\n\nSeveral\n   **physical characteristics**\n   of data storage are important. In a volatile memory, information decays naturally or is lost when electrical power is switched off. In a nonvolatile memory, information once recorded remains without deterioration until deliberately changed; no electrical power is needed to retain information. Magnetic-surface memories are nonvolatile. Semiconductor memory (memory on integrated circuits) may be either volatile or nonvolatile. Nonerasable memory cannot be altered, except by destroying the storage unit. Semiconductor memory of this type is known as\n   *read-only memory*\n   (ROM). Of necessity, a practical nonerasable memory must also be nonvolatile.\n\n\nFor random-access memory, the\n   **organization**\n   is a key design issue. In this context,\n   *organization*\n   refers to the physical arrangement of bits to form words. The obvious arrangement is not always used, as is explained in Chapter 5.\n\n\n\n\n**The Memory Hierarchy**\n\n\nThe design constraints on a computer's memory can be summed up by three questions: How much? How fast? How expensive?\n\n\nThe question of how much is somewhat open ended. If the capacity is there, applications will likely be developed to use it. The question of how fast is, in a sense, easier to answer. To achieve greatest performance, the memory must be able to keep up with the processor. That is, as the processor is executing instructions, we would not want it to have to pause waiting for instructions or operands. The final question must also be considered. For a practical system, the cost of memory must be reasonable in relationship to other components.\n\n\nAs might be expected, there is a trade-off among the three key characteristics of memory: capacity, access time, and cost. A variety of technologies are used to implement memory systems, and across this spectrum of technologies, the following relationships hold:\n\n\n  * ■ Faster access time, greater cost per bit;\n  * ■ Greater capacity, smaller cost per bit;\n  * ■ Greater capacity, slower access time.\n\n\nThe dilemma facing the designer is clear. The designer would like to use memory technologies that provide for large-capacity memory, both because the capacity is needed and because the cost per bit is low. However, to meet performance requirements, the designer needs to use expensive, relatively lower-capacity memories with short access times.\n\n\nThe way out of this dilemma is not to rely on a single memory component or technology, but to employ a\n   **memory hierarchy**\n   . A typical hierarchy is illustrated in Figure 4.1. As one goes down the hierarchy, the following occur:\n\n\n  * a. Decreasing cost per bit;\n  * b. Increasing capacity;\n  * c. Increasing access time;\n  * d. Decreasing frequency of access of the memory by the processor.\n\n\nThus, smaller, more expensive, faster memories are supplemented by larger, cheaper, slower memories. The key to the success of this organization\n\n\n\n\n![Figure 4.1: The Memory Hierarchy. A pyramid diagram showing levels of memory from fastest/smallest at the top to slowest/largest at the bottom. The top level is Registers. Below it is Cache, then Main memory. The next level is Inboard memory. The next level is Outboard storage, which includes Magnetic disk, CD-ROM, CD-RW, DVD-RW, DVD-RAM, and Blu-Ray. The bottom level is Off-line storage, which includes Magnetic tape.](images/image_0051.jpeg)\n\n\nThe diagram illustrates the Memory Hierarchy as a pyramid with three main levels of storage, each subdivided into specific types of memory:\n\n\n  * **Top Level (Registers, Cache, Main memory):**\n     This level represents the fastest and smallest memory, closest to the CPU.\n  * **Middle Level (Inboard memory, Outboard storage):**\n  * **Inboard memory:**\n       Includes Registers, Cache, and Main memory.\n  * **Outboard storage:**\n       Includes Magnetic disk, CD-ROM, CD-RW, DVD-RW, DVD-RAM, and Blu-Ray.\n  * **Bottom Level (Off-line storage):**\n     Includes Magnetic tape.\n\n\nFigure 4.1: The Memory Hierarchy. A pyramid diagram showing levels of memory from fastest/smallest at the top to slowest/largest at the bottom. The top level is Registers. Below it is Cache, then Main memory. The next level is Inboard memory. The next level is Outboard storage, which includes Magnetic disk, CD-ROM, CD-RW, DVD-RW, DVD-RAM, and Blu-Ray. The bottom level is Off-line storage, which includes Magnetic tape.\n\n\n**Figure 4.1**\n   The Memory Hierarchy\n\n\nis item (d): decreasing frequency of access. We examine this concept in greater detail when we discuss the cache, later in this chapter, and virtual memory in Chapter 8. A brief explanation is provided at this point.\n\n\nThe use of two levels of memory to reduce average access time works in principle, but only if conditions (a) through (d) apply. By employing a variety of technologies, a spectrum of memory systems exists that satisfies conditions (a) through (c). Fortunately, condition (d) is also generally valid.\n\n\nThe basis for the validity of condition (d) is a principle known as\n   **locality of reference**\n   [DENN68]. During the course of execution of a program, memory references by the processor, for both instructions and data, tend to cluster. Programs typically contain a number of iterative loops and subroutines. Once a loop or subroutine is entered, there are repeated references to a small set of instructions. Similarly, operations on tables and arrays involve access to a clustered set of data words. Over a long period of time, the clusters in use change, but over a short period of time, the processor is primarily working with fixed clusters of memory references.\n\n\n**EXAMPLE 4.1**\n   Suppose that the processor has access to two levels of memory. Level 1 contains 1000 words and has an access time of\n   \n    0.01 \\mu\\text{s}\n   \n   ; level 2 contains 100,000 words and has an access time of\n   \n    0.1 \\mu\\text{s}\n   \n   . Assume that if a word to be accessed is in level 1, then the processor accesses it directly. If it is in level 2, then the word is first transferred to level 1 and then accessed by the processor. For simplicity, we ignore the time required for the processor to determine whether the word is in level 1 or level 2. Figure 4.2 shows the general shape of the curve that covers this situation. The figure shows the average access time to a two-level memory as a function of the hit ratio\n   \n    H\n   \n   , where\n   \n    H\n   \n   is defined as the fraction of all memory accesses that are found in the faster memory (e.g., the cache),\n   \n    T_1\n   \n   is the access time to level 1, and\n   \n    T_2\n   \n   is the access time to level 2.\n   \n    1\n   \n   As can be seen, for high percentages of level 1 access, the average total access time is much closer to that of level 1 than that of level 2.\n\n\nIn our example, suppose 95% of the memory accesses are found in level 1. Then the average time to access a word can be expressed as\n\n\n(0.95)(0.01 \\mu\\text{s}) + (0.05)(0.01 \\mu\\text{s} + 0.1 \\mu\\text{s}) = 0.0095 + 0.0055 = 0.015 \\mu\\text{s}\n\n\nThe average access time is much closer to\n   \n    0.01 \\mu\\text{s}\n   \n   than to\n   \n    0.1 \\mu\\text{s}\n   \n   , as desired.\n\n\nAccordingly, it is possible to organize data across the hierarchy such that the percentage of accesses to each successively lower level is substantially less than that of the level above. Consider the two-level example already presented. Let level 2\n\n\n\n\n![Figure 4.2: A line graph showing the average access time as a function of the hit ratio. The x-axis is labeled 'Fraction of accesses involving only level 1 (hit ratio)' and ranges from 0 to 1. The y-axis is labeled 'Average access time' and has three points marked: T1 at the bottom, T2 in the middle, and T1 + T2 at the top. A straight line connects the point (0, T1 + T2) to the point (1, T1).](images/image_0052.jpeg)\n\n\nThe figure is a line graph with the x-axis representing the 'Fraction of accesses involving only level 1 (hit ratio)' from 0 to 1, and the y-axis representing 'Average access time'. A straight line starts at the point (0,\n    \n     T_1 + T_2\n    \n    ) and ends at the point (1,\n    \n     T_1\n    \n    ). The y-axis has three labels:\n    \n     T_1\n    \n    at the bottom,\n    \n     T_2\n    \n    in the middle, and\n    \n     T_1 + T_2\n    \n    at the top. The line shows that as the hit ratio increases from 0 to 1, the average access time decreases linearly from\n    \n     T_1 + T_2\n    \n    to\n    \n     T_1\n    \n    .\n\n\nFigure 4.2: A line graph showing the average access time as a function of the hit ratio. The x-axis is labeled 'Fraction of accesses involving only level 1 (hit ratio)' and ranges from 0 to 1. The y-axis is labeled 'Average access time' and has three points marked: T1 at the bottom, T2 in the middle, and T1 + T2 at the top. A straight line connects the point (0, T1 + T2) to the point (1, T1).\n\n\n**Figure 4.2**\n   Performance of Accesses Involving only Level 1 (hit ratio)\n\n\n1\n   \n   If the accessed word is found in the faster memory, that is defined as a\n   **hit**\n   . A\n   **miss**\n   occurs if the accessed word is not found in the faster memory.\n\n\nmemory contain all program instructions and data. The current clusters can be temporarily placed in level 1. From time to time, one of the clusters in level 1 will have to be swapped back to level 2 to make room for a new cluster coming in to level 1. On average, however, most references will be to instructions and data contained in level 1.\n\n\nThis principle can be applied across more than two levels of memory, as suggested by the hierarchy shown in Figure 4.1. The fastest, smallest, and most expensive type of memory consists of the registers internal to the processor. Typically, a processor will contain a few dozen such registers, although some machines contain hundreds of registers. Main memory is the principal internal memory system of the computer. Each location in main memory has a unique address. Main memory is usually extended with a higher-speed, smaller cache. The cache is not usually visible to the programmer or, indeed, to the processor. It is a device for staging the movement of data between main memory and processor registers to improve performance.\n\n\nThe three forms of memory just described are, typically, volatile and employ semiconductor technology. The use of three levels exploits the fact that semiconductor memory comes in a variety of types, which differ in speed and cost. Data are stored more permanently on external mass storage devices, of which the most common are hard disk and removable media, such as removable magnetic disk, tape, and optical storage. External, nonvolatile memory is also referred to as\n   **secondary memory**\n   or\n   **auxiliary memory**\n   . These are used to store program and data files and are usually visible to the programmer only in terms of files and records, as opposed to individual bytes or words. Disk is also used to provide an extension to main memory known as virtual memory, which is discussed in Chapter 8.\n\n\nOther forms of memory may be included in the hierarchy. For example, large IBM mainframes include a form of internal memory known as expanded storage. This uses a semiconductor technology that is slower and less expensive than that of main memory. Strictly speaking, this memory does not fit into the hierarchy but is a side branch: Data can be moved between main memory and expanded storage but not between expanded storage and external memory. Other forms of secondary memory include optical and magneto-optical disks. Finally, additional levels can be effectively added to the hierarchy in software. A portion of main memory can be used as a buffer to hold data temporarily that is to be read out to disk. Such a technique, sometimes referred to as a disk cache,\n   \n    2\n   \n   improves performance in two ways:\n\n\n  * ■ Disk writes are clustered. Instead of many small transfers of data, we have a few large transfers of data. This improves disk performance and minimizes processor involvement.\n  * ■ Some data destined for write-out may be referenced by a program before the next dump to disk. In that case, the data are retrieved rapidly from the software cache rather than slowly from the disk.\n\n\nAppendix 4A examines the performance implications of multilevel memory structures.\n\n\n2\n   \n   Disk cache is generally a purely software technique and is not examined in this book. See [STAL15] for a discussion."
        },
        {
          "name": "Cache Memory Principles",
          "content": "Cache memory is designed to combine the memory access time of expensive, high-speed memory combined with the large memory size of less expensive, lower-speed memory. The concept is illustrated in Figure 4.3a. There is a relatively large and slow main memory together with a smaller, faster cache memory. The cache contains a copy of portions of main memory. When the processor attempts to read a word of memory, a check is made to determine if the word is in the cache. If so, the word is delivered to the processor. If not, a block of main memory, consisting of some fixed number of words, is read into the cache and then the word is delivered to the processor. Because of the phenomenon of locality of reference, when a block of data is fetched into the cache to satisfy a single memory reference, it is likely that there will be future references to that same memory location or to other words in the block.\n\n\nFigure 4.3b depicts the use of multiple levels of cache. The L2 cache is slower and typically larger than the L1 cache, and the L3 cache is slower and typically larger than the L2 cache.\n\n\nFigure 4.4 depicts the structure of a cache/main-memory system. Main memory consists of up to\n   \n    2^n\n   \n   addressable words, with each word having a unique\n   \n    n\n   \n   -bit address. For mapping purposes, this memory is considered to consist of a number of fixed-length blocks of\n   \n    K\n   \n   words each. That is, there are\n   \n    M = 2^n/K\n   \n   blocks in main memory. The cache consists of\n   \n    m\n   \n   blocks, called\n   **lines**\n   .\n   \n    3\n   \n   Each line contains\n   \n    K\n   \n   words,\n\n\n\n\n![Figure 4.3: Cache and Main Memory. (a) Single cache: CPU, Cache, and Main memory. (b) Three-level cache organization: CPU, Level 1 (L1) cache, Level 2 (L2) cache, Level 3 (L3) cache, and Main memory.](images/image_0053.jpeg)\n\n\nFigure 4.3 consists of two diagrams illustrating cache and main memory organization.\n\n\n**(a) Single cache:**\n    This diagram shows three rectangular blocks: 'CPU' on the left, 'Cache' in the middle, and 'Main memory' on the right. Bidirectional arrows connect the CPU and Cache, labeled 'Fast' below. Bidirectional arrows connect the Cache and Main memory, labeled 'Slow' below. A bracket above the CPU and Cache is labeled 'Word transfer'. A bracket above the Cache and Main memory is labeled 'Block transfer'.\n\n\n**(b) Three-level cache organization:**\n    This diagram shows five rectangular blocks in a row: 'CPU' on the far left, followed by 'Level 1 (L1) cache', 'Level 2 (L2) cache', 'Level 3 (L3) cache', and 'Main memory' on the far right. Bidirectional arrows connect the CPU and L1 cache, labeled 'Fastest' below. Bidirectional arrows connect L1 cache and L2 cache, labeled 'Fast' below. Bidirectional arrows connect L2 cache and L3 cache, labeled 'Less fast' below. Bidirectional arrows connect L3 cache and Main memory, labeled 'Slow' below.\n\n\nFigure 4.3: Cache and Main Memory. (a) Single cache: CPU, Cache, and Main memory. (b) Three-level cache organization: CPU, Level 1 (L1) cache, Level 2 (L2) cache, Level 3 (L3) cache, and Main memory.\n\n\n**Figure 4.3**\n   Cache and Main Memory\n\n\n3\n   \n   In referring to the basic unit of the cache, the term\n   *line*\n   is used, rather than the term\n   *block*\n   , for two reasons: (1) to avoid confusion with a main memory block, which contains the same number of data words as a cache line; and (2) because a cache line includes not only\n   \n    K\n   \n   words of data, just as a main memory block, but also includes tag and control bits.\n\n\n\n\n![Figure 4.4: Cache/Main Memory Structure. (a) Cache: A table with columns 'Line number', 'Tag', and 'Block'. Line numbers are 0, 1, 2, ..., C-1. The 'Tag' column has shaded cells for lines 0, 1, 2, and C-1. The 'Block' column has a shaded cell for line 0 and a large shaded area for lines 2 to C-1. A double-headed arrow below the table indicates 'Block length (K words)'. (b) Main memory: A vertical stack of memory addresses from 0 to 2^n - 1. Addresses 0, 1, 2, 3 are grouped as 'Block 0 (K words)'. Addresses 2^n - K to 2^n - 1 are grouped as 'Block M-1'. A double-headed arrow at the bottom indicates 'Word length'.](images/image_0054.jpeg)\n\n\n(a) Cache\n\n\n(b) Main memory\n\n\nFigure 4.4: Cache/Main Memory Structure. (a) Cache: A table with columns 'Line number', 'Tag', and 'Block'. Line numbers are 0, 1, 2, ..., C-1. The 'Tag' column has shaded cells for lines 0, 1, 2, and C-1. The 'Block' column has a shaded cell for line 0 and a large shaded area for lines 2 to C-1. A double-headed arrow below the table indicates 'Block length (K words)'. (b) Main memory: A vertical stack of memory addresses from 0 to 2^n - 1. Addresses 0, 1, 2, 3 are grouped as 'Block 0 (K words)'. Addresses 2^n - K to 2^n - 1 are grouped as 'Block M-1'. A double-headed arrow at the bottom indicates 'Word length'.\n\n\n**Figure 4.4**\n   Cache/Main Memory Structure\n\n\nplus a tag of a few bits. Each line also includes control bits (not shown), such as a bit to indicate whether the line has been modified since being loaded into the cache. The length of a line, not including tag and control bits, is the\n   **line size**\n   . The line size may be as small as 32 bits, with each “word” being a single byte; in this case the line size is 4 bytes. The number of lines is considerably less than the number of main memory blocks (\n   \n    m \\ll M\n   \n   ). At any time, some subset of the blocks of memory resides in lines in the cache. If a word in a block of memory is read, that block is transferred to one of the lines of the cache. Because there are more blocks than lines, an individual line cannot be uniquely and permanently dedicated to a particular block. Thus, each line includes a\n   **tag**\n   that identifies which particular block is currently being stored. The tag is usually a portion of the main memory address, as described later in this section.\n\n\nFigure 4.5 illustrates the read operation. The processor generates the read address (RA) of a word to be read. If the word is contained in the cache, it is delivered to the processor. Otherwise, the block containing that word is loaded into the cache, and the word is delivered to the processor. Figure 4.5 shows these last two operations occurring in parallel and reflects the organization shown in Figure 4.6, which is typical of contemporary cache organizations. In this organization, the cache connects to the processor via data, control, and address lines. The data and address lines also attach to data and address buffers, which attach to a system bus from\n\n\n\n\n![Flowchart of the Cache Read Operation. The process starts with a START oval, followed by a Receive address RA from CPU rectangle. A decision diamond asks 'Is block containing RA in cache?'. If Yes, it goes to Fetch RA word and deliver to CPU, then to DONE. If No, it goes to Access main memory for block containing RA, then to Allocate cache line for main memory block. From there, it splits into Load main memory block into cache line and Deliver RA word to CPU, both leading to DONE.](images/image_0055.jpeg)\n\n\ngraph TD\n    START([START]) --> RA[Receive address RA from CPU]\n    RA --> Decision{Is block containing RA in cache?}\n    Decision -- Yes --> Fetch[Fetch RA word and deliver to CPU]\n    Fetch --> DONE([DONE])\n    Decision -- No --> Access[Access main memory for block containing RA]\n    Access --> Allocate[Allocate cache line for main memory block]\n    Allocate --> Load[Load main memory block into cache line]\n    Allocate --> Deliver[Deliver RA word to CPU]\n    Load --> DONE\n    Deliver --> DONE\n  \nFlowchart of the Cache Read Operation. The process starts with a START oval, followed by a Receive address RA from CPU rectangle. A decision diamond asks 'Is block containing RA in cache?'. If Yes, it goes to Fetch RA word and deliver to CPU, then to DONE. If No, it goes to Access main memory for block containing RA, then to Allocate cache line for main memory block. From there, it splits into Load main memory block into cache line and Deliver RA word to CPU, both leading to DONE.\n\n\n**Figure 4.5**\n   Cache Read Operation\n\n\nwhich main memory is reached. When a cache hit occurs, the data and address buffers are disabled and communication is only between processor and cache, with no system bus traffic. When a cache miss occurs, the desired address is loaded onto the system bus and the data are returned through the data buffer to both the cache and the processor. In other organizations, the cache is physically interposed between the processor and the main memory for all data, address, and control lines. In this latter case, for a cache miss, the desired word is first read into the cache and then transferred from cache to processor.\n\n\nA discussion of the performance parameters related to cache use is contained in Appendix 4A.\n\n\n\n\n![Diagram of Typical Cache Organization showing the flow of Address, Control, and Data between a Processor, a Cache, and a System bus.](images/image_0056.jpeg)\n\n\nThe diagram illustrates the typical organization of a cache. A large vertical rectangle on the left is labeled 'Processor'. In the center is a smaller vertical rectangle labeled 'Cache'. To the right is a thick vertical bar labeled 'System bus'. \n  - An arrow labeled 'Address' points from the Processor to the Cache, and then from the Cache to an 'Address buffer' (a small box with a triangle) which is connected to the System bus.\n  - A double-headed arrow labeled 'Control' connects the Processor and the Cache, and another double-headed arrow labeled 'Control' connects the Cache and the System bus.\n  - A double-headed arrow labeled 'Data' connects the Cache and the System bus, passing through a 'Data buffer' (a small box with two triangles) which is also connected to the System bus.\n\n\nDiagram of Typical Cache Organization showing the flow of Address, Control, and Data between a Processor, a Cache, and a System bus.\n\n\nFigure 4.6 Typical Cache Organization"
        },
        {
          "name": "Elements of Cache Design",
          "content": "This section provides an overview of cache design parameters and reports some typical results. We occasionally refer to the use of caches in\n   **high-performance computing (HPC)**\n   . HPC deals with supercomputers and their software, especially for scientific applications that involve large amounts of data, vector and matrix computation, and the use of parallel algorithms. Cache design for HPC is quite different than for other hardware platforms and applications. Indeed, many researchers have found that HPC applications perform poorly on computer architectures that employ caches [BAIL93]. Other researchers have since shown that a cache hierarchy can be useful in improving performance if the application software is tuned to exploit the cache [WANG99, PRES01].\n   \n    4\n\n\nAlthough there are a large number of cache implementations, there are a few basic design elements that serve to classify and differentiate cache architectures. Table 4.2 lists key elements.\n\n\n\n\n**Cache Addresses**\n\n\nAlmost all nonembedded processors, and many embedded processors, support virtual memory, a concept discussed in Chapter 8. In essence, virtual memory is a facility that allows programs to address memory from a logical point of view, without regard to the amount of main memory physically available. When virtual memory is used, the address fields of machine instructions contain virtual addresses. For reads\n\n\n4\n   \n   For a general discussion of HPC, see [DOWD98].\n\n\n**Table 4.2**\n\nCache Addresses | Write Policy\nLogical | Write through\nPhysical | Write back\nCache Size | Line Size\nMapping Function | Number of Caches\nDirect | Single or two level\nAssociative | Unified or split\nSet associative | \nReplacement Algorithm | \nLeast recently used (LRU) | \nFirst in first out (FIFO) | \nLeast frequently used (LFU) | \nRandom | \n\n\nto and writes from main memory, a hardware memory management unit (MMU) translates each virtual address into a physical address in main memory.\n\n\nWhen virtual addresses are used, the system designer may choose to place the cache between the processor and the MMU or between the MMU and main memory (Figure 4.7). A\n   **logical cache**\n   , also known as a\n   **virtual cache**\n   , stores data using\n\n\n\n\n![Diagram illustrating Logical and Physical Caches. (a) Logical cache: The Processor sends a Logical address to the Cache, which then sends it to the MMU. The MMU sends a Physical address to Main memory. Data flows from Main memory to the Cache and then to the Processor. (b) Physical cache: The Processor sends a Logical address to the MMU, which sends a Physical address to the Cache. The Cache sends a Physical address to Main memory. Data flows from Main memory to the Cache and then to the Processor.](images/image_0057.jpeg)\n\n\nThe diagram consists of two parts, (a) and (b), showing different cache placement strategies relative to the MMU and main memory.\n\n\n**(a) Logical cache:**\n    The Processor sends a Logical address to the Cache. The Cache then sends this Logical address to the MMU. The MMU translates it into a Physical address and sends it to Main memory. Data flows from Main memory to the Cache and then to the Processor.\n\n\n**(b) Physical cache:**\n    The Processor sends a Logical address to the MMU. The MMU translates it into a Physical address and sends it to the Cache. The Cache then sends this Physical address to Main memory. Data flows from Main memory to the Cache and then to the Processor.\n\n\nDiagram illustrating Logical and Physical Caches. (a) Logical cache: The Processor sends a Logical address to the Cache, which then sends it to the MMU. The MMU sends a Physical address to Main memory. Data flows from Main memory to the Cache and then to the Processor. (b) Physical cache: The Processor sends a Logical address to the MMU, which sends a Physical address to the Cache. The Cache sends a Physical address to Main memory. Data flows from Main memory to the Cache and then to the Processor.\n\n\n**Figure 4.7**\n**virtual addresses.**\n   The processor accesses the cache directly, without going through the MMU. A\n   **physical cache**\n   stores data using main memory\n   **physical addresses**\n   .\n\n\nOne obvious advantage of the logical cache is that cache access speed is faster than for a physical cache, because the cache can respond before the MMU performs an address translation. The disadvantage has to do with the fact that most virtual memory systems supply each application with the same virtual memory address space. That is, each application sees a virtual memory that starts at address 0. Thus, the same virtual address in two different applications refers to two different physical addresses. The cache memory must therefore be completely flushed with each application context switch, or extra bits must be added to each line of the cache to identify which virtual address space this address refers to.\n\n\nThe subject of logical versus physical cache is a complex one, and beyond the scope of this book. For a more in-depth discussion, see [CEKL97] and [JACO08].\n\n\n\n\n**Cache Size**\n\n\nThe second item in Table 4.2, cache size, has already been discussed. We would like the size of the cache to be small enough so that the overall average cost per bit is close to that of main memory alone and large enough so that the overall average access time is close to that of the cache alone. There are several other motivations for minimizing cache size. The larger the cache, the larger the number of gates involved in addressing the cache. The result is that large caches tend to be slightly slower than small ones—even when built with the same integrated circuit technology and put in the same place on chip and circuit board. The available chip and board area also limits cache size. Because the performance of the cache is very sensitive to the nature of the workload, it is impossible to arrive at a single “optimum” cache size. Table 4.3 lists the cache sizes of some current and past processors.\n\n\n\n\n**Mapping Function**\n\n\nBecause there are fewer cache lines than main memory blocks, an algorithm is needed for mapping main memory blocks into cache lines. Further, a means is needed for determining which main memory block currently occupies a cache line. The choice of the mapping function dictates how the cache is organized. Three techniques can be used: direct, associative, and set-associative. We examine each of these in turn. In each case, we look at the general structure and then a specific example.\n\n\n**EXAMPLE 4.2**\n   For all three cases, the example includes the following elements:\n\n\n  * ■ The cache can hold 64 kB.\n  * ■ Data are transferred between main memory and the cache in blocks of 4 bytes each. This means that the cache is organized as\n    \n     16K = 2^{14}\n    \n    lines of 4 bytes each.\n  * ■ The main memory consists of 16 MB, with each byte directly addressable by a 24-bit address (\n    \n     2^{24} = 16M\n    \n    ). Thus, for mapping purposes, we can consider main memory to consist of 4M blocks of 4 bytes each.\n\n\n**Table 4.3**\n\nProcessor | Type | Year of Introduction | L1 Cache\n      \n       a | L2 Cache | L3 Cache\nIBM 360/85 | Mainframe | 1968 | 16–32 kB | — | —\nPDP-11/70 | Minicomputer | 1975 | 1 kB | — | —\nVAX 11/780 | Minicomputer | 1978 | 16 kB | — | —\nIBM 3033 | Mainframe | 1978 | 64 kB | — | —\nIBM 3090 | Mainframe | 1985 | 128–256 kB | — | —\nIntel 80486 | PC | 1989 | 8 kB | — | —\nPentium | PC | 1993 | 8 kB/8 kB | 256–512 kB | —\nPowerPC 601 | PC | 1993 | 32 kB | — | —\nPowerPC 620 | PC | 1996 | 32 kB/32 kB | — | —\nPowerPC G4 | PC/server | 1999 | 32 kB/32 kB | 256 kB to 1 MB | 2 MB\nIBM S/390 G6 | Mainframe | 1999 | 256 kB | 8 MB | —\nPentium 4 | PC/server | 2000 | 8 kB/8 kB | 256 kB | —\nIBM SP | High-end server/\n      \n      supercomputer | 2000 | 64 kB/32 kB | 8 MB | —\nCRAY MTA\n      \n       b | Supercomputer | 2000 | 8 kB | 2 MB | —\nItanium | PC/server | 2001 | 16 kB/16 kB | 96 kB | 4 MB\nItanium 2 | PC/server | 2002 | 32 kB | 256 kB | 6 MB\nIBM POWER5 | High-end server | 2003 | 64 kB | 1.9 MB | 36 MB\nCRAY XD-1 | Supercomputer | 2004 | 64 kB/64 kB | 1 MB | —\nIBM POWER6 | PC/server | 2007 | 64 kB/64 kB | 4 MB | 32 MB\nIBM z10 | Mainframe | 2008 | 64 kB/128 kB | 3 MB | 24–48 MB\nIntel Core i7 EE 990 | Workstation/\n      \n      server | 2011 | 6 × 32 kB/\n      \n      32 kB | 1.5 MB | 12 MB\nIBM zEnterprise 196 | Mainframe/\n      \n      server | 2011 | 24 × 64 kB/\n      \n      128 kB | 24 × 1.5 MB | 24 MB L3\n      \n      192 MB L4\n\n\nNotes:\n   \n    a\n   \n   Two values separated by a slash refer to instruction and data caches.\n   \n    b\n   \n   Both caches are instruction only; no data caches.\n\n\n**DIRECT MAPPING**\n   The simplest technique, known as direct mapping, maps each block of main memory into only one possible cache line. The mapping is expressed as\n\n\ni = j \\text{ modulo } m\n\n\nwhere\n\n\ni\n   \n   = cache line number\n\n\nj\n   \n   = main memory block number\n\n\nm\n   \n   = number of lines in the cache\n\n\nFigure 4.8a shows the mapping for the first\n   \n    m\n   \n   blocks of main memory. Each block of main memory maps into one unique line of the cache. The next\n   \n    m\n   \n   blocks\n\n\n\n\n![Diagram (a) Direct mapping showing the mapping of main memory blocks to cache lines.](images/image_0058.jpeg)\n\n\nDiagram (a) illustrates direct mapping. On the left, the 'First\n    \n     m\n    \n    blocks of main memory' are shown as blocks\n    \n     B_0, \\dots, B_{m-1}\n    \n    , each of size\n    \n     b\n    \n    bits. On the right, the 'Cache memory' is shown as lines\n    \n     L_0, \\dots, L_{m-1}\n    \n    , each of size\n    \n     b\n    \n    bits. The tag field for each line is\n    \n     t\n    \n    bits. Arrows show a one-to-one mapping:\n    \n     B_0\n    \n    maps to\n    \n     L_0\n    \n    ,\n    \n     B_1\n    \n    maps to\n    \n     L_1\n    \n    , and so on, up to\n    \n     B_{m-1}\n    \n    mapping to\n    \n     L_{m-1}\n    \n    .\n\n\nDiagram (a) Direct mapping showing the mapping of main memory blocks to cache lines.\n\n\n(a) Direct mapping\n\n\n\n\n![Diagram (b) Associative mapping showing the mapping of a main memory block to any cache line.](images/image_0059.jpeg)\n\n\nDiagram (b) illustrates associative mapping. A single 'One block of main memory' of size\n    \n     b\n    \n    bits is shown on the left. On the right, the 'Cache memory' is shown as lines\n    \n     L_0, \\dots, L_{m-1}\n    \n    , each of size\n    \n     b\n    \n    bits with a tag field of\n    \n     t\n    \n    bits. Multiple arrows from the main memory block point to different lines in the cache, indicating that any block can be placed in any line.\n\n\nDiagram (b) Associative mapping showing the mapping of a main memory block to any cache line.\n\n\n(b) Associative mapping\n\n\n**Figure 4.8**\nof main memory map into the cache in the same fashion; that is, block\n   \n    B_m\n   \n   of main memory maps into line\n   \n    L_0\n   \n   of cache, block\n   \n    B_{m+1}\n   \n   maps into line\n   \n    L_1\n   \n   , and so on.\n\n\nThe mapping function is easily implemented using the main memory address. Figure 4.9 illustrates the general mechanism. For purposes of cache access, each main memory address can be viewed as consisting of three fields. The least significant\n   \n    w\n   \n   bits identify a unique word or byte within a block of main memory; in most contemporary machines, the address is at the byte level. The remaining\n   \n    s\n   \n   bits specify one of the\n   \n    2^s\n   \n   blocks of main memory. The cache logic interprets these\n   \n    s\n   \n   bits as a tag of\n   \n    s - r\n   \n   bits (most significant portion) and a line field of\n   \n    r\n   \n   bits. This latter field identifies one of the\n   \n    m = 2^r\n   \n   lines of the cache. To summarize,\n\n\n  * ■ Address length =\n    \n     (s + w)\n    \n    bits\n  * ■ Number of addressable units =\n    \n     2^{s+w}\n    \n    words or bytes\n  * ■ Block size = line size =\n    \n     2^w\n    \n    words or bytes\n  * ■ Number of blocks in main memory =\n    \n     \\frac{2^{s+w}}{2^w} = 2^s\n  * ■ Number of lines in cache =\n    \n     m = 2^r\n  * ■ Size of cache =\n    \n     2^{r+w}\n    \n    words or bytes\n  * ■ Size of tag =\n    \n     (s - r)\n    \n    bits\n\n\n\n\n![](images/image_0060.jpeg)\n\n\n**Figure 4.9**\n    Direct-Mapping Cache Organization\n\n\n**EXAMPLE 4.2a**\n    Figure 4.10 shows our example system using direct mapping.\n    \n     5\n    \n    In the example,\n    \n     m = 16\\text{K} = 2^{14}\n    \n    and\n    \n     i = j \\text{ modulo } 2^{14}\n    \n    . The mapping becomes\n\n\nCache Line | Starting Memory Address of Block\n0 | 000000, 010000, ..., FF0000\n1 | 000004, 010004, ..., FF0004\n⋮ | ⋮\n2^{14} - 1 | 00FFFF, 01FFFF, ..., FFFFFC\n\n\nNote that no two blocks that map into the same line number have the same tag number. Thus, blocks with starting addresses 000000, 010000, ..., FF0000 have tag numbers 00, 01, ..., FF, respectively.\n\n\nReferring back to Figure 4.5, a read operation works as follows. The cache system is presented with a 24-bit address. The 14-bit line number is used as an index into the cache to access a particular line. If the 8-bit tag number matches the tag number currently stored in that line, then the 2-bit word number is used to select one of the 4 bytes in that line. Otherwise, the 22-bit tag-plus-line field is used to fetch a block from main memory. The actual address that is used for the fetch is the 22-bit tag-plus-line concatenated with two 0 bits, so that 4 bytes are fetched starting on a block boundary.\n\n\n5\n    \n    In this and subsequent figures, memory values are represented in hexadecimal notation. See Chapter 9 for a basic refresher on number systems (decimal, binary, hexadecimal).\n\n\n\n\n![](images/image_0061.jpeg)\n\n\n**Main memory address (binary)**\n\n\n\nTag (hex) | Tag | Line + Word | Data\n00 | 00000000000000000000000000000000 | 00000000000000000000000000000100 | 13579246\n00 | 00000000000000000000000000000000 | 00000000000000000000000000000100 | ...\n16 | 00010100000000000000000000000000 | 00010100000000000000000000000100 | 77777777\n        \n        11235813\n16 | 00010100000000000000000000000000 | 00010100010011100110011100 | FEDCBA98\n16 | 00010100000000000000000000000000 | 000101001111111111111111100 | 12345678\nFF | 11111111000000000000000000000000 | 11111111000000000000000000000100 | ...\nFF | 11111111000000000000000000000000 | 1111111111111111111111111100 | 11223344\n        \n        24682468\n\n\n**16K line cache**\n\n\n\nTag | Data | Line number\n00 | 13579246 | 0000\n16 | 11235813 | 0001\n16 | FEDCBA98 | 0CE7\nFF | 11223344 | 3FFE\n16 | 12345678 | 3FFF\n\n\n8 bits      32 bits\n\n\n**16-Mb main memory**\n\n\n**Note:**\n     Memory address values are in binary representation; other values are in hexadecimal.\n\n\n**Main memory address =**\n\n\n\nTag | Line | Word\n\n\n8 bits      14 bits      2 bits\n\n\n**Figure 4.10**\n    Direct Mapping Example\n\n\nThe effect of this mapping is that blocks of main memory are assigned to lines of the cache as follows:\n\n\nCache line | Main memory blocks assigned\n0 | 0, m, 2m, \\dots, 2^s - m\n1 | 1, m + 1, 2m + 1, \\dots, 2^s - m + 1\n\\vdots | \\vdots\nm - 1 | m - 1, 2m - 1, 3m - 1, \\dots, 2^s - 1\n\n\nThus, the use of a portion of the address as a line number provides a unique mapping of each block of main memory into the cache. When a block is actually\n\n\nread into its assigned line, it is necessary to tag the data to distinguish it from other blocks that can fit into that line. The most significant\n   \n    s - r\n   \n   bits serve this purpose.\n\n\nThe direct mapping technique is simple and inexpensive to implement. Its main disadvantage is that there is a fixed cache location for any given block. Thus, if a program happens to reference words repeatedly from two different blocks that map into the same line, then the blocks will be continually swapped in the cache, and the hit ratio will be low (a phenomenon known as\n   *thrashing*\n   ).\n\n\n\n\n![Logo for Online Interactive Simulation, featuring a globe and the text 'www'.](images/image_0062.jpeg)\n\n\nLogo for Online Interactive Simulation, featuring a globe and the text 'www'.\n\n\n\n\n**Selective Victim Cache Simulator**\n\n\nOne approach to lower the\n   **miss**\n   penalty is to remember what was discarded in case it is needed again. Since the discarded data has already been fetched, it can be used again at a small cost. Such recycling is possible using a victim cache. Victim cache was originally proposed as an approach to reduce the conflict misses of direct mapped caches without affecting its fast access time. Victim cache is a fully associative cache, whose size is typically 4 to 16 cache lines, residing between a direct mapped L1 cache and the next level of memory. This concept is explored in Appendix F.\n\n\n**ASSOCIATIVE MAPPING**\n   Associative mapping overcomes the disadvantage of direct mapping by permitting each main memory block to be loaded into any line of the cache (Figure 4.8b). In this case, the cache control logic interprets a memory address simply as a Tag and a Word field. The Tag field uniquely identifies a block of main memory. To determine whether a block is in the cache, the cache control logic must simultaneously examine every line's tag for a match. Figure 4.11 illustrates the logic.\n\n\n\n\n![Diagram of Fully Associative Cache Organization showing the flow of memory addresses and data between main memory, a cache, and a victim cache.](images/image_0063.jpeg)\n\n\nThe diagram illustrates the Fully Associative Cache Organization. A\n    **Memory address**\n    is split into a\n    **Tag**\n    field of size\n    \n     s\n    \n    and a\n    **Word**\n    field of size\n    \n     w\n    \n    . The\n    **Tag**\n    is compared against the\n    **Tag**\n    fields of all\n    \n     m\n    \n    lines in the\n    **Cache**\n    (labeled\n    \n     L_0, L_j, L_{m-1}\n    \n    ). The\n    **Word**\n    field is used to select a specific word from the main memory block\n    \n     B_0\n    \n    (containing words\n    \n     W0, W1, W2, W3, \\dots\n    \n    ) or block\n    \n     B_j\n    \n    (containing words\n    \n     W4j, W(4j+1), W(4j+2), W(4j+3), \\dots\n    \n    ). The comparison logic outputs a signal indicating a hit or miss. The\n    **Cache**\n    consists of\n    **Tag**\n    and\n    **Data**\n    fields for each line. The\n    **Victim Cache**\n    (labeled\n    \n     B_j\n    \n    ) is accessed when a conflict miss occurs, with its\n    **Tag**\n    also being compared against the incoming\n    **Tag**\n    . The final output is a\n    **Hit in cache**\n    or\n    **Miss in cache**\n    signal.\n\n\nDiagram of Fully Associative Cache Organization showing the flow of memory addresses and data between main memory, a cache, and a victim cache.\n\n\n**Figure 4.11**\n   Fully Associative Cache Organization\n\n\n**EXAMPLE 4.2b**\n    Figure 4.12 shows our example using associative mapping. A main memory address consists of a 22-bit tag and a 2-bit byte number. The 22-bit tag must be stored with the 32-bit block of data for each line in the cache. Note that it is the leftmost (most significant) 22 bits of the address that form the tag. Thus, the 24-bit hexadecimal address 16339C has the 22-bit tag 058CE7. This is easily seen in binary notation:\n\n\nMemory address | 0001 | 0110 | 0011 | 0011 | 1001 | 1100 | (binary)\n | 1 | 6 | 3 | 3 | 9 | C | (hex)\nTag (leftmost 22 bits) | 00 | 0101 | 1000 | 1100 | 1110 | 0111 | (binary)\n | 0 | 5 | 8 | C | E | 7 | (hex)\n\n\n\n\n![](images/image_0064.jpeg)\n\n\nMain memory address (binary)\n\n\nTag Word\n\n\n000000 000001\n\n\n00000000000000000000000000000000\n     \n\n     00000100000000000000000000000000\n\n\nData\n\n\n13579246\n\n\n058CE6 058CE7 058CE8\n\n\n00010100011001110011000\n     \n\n     00010100011001110011100\n     \n\n     00010100011001110011100\n\n\nFEDCBA98\n\n\n33333333 11223344 24682468\n\n\nTag Data Line number\n\n\n3FFFFE 058CE7 11223344 0000\n     \n\n     000000 FEDCBA98 0001\n\n\n3FFFFD 33333333 3FFD\n     \n\n     000000 13579246 3FFE\n     \n\n     3FFFFF 24682468 3FFF\n\n\n22 bits 32 bits\n     \n\n     16K line cache\n\n\nNote: Memory address values are in binary representation; other values are in hexadecimal.\n\n\n32 bits\n     \n\n     16-Mb main memory\n\n\nMain memory address =\n\n\nTag Word\n\n\n22 bits 2 bits\n\n\n**Figure 4.12**\n    Associative Mapping Example\n\n\nNote that no field in the address corresponds to the line number, so that the number of lines in the cache is not determined by the address format. To summarize,\n\n\n  * ■ Address length =\n    \n     (s + w)\n    \n    bits\n  * ■ Number of addressable units =\n    \n     2^{s+w}\n    \n    words or bytes\n  * ■ Block size = line size =\n    \n     2^w\n    \n    words or bytes\n  * ■ Number of blocks in main memory =\n    \n     \\frac{2^{s+w}}{2^w} = 2^s\n  * ■ Number of lines in cache = undetermined\n  * ■ Size of tag =\n    \n     s\n    \n    bits\n\n\nWith associative mapping, there is flexibility as to which block to replace when a new block is read into the cache. Replacement algorithms, discussed later in this section, are designed to maximize the hit ratio. The principal disadvantage of associative mapping is the complex circuitry required to examine the tags of all cache lines in parallel.\n\n\n\n\n![Online Interactive Simulation logo featuring a globe and the text 'Online Interactive Simulation' and 'www'.](images/image_0065.jpeg)\n\n\nOnline Interactive Simulation logo featuring a globe and the text 'Online Interactive Simulation' and 'www'.\n\n\n\n\n**Cache Time Analysis Simulator**\n\n\n**SET-ASSOCIATIVE MAPPING**\n   Set-associative mapping is a compromise that exhibits the strengths of both the direct and associative approaches while reducing their disadvantages.\n\n\nIn this case, the cache consists of number sets, each of which consists of a number of lines. The relationships are\n\n\nm = v \\times k\n\n\ni = j \\text{ modulo } v\n\n\nwhere\n\n\ni\n   \n   = cache set number\n\n\nj\n   \n   = main memory block number\n\n\nm\n   \n   = number of lines in the cache\n\n\nv\n   \n   = number of sets\n\n\nk\n   \n   = number of lines in each set\n\n\nThis is referred to as\n   \n    k\n   \n   -way set-associative mapping. With set-associative mapping, block\n   \n    B_j\n   \n   can be mapped into any of the lines of set\n   \n    j\n   \n   . Figure 4.13a illustrates this mapping for the first\n   \n    v\n   \n   blocks of main memory. As with associative mapping, each word maps into multiple cache lines. For set-associative mapping, each word maps into all the cache lines in a specific set, so that main memory block\n   \n    B_0\n   \n   maps into set 0, and so on. Thus, the set-associative cache can be physically implemented as\n   \n    v\n   \n   associative caches. It is also possible to implement the set-associative cache as\n   \n    k\n   \n   direct mapping caches, as shown in Figure 4.13b. Each direct-mapped cache is referred to as a\n   *way*\n   , consisting of\n   \n    v\n   \n   lines. The first\n   \n    v\n   \n   lines of main memory are direct mapped into the\n   \n    v\n   \n   lines of each way; the next group of\n   \n    v\n   \n   lines of main memory are similarly mapped, and so on. The direct-mapped implementation is typically used\n\n\n\n\n![Figure 4.13: Mapping from Main Memory to Cache: k-Way Set Associative. (a) v associative-mapped caches: Main memory blocks B0 to B_{v-1} are mapped to Cache memory-set 0 to Cache memory-set v-1. (b) k direct-mapped caches: Main memory blocks B0 to B_{v-1} are mapped to Cache memory-way 1 to Cache memory-way k.](images/image_0066.jpeg)\n\n\n(a)\n    \n     v\n    \n    associative-mapped caches\n\n\n(b)\n    \n     k\n    \n    direct-mapped caches\n\n\nFigure 4.13: Mapping from Main Memory to Cache: k-Way Set Associative. (a) v associative-mapped caches: Main memory blocks B0 to B_{v-1} are mapped to Cache memory-set 0 to Cache memory-set v-1. (b) k direct-mapped caches: Main memory blocks B0 to B_{v-1} are mapped to Cache memory-way 1 to Cache memory-way k.\n\n\n**Figure 4.13**\n   Mapping from Main Memory to Cache:\n   \n    k\n   \n   -Way Set Associative\n\n\nfor small degrees of associativity (small values of\n   \n    k\n   \n   ) while the associative-mapped implementation is typically used for higher degrees of associativity [JACO08].\n\n\nFor set-associative mapping, the cache control logic interprets a memory address as three fields: Tag, Set, and Word. The\n   \n    d\n   \n   set bits specify one of\n   \n    v = 2^d\n   \n   sets. The\n   \n    s\n   \n   bits of the Tag and Set fields specify one of the\n   \n    2^s\n   \n   blocks of main memory. Figure 4.14 illustrates the cache control logic. With fully associative mapping, the tag in a memory address is quite large and must be compared to the tag of every line in the cache. With\n   \n    k\n   \n   -way set-associative mapping, the tag in a memory address is much smaller and is only compared to the\n   \n    k\n   \n   tags within a single set. To summarize,\n\n\n  * ■ Address length =\n    \n     (s + w)\n    \n    bits\n  * ■ Number of addressable units =\n    \n     2^{s+w}\n    \n    words or bytes\n\n\n\n\n![Diagram of k-Way Set-Associative Cache Organization. A memory address is split into Tag (s-d bits), Set (d bits), and Word (w bits). The Set bits are used to select a set in the cache. The Tag bits are compared with the tags of all k lines in the selected set. If a match is found, it's a hit; otherwise, it's a miss. The Word bits are used to select a specific word within the block. The cache is organized into sets, each containing k lines. Main memory blocks are mapped to cache sets based on the set number.](images/image_0067.jpeg)\n\n\nThe diagram illustrates the\n    \n     k\n    \n    -Way Set-Associative Cache Organization. A memory address is divided into three fields: Tag (length\n    \n     s-d\n    \n    ), Set (length\n    \n     d\n    \n    ), and Word (length\n    \n     w\n    \n    ). The Set field is used to select a specific set within the cache. The Tag field is compared against the tags of all\n    \n     k\n    \n    lines in the selected set. The Word field is used to select a specific word within the block. The cache is organized into sets, each containing\n    \n     k\n    \n    lines. Main memory blocks are mapped to cache sets based on the set number. The diagram shows two sets, Set 0 and Set 1, each containing\n    \n     k\n    \n    lines. The lines in Set 0 are labeled\n    \n     F_0, F_1, \\dots, F_{k-1}\n    \n    , and the lines in Set 1 are labeled\n    \n     F_k, F_{k+1}, \\dots, F_{2k-1}\n    \n    . The main memory is shown as a stack of blocks\n    \n     B_0, B_1, \\dots, B_j, \\dots\n    \n    . The address\n    \n     s+w\n    \n    is used to access a block in main memory. The cache and main memory are connected via a bus with a width of\n    \n     s+w\n    \n    . The diagram also shows the logic for determining a hit or miss: if any line in the set matches the tag, it's a hit; otherwise, it's a miss.\n\n\nDiagram of k-Way Set-Associative Cache Organization. A memory address is split into Tag (s-d bits), Set (d bits), and Word (w bits). The Set bits are used to select a set in the cache. The Tag bits are compared with the tags of all k lines in the selected set. If a match is found, it's a hit; otherwise, it's a miss. The Word bits are used to select a specific word within the block. The cache is organized into sets, each containing k lines. Main memory blocks are mapped to cache sets based on the set number.\n\n\n**Figure 4.14**\n\n    k\n   \n   -Way Set-Associative Cache Organization\n\n\n  * ■ Block size = line size =\n    \n     2^w\n    \n    words or bytes\n  * ■ Number of blocks in main memory =\n    \n     \\frac{2^{s+w}}{2^w} = 2^s\n  * ■ Number of lines in set =\n    \n     k\n  * ■ Number of sets =\n    \n     v = 2^d\n  * ■ Number of lines in cache =\n    \n     m = kv = k \\times 2^d\n  * ■ Size of cache =\n    \n     k \\times 2^{d+w}\n    \n    words or bytes\n  * ■ Size of tag =\n    \n     (s - d)\n    \n    bits\n\n\n**EXAMPLE 4.2c**\n   Figure 4.15 shows our example using set-associative mapping with two lines in each set, referred to as two-way set-associative. The 13-bit set number identifies a unique set of two lines within the cache. It also gives the number of the block in main memory, modulo\n   \n    2^{13}\n   \n   . This determines the mapping of blocks into lines. Thus, blocks 000000, 008000, ..., FF8000 of two memory map into cache set 0. Any of those blocks can be loaded into either of the two lines in the set. Note that no two blocks that map into the same cache set have the same tag number. For a read operation, the 13-bit set number is used to determine which set of two lines is to be examined. Both lines in the set are examined for a match with the tag number of the address to be accessed.\n\n\n\n\n![](images/image_0068.jpeg)\n\n\n**Main memory address (binary)**\n\n\n**Tag (hex)**\n**Tag**\n**Set + Word**\n**Data**\n\n\n000 00000000000000000000000000000000 13579246\n\n\n000 000000000000000000000000000000001000\n\n\n000 0000000011111111111111111000\n\n\n000 0000000011111111111111111100\n\n\n02C 00010110000000000000000000000000 77777777\n\n\n02C 000101100000000000000000000000001000 11235813\n\n\n02C 000101100011001110011100111000 FEDCBA98\n\n\n02C 00010110011111111111111111111000 12345678\n\n\n1FF 11111111111000000000000000000000\n\n\n1FF 111111111110000000000000000000001000\n\n\n1FF 11111111111111111111111111111000 11223344\n\n\n1FF 11111111111111111111111111111100 24682468\n\n\n**16-Mb main memory**\n**32 bits**\n\n\n**Main memory address =**\n\n\n**Tag**\n**Set**\n**Word**\n\n\n9 bits 13 bits 2 bits\n\n\n**Set number**\n\n\n**Tag Data Tag Data**\n\n\n000 13579246 0000 02C 77777777\n\n\n02C 11235813 0001\n\n\n02C FEDCBA98 0CE7\n\n\n1FF 11223344 1FFE\n\n\n02C 12345678 1FFF\n\n\n1FF 24682468\n\n\n9 bits 32 bits 9 bits 32 bits\n\n\n**16K line cache**\n\n\n*Note: Memory address values are in binary representation; other values are in hexadecimal.*\n\n\n**Figure 4.15**\n   Two-Way Set-Associative Mapping Example\n\n\n\n\n![Bar chart showing Hit ratio versus Cache size (bytes) for different cache associativities: Direct, Two-way, Four-way, Eight-way, and Sixteen-way. The hit ratio increases with cache size and associativity, leveling off around 0.95 for sizes of 32k and above.](images/image_0069.jpeg)\n\n\nCache size (bytes) | Direct | Two-way | Four-way | Eight-way | Sixteen-way\n1k | 0.48 | 0.49 | 0.50 | 0.50 | 0.50\n2k | 0.55 | 0.56 | 0.57 | 0.57 | 0.58\n4k | 0.65 | 0.66 | 0.67 | 0.68 | 0.69\n8k | 0.75 | 0.76 | 0.77 | 0.78 | 0.79\n16k | 0.85 | 0.86 | 0.87 | 0.88 | 0.89\n32k | 0.90 | 0.91 | 0.92 | 0.93 | 0.94\n64k | 0.92 | 0.93 | 0.94 | 0.95 | 0.95\n128k | 0.94 | 0.95 | 0.95 | 0.95 | 0.95\n256k | 0.95 | 0.95 | 0.95 | 0.95 | 0.95\n512k | 0.95 | 0.95 | 0.95 | 0.95 | 0.95\n1M | 0.95 | 0.95 | 0.95 | 0.95 | 0.95\n\n\nBar chart showing Hit ratio versus Cache size (bytes) for different cache associativities: Direct, Two-way, Four-way, Eight-way, and Sixteen-way. The hit ratio increases with cache size and associativity, leveling off around 0.95 for sizes of 32k and above.\n\n\n**Figure 4.16**\n   Varying Associativity over Cache Size\n\n\nIn the extreme case of\n   \n    v = m, k = 1\n   \n   , the set-associative technique reduces to direct mapping, and for\n   \n    v = 1, k = m\n   \n   , it reduces to associative mapping. The use of two lines per set (\n   \n    v = m/2, k = 2\n   \n   ) is the most common set-associative organization. It significantly improves the hit ratio over direct mapping. Four-way set associative (\n   \n    v = m/4, k = 4\n   \n   ) makes a modest additional improvement for a relatively small additional cost [MAYB84, HILL89]. Further increases in the number of lines per set have little effect.\n\n\nFigure 4.16 shows the results of one simulation study of set-associative cache performance as a function of cache size [GENU04]. The difference in performance between direct and two-way set associative is significant up to at least a cache size of 64 kB. Note also that the difference between two-way and four-way at 4 kB is much less than the difference in going from 4 kB to 8 kB in cache size. The complexity of the cache increases in proportion to the associativity, and in this case would not be justifiable against increasing cache size to 8 or even 16 kB. A final point to note is that beyond about 32 kB, increase in cache size brings no significant increase in performance.\n\n\nThe results of Figure 4.16 are based on simulating the execution of a GCC compiler. Different applications may yield different results. For example, [CANT01] reports on the results for cache performance using many of the CPU2000 SPEC benchmarks. The results of [CANT01] in comparing hit ratio to cache size follow the same pattern as Figure 4.16, but the specific values are somewhat different.\n\n\n\n\n![Logo for Online Interactive Simulator, featuring a globe and the text 'Online Interactive Simulator' and 'www'.](images/image_0070.jpeg)\n\n\nLogo for Online Interactive Simulator, featuring a globe and the text 'Online Interactive Simulator' and 'www'.\n\n\n\n\n**Replacement Algorithms**\n\n\nOnce the cache has been filled, when a new block is brought into the cache, one of the existing blocks must be replaced. For direct mapping, there is only one possible line for any particular block, and no choice is possible. For the associative and set-associative techniques, a replacement algorithm is needed. To achieve high speed, such an algorithm must be implemented in hardware. A number of algorithms have been tried. We mention four of the most common. Probably the most effective is\n   **least recently used (LRU)**\n   : Replace that block in the set that has been in the cache longest with no reference to it. For two-way set associative, this is easily implemented. Each line includes a USE bit. When a line is referenced, its USE bit is set to 1 and the USE bit of the other line in that set is set to 0. When a block is to be read into the set, the line whose USE bit is 0 is used. Because we are assuming that more recently used memory locations are more likely to be referenced, LRU should give the best hit ratio. LRU is also relatively easy to implement for a fully associative cache. The cache mechanism maintains a separate list of indexes to all the lines in the cache. When a line is referenced, it moves to the front of the list. For replacement, the line at the back of the list is used. Because of its simplicity of implementation, LRU is the most popular replacement algorithm.\n\n\nAnother possibility is first-in-first-out (FIFO): Replace that block in the set that has been in the cache longest. FIFO is easily implemented as a round-robin or circular buffer technique. Still another possibility is least frequently used (LFU): Replace that block in the set that has experienced the fewest references. LFU could be implemented by associating a counter with each line. A technique not based on usage (i.e., not LRU, LFU, FIFO, or some variant) is to pick a line at random from among the candidate lines. Simulation studies have shown that random replacement provides only slightly inferior performance to an algorithm based on usage [SMIT82].\n\n\n\n\n**Write Policy**\n\n\nWhen a block that is resident in the cache is to be replaced, there are two cases to consider. If the old block in the cache has not been altered, then it may be overwritten with a new block without first writing out the old block. If at least one write operation has been performed on a word in that line of the cache, then main memory must be updated by writing the line of cache out to the block of memory before bringing in the new block. A variety of write policies, with performance and economic trade-offs, is possible. There are two problems to contend with. First, more than one device may have access to main memory. For example, an I/O module may be able to read-write directly to memory. If a word has been altered only in the cache, then the corresponding memory word is invalid. Further, if the I/O device has altered main memory, then the cache word is invalid. A more complex problem occurs when multiple processors are attached to the same bus and each processor has its own local cache. Then, if a word is altered in one cache, it could conceivably invalidate a word in other caches.\n\n\nThe simplest technique is called\n   **write through**\n   . Using this technique, all write operations are made to main memory as well as to the cache, ensuring that main memory is always valid. Any other processor-cache module can monitor traffic to main memory to maintain consistency within its own cache. The main disadvantage\n\n\nof this technique is that it generates substantial memory traffic and may create a bottleneck. An alternative technique, known as\n   **write back**\n   , minimizes memory writes. With write back, updates are made only in the cache. When an update occurs, a\n   **dirty bit**\n   , or\n   **use bit**\n   , associated with the line is set. Then, when a block is replaced, it is written back to main memory if and only if the dirty bit is set. The problem with write back is that portions of main memory are invalid, and hence accesses by I/O modules can be allowed only through the cache. This makes for complex circuitry and a potential bottleneck. Experience has shown that the percentage of memory references that are writes is on the order of 15% [SMIT82]. However, for HPC applications, this number may approach 33% (vector-vector multiplication) and can go as high as 50% (matrix transposition).\n\n\n**EXAMPLE 4.3**\n   Consider a cache with a line size of 32 bytes and a main memory that requires 30 ns to transfer a 4-byte word. For any line that is written at least once before being swapped out of the cache, what is the average number of times that the line must be written before being swapped out for a write-back cache to be more efficient than a write-through cache?\n\n\nFor the write-back case, each dirty line is written back once, at swap-out time, taking\n   \n    8 \\times 30 = 240\n   \n   ns. For the write-through case, each update of the line requires that one word be written out to main memory, taking 30 ns. Therefore, if the average line that gets written at least once gets written more than 8 times before swap out, then write back is more efficient.\n\n\nIn a bus organization in which more than one device (typically a processor) has a cache and main memory is shared, a new problem is introduced. If data in one cache are altered, this invalidates not only the corresponding word in main memory, but also that same word in other caches (if any other cache happens to have that same word). Even if a write-through policy is used, the other caches may contain invalid data. A system that prevents this problem is said to maintain cache coherency. Possible approaches to cache coherency include the following:\n\n\n  * ■\n    **Bus watching with write through:**\n    Each cache controller monitors the address lines to detect write operations to memory by other bus masters. If another master writes to a location in shared memory that also resides in the cache memory, the cache controller invalidates that cache entry. This strategy depends on the use of a write-through policy by all cache controllers.\n  * ■\n    **Hardware transparency:**\n    Additional hardware is used to ensure that all updates to main memory via cache are reflected in all caches. Thus, if one processor modifies a word in its cache, this update is written to main memory. In addition, any matching words in other caches are similarly updated.\n  * ■\n    **Noncacheable memory:**\n    Only a portion of main memory is shared by more than one processor, and this is designated as noncacheable. In such a system, all accesses to shared memory are cache misses, because the shared memory is never copied into the cache. The noncacheable memory can be identified using chip-select logic or high-address bits.\n\n\nCache coherency is an active field of research. This topic is explored further in Part Five.\n\n\n\n\n**Line Size**\n\n\nAnother design element is the line size. When a block of data is retrieved and placed in the cache, not only the desired word but also some number of adjacent words are retrieved. As the block size increases from very small to larger sizes, the hit ratio will at first increase because of the principle of locality, which states that data in the vicinity of a referenced word are likely to be referenced in the near future. As the block size increases, more useful data are brought into the cache. The hit ratio will begin to decrease, however, as the block becomes even bigger and the probability of using the newly fetched information becomes less than the probability of reusing the information that has to be replaced. Two specific effects come into play:\n\n\n  * ■ Larger blocks reduce the number of blocks that fit into a cache. Because each block fetch overwrites older cache contents, a small number of blocks results in data being overwritten shortly after they are fetched.\n  * ■ As a block becomes larger, each additional word is farther from the requested word and therefore less likely to be needed in the near future.\n\n\nThe relationship between block size and hit ratio is complex, depending on the locality characteristics of a particular program, and no definitive optimum value has been found. A size of from 8 to 64 bytes seems reasonably close to optimum [SMIT87, PRZY88, PRZY90, HAND98]. For HPC systems, 64- and 128-byte cache line sizes are most frequently used.\n\n\n\n\n**Number of Caches**\n\n\nWhen caches were originally introduced, the typical system had a single cache. More recently, the use of multiple caches has become the norm. Two aspects of this design issue concern the number of levels of caches and the use of unified versus split caches.\n\n\n***MULTILEVEL CACHES***\n   As logic density has increased, it has become possible to have a cache on the same chip as the processor: the on-chip cache. Compared with a cache reachable via an external bus, the on-chip cache reduces the processor's external bus activity and therefore speeds up execution times and increases overall system performance. When the requested instruction or data is found in the on-chip cache, the bus access is eliminated. Because of the short data paths internal to the processor, compared with bus lengths, on-chip cache accesses will complete appreciably faster than would even zero-wait state bus cycles. Furthermore, during this period the bus is free to support other transfers.\n\n\nThe inclusion of an on-chip cache leaves open the question of whether an off-chip, or external, cache is still desirable. Typically, the answer is yes, and most contemporary designs include both on-chip and external caches. The simplest such organization is known as a two-level cache, with the internal level 1 (L1) and the external cache designated as level 2 (L2). The reason for including an L2 cache is the following: If there is no L2 cache and the processor makes an access request for a memory location not in the L1 cache, then the processor must access DRAM or\n\n\nROM memory across the bus. Due to the typically slow bus speed and slow memory access time, this results in poor performance. On the other hand, if an L2 SRAM (static RAM) cache is used, then frequently the missing information can be quickly retrieved. If the SRAM is fast enough to match the bus speed, then the data can be accessed using a zero-wait state transaction, the fastest type of bus transfer.\n\n\nTwo features of contemporary cache design for multilevel caches are noteworthy. First, for an off-chip L2 cache, many designs do not use the system bus as the path for transfer between the L2 cache and the processor, but use a separate data path, so as to reduce the burden on the system bus. Second, with the continued shrinkage of processor components, a number of processors now incorporate the L2 cache on the processor chip, improving performance.\n\n\nThe potential savings due to the use of an L2 cache depends on the hit rates in both the L1 and L2 caches. Several studies have shown that, in general, the use of a second-level cache does improve performance (e.g., see [AZIM92], [NOVI93], [HAND98]). However, the use of multilevel caches does complicate all of the design issues related to caches, including size, replacement algorithm, and write policy; see [HAND98] and [PEIR99] for discussions.\n\n\nFigure 4.17 shows the results of one simulation study of two-level cache performance as a function of cache size [GENU04]. The figure assumes that both caches have the same line size and shows the total hit ratio. That is, a hit is counted if the desired data appears in either the L1 or the L2 cache. The figure shows the impact of L2 on total hits with respect to L1 size. L2 has little effect on the total number of cache hits until it is at least double the L1 cache size. Note that the steepest part of the slope for an L1 cache of 8 kB is for an L2 cache of 16 kB. Again for an L1 cache of 16 kB, the steepest part of the curve is for an L2 cache size of 32 kB. Prior to that point, the L2 cache has little, if any, impact on total cache performance. The need for the L2 cache to be larger than\n\n\n\n\n![Figure 4.17: Total Hit Ratio (L1 and L2) for 8-kB and 16-kB L1 caches. The graph plots Hit ratio (y-axis, 0.78 to 0.98) against L2 cache size (bytes) (x-axis, 1k to 2M). Two curves are shown: L1 = 16k (dashed line) and L1 = 8k (solid line). The L1 = 8k curve starts at a hit ratio of ~0.85 and rises sharply, reaching ~0.96 at 1M L2 size. The L1 = 16k curve starts at a hit ratio of ~0.92 and rises more gradually, reaching ~0.96 at 2M L2 size.](images/image_0071.jpeg)\n\n\nL2 cache size (bytes) | Hit ratio (L1 = 8k) | Hit ratio (L1 = 16k)\n1k | 0.85 | 0.92\n2k | 0.85 | 0.92\n4k | 0.85 | 0.92\n8k | 0.85 | 0.92\n16k | 0.90 | 0.92\n32k | 0.94 | 0.93\n64k | 0.95 | 0.94\n128k | 0.96 | 0.95\n256k | 0.96 | 0.95\n512k | 0.96 | 0.95\n1M | 0.96 | 0.95\n2M | 0.96 | 0.96\n\n\nFigure 4.17: Total Hit Ratio (L1 and L2) for 8-kB and 16-kB L1 caches. The graph plots Hit ratio (y-axis, 0.78 to 0.98) against L2 cache size (bytes) (x-axis, 1k to 2M). Two curves are shown: L1 = 16k (dashed line) and L1 = 8k (solid line). The L1 = 8k curve starts at a hit ratio of ~0.85 and rises sharply, reaching ~0.96 at 1M L2 size. The L1 = 16k curve starts at a hit ratio of ~0.92 and rises more gradually, reaching ~0.96 at 2M L2 size.\n\n\n**Figure 4.17**\n   Total Hit Ratio (L1 and L2) for 8-kB and 16-kB L1\n\n\nthe L1 cache to affect performance makes sense. If the L2 cache has the same line size and capacity as the L1 cache, its contents will more or less mirror those of the L1 cache.\n\n\nWith the increasing availability of on-chip area available for cache, most contemporary microprocessors have moved the L2 cache onto the processor chip and added an L3 cache. Originally, the L3 cache was accessible over the external bus. More recently, most microprocessors have incorporated an on-chip L3 cache. In either case, there appears to be a performance advantage to adding the third level (e.g., see [GHAI98]). Further, large systems, such as the IBM mainframe zEnterprise systems, now incorporate 3 on-chip cache levels and a fourth level of cache shared across multiple chips [CURR11].\n\n\n**UNIFIED VERSUS SPLIT CACHES**\n   When the on-chip cache first made an appearance, many of the designs consisted of a single cache used to store references to both data and instructions. More recently, it has become common to split the cache into two: one dedicated to instructions and one dedicated to data. These two caches both exist at the same level, typically as two L1 caches. When the processor attempts to fetch an instruction from main memory, it first consults the instruction L1 cache, and when the processor attempts to fetch data from main memory, it first consults the data L1 cache.\n\n\nThere are two potential advantages of a unified cache:\n\n\n  * ■ For a given cache size, a unified cache has a higher hit rate than split caches because it balances the load between instruction and data fetches automatically. That is, if an execution pattern involves many more instruction fetches than data fetches, then the cache will tend to fill up with instructions, and if an execution pattern involves relatively more data fetches, the opposite will occur.\n  * ■ Only one cache needs to be designed and implemented.\n\n\nThe trend is toward split caches at the L1 and unified caches for higher levels, particularly for superscalar machines, which emphasize parallel instruction execution and the prefetching of predicted future instructions. The key advantage of the split cache design is that it eliminates contention for the cache between the instruction fetch/decode unit and the execution unit. This is important in any design that relies on the pipelining of instructions. Typically, the processor will fetch instructions ahead of time and fill a buffer, or pipeline, with instructions to be executed. Suppose now that we have a unified instruction/data cache. When the execution unit performs a memory access to load and store data, the request is submitted to the unified cache. If, at the same time, the instruction prefetcher issues a read request to the cache for an instruction, that request will be temporarily blocked so that the cache can service the execution unit first, enabling it to complete the currently executing instruction. This cache contention can degrade performance by interfering with efficient use of the instruction pipeline. The split cache structure overcomes this difficulty."
        },
        {
          "name": "Pentium 4 Cache Organization",
          "content": "The evolution of cache organization is seen clearly in the evolution of Intel microprocessors (Table 4.4). The 80386 does not include an on-chip cache. The 80486 includes a single on-chip cache of 8 kB, using a line size of 16 bytes and a four-way\n\n\n**Table 4.4**\n\nProblem | Solution | Processor on Which Feature First Appears\nExternal memory slower than the system bus. | Add external cache using faster memory technology. | 386\nIncreased processor speed results in external bus becoming a bottleneck for cache access. | Move external cache on-chip, operating at the same speed as the processor. | 486\nInternal cache is rather small, due to limited space on chip. | Add external L2 cache using faster technology than main memory. | 486\nContention occurs when both the Instruction Prefetcher and the Execution Unit simultaneously require access to the cache. In that case, the Prefetcher is stalled while the Execution Unit's data access takes place. | Create separate data and instruction caches. | Pentium\nIncreased processor speed results in external bus becoming a bottleneck for L2 cache access. | Create separate back-side bus that runs at higher speed than the main (front-side) external bus. The BSB is dedicated to the L2 cache. | Pentium Pro\nMove L2 cache on to the processor chip. | Pentium II\nSome applications deal with massive databases and must have rapid access to large amounts of data. The on-chip caches are too small. | Add external L3 cache. | Pentium III\nMove L3 cache on-chip. | Pentium 4\n\n\nset-associative organization. All of the Pentium processors include two on-chip L1 caches, one for data and one for instructions. For the Pentium 4, the L1 data cache is 16 kB, using a line size of 64 bytes and a four-way set-associative organization. The Pentium 4 instruction cache is described subsequently. The Pentium II also includes an L2 cache that feeds both of the L1 caches. The L2 cache is eight-way set associative with a size of 512 kB and a line size of 128 bytes. An L3 cache was added for the Pentium III and became on-chip with high-end versions of the Pentium 4.\n\n\nFigure 4.18 provides a simplified view of the Pentium 4 organization, highlighting the placement of the three caches. The processor core consists of four major components:\n\n\n  * ■\n    **Fetch/decode unit:**\n    Fetches program instructions in order from the L2 cache, decodes these into a series of micro-operations, and stores the results in the L1 instruction cache.\n  * ■\n    **Out-of-order execution logic:**\n    Schedules execution of the micro-operations subject to data dependencies and resource availability; thus, micro-operations may be scheduled for execution in a different order than they were fetched from the instruction stream. As time permits, this unit schedules speculative execution of micro-operations that may be required in the future.\n\n\n\n\n![Pentium 4 Block Diagram showing internal architecture and cache hierarchy.](images/image_0072.jpeg)\n\n\nThe diagram illustrates the internal architecture of the Pentium 4 processor, highlighting its out-of-order execution engine and multi-level cache hierarchy.\n\n\n**Internal Execution Engine:**\n\n\n  * **Out-of-order execution logic**\n     (top center) coordinates the execution units.\n  * **L1 instruction cache (12K μops)**\n     feeds the\n     **Instruction fetch/decode unit**\n     .\n  * The\n     **Instruction fetch/decode unit**\n     provides instructions to the\n     **Out-of-order execution logic**\n     .\n  * The\n     **Out-of-order execution logic**\n     manages the\n     **Integer register file**\n     and\n     **FP register file**\n     .\n  * The\n     **Integer register file**\n     feeds the\n     **Load address unit**\n     ,\n     **Store address unit**\n     ,\n     **Simple integer ALU**\n     , and\n     **Complex integer ALU**\n     .\n  * The\n     **FP register file**\n     feeds the\n     **FP/ MMX unit**\n     and\n     **FP move unit**\n     .\n  * The\n     **Load address unit**\n     and\n     **Store address unit**\n     feed the\n     **L1 data cache (16 kB)**\n     .\n  * The\n     **Simple integer ALU**\n     ,\n     **Complex integer ALU**\n     ,\n     **FP/ MMX unit**\n     , and\n     **FP move unit**\n     feed the\n     **L1 data cache (16 kB)**\n     .\n\n\n**Cache Hierarchy:**\n\n\n  * **L1 data cache (16 kB)**\n     is the primary data cache, connected to the execution engine and the\n     **L2 cache (512 kB)**\n     .\n  * **L2 cache (512 kB)**\n     is the secondary cache, connected to the\n     **L1 data cache**\n     and the\n     **L3 cache (1 MB)**\n     .\n  * **L3 cache (1 MB)**\n     is the tertiary cache, connected to the\n     **L2 cache**\n     and the\n     **System bus**\n     .\n\n\n**External Connections:**\n\n\n  * The\n     **System bus**\n     connects to the\n     **L3 cache (1 MB)**\n     .\n  * A\n     **64 bits**\n     wide bus connects the\n     **Instruction fetch/decode unit**\n     to the\n     **L3 cache (1 MB)**\n     .\n  * A\n     **256 bits**\n     wide bus connects the\n     **L1 data cache (16 kB)**\n     to the\n     **L2 cache (512 kB)**\n     .\n\n\nPentium 4 Block Diagram showing internal architecture and cache hierarchy.\n\n\n**Figure 4.18**\n   Pentium 4 Block Diagram\n\n\n**Table 4.5**\n\nControl Bits | Operating Mode\nCD | NW | Cache Fills | Write Throughs | Invalidates\n0 | 0 | Enabled | Enabled | Enabled\n1 | 0 | Disabled | Enabled | Enabled\n1 | 1 | Disabled | Disabled | Disabled\n\n\nNote: CD = 0; NW = 1 is an invalid combination.\n\n\n  * ■\n    **Execution units:**\n    These units execute micro-operations, fetching the required data from the L1 data cache and temporarily storing results in registers.\n  * ■\n    **Memory subsystem:**\n    This unit includes the L2 and L3 caches and the system bus, which is used to access main memory when the L1 and L2 caches have a cache miss and to access the system I/O resources.\n\n\nUnlike the organization used in all previous Pentium models, and in most other processors, the Pentium 4 instruction cache sits between the instruction decode logic and the execution core. The reasoning behind this design decision is as follows: As discussed more fully in Chapter 16, the Pentium process decodes, or translates, Pentium machine instructions into simple RISC-like instructions called micro-operations. The use of simple, fixed-length micro-operations enables the use of superscalar pipelining and scheduling techniques that enhance performance. However, the Pentium machine instructions are cumbersome to decode; they have a variable number of bytes and many different options. It turns out that performance is enhanced if this decoding is done independently of the scheduling and pipelining logic. We return to this topic in Chapter 16.\n\n\nThe data cache employs a write-back policy: Data are written to main memory only when they are removed from the cache and there has been an update. The Pentium 4 processor can be dynamically configured to support write-through caching.\n\n\nThe L1 data cache is controlled by two bits in one of the control registers, labeled the CD (cache disable) and NW (not write-through) bits (Table 4.5). There are also two Pentium 4 instructions that can be used to control the data cache: INVD invalidates (flushes) the internal cache memory and signals the external cache (if any) to invalidate. WBINVD writes back and invalidates internal cache and then writes back and invalidates external cache.\n\n\nBoth the L2 and L3 caches are eight-way set-associative with a line size of 128 bytes."
        }
      ]
    },
    {
      "name": "Internal Memory",
      "sections": [
        {
          "name": "Semiconductor Main Memory",
          "content": "In earlier computers, the most common form of random-access storage for computer main memory employed an array of doughnut-shaped ferromagnetic loops referred to as\n   *cores*\n   . Hence, main memory was often referred to as\n   *core*\n   , a term that persists to this day. The advent of, and advantages of, microelectronics has long since vanquished the magnetic core memory. Today, the use of semiconductor chips for main memory is almost universal. Key aspects of this technology are explored in this section.\n\n\n\n\n**Organization**\n\n\nThe basic element of a\n   **semiconductor memory**\n   is the memory cell. Although a variety of electronic technologies are used, all semiconductor memory cells share certain properties:\n\n\n  * ■ They exhibit two stable (or semistable) states, which can be used to represent binary 1 and 0.\n  * ■ They are capable of being written into (at least once), to set the state.\n  * ■ They are capable of being read to sense the state.\n\n\nFigure 5.1 depicts the operation of a memory cell. Most commonly, the cell has three functional terminals capable of carrying an electrical signal. The select terminal, as the name suggests, selects a memory cell for a read or write operation. The control terminal indicates read or write. For writing, the other terminal provides an electrical signal that sets the state of the cell to 1 or 0. For reading, that terminal is used for output of the cell's state. The details of the internal organization, functioning, and timing of the memory cell depend on the specific integrated circuit technology used and are beyond the scope of this book, except for a brief summary. For our purposes, we will take it as given that individual cells can be selected for reading and writing operations.\n\n\n\n\n![Figure 5.1: Memory Cell Operation. (a) Write: A 'Control' signal points to a 'Cell' block. A 'Select' signal points to the 'Cell' from the left, and 'Data in' points to the 'Cell' from the right. (b) Read: A 'Control' signal points to a 'Cell' block. A 'Select' signal points to the 'Cell' from the left, and a 'Sense' signal points away from the 'Cell' to the right.](images/image_0073.jpeg)\n\n\nFigure 5.1: Memory Cell Operation. (a) Write: A 'Control' signal points to a 'Cell' block. A 'Select' signal points to the 'Cell' from the left, and 'Data in' points to the 'Cell' from the right. (b) Read: A 'Control' signal points to a 'Cell' block. A 'Select' signal points to the 'Cell' from the left, and a 'Sense' signal points away from the 'Cell' to the right.\n\n\n**Figure 5.1**\n   Memory Cell Operation\n\n\n\n\n**DRAM and SRAM**\n\n\nAll of the memory types that we will explore in this chapter are random access. That is, individual words of memory are directly accessed through wired-in addressing logic.\n\n\nTable 5.1 lists the major types of semiconductor memory. The most common is referred to as\n   **random-access memory (RAM)**\n   . This is, in fact, a misuse of the term, because all of the types listed in the table are random access. One distinguishing characteristic of memory that is designated as RAM is that it is possible both to read data from the memory and to write new data into the memory easily and rapidly. Both the reading and writing are accomplished through the use of electrical signals.\n\n\nThe other distinguishing characteristic of traditional RAM is that it is volatile. A RAM must be provided with a constant power supply. If the power is interrupted, then the data are lost. Thus, RAM can be used only as temporary storage. The two traditional forms of RAM used in computers are DRAM and SRAM. Newer forms of RAM, discussed in Section 5.5, are nonvolatile.\n\n\n**DYNAMIC RAM**\n   RAM technology is divided into two technologies: dynamic and static. A\n   **dynamic RAM (DRAM)**\n   is made with cells that store data as charge on capacitors. The presence or absence of charge in a capacitor is interpreted as a binary 1 or 0. Because capacitors have a natural tendency to discharge, dynamic RAMs require periodic charge refreshing to maintain data storage. The term\n\n\n**Table 5.1**\n   Semiconductor Memory Types\n\n\n\nMemory Type | Category | Erasure | Write Mechanism | Volatility\nRandom-access memory (RAM) | Read-write memory | Electrically, byte-level | Electrically | Nonvolatile\nRead-only memory (ROM) | Read-only memory | Not possible | Masks\nProgrammable ROM (PROM) |  |  | \nErasable PROM (EPROM) | Read-mostly memory | UV light, chip-level | Electrically\nElectrically Erasable PROM (EEPROM) | Electrically, byte-level\nFlash memory | Electrically, block-level | \n\n\n*dynamic*\n   refers to this tendency of the stored charge to leak away, even with power continuously applied.\n\n\nFigure 5.2a is a typical DRAM structure for an individual cell that stores one bit. The address line is activated when the bit value from this cell is to be read or written. The transistor acts as a switch that is closed (allowing current to flow) if a voltage is applied to the address line and open (no current flows) if no voltage is present on the address line.\n\n\nFor the write operation, a voltage signal is applied to the bit line; a high voltage represents 1, and a low voltage represents 0. A signal is then applied to the address line, allowing a charge to be transferred to the capacitor.\n\n\nFor the read operation, when the address line is selected, the transistor turns on and the charge stored on the capacitor is fed out onto a bit line and to a sense amplifier. The sense amplifier compares the capacitor voltage to a reference value and determines if the cell contains a logic 1 or a logic 0. The readout from the cell discharges the capacitor, which must be restored to complete the operation.\n\n\nAlthough the DRAM cell is used to store a single bit (0 or 1), it is essentially an analog device. The capacitor can store any charge value within a range; a threshold value determines whether the charge is interpreted as 1 or 0.\n\n\n**STATIC RAM**\n   In contrast, a\n   **static RAM (SRAM)**\n   is a digital device that uses the same logic elements used in the processor. In a SRAM, binary values are stored using traditional flip-flop logic-gate configurations (see Chapter 11 for a description of flip-flops). A static RAM will hold its data as long as power is supplied to it.\n\n\nFigure 5.2b is a typical SRAM structure for an individual cell. Four transistors (\n   \n    T_1, T_2, T_3, T_4\n   \n   ) are cross connected in an arrangement that produces a stable logic\n\n\n\n\n![Figure 5.2: Typical Memory Cell Structures. (a) Dynamic RAM (DRAM) cell: A transistor connected between a bit line (B) and a storage capacitor. The gate of the transistor is connected to an address line. The capacitor is connected to ground. (b) Static RAM (SRAM) cell: A 6-transistor (6T) SRAM cell. It consists of two cross-coupled inverters (formed by transistors T1, T2 and T3, T4) and two access transistors (T5, T6). The access transistors connect the cross-coupled nodes to a bit line (B) and an address line. A dc voltage source is connected to the gates of T3 and T4, and the circuit is connected to ground.](images/image_0074.jpeg)\n\n\n(a) Dynamic RAM (DRAM) cell\n    \n\n     (b) Static RAM (SRAM) cell\n\n\nFigure 5.2: Typical Memory Cell Structures. (a) Dynamic RAM (DRAM) cell: A transistor connected between a bit line (B) and a storage capacitor. The gate of the transistor is connected to an address line. The capacitor is connected to ground. (b) Static RAM (SRAM) cell: A 6-transistor (6T) SRAM cell. It consists of two cross-coupled inverters (formed by transistors T1, T2 and T3, T4) and two access transistors (T5, T6). The access transistors connect the cross-coupled nodes to a bit line (B) and an address line. A dc voltage source is connected to the gates of T3 and T4, and the circuit is connected to ground.\n\n\n**Figure 5.2**\n   Typical Memory Cell Structures\n\n\nstate. In logic state 1, point\n   \n    C_1\n   \n   is high and point\n   \n    C_2\n   \n   is low; in this state,\n   \n    T_1\n   \n   and\n   \n    T_4\n   \n   are off and\n   \n    T_2\n   \n   and\n   \n    T_3\n   \n   are on.\n   \n    1\n   \n   In logic state 0, point\n   \n    C_1\n   \n   is low and point\n   \n    C_2\n   \n   is high; in this state,\n   \n    T_1\n   \n   and\n   \n    T_4\n   \n   are on and\n   \n    T_2\n   \n   and\n   \n    T_3\n   \n   are off. Both states are stable as long as the direct current (dc) voltage is applied. Unlike the DRAM, no refresh is needed to retain data.\n\n\nAs in the DRAM, the SRAM address line is used to open or close a switch. The address line controls two transistors (\n   \n    T_5\n   \n   and\n   \n    T_6\n   \n   ). When a signal is applied to this line, the two transistors are switched on, allowing a read or write operation. For a write operation, the desired bit value is applied to line B, while its complement is applied to line\n   \n    \\bar{B}\n   \n   . This forces the four transistors (\n   \n    T_1\n   \n   ,\n   \n    T_2\n   \n   ,\n   \n    T_3\n   \n   ,\n   \n    T_4\n   \n   ) into the proper state. For a read operation, the bit value is read from line B.\n\n\n**SRAM VERSUS DRAM**\n   Both static and dynamic RAMs are volatile; that is, power must be continuously supplied to the memory to preserve the bit values. A dynamic memory cell is simpler and smaller than a static memory cell. Thus, a DRAM is more dense (smaller cells = more cells per unit area) and less expensive than a corresponding SRAM. On the other hand, a DRAM requires the supporting refresh circuitry. For larger memories, the fixed cost of the refresh circuitry is more than compensated for by the smaller variable cost of DRAM cells. Thus, DRAMs tend to be favored for large memory requirements. A final point is that SRAMs are somewhat faster than DRAMs. Because of these relative characteristics, SRAM is used for cache memory (both on and off chip), and DRAM is used for main memory.\n\n\n\n\n**Types of ROM**\n\n\nAs the name suggests, a\n   **read-only memory (ROM)**\n   contains a permanent pattern of data that cannot be changed. A ROM is nonvolatile; that is, no power source is required to maintain the bit values in memory. While it is possible to read a ROM, it is not possible to write new data into it. An important application of ROMs is microprogramming, discussed in Part Four. Other potential applications include\n\n\n  * ■ Library subroutines for frequently wanted functions\n  * ■ System programs\n  * ■ Function tables\n\n\nFor a modest-sized requirement, the advantage of ROM is that the data or program is permanently in main memory and need never be loaded from a secondary storage device.\n\n\nA ROM is created like any other integrated circuit chip, with the data actually wired into the chip as part of the fabrication process. This presents two problems:\n\n\n  * ■ The data insertion step includes a relatively large fixed cost, whether one or thousands of copies of a particular ROM are fabricated.\n  * ■ There is no room for error. If one bit is wrong, the whole batch of ROMs must be thrown out.\n\n\nWhen only a small number of ROMs with a particular memory content is needed, a less expensive alternative is the\n   **programmable ROM (PROM)**\n   . Like the\n\n\n1\n   \n   The circles associated with\n   \n    T_3\n   \n   and\n   \n    T_4\n   \n   in Figure 5.2b indicate signal negation.\n\n\nROM, the PROM is\n   **nonvolatile**\n   and may be written into only once. For the PROM, the writing process is performed electrically and may be performed by a supplier or customer at a time later than the original chip fabrication. Special equipment is required for the writing or “programming” process. PROMs provide flexibility and convenience. The ROM remains attractive for high-volume production runs.\n\n\nAnother variation on read-only memory is the\n   **read-mostly memory**\n   , which is useful for applications in which read operations are far more frequent than write operations but for which nonvolatile storage is required. There are three common forms of read-mostly memory: EPROM, EEPROM, and flash memory.\n\n\nThe optically\n   **erasable programmable read-only memory (EPROM)**\n   is read and written electrically, as with PROM. However, before a write operation, all the storage cells must be erased to the same initial state by exposure of the packaged chip to ultraviolet radiation. Erasure is performed by shining an intense ultraviolet light through a window that is designed into the memory chip. This erasure process can be performed repeatedly; each erasure can take as much as 20 minutes to perform. Thus, the EPROM can be altered multiple times and, like the ROM and PROM, holds its data virtually indefinitely. For comparable amounts of storage, the EPROM is more expensive than PROM, but it has the advantage of the multiple update capability.\n\n\nA more attractive form of read-mostly memory is\n   **electrically erasable programmable read-only memory (EEPROM)**\n   . This is a read-mostly memory that can be written into at any time without erasing prior contents; only the byte or bytes addressed are updated. The write operation takes considerably longer than the read operation, on the order of several hundred microseconds per byte. The EEPROM combines the advantage of nonvolatility with the flexibility of being updatable in place, using ordinary bus control, address, and data lines. EEPROM is more expensive than EPROM and also is less dense, supporting fewer bits per chip.\n\n\nAnother form of semiconductor memory is\n   **flash memory**\n   (so named because of the speed with which it can be reprogrammed). First introduced in the mid-1980s, flash memory is intermediate between EPROM and EEPROM in both cost and functionality. Like EEPROM, flash memory uses an electrical erasing technology. An entire flash memory can be erased in one or a few seconds, which is much faster than EPROM. In addition, it is possible to erase just blocks of memory rather than an entire chip. Flash memory gets its name because the microchip is organized so that a section of memory cells are erased in a single action or “flash.” However, flash memory does not provide byte-level erasure. Like EPROM, flash memory uses only one transistor per bit, and so achieves the high density (compared with EEPROM) of EPROM.\n\n\n\n\n**Chip Logic**\n\n\nAs with other integrated circuit products, semiconductor memory comes in packaged chips (Figure 1.11). Each chip contains an array of memory cells.\n\n\nIn the memory hierarchy as a whole, we saw that there are trade-offs among speed, density, and cost. These trade-offs also exist when we consider the organization of memory cells and functional logic on a chip. For semiconductor memories, one of the key design issues is the number of bits of data that may be read/written at a time. At one extreme is an organization in which the physical arrangement of cells in the array is the same as the logical arrangement (as perceived by the processor) of words in memory. The array is organized into\n   \n    W\n   \n   words of\n   \n    B\n   \n   bits each.\n\n\nFor example, a 16-Mbit chip could be organized as 1M 16-bit words. At the other extreme is the so-called 1-bit-per-chip organization, in which data are read/written one bit at a time. We will illustrate memory chip organization with a DRAM; ROM organization is similar, though simpler.\n\n\nFigure 5.3 shows a typical organization of a 16-Mbit DRAM. In this case, 4 bits are read or written at a time. Logically, the memory array is organized as four square arrays of 2048 by 2048 elements. Various physical arrangements are possible. In any case, the elements of the array are connected by both horizontal (row) and vertical (column) lines. Each horizontal line connects to the Select terminal of each cell in its row; each vertical line connects to the Data-In/Sense terminal of each cell in its column.\n\n\nAddress lines supply the address of the word to be selected. A total of\n   \n    \\log_2 W\n   \n   lines are needed. In our example, 11 address lines are needed to select one of 2048 rows. These 11 lines are fed into a row decoder, which has 11 lines of input and 2048 lines for output. The logic of the decoder activates a single one of the 2048 outputs depending on the bit pattern on the 11 input lines (\n   \n    2^{11} = 2048\n   \n   ).\n\n\nAn additional 11 address lines select one of 2048 columns of 4 bits per column. Four data lines are used for the input and output of 4 bits to and from a data buffer. On input (write), the bit driver of each bit line is activated for a 1 or 0 according to the value of the corresponding data line. On output (read), the value of each bit line is passed through a sense amplifier and presented to the data lines. The row line selects which row of cells is used for reading or writing.\n\n\n\n\n![Block diagram of a typical 16-Mbit DRAM (4M x 4) organization. The diagram shows the flow of address lines (A0-A10) through address buffers to a row decoder and a column decoder. A refresh counter feeds into a multiplexer (MUX) which selects between the row address buffer and the refresh counter. The row decoder and column decoder select a specific cell in the memory array (2048 x 2048 x 4). The memory array is connected to refresh circuitry. Data input and output are handled by data input and output buffers (D1-D4). Timing and control signals (RAS, CAS, WE, OE) are at the top.](images/image_0075.jpeg)\n\n\nThe diagram illustrates the internal structure of a 16-Mbit DRAM chip. It features 11 address input lines (A0 through A10) that are fed into two address buffers: a Row address buffer and a Column address buffer. The Row address buffer's output goes to a Row decoder, while the Column address buffer's output goes to a Column decoder. A Refresh counter provides a refresh address to a Multiplexer (MUX), which also receives the Row address from the Row address buffer. The MUX selects either the refresh address or the row address based on the control signals. The Row decoder and Column decoder work together to select a specific row and column in the Memory array, which is organized as four 2048x2048 arrays. The Memory array is connected to Refresh circuitry. Data input and output are managed by Data input buffer and Data output buffer blocks, which interface with four data lines (D1, D2, D3, D4). At the top, four control signals (RAS, CAS, WE, OE) are connected to a Timing and control block, which coordinates the operations of the various components.\n\n\nBlock diagram of a typical 16-Mbit DRAM (4M x 4) organization. The diagram shows the flow of address lines (A0-A10) through address buffers to a row decoder and a column decoder. A refresh counter feeds into a multiplexer (MUX) which selects between the row address buffer and the refresh counter. The row decoder and column decoder select a specific cell in the memory array (2048 x 2048 x 4). The memory array is connected to refresh circuitry. Data input and output are handled by data input and output buffers (D1-D4). Timing and control signals (RAS, CAS, WE, OE) are at the top.\n\n\n**Figure 5.3**\n   Typical 16-Mbit DRAM (4M\n   \n    \\times\n   \n   4)\n\n\nBecause only 4 bits are read/written to this DRAM, there must be multiple DRAMs connected to the memory controller to read/write a word of data to the bus.\n\n\nNote that there are only 11 address lines (A0–A10), half the number you would expect for a\n   \n    2048 \\times 2048\n   \n   array. This is done to save on the number of pins. The 22 required address lines are passed through select logic external to the chip and multiplexed onto the 11 address lines. First, 11 address signals are passed to the chip to define the row address of the array, and then the other 11 address signals are presented for the column address. These signals are accompanied by row address select (RAS) and column address select (CAS) signals to provide timing to the chip.\n\n\nThe write enable (WE) and output enable (OE) pins determine whether a write or read operation is performed. Two other pins, not shown in Figure 5.3, are ground (Vss) and a voltage source (Vcc).\n\n\nAs an aside, multiplexed addressing plus the use of square arrays result in a quadrupling of memory size with each new generation of memory chips. One more pin devoted to addressing doubles the number of rows and columns, and so the size of the chip memory grows by a factor of 4.\n\n\nFigure 5.3 also indicates the inclusion of refresh circuitry. All DRAMs require a refresh operation. A simple technique for refreshing is, in effect, to disable the DRAM chip while all data cells are refreshed. The refresh counter steps through all of the row values. For each row, the output lines from the refresh counter are supplied to the row decoder and the RAS line is activated. The data are read out and written back into the same location. This causes each cell in the row to be refreshed.\n\n\n\n\n**Chip Packaging**\n\n\nAs was mentioned in Chapter 2, an integrated circuit is mounted on a package that contains pins for connection to the outside world.\n\n\nFigure 5.4a shows an example EPROM package, which is an 8-Mbit chip organized as\n   \n    1M \\times 8\n   \n   . In this case, the organization is treated as a one-word-per-chip package. The package includes 32 pins, which is one of the standard chip package sizes. The pins support the following signal lines:\n\n\n  * ■ The address of the word being accessed. For 1M words, a total of 20 (\n    \n     2^{20} = 1M\n    \n    ) pins are needed (A0–A19).\n  * ■ The data to be read out, consisting of 8 lines (D0–D7).\n  * ■ The power supply to the chip (Vcc).\n  * ■ A ground pin (Vss).\n  * ■ A chip enable (CE) pin. Because there may be more than one memory chip, each of which is connected to the same address bus, the CE pin is used to indicate whether or not the address is valid for this chip. The CE pin is activated by logic connected to the higher-order bits of the address bus (i.e., address bits above A19). The use of this signal is illustrated presently.\n  * ■ A program voltage (\n    \n     V_{pp}\n    \n    ) that is supplied during programming (write operations).\n\n\nA typical DRAM pin configuration is shown in Figure 5.4b, for a 16-Mbit chip organized as\n   \n    4M \\times 4\n   \n   . There are several differences from a ROM chip. Because a RAM can be updated, the data pins are input/output. The write enable (WE) and output enable (OE) pins indicate whether this is a write or read operation.\n\n\n\n\n![Figure 5.4: Typical Memory Package Pins and Signals. (a) 8-Mbit EPROM pin diagram showing 32 pins, 16 address lines (A19-A0), 8 data lines (D7-D0), and power/ground lines (Vcc, Vss, Vpp). (b) 16-Mbit DRAM pin diagram showing 24 pins, 11 address lines (A10-A0), 4 data lines (D3-D0), and control lines (WE, RAS, CAS, OE, NC).](images/image_0076.jpeg)\n\n\n(a) 8-Mbit EPROM\n\n\n(b) 16-Mbit DRAM\n\n\nFigure 5.4: Typical Memory Package Pins and Signals. (a) 8-Mbit EPROM pin diagram showing 32 pins, 16 address lines (A19-A0), 8 data lines (D7-D0), and power/ground lines (Vcc, Vss, Vpp). (b) 16-Mbit DRAM pin diagram showing 24 pins, 11 address lines (A10-A0), 4 data lines (D3-D0), and control lines (WE, RAS, CAS, OE, NC).\n\n\n**Figure 5.4**\n   Typical Memory Package Pins and Signals\n\n\nBecause the DRAM is accessed by row and column, and the address is multiplexed, only 11 address pins are needed to specify the 4M row/column combinations (\n   \n    2^{11} \\times 2^{11} = 2^{22} = 4M\n   \n   ). The functions of the row address select (RAS) and column address select (CAS) pins were discussed previously. Finally, the no connect (NC) pin is provided so that there are an even number of pins.\n\n\n\n\n**Module Organization**\n\n\nIf a RAM chip contains only one bit per word, then clearly we will need at least a number of chips equal to the number of bits per word. As an example, Figure 5.5 shows how a memory module consisting of 256K 8-bit words could be organized. For 256K words, an 18-bit address is needed and is supplied to the module from some external source (e.g., the address lines of a bus to which the module is attached). The address is presented to 8 256K\n   \n    \\times\n   \n   1-bit chips, each of which provides the input/output of one bit.\n\n\nThis organization works as long as the size of memory equals the number of bits per chip. In the case in which larger memory is required, an array of chips is needed. Figure 5.6 shows the possible organization of a memory consisting of 1M word by 8 bits per word. In this case, we have four columns of chips, each column containing 256K words arranged as in Figure 5.5. For 1M word, 20 address lines are needed. The 18 least significant bits are routed to all 32 modules. The high-order 2 bits are input to a group select logic module that sends a chip enable signal to one of the four columns of modules.\n\n\n\n\n**Interleaved Memory**\n\n\nMain memory is composed of a collection of DRAM memory chips. A number of chips can be grouped together to form a\n   *memory bank*\n   . It is possible to organize the memory\n\n\n\n\n![Diagram of 256-KByte Memory Organization showing two interleaved banks of 512 words by 512 bits each, Chip #1 and Chip #8. A 10-bit Memory Address Register (MAR) provides addresses to both chips. Each chip has a 512-word decoder and a 512-bit-sense decoder. The outputs of the 512-bit-sense decoders feed into a Memory Buffer Register (MBR) with 8 slots.](images/image_0077.jpeg)\n\n\nThe diagram illustrates the organization of a 256-KByte memory system using two interleaved banks, Chip #1 and Chip #8. Each chip is a 512-word by 512-bit memory. A 10-bit Memory Address Register (MAR) provides the address to both chips. The MAR is shown as two 9-bit registers with a vertical ellipsis between them, indicating a 10-bit address. Each chip has a 'Decode 1 of 512' block and a 'Decode 1 of 512 bit-sense' block. The outputs of the 'Decode 1 of 512 bit-sense' blocks feed into a Memory Buffer Register (MBR) which consists of 8 slots numbered 1 through 8, with a vertical ellipsis between slots 5 and 6.\n\n\nDiagram of 256-KByte Memory Organization showing two interleaved banks of 512 words by 512 bits each, Chip #1 and Chip #8. A 10-bit Memory Address Register (MAR) provides addresses to both chips. Each chip has a 512-word decoder and a 512-bit-sense decoder. The outputs of the 512-bit-sense decoders feed into a Memory Buffer Register (MBR) with 8 slots.\n\n\n**Figure 5.5**\n   256-KByte Memory Organization\n\n\nbanks in a way known as interleaved memory. Each bank is independently able to service a memory read or write request, so that a system with\n   \n    K\n   \n   banks can service\n   \n    K\n   \n   requests simultaneously, increasing memory read or write rates by a factor of\n   \n    K\n   \n   . If consecutive words of memory are stored in different banks, then the transfer of a block of memory is speeded up. Appendix G explores the topic of interleaved memory.\n\n\n\n\n![Logo for Online Interactive Simulation, featuring a globe and the text 'Online Interactive Simulation' and 'www'.](images/image_0078.jpeg)\n\n\nLogo for Online Interactive Simulation, featuring a globe and the text 'Online Interactive Simulation' and 'www'.\n\n\n**Interleaved Memory Simulator**"
        },
        {
          "name": "Error Correction",
          "content": "A semiconductor memory system is subject to errors. These can be categorized as hard failures and soft errors. A\n   **hard failure**\n   is a permanent physical defect so that the memory cell or cells affected cannot reliably store data but become stuck at 0 or 1 or\n\n\n\n\n![Figure 5.6: 1-MB Memory Organization. The diagram shows a 1-MB memory organized into 8 groups of 128 words each. Each group contains 8 chips of 512 words each. The Memory Address Register (MAR) provides 11 address lines (bits 9, 9, 2) to select a group and a word within the group. The Chip group enable signal selects one of four groups (A, B, C, D). The Memory Buffer Register (MBR) provides 8 data lines (bits 1, 2, 7, 8) to read from or write to the memory. The diagram also shows the internal structure of the chips, with each chip having 1/512 and 1/512 labels, and the overall organization being 'All chips 512 words by 512 bits. 2-terminal cells'.](images/image_0079.jpeg)\n\n\nFigure 5.6: 1-MB Memory Organization. The diagram shows a 1-MB memory organized into 8 groups of 128 words each. Each group contains 8 chips of 512 words each. The Memory Address Register (MAR) provides 11 address lines (bits 9, 9, 2) to select a group and a word within the group. The Chip group enable signal selects one of four groups (A, B, C, D). The Memory Buffer Register (MBR) provides 8 data lines (bits 1, 2, 7, 8) to read from or write to the memory. The diagram also shows the internal structure of the chips, with each chip having 1/512 and 1/512 labels, and the overall organization being 'All chips 512 words by 512 bits. 2-terminal cells'.\n\n\n**Figure 5.6**\n   1-MB Memory Organization\n\n\nswitch erratically between 0 and 1. Hard errors can be caused by harsh environmental abuse, manufacturing defects, and wear. A\n   **soft error**\n   is a random, nondestructive event that alters the contents of one or more memory cells without damaging the memory. Soft errors can be caused by power supply problems or alpha particles. These particles result from radioactive decay and are distressingly common because radioactive nuclei are found in small quantities in nearly all materials. Both hard and soft errors are clearly undesirable, and most modern main memory systems include logic for both detecting and correcting errors.\n\n\nFigure 5.7 illustrates in general terms how the process is carried out. When data are to be written into memory, a calculation, depicted as a function\n   \n    f\n   \n   , is performed on the data to produce a code. Both the code and the data are stored. Thus, if an\n   \n    M\n   \n   -bit word of data is to be stored and the code is of length\n   \n    K\n   \n   bits, then the actual size of the stored word is\n   \n    M + K\n   \n   bits.\n\n\nWhen the previously stored word is read out, the code is used to detect and possibly correct errors. A new set of\n   \n    K\n   \n   code bits is generated from the\n   \n    M\n   \n   data bits and compared with the fetched code bits. The comparison yields one of three results:\n\n\n  * ■ No errors are detected. The fetched data bits are sent out.\n  * ■ An error is detected, and it is possible to correct the error. The data bits plus\n    **error correction**\n    bits are fed into a corrector, which produces a corrected set of\n    \n     M\n    \n    bits to be sent out.\n  * ■ An error is detected, but it is not possible to correct it. This condition is reported.\n\n\nCodes that operate in this fashion are referred to as\n   **error-correcting codes**\n   . A code is characterized by the number of bit errors in a word that it can correct and detect.\n\n\n\n\n![Block diagram of an error-correcting code function. Data in (M bits) enters a function block 'f' which also receives K check bits. The output of 'f' is sent to Memory (M bits) and to a Compare block (K bits). Memory outputs M bits to a Correcor block and K check bits to the Compare block. The Correcor block outputs an Error signal. The Compare block outputs a syndrome word (K bits) to the Correcor block. The Correcor block outputs corrected Data out (M bits).](images/image_0080.jpeg)\n\n\nBlock diagram of an error-correcting code function. Data in (M bits) enters a function block 'f' which also receives K check bits. The output of 'f' is sent to Memory (M bits) and to a Compare block (K bits). Memory outputs M bits to a Correcor block and K check bits to the Compare block. The Correcor block outputs an Error signal. The Compare block outputs a syndrome word (K bits) to the Correcor block. The Correcor block outputs corrected Data out (M bits).\n\n\n**Figure 5.7**\n   Error-Correcting Code Function\n\n\nThe simplest of the error-correcting codes is the\n   **Hamming code**\n   devised by Richard Hamming at Bell Laboratories. Figure 5.8 uses Venn diagrams to illustrate the use of this code on 4-bit words (\n   \n    M = 4\n   \n   ). With three intersecting circles, there are seven compartments. We assign the 4 data bits to the inner compartments (Figure 5.8a). The remaining compartments are filled with what are called\n   *parity bits*\n   . Each parity bit is chosen so that the total number of 1s in its circle is even (Figure 5.8b). Thus, because circle A includes three data 1s, the parity bit in that circle is set to 1. Now, if an error changes one of the data bits (Figure 5.8c), it is easily found. By checking the parity bits, discrepancies are found in circle A and circle C but not in circle B. Only one of the seven compartments is in A and C but not B (Figure 5.8d). The error can therefore be corrected by changing that bit.\n\n\nTo clarify the concepts involved, we will develop a code that can detect and correct single-bit errors in 8-bit words.\n\n\nTo start, let us determine how long the code must be. Referring to Figure 5.7, the comparison logic receives as input two\n   \n    K\n   \n   -bit values. A bit-by-bit comparison is done by taking the exclusive-OR of the two inputs. The result is called the\n   *syndrome word*\n   . Thus, each bit of the\n   **syndrome**\n   is 0 or 1 according to if there is or is not a match in that bit position for the two inputs.\n\n\nThe syndrome word is therefore\n   \n    K\n   \n   bits wide and has a range between 0 and\n   \n    2^K - 1\n   \n   . The value 0 indicates that no error was detected, leaving\n   \n    2^K - 1\n   \n   values to indicate, if there is an error, which bit was in error. Now, because an error could occur on any of the\n   \n    M\n   \n   data bits or\n   \n    K\n   \n   check bits, we must have\n\n\n2^K - 1 \\ge M + K\n\n\nThis inequality gives the number of bits needed to correct a single bit error in a word containing\n   \n    M\n   \n   data bits. For example, for a word of 8 data bits (\n   \n    M = 8\n   \n   ), we have\n\n\n  * ■\n    \n     K = 3: 2^3 - 1 < 8 + 3\n  * ■\n    \n     K = 4: 2^4 - 1 > 8 + 4\n\n\n\n\n![Figure 5.8: Hamming Error-Correcting Code. Four Venn diagrams (a, b, c, d) showing the placement of 1s and 0s in overlapping circles A, B, and C.](images/image_0081.jpeg)\n\n\nFigure 5.8 consists of four Venn diagrams labeled (a), (b), (c), and (d), each showing three overlapping circles labeled A, B, and C. The regions are labeled with binary values (0 or 1) as follows:\n\n\n  * (a) Region A only: 1; Region B only: 1; Region C only: 1; Region A ∩ B: 1; Region A ∩ C: 1; Region B ∩ C: 0; Region A ∩ B ∩ C: 1.\n  * (b) Region A only: 1; Region B only: 0; Region C only: 0; Region A ∩ B: 1; Region A ∩ C: 1; Region B ∩ C: 1; Region A ∩ B ∩ C: 1.\n  * (c) Region A only: 1; Region B only: 0; Region C only: 0; Region A ∩ B: 1; Region A ∩ C: 0; Region B ∩ C: 0; Region A ∩ B ∩ C: 1.\n  * (d) Region A only: 1; Region B only: 0; Region C only: 0; Region A ∩ B: 1; Region A ∩ C: 0; Region B ∩ C: 1; Region A ∩ B ∩ C: 0.\n\n\nFigure 5.8: Hamming Error-Correcting Code. Four Venn diagrams (a, b, c, d) showing the placement of 1s and 0s in overlapping circles A, B, and C.\n\n\n**Figure 5.8**\n   Hamming Error-Correcting Code\n\n\nThus, eight data bits require four check bits. The first three columns of Table 5.2 lists the number of check bits required for various data word lengths.\n\n\nFor convenience, we would like to generate a 4-bit syndrome for an 8-bit data word with the following characteristics:\n\n\n  * ■ If the syndrome contains all 0s, no error has been detected.\n  * ■ If the syndrome contains one and only one bit set to 1, then an error has occurred in one of the 4 check bits. No correction is needed.\n  * ■ If the syndrome contains more than one bit set to 1, then the numerical value of the syndrome indicates the position of the data bit in error. This data bit is inverted for correction.\n\n\nTo achieve these characteristics, the data and check bits are arranged into a 12-bit word as depicted in Figure 5.9. The bit positions are numbered from 1 to 12. Those bit positions whose position numbers are powers of 2 are designated as check\n\n\n**Table 5.2**\n\nData Bits | Single-Error Correction | Single-Error Correction/\n      \n      Double-Error Detection\nCheck Bits | % Increase | Check Bits | % Increase\n8 | 4 | 50.0 | 5 | 62.5\n16 | 5 | 31.25 | 6 | 37.5\n32 | 6 | 18.75 | 7 | 21.875\n64 | 7 | 10.94 | 8 | 12.5\n128 | 8 | 6.25 | 9 | 7.03\n256 | 9 | 3.52 | 10 | 3.91\n\n\nbits. The check bits are calculated as follows, where the symbol\n   \n    \\oplus\n   \n   designates the exclusive-OR operation:\n\n\n\\begin{aligned}\n    C1 &= D1 \\oplus D2 \\oplus D4 \\oplus D5 \\oplus D7 \\\\\n    C2 &= D1 \\oplus D3 \\oplus D4 \\oplus D6 \\oplus D7 \\\\\n    C4 &= D2 \\oplus D3 \\oplus D4 \\oplus D8 \\\\\n    C8 &= D5 \\oplus D6 \\oplus D7 \\oplus D8\n    \\end{aligned}\n\n\nEach check bit operates on every data bit whose position number contains a 1 in the same bit position as the position number of that check bit. Thus, data bit positions 3, 5, 7, 9, and 11 (D1, D2, D4, D5, D7) all contain a 1 in the least significant bit of their position number as does C1; bit positions 3, 6, 7, 10, and 11 all contain a 1 in the second bit position, as does C2; and so on. Looked at another way, bit position\n   \n    n\n   \n   is checked by those bits\n   \n    C_i\n   \n   such that\n   \n    \\sum_i = n\n   \n   . For example, position 7 is checked by bits in position 4, 2, and 1; and\n   \n    7 = 4 + 2 + 1\n   \n   .\n\n\nLet us verify that this scheme works with an example. Assume that the 8-bit input word is 00111001, with data bit D1 in the rightmost position. The calculations are as follows:\n\n\n\\begin{aligned}\n    C1 &= 1 \\oplus 0 \\oplus 1 \\oplus 1 \\oplus 0 = 1 \\\\\n    C2 &= 1 \\oplus 0 \\oplus 1 \\oplus 1 \\oplus 0 = 1 \\\\\n    C4 &= 0 \\oplus 0 \\oplus 1 \\oplus 0 = 1 \\\\\n    C8 &= 1 \\oplus 1 \\oplus 0 \\oplus 0 = 0\n    \\end{aligned}\n\n\n\nBit position | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1\nPosition number | 1100 | 1011 | 1010 | 1001 | 1000 | 0111 | 0110 | 0101 | 0100 | 0011 | 0010 | 0001\nData bit | D8 | D7 | D6 | D5 |  | D4 | D3 | D2 |  | D1 |  | \nCheck bit |  |  |  |  | C8 |  |  |  | C4 |  | C2 | C1\n\n\n**Figure 5.9**\nSuppose now that data bit 3 sustains an error and is changed from 0 to 1. When the check bits are recalculated, we have\n\n\nC1 = 1 \\oplus 0 \\oplus 1 \\oplus 1 \\oplus 0 = 1\n\n\nC2 = 1 \\oplus 1 \\oplus 1 \\oplus 1 \\oplus 0 = 0\n\n\nC4 = 0 \\oplus 1 \\oplus 1 \\oplus 0 = 0\n\n\nC8 = 1 \\oplus 1 \\oplus 0 \\oplus 0 = 0\n\n\nWhen the new check bits are compared with the old check bits, the syndrome word is formed:\n\n\n\\begin{array}{cccc} C8 & C4 & C2 & C1 \\\\ 0 & 1 & 1 & 1 \\\\ \\oplus & 0 & 0 & 0 \\\\ \\hline 0 & 1 & 1 & 0 \\end{array}\n\n\nThe result is 0110, indicating that bit position 6, which contains data bit 3, is in error.\n\n\nFigure 5.10 illustrates the preceding calculation. The data and check bits are positioned properly in the 12-bit word. Four of the data bits have a value 1 (shaded in the table), and their bit position values are XORed to produce the Hamming code 0111, which forms the four check digits. The entire block that is stored is 001101001111. Suppose now that data bit 3, in bit position 6, sustains an error and is changed from 0 to 1. The resulting block is 001101101111, with a Hamming code of 0001. An XOR of the Hamming code and all of the bit position values for nonzero data bits results in 0110. The nonzero result detects an error and indicates that the error is in bit position 6.\n\n\nThe code just described is known as a\n   **single-error-correcting (SEC) code**\n   . More commonly, semiconductor memory is equipped with a\n   **single-error-correcting, double-error-detecting (SEC-DED) code**\n   . As Table 5.2 shows, such codes require one additional bit compared with SEC codes.\n\n\nFigure 5.11 illustrates how such a code works, again with a 4-bit data word. The sequence shows that if two errors occur (Figure 5.11c), the checking procedure goes astray (d) and worsens the problem by creating a third error (e). To overcome\n\n\n\nBit position | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1\nPosition number | 1100 | 1011 | 1010 | 1001 | 1000 | 0111 | 0110 | 0101 | 0100 | 0011 | 0010 | 0001\nData bit | D8 | D7 | D6 | D5 |  | D4 | D3 | D2 |  | D1 |  | \nCheck bit |  |  |  |  | C8 |  |  |  | C4 |  | C2 | C1\nWord stored as | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 1\nWord fetched as | 0 | 0 | 1 | 1 | 0 | 1 | 1 | 0 | 1 | 1 | 1 | 1\nPosition number | 1100 | 1011 | 1010 | 1001 | 1000 | 0111 | 0110 | 0101 | 0100 | 0011 | 0010 | 0001\nCheck bit |  |  |  |  | 0 |  |  |  | 0 |  | 0 | 1\n\n\n**Figure 5.10**\n   Check Bit Calculation\n\n\n\n\n![Figure 5.11: Hamming SEC-DEC Code. Six Venn diagrams (a-f) showing parity checks for data bits 1, 0, 1, 0, 1, 0 and an error in diagram (d).](images/image_0082.jpeg)\n\n\nFigure 5.11 consists of six Venn diagrams arranged in a 2x3 grid, labeled (a) through (f). Each diagram contains three overlapping circles. The regions of the circles are labeled with binary values (0 or 1) representing parity checks. Below each diagram is a small square containing a binary value.\n\n\n  * (a) Top-left: Regions are labeled 0, 1, 0, 0. Bottom square is empty.\n  * (b) Top-middle: Regions are labeled 0, 0, 1, 1. Bottom square contains '1'.\n  * (c) Top-right: Regions are labeled 1, 0, 1, 0. Bottom square contains '1'.\n  * (d) Bottom-left: Regions are labeled 1, 0, 1, 0. The bottom-right region (intersection of all three circles) is shaded gray. Bottom square contains '1'.\n  * (e) Bottom-middle: Regions are labeled 1, 0, 1, 0. Bottom square contains '1'.\n  * (f) Bottom-right: Regions are labeled 1, 0, 1, 0. Bottom square contains '1'.\n\n\nFigure 5.11: Hamming SEC-DEC Code. Six Venn diagrams (a-f) showing parity checks for data bits 1, 0, 1, 0, 1, 0 and an error in diagram (d).\n\n\n**Figure 5.11**\n   Hamming SEC-DEC Code\n\n\nthe problem, an eighth bit is added that is set so that the total number of 1s in the diagram is even. The extra parity bit catches the error (f).\n\n\nAn error-correcting code enhances the reliability of the memory at the cost of added complexity. With a 1-bit-per-chip organization, an SEC-DED code is generally considered adequate. For example, the IBM 30xx implementations used an 8-bit SEC-DED code for each 64 bits of data in main memory. Thus, the size of main memory is actually about 12% larger than is apparent to the user. The VAX computers used a 7-bit SEC-DED for each 32 bits of memory, for a 22% overhead. Contemporary DRAM systems may have anywhere from 7% to 20% overhead [SHAR03]."
        },
        {
          "name": "DDR DRAM",
          "content": "As discussed in Chapter 1, one of the most critical system bottlenecks when using high-performance processors is the interface to internal main memory. This interface is the most important pathway in the entire computer system. The basic building block of main memory remains the DRAM chip, as it has for decades; until recently, there had been no significant changes in DRAM architecture since the early 1970s. The traditional DRAM chip is constrained both by its internal architecture and by its interface to the processor's memory bus.\n\n\nWe have seen that one attack on the performance problem of DRAM main memory has been to insert one or more levels of high-speed SRAM cache between the DRAM main memory and the processor. But SRAM is much costlier than DRAM, and expanding cache size beyond a certain point yields diminishing returns.\n\n\nIn recent years, a number of enhancements to the basic DRAM architecture have been explored. The schemes that currently dominate the market are SDRAM and DDR-DRAM. We examine each of these in turn.\n\n\n\n\n**Synchronous DRAM**\n\n\nOne of the most widely used forms of DRAM is the\n   **synchronous DRAM (SDRAM)**\n   . Unlike the traditional DRAM, which is asynchronous, the SDRAM exchanges data with the processor synchronized to an external clock signal and running at the full speed of the processor/memory bus without imposing wait states.\n\n\nIn a typical DRAM, the processor presents addresses and control levels to the memory, indicating that a set of data at a particular location in memory should be either read from or written into the DRAM. After a delay, the access time, the DRAM either writes or reads the data. During the access-time delay, the DRAM performs various internal functions, such as activating the high capacitance of the row and column lines, sensing the data, and routing the data out through the output buffers. The processor must simply wait through this delay, slowing system performance.\n\n\nWith synchronous access, the DRAM moves data in and out under control of the system clock. The processor or other master issues the instruction and address information, which is latched by the DRAM. The DRAM then responds after a set number of clock cycles. Meanwhile, the master can safely do other tasks while the SDRAM is processing the request.\n\n\nFigure 5.12 shows the internal logic of a typical 256-Mb SDRAM typical of SDRAM organization, and Table 5.3 defines the various pin assignments. The\n\n\n\n\n![Block diagram of a 256-Mb Synchronous Dynamic RAM (SDRAM) showing internal logic and data flow.](images/image_0083.jpeg)\n\n\nThe diagram illustrates the internal architecture of a 256-Mb SDRAM. It features several key components and data paths:\n\n\n  * **External Interface:**\n     Includes control signals (CLK, CKE, CS, RAS, CAS, WE) and address lines (A10-A12, A9-A7, A6-A4, A3-A1, BA0, BA1). Data lines are labeled DQML, DQMH, and DQ 0-15.\n  * **Command Decoder & Clock Generator:**\n     Receives external control signals and generates internal clock and control signals for the memory array.\n  * **Mode Register:**\n     Receives a 13-bit input (A10-A7, A6-A4, A3-A1) and stores configuration settings.\n  * **Refresh Controller & Self-Refresh Controller:**\n     Manages refresh operations, including a Refresh Counter.\n  * **Address Latching & Buffering:**\n  * **Row Address Latch & Buffer:**\n       Latches row addresses (A10-A7) and feeds them into a Multiplexer and a Row Address Buffer. The Row Address Buffer feeds into the Row Decoder.\n  * **Column Address Latch & Buffer:**\n       Latches column addresses (A6-A4) and feeds them into a Burst Counter and a Column Address Buffer. The Column Address Buffer feeds into the Column Decoder.\n  * **Bank Control Logic:**\n     Coordinates access between the Row Decoder, Column Decoder, and the Memory Cell Array.\n  * **Row Decoder:**\n     Decodes the row address to select a row in the memory array. It has 13 inputs from the Row Address Buffer.\n  * **Column Decoder:**\n     Decodes the column address to select a column in the memory array. It has 9 inputs from the Column Address Buffer.\n  * **Memory Cell Array (DRAM BANK 0):**\n     The core storage area, organized as 8192 rows by 8192 columns, totaling 4 Mb x 16. It includes Sense Amps for reading data.\n  * **Data I/O Path:**\n     Data flows from the Memory Cell Array through Sense Amps, a Column Decoder, a Bank Control Logic, a Row Decoder, and a Refresh Controller to the Data in Buffer. The Data in Buffer (16 bits) feeds into the Data out Buffer (16 bits), which then outputs to the external data lines (DQ 0-15).\n\n\nBlock diagram of a 256-Mb Synchronous Dynamic RAM (SDRAM) showing internal logic and data flow.\n\n\n**Figure 5.12**\n   256-Mb Synchronous Dynamic RAM (SDRAM)\n\n\n**Table 5.3**\n\nA0 to A13 | Address inputs\nBA0, BA1 | Bank address lines\nCLK | Clock input\nCKE | Clock enable\n\\overline{CS} | Chip select\n\\overline{RAS} | Row address strobe\n\\overline{CAS} | Column address strobe\n\\overline{WE} | Write enable\nDQ0 to DQ7 | Data input/output\nDQM | Data mask\n\n\nSDRAM employs a burst mode to eliminate the address setup time and row and column line precharge time after the first access. In burst mode, a series of data bits can be clocked out rapidly after the first bit has been accessed. This mode is useful when all the bits to be accessed are in sequence and in the same row of the array as the initial access. In addition, the SDRAM has a multiple-bank internal architecture that improves opportunities for on-chip parallelism.\n\n\nThe mode register and associated control logic is another key feature differentiating SDRAMs from conventional DRAMs. It provides a mechanism to customize the SDRAM to suit specific system needs. The mode register specifies the burst length, which is the number of separate units of data synchronously fed onto the bus. The register also allows the programmer to adjust the latency between receipt of a read request and the beginning of data transfer.\n\n\nThe SDRAM performs best when it is transferring large blocks of data sequentially, such as for applications like word processing, spreadsheets, and multimedia.\n\n\nFigure 5.13 shows an example of SDRAM operation. In this case, the burst length is 4 and the latency is 2. The burst read command is initiated by having\n   \n    \\overline{CS}\n   \n   and\n   \n    \\overline{CAS}\n   \n   low while holding\n   \n    \\overline{RAS}\n   \n   and\n   \n    \\overline{WE}\n   \n   high at the rising edge of the clock. The address inputs determine the starting column address for the burst, and the mode register sets the type of burst (sequential or interleaved) and the burst length (1, 2, 4, 8, full page). The delay from the start of the command to when the data from the first cell appears on the outputs is equal to the value of the\n   \n    \\overline{CAS}\n   \n   latency that is set in the mode register.\n\n\n\n\n![Figure 5.13: SDRAM Read Timing diagram showing CLK, COMMAND, and DQs signals over time slots T0 to T8. The COMMAND signal shows a READ A command at T0, followed by NOP commands. The DQs signal shows data outputs DOUT A0, DOUT A1, DOUT A2, and DOUT A3 starting at T4, with a delay of 2 clock cycles (latency) from the start of the READ command at T0.](images/image_0084.jpeg)\n\n\nThe diagram illustrates the timing of an SDRAM read operation. It shows three signals over time slots T0 through T8:\n\n\n  * **CLK:**\n     A square wave clock signal.\n  * **COMMAND:**\n     A sequence of commands: READ A at T0, followed by NOP (No Operation) commands at T1, T2, T3, T4, T5, T6, T7, and T8.\n  * **DQs:**\n     Data output lines. Data is output starting at T4, with outputs labeled DOUT A0, DOUT A1, DOUT A2, and DOUT A3. An arrow indicates that the data output begins 2 clock cycles (latency) after the start of the READ command at T0.\n\n\nFigure 5.13: SDRAM Read Timing diagram showing CLK, COMMAND, and DQs signals over time slots T0 to T8. The COMMAND signal shows a READ A command at T0, followed by NOP commands. The DQs signal shows data outputs DOUT A0, DOUT A1, DOUT A2, and DOUT A3 starting at T4, with a delay of 2 clock cycles (latency) from the start of the READ command at T0.\n\n\n**Figure 5.13**\n\n   \\overline{CAS}\n  \n\n\n**DDR SDRAM**\n\n\nAlthough SDRAM is a significant improvement on asynchronous RAM, it still has shortcomings that unnecessarily limit that I/O data rate that can be achieved. To address these shortcomings a newer version of SDRAM, referred to as double-data-rate DRAM (DDR DRAM) provides several features that dramatically increase the data rate. DDR DRAM was developed by the JEDEC Solid State Technology Association, the Electronic Industries Alliance's semiconductor-engineering-standardization body. Numerous companies make DDR chips, which are widely used in desktop computers and servers.\n\n\nDDR achieves higher data rates in three ways. First, the data transfer is synchronized to both the rising and falling edge of the clock, rather than just the rising edge. This doubles the data rate; hence the term\n   *double data rate*\n   . Second, DDR uses higher clock rate on the bus to increase the transfer rate. Third, a buffering scheme is used, as explained subsequently.\n\n\nJEDEC has thus far defined four generations of the DDR technology (Table 5.4). The initial DDR version makes use of a 2-bit prefetch buffer. The prefetch buffer is a memory cache located on the SDRAM chip. It enables the SDRAM chip to pre-position bits to be placed on the data bus as rapidly as possible. The DDR I/O bus uses the same clock rate as the memory chip, but because it can handle two bits per cycle, it achieves a data rate that is double the clock rate. The 2-bit prefetch buffer enables the SDRAM chip to keep up with the I/O bus.\n\n\nTo understand the operation of the prefetch buffer, we need to look at it from the point of view of a word transfer. The prefetch buffer size determines how many words of data are fetched (across multiple SDRAM chips) every time a column command is performed with DDR memories. Because the core of the DRAM is much slower than the interface, the difference is bridged by accessing information in parallel and then serializing it out the interface through a multiplexor (MUX). Thus, DDR prefetches two words, which means that every time a read or a write operation is performed, it is performed on two words of data, and bursts out of, or into, the SDRAM over one clock cycle on both clock edges for a total of two consecutive operations. As a result, the DDR I/O interface is twice as fast as the SDRAM core.\n\n\nAlthough each new generation of SDRAM results in much greater capacity, the core speed of the SDRAM has not changed significantly from generation to generation. To achieve greater data rates than those afforded by the rather modest increases in SDRAM clock rate, JEDEC increased the buffer size. For DDR2, a 4-bit buffer is used, allowing for words to be transferred in parallel, increasing the effective data rate by a factor of 4. For DDR3, an 8-bit buffer is used and a factor of 8 speedup is achieved (Figure 5.14).\n\n\n**Table 5.4**\n   DDR Characteristics\n\n\n\n | DDR1 | DDR2 | DDR3 | DDR4\nPrefetch buffer (bits) | 2 | 4 | 8 | 8\nVoltage level (V) | 2.5 | 1.8 | 1.5 | 1.2\nFront side bus data rates (Mbps) | 200–400 | 400–1066 | 800–2133 | 2133–4266\n\n\n\n\n![Diagram illustrating DDR Generations from SDRAM to DDR4, showing the evolution of memory array, I/O, and bandwidth specifications across generations.](images/image_0085.jpeg)\n\n\nThe diagram illustrates the evolution of DDR memory generations, showing the relationship between memory arrays, I/O interfaces, and bandwidth across different generations. The generations are separated by dashed green lines.\n\n\n\nGeneration | Memory Array (MHz) | I/O (MHz) | Bandwidth (Mbps)\nSDRAM (1N) | 100–150 MHz | 100–150 MHz | 100–150 Mbps\nDDR (2N) | 100–200 MHz | 100–200 MHz | 200–400 Mbps\nDDR2 (4N) | 100–266 MHz | 200–533 MHz | 400–1066 Mbps\nDDR3 (8N) | 100–266 MHz | 400–1066 MHz | 800–2133 Mbps\nDDR4 (8N) | 100–266 MHz | 667–1600 MHz | 1333–3200 Mbps\n\n\nThe diagram also shows the internal structure, including the use of MUX (Multiplexer) blocks to combine multiple memory array signals into a single I/O path. The number of memory arrays increases from 1N to 8N across the generations, and the I/O frequency and bandwidth increase significantly.\n\n\nDiagram illustrating DDR Generations from SDRAM to DDR4, showing the evolution of memory array, I/O, and bandwidth specifications across generations.\n\n\nFigure 5.14 DDR Generations\n\n\nThe downside to the prefetch is that it effectively determines the minimum burst length for the SDRAMs. For example, it is very difficult to have an efficient burst length of four words with DDR3's prefetch of eight. Accordingly, the JEDEC designers chose not to increase the buffer size to 16 bits for DDR4, but rather to introduce the concept of a\n   **bank group**\n   [ALLA13]. Bank groups are separate entities such that they allow a column cycle to complete within a bank group, but that column cycle does not impact what is happening in another bank group. Thus, two prefetches of eight can be operating in parallel in the two bank groups. This arrangement keeps the prefetch buffer size the same as for DDR3, while increasing performance as if the prefetch is larger.\n\n\nFigure 5.14 shows a configuration with two bank groups. With DDR4, up to 4 bank groups can be used."
        },
        {
          "name": "Flash Memory",
          "content": "Another form of semiconductor memory is flash memory. Flash memory is used both for internal memory and external memory applications. Here, we provide a technical overview and look at its use for internal memory.\n\n\nFirst introduced in the mid-1980s, flash memory is intermediate between EPROM and EEPROM in both cost and functionality. Like EEPROM, flash memory uses an electrical erasing technology. An entire flash memory can be erased in one or a few seconds, which is much faster than EPROM. In addition, it is possible to erase just blocks of memory rather than an entire chip. Flash memory gets its name because the microchip is organized so that a section of memory cells are erased in a single action or “flash.” However, flash memory does not provide byte-level erasure. Like EPROM, flash memory uses only one transistor per bit, and so achieves the high density (compared with EEPROM) of EPROM.\n\n\n\n\n**Operation**\n\n\nFigure 5.15 illustrates the basic operation of a flash memory. For comparison, Figure 5.15a depicts the operation of a transistor. Transistors exploit the properties of semiconductors so that a small voltage applied to the gate can be used to control the flow of a large current between the source and the drain.\n\n\nIn a flash memory cell, a second gate—called a floating gate, because it is insulated by a thin oxide layer—is added to the transistor. Initially, the floating gate does not interfere with the operation of the transistor (Figure 5.15b). In this state, the cell is deemed to represent binary 1. Applying a large voltage across the oxide layer causes electrons to tunnel through it and become trapped on the floating gate, where they remain even if the power is disconnected (Figure 5.15c). In this state, the cell is deemed to represent binary 0. The state of the cell can be read by using external circuitry to test whether the transistor is working or not. Applying a large voltage in the opposite direction removes the electrons from the floating gate, returning to a state of binary 1.\n\n\n\n\n![Figure 5.15: Flash Memory Operation. (a) Transistor structure: A cross-section showing a P-substrate with N+ Drain and N+ Source regions. A Control gate is placed on top of the channel region. (b) Flash memory cell in one state: The Control gate is on top, and a Floating gate is placed on top of the Control gate. The Floating gate is empty. (c) Flash memory cell in zero state: The Control gate is on top, and a Floating gate is placed on top of the Control gate. The Floating gate is filled with electrons, represented by circles with minus signs.](images/image_0086.jpeg)\n\n\n(a) Transistor structure\n\n\n(b) Flash memory cell in one state\n\n\n(c) Flash memory cell in zero state\n\n\nFigure 5.15: Flash Memory Operation. (a) Transistor structure: A cross-section showing a P-substrate with N+ Drain and N+ Source regions. A Control gate is placed on top of the channel region. (b) Flash memory cell in one state: The Control gate is on top, and a Floating gate is placed on top of the Control gate. The Floating gate is empty. (c) Flash memory cell in zero state: The Control gate is on top, and a Floating gate is placed on top of the Control gate. The Floating gate is filled with electrons, represented by circles with minus signs.\n\n\n**Figure 5.15**\n   Flash Memory Operation\n\n\nAn important characteristic of flash memory is that it is persistent memory, which means that it retains data when there is no power applied to the memory. Thus, it is useful for secondary (external) storage, and as an alternative to random access memory in computers.\n\n\n\n\n**NOR and NAND Flash Memory**\n\n\nThere are two distinctive types of flash memory, designated as NOR and NAND (Figure 5.16). In\n   **NOR flash memory**\n   , the basic unit of access is a bit, referred to as a\n   *memory cell*\n   . Cells in NOR flash are connected in parallel to the bit lines so that each cell can be read/write/erased individually. If any memory cell of the device is turned on by the corresponding word line, the bit line goes low. This is similar in function to a NOR logic gate.\n   \n    2\n\n\n**NAND flash memory**\n   is organized in transistor arrays with 16 or 32 transistors in series. The bit line goes low only if all the transistors in the corresponding word lines are turned on. This is similar in function to a NAND logic gate.\n\n\nAlthough the specific quantitative values of various characteristics of NOR and NAND are changing year by year, the relative differences between the two types has remained stable. These differences are usefully illustrated by the Kiviat graphs\n   \n    3\n   \n   shown in Figure 5.17.\n\n\n\n\n![Figure 5.16 Flash Memory Structures. (a) NOR flash structure: A bit line is connected to multiple memory cells in parallel. Each cell is connected to a word line (0 through 5). A dashed box highlights one cell. (b) NAND flash structure: A bit line is connected to a series of transistors (word lines 0 through 7) in series. A dashed box highlights one cell. A 'Ground select transistor' is connected to the bit line, and a 'Bit-line select transistor' is connected to the bit line at the end of the series.](images/image_0087.jpeg)\n\n\n(a) NOR flash structure\n\n\n(b) NAND flash structure\n\n\nFigure 5.16 Flash Memory Structures. (a) NOR flash structure: A bit line is connected to multiple memory cells in parallel. Each cell is connected to a word line (0 through 5). A dashed box highlights one cell. (b) NAND flash structure: A bit line is connected to a series of transistors (word lines 0 through 7) in series. A dashed box highlights one cell. A 'Ground select transistor' is connected to the bit line, and a 'Bit-line select transistor' is connected to the bit line at the end of the series.\n\n\n**Figure 5.16**\n   Flash Memory Structures\n\n\n2\n   \n   The circles associated with and in Figure 5.2b indicate signal negation.\n\n\n3\n   \n   A Kiviat graph provides a pictorial means of comparing systems along multiple variables [MORR74]. The variables are laid out at as lines of equal angular intervals within a circle, each line going from the center of the circle to the circumference. A given system is defined by one point on each line; the closer to the circumference, the better the value. The points are connected to yield a shape that is characteristic of that system. The more area enclosed in the shape, the “better” is the system.\n\n\n\n\n![Figure 5.17: Kiviat Graphs for Flash Memory. (a) NOR and (b) NAND. Both graphs plot Cost per bit, Active power, Read speed, and Write speed. (a) NOR: Cost per bit is Low, Active power is Low, Read speed is High, Write speed is High. (b) NAND: Cost per bit is Low, Active power is Low, Read speed is High, Write speed is High. Both graphs also show 'File storage use Easy' and 'Code execution' as high.](images/image_0088.jpeg)\n\n\n(a) NOR\n\n\n(b) NAND\n\n\nFigure 5.17: Kiviat Graphs for Flash Memory. (a) NOR and (b) NAND. Both graphs plot Cost per bit, Active power, Read speed, and Write speed. (a) NOR: Cost per bit is Low, Active power is Low, Read speed is High, Write speed is High. (b) NAND: Cost per bit is Low, Active power is Low, Read speed is High, Write speed is High. Both graphs also show 'File storage use Easy' and 'Code execution' as high.\n\n\nFigure 5.17 Kiviat Graphs for Flash Memory\n\n\nNOR flash memory provides high-speed random access. It can read and write data to specific locations, and can reference and retrieve a single byte. NAND reads and writes in small blocks. NAND provides higher bit density than NOR and greater write speed. NAND flash does not provide a random-access external address bus so the data must be read on a blockwise basis (also known as page access), where each block holds hundreds to thousands of bits.\n\n\nFor internal memory in embedded systems, NOR flash memory has traditionally been preferred. NAND memory has made some inroads, but NOR remains the dominant technology for internal memory. It is ideally suited for microcontrollers where the amount of program code is relatively small and a certain amount of application data does not vary. For example, the flash memory in Figure 1.16 is NOR memory.\n\n\nNAND memory is better suited for external memory, such as USB flash drives, memory cards (in digital cameras, MP3 players, etc.), and in what are known as solid-state disks (SSDs). We discuss SSDs in Chapter 6."
        },
        {
          "name": "Newer Nonvolatile Solid-State Memory Technologies",
          "content": "The traditional memory hierarchy has consisted of three levels (Figure 5.18):\n\n\n  * ■\n    **Static RAM (SRAM):**\n    SRAM provides rapid access time, but is the most expensive and the least dense (bit density). SRAM is suitable for cache memory.\n  * ■\n    **Dynamic RAM (DRAM):**\n    Cheaper, denser, and slower than SRAM, DRAM has traditionally been the choice off-chip main memory.\n  * ■\n    **Hard disk:**\n    A magnetic disk provides very high bit density and very low cost per bit, with relatively slow access times. It is the traditional choice for external storage as part of the memory hierarchy.\n\n\n\n\n![Figure 5.18: Nonvolatile RAM within the Memory Hierarchy. The diagram shows a pyramid representing the memory hierarchy. The pyramid is divided into five horizontal layers from top to bottom: SRAM, DRAM, NAND FLASH, HARD DISK, and a bottom-most layer. To the right of the pyramid, three new memory technologies are listed: STT-RAM, PCRAM, and ReRAM. Dashed lines connect these three technologies to the SRAM, DRAM, and NAND FLASH layers respectively. An arrow on the left points upwards, labeled 'Increasing performance and endurance'. Another arrow on the left points downwards, labeled 'Decreasing cost per bit, increasing capacity or density'.](images/image_0089.jpeg)\n\n\nFigure 5.18: Nonvolatile RAM within the Memory Hierarchy. The diagram shows a pyramid representing the memory hierarchy. The pyramid is divided into five horizontal layers from top to bottom: SRAM, DRAM, NAND FLASH, HARD DISK, and a bottom-most layer. To the right of the pyramid, three new memory technologies are listed: STT-RAM, PCRAM, and ReRAM. Dashed lines connect these three technologies to the SRAM, DRAM, and NAND FLASH layers respectively. An arrow on the left points upwards, labeled 'Increasing performance and endurance'. Another arrow on the left points downwards, labeled 'Decreasing cost per bit, increasing capacity or density'.\n\n\n**Figure 5.18**\n   Nonvolatile RAM within the Memory Hierarchy\n\n\nInto this mix, as we have seen, has been added flash memory. Flash memory has the advantage over traditional memory that it is nonvolatile. NOR flash is best suited to storing programs and static application data in embedded systems, while NAND flash has characteristics intermediate between DRAM and hard disks.\n\n\nOver time, each of these technologies has seen improvements in scaling: higher bit density, higher speed, lower power consumption, and lower cost. However, for semiconductor memory, it is becoming increasingly difficult to continue the pace of improvement [ITRS14].\n\n\nRecently, there have been breakthroughs in developing new forms of non-volatile semiconductor memory that continue scaling beyond flash memory. The most promising technologies are spin-transfer torque RAM (STT-RAM), phase-change RAM (PCRAM), and resistive RAM (ReRAM) ([ITRS14], [GOER12]). All of these are in volume production. However, because NAND Flash and to some extent NOR Flash are still dominating the applications, these emerging memories have been used in specialty applications and have not yet fulfilled their original promise to become dominating mainstream high-density nonvolatile memory. This is likely to change in the next few years.\n\n\nFigure 5.18 shows how these three technologies are likely to fit into the memory hierarchy.\n\n\n\n\n**STT-RAM**\n\n\nSTT-RAM is a new type of\n   **magnetic RAM (MRAM)**\n   , which features non-volatility, fast writing/reading speed (\n   \n    < 10\n   \n   ns), and high programming endurance (\n   \n    > 10^{15}\n   \n   cycles) and zero standby power [KULT13]. The storage capability or programmability of MRAM arises from magnetic tunneling junction (MTJ), in which a thin tunneling dielectric is sandwiched between two ferromagnetic layers. One ferromagnetic layer (pinned or reference layer) is designed to have its magnetization pinned, while the magnetization of the other layer (free layer) can be flipped by a write event. An MTJ has a low (high) resistance if the magnetizations of the free layer and the pinned layer are parallel (anti-parallel). In first-generation MRAM design, the magnetization of the free layer is changed by the current-induced magnetic field. In STT-RAM, a new write mechanism, called\n   *polarization-current-induced magnetization switching*\n   , is introduced. For STT-RAM, the magnetization of the free layer is flipped by the electrical current directly. Because the current required to switch an MTJ resistance state is proportional to the MTJ cell area, STT-RAM is believed to have a better scaling property than the first-generation MRAM. Figure 5.19a illustrates the general configuration.\n\n\nSTT-RAM is a good candidate for either cache or main memory.\n\n\n\n\n**PCRAM**\n\n\n**Phase-change RAM (PCRAM)**\n   is the most mature of the new technologies, with an extensive technical literature ([RAOU09], [ZHOU09], [LEE10]).\n\n\nPCRAM technology is based on a chalcogenide alloy material, which is similar to those commonly used in optical storage media (compact discs and digital versatile discs). The data storage capability is achieved from the resistance differences between an amorphous (high-resistance) and a crystalline (low-resistance) phase of the chalcogenide-based material. In SET operation, the phase change material is crystallized by applying an electrical pulse that heats a significant portion of the cell above its crystallization temperature. In RESET operation, a larger electrical current is applied and then abruptly cut off in order to melt and then quench the material, leaving it in the amorphous state. Figure 5.19b illustrates the general configuration.\n\n\nPCRAM is a good candidate to replace or supplement DRAM for main memory.\n\n\n\n\n**ReRAM**\n\n\nReRAM (also known as RRAM) works by creating resistance rather than directly storing charge. An electric current is applied to a material, changing the resistance of that material. The resistance state can then be measured and a 1 or 0 is read as the result. Much of the work done on ReRAM to date has focused on finding appropriate materials and measuring the resistance state of the cells. ReRAM designs are low voltage, endurance is far superior to flash memory, and the cells are much smaller—at least in theory. Figure 5.19c shows one ReRam configuration.\n\n\nReRAM is a good candidate to replace or supplement both secondary storage and main memory.\n\n\n\n\n![Figure 5.19: Nonvolatile RAM Technologies. (a) STT-RAM: Shows two cross-sections of a cell. The left one is labeled 'binary 0' with the free layer magnetization pointing down. The right one is labeled 'binary 1' with the free layer magnetization pointing up. The cell consists of a Bit line, Free layer (Perpendicular magnetic layer), Interface layer, Insulating layer, Interface layer, Reference layer (Perpendicular magnetic layer), and Base electrode. An Electric current arrow points up through the cell. (b) PCRAM: Shows two cross-sections of a cell. The left one is labeled 'Polycrystalline chalcogenide' and the right one is labeled 'Amorphous chalcogenide'. The cell consists of a Top electrode, a chalcogenide layer, a Heater, an Insulator, and a Bottom electrode. (c) ReRAM: Shows two cross-sections of a cell. The left one is labeled 'Reduction: low resistance' and the right one is labeled 'Oxidation: high resistance'. The cell consists of a Top electrode, an Insulator, a Metal oxide layer, and a Bottom electrode. A Filament is shown within the Metal oxide layer.](images/image_0090.jpeg)\n\n\n(a) STT-RAM\n\n\n(b) PCRAM\n\n\n(c) ReRAM\n\n\nFigure 5.19: Nonvolatile RAM Technologies. (a) STT-RAM: Shows two cross-sections of a cell. The left one is labeled 'binary 0' with the free layer magnetization pointing down. The right one is labeled 'binary 1' with the free layer magnetization pointing up. The cell consists of a Bit line, Free layer (Perpendicular magnetic layer), Interface layer, Insulating layer, Interface layer, Reference layer (Perpendicular magnetic layer), and Base electrode. An Electric current arrow points up through the cell. (b) PCRAM: Shows two cross-sections of a cell. The left one is labeled 'Polycrystalline chalcogenide' and the right one is labeled 'Amorphous chalcogenide'. The cell consists of a Top electrode, a chalcogenide layer, a Heater, an Insulator, and a Bottom electrode. (c) ReRAM: Shows two cross-sections of a cell. The left one is labeled 'Reduction: low resistance' and the right one is labeled 'Oxidation: high resistance'. The cell consists of a Top electrode, an Insulator, a Metal oxide layer, and a Bottom electrode. A Filament is shown within the Metal oxide layer.\n\n\nFigure 5.19 Nonvolatile RAM Technologies"
        }
      ]
    },
    {
      "name": "External Memory",
      "sections": [
        {
          "name": "Magnetic Disk",
          "content": "A disk is a circular\n   **platter**\n   constructed of nonmagnetic material, called the\n   **substrate**\n   , coated with a magnetizable material. Traditionally, the substrate has been an aluminum or aluminum alloy material. More recently, glass substrates have been introduced. The glass substrate has a number of benefits, including the following:\n\n\n  * ■ Improvement in the uniformity of the magnetic film surface to increase disk reliability.\n  * ■ A significant reduction in overall surface defects to help reduce read-write errors.\n  * ■ Ability to support lower fly heights (described subsequently).\n  * ■ Better stiffness to reduce disk dynamics.\n  * ■ Greater ability to withstand shock and damage.\n\n\n\n\n**Magnetic Read and Write Mechanisms**\n\n\nData are recorded on and later retrieved from the disk via a conducting coil named the\n   **head**\n   ; in many systems, there are two heads, a read head and a write head. During a read or write operation, the head is stationary while the platter rotates beneath it.\n\n\nThe write mechanism exploits the fact that electricity flowing through a coil produces a magnetic field. Electric pulses are sent to the write head, and the resulting magnetic patterns are recorded on the surface below, with different patterns for positive and negative currents. The write head itself is made of easily magnetizable\n\n\n\n\n![Figure 6.1: Inductive Write/Magneto resistive Read Head. This 3D diagram illustrates the components of a hard disk drive head. The 'Recording medium' is shown as a series of rectangular blocks with alternating North (N) and South (S) poles, representing the magnetic tracks. A 'Track width' is indicated by a double-headed arrow. The 'Inductive write element' is a rectangular block with a central gap, through which a 'Write current' flows. The 'MR sensor' is a smaller rectangular block positioned close to the write element, with a 'Read current' flowing through it. A 'Shield' is placed between the write element and the MR sensor to prevent interference. The 'Magnetization' of the recording medium is shown with arrows indicating the direction of the magnetic field.](images/image_0091.jpeg)\n\n\nFigure 6.1: Inductive Write/Magneto resistive Read Head. This 3D diagram illustrates the components of a hard disk drive head. The 'Recording medium' is shown as a series of rectangular blocks with alternating North (N) and South (S) poles, representing the magnetic tracks. A 'Track width' is indicated by a double-headed arrow. The 'Inductive write element' is a rectangular block with a central gap, through which a 'Write current' flows. The 'MR sensor' is a smaller rectangular block positioned close to the write element, with a 'Read current' flowing through it. A 'Shield' is placed between the write element and the MR sensor to prevent interference. The 'Magnetization' of the recording medium is shown with arrows indicating the direction of the magnetic field.\n\n\n**Figure 6.1**\n   Inductive Write/Magneto resistive Read Head\n\n\nmaterial and is in the shape of a rectangular doughnut with a gap along one side and a few turns of conducting wire along the opposite side (Figure 6.1). An electric current in the wire induces a magnetic field across the gap, which in turn magnetizes a small area of the recording medium. Reversing the direction of the current reverses the direction of the magnetization on the recording medium.\n\n\nThe traditional read mechanism exploits the fact that a magnetic field moving relative to a coil produces an electrical current in the coil. When the surface of the disk rotates under the head, it generates a current of the same polarity as the one already recorded. The structure of the head for reading is in this case essentially the same as for writing and therefore the same head can be used for both. Such single heads are used in floppy disk systems and in older rigid disk systems.\n\n\nContemporary rigid disk systems use a different read mechanism, requiring a separate read head, positioned for convenience close to the write head. The read head consists of a partially shielded\n   **magneto resistive (MR)**\n   sensor. The MR material has an electrical resistance that depends on the direction of the magnetization of the medium moving under it. By passing a current through the MR sensor, resistance changes are detected as voltage signals. The MR design allows higher-frequency operation, which equates to greater storage densities and operating speeds.\n\n\n\n\n**Data Organization and Formatting**\n\n\nThe head is a relatively small device capable of reading from or writing to a portion of the platter rotating beneath it. This gives rise to the organization of data on the platter in a concentric set of rings, called\n   **tracks**\n   . Each track is the same width as the head. There are thousands of tracks per surface.\n\n\nFigure 6.2 depicts this data layout. Adjacent tracks are separated by\n   **intertrack gaps**\n   . This prevents, or at least minimizes, errors due to misalignment of the head or simply interference of magnetic fields. Data are transferred to and from the disk in\n   **sectors**\n   . There are typically hundreds of sectors per track, and these may be of either fixed or variable length. In most contemporary systems, fixed-length sectors are used, with 512 bytes being the nearly universal sector size. To avoid imposing unreasonable precision requirements on the system, adjacent sectors are separated by intersector gaps.\n\n\nA bit near the center of a rotating disk travels past a fixed point (such as a read-write head) slower than a bit on the outside. Therefore, some way must be found to compensate for the variation in speed so that the head can read all the bits at the same rate. This can be done by defining a variable spacing between bits of information recorded in\n\n\n\n\n![Diagram of disk data layout and physical structure.](images/image_0092.jpeg)\n\n\nThe diagram illustrates the data layout and physical structure of a magnetic disk. The top part shows a cross-section of the disk platter with concentric tracks. Each track is divided into sectors. Labels include 'Inter-track gap' (space between tracks), 'Inter-sector gap' (space between sectors on a track), 'Sector' (a segment of a track), 'Track' (a full circle of data), and 'Track sector' (a sector on a specific track). An arrow labeled 'Rotation' indicates the counter-clockwise direction of the disk's spin. The bottom part shows the physical components: the 'Platter' (the disk itself), the 'Spindle' (the central axis), and the 'Boom' (the arm that holds the read-write head). A 'Read-write head' is shown positioned over a track. The 'Cylinder' is indicated by a vertical dashed line passing through the centers of the tracks. The 'Direction of arm motion' is shown as a horizontal arrow along the boom.\n\n\nDiagram of disk data layout and physical structure.\n\n\n**Figure 6.2**\n   Disk Data Layout\n\n\nlocations on the disk, in a way that the outermost tracks has sectors with bigger spacing. The information can then be scanned at the same rate by rotating the disk at a fixed speed, known as the\n   **constant angular velocity (CAV)**\n   . Figure 6.3a shows the layout of a disk using CAV. The disk is divided into a number of pie-shaped sectors and into a series of concentric tracks. The advantage of using CAV is that individual blocks of data can be directly addressed by track and sector. To move the head from its current location to a specific address, it only takes a short movement of the head to a specific track and a short wait for the proper sector to spin under the head. The disadvantage of CAV is that the amount of data that can be stored on the long outer tracks is the only same as what can be stored on the short inner tracks.\n\n\nBecause the\n   **density**\n   , in bits per linear inch, increases in moving from the outermost track to the innermost track, disk storage capacity in a straightforward CAV system is limited by the maximum recording density that can be achieved on the innermost track. To maximize storage capacity, it would be preferable to have the same linear bit density on each track. This would require unacceptably complex circuitry. Modern hard disk systems use simpler technique, which approximates equal bit density per track, known as multiple zone recording (MZR), in which the surface is divided into a number of concentric zones (16 is typical). Each zone contains a number of contiguous tracks, typically in the thousands. Within a zone, the number of bits per track is constant. Zones farther from the center contain more bits (more sectors) than zones closer to the center. Zones are defined in such a way that the linear bit density is approximately the same on all tracks of the disk. MZR allows for greater overall storage capacity at the expense of somewhat more complex circuitry. As the disk head moves from one zone to another, the length (along the track) of individual bits changes, causing a change in the timing for reads and writes.\n\n\nFigure 6.3b is a simplified MZR layout, with 15 tracks organized into 5 zones. The innermost two zones have two tracks each, with each track having nine sectors; the next zone has 3 tracks, each with 12 sectors; and the outermost 2 zones have 4 tracks each, with each track having 16 sectors.\n\n\n\n\n![Figure 6.3 Comparison of Disk Layout Methods. (a) Constant angular velocity: A disk layout with concentric tracks of equal length and sectors of equal angular size. (b) Multiple zone recording: A disk layout with concentric zones of equal length, where tracks within a zone have equal length but tracks in different zones have different lengths.](images/image_0093.jpeg)\n\n\nFigure 6.3 consists of two diagrams comparing disk layout methods. Diagram (a), titled 'Constant angular velocity', shows a disk with concentric tracks. Each track is divided into sectors of equal angular size. The tracks are of different lengths, with the outermost tracks being longer than the innermost ones. Diagram (b), titled 'Multiple zone recording', shows a disk with concentric zones. Each zone is divided into tracks of equal length. The number of tracks per zone varies, with the outer zones having more tracks than the inner zones. The tracks are of different lengths, with the outermost tracks being longer than the innermost ones.\n\n\nFigure 6.3 Comparison of Disk Layout Methods. (a) Constant angular velocity: A disk layout with concentric tracks of equal length and sectors of equal angular size. (b) Multiple zone recording: A disk layout with concentric zones of equal length, where tracks within a zone have equal length but tracks in different zones have different lengths.\n\n\n**Figure 6.3**\n   Comparison of Disk Layout Methods\n\n\nSome means is needed to locate sector positions within a track. Clearly, there must be some starting point on the track and a way of identifying the start and end of each sector. These requirements are handled by means of control data recorded on the disk. Thus, the disk is formatted with some extra data used only by the disk drive and not accessible to the user.\n\n\nAn example of disk formatting is shown in Figure 6.4. In this case, each track contains 30 fixed-length sectors of 600 bytes each. Each sector holds 512 bytes of data plus control information useful to the disk controller. The ID field is a unique identifier or address used to locate a particular sector. The SYNCH byte is a special bit pattern that delimits the beginning of the field. The track number identifies a track on a surface. The head number identifies a head, because this disk has multiple surfaces (explained presently). The ID and data fields each contain an error-detecting code.\n\n\n\n\n**Physical Characteristics**\n\n\nTable 6.1 lists the major characteristics that differentiate among the various types of magnetic disks. First, the head may either be fixed or movable with respect to the radial direction of the platter. In a\n   **fixed-head disk**\n   , there is one read-write head per track. All of the heads are mounted on a rigid arm that extends across all tracks; such systems are rare today. In a\n   **movable-head disk**\n   , there is only one read-write head. Again, the head is mounted on an arm. Because the head must be able to be positioned above any track, the arm can be extended or retracted for this purpose.\n\n\nThe disk itself is mounted in a disk drive, which consists of the arm, a spindle that rotates the disk, and the electronics needed for input and output of binary data. A\n   **nonremovable disk**\n   is permanently mounted in the disk drive; the hard disk in a personal computer is a nonremovable disk. A\n   **removable disk**\n   can be removed and replaced with another disk. The advantage of the latter type is that unlimited amounts of data are available with a limited number of disk systems. Furthermore, such a disk may be moved from one computer system to another. Floppy disks and ZIP cartridge disks are examples of removable disks.\n\n\n\n\n![Diagram of Winchester Disk Format (Seagate ST506) showing the layout of sectors on a track. The diagram illustrates the physical structure of a disk track, showing the Index and Sector markers. It details the composition of a sector, including the Gap 1, ID field, Gap 2, Data field, and Gap 3. The diagram shows two sectors: Physical sector 0 and Physical sector 1. Physical sector 0 is shown in full, while Physical sector 1 is partially shown. The diagram also shows the byte breakdown for each field in a sector, totaling 600 bytes per sector. Below the main diagram, two smaller tables show the byte breakdown for the ID field and the Data field, respectively.](images/image_0094.jpeg)\n\n\nThe diagram illustrates the Winchester Disk Format (Seagate ST506) for a track. It shows the physical layout of sectors and the internal structure of each sector's data fields.\n\n\n**Track Layout:**\n\n\n  * **Index:**\n     Marks the start of the track.\n  * **Sector:**\n     Marks the start of each sector. The diagram shows Physical sector 0, Physical sector 1, and Physical sector 29.\n\n\n**Sector Structure (600 bytes/sector):**\n\n\n\nField | Bytes\nGap 1 | 17\nID field | 7\nGap 2 | 41\nData field | 515\nGap 3 | 20\n\n\n**Physical Sector 0 Breakdown:**\n\n\n\nField | Bytes\nGap 1 | 17\nID field 0 | 7\nGap 2 | 41\nData field 0 | 515\nGap 3 | 20\n\n\n**Physical Sector 1 Breakdown:**\n\n\n\nField | Bytes\nGap 1 | 17\nID field 1 | 7\nGap 2 | 41\nData field 1 | 515\nGap 3 | 20\n\n\n**Physical Sector 29 Breakdown:**\n\n\n\nField | Bytes\nGap 1 | 17\nID field 29 | 7\nGap 2 | 41\nData field 29 | 515\nGap 3 | 20\n\n\n**ID Field Breakdown (7 bytes):**\n\n\n\nField | Bytes\nSync byte | 1\nTrack # | 2\nHead # | 1\nSector # | 1\nCRC | 2\n\n\n**Data Field Breakdown (512 bytes):**\n\n\n\nField | Bytes\nSync byte | 1\nData | 512\nCRC | 2\n\n\nDiagram of Winchester Disk Format (Seagate ST506) showing the layout of sectors on a track. The diagram illustrates the physical structure of a disk track, showing the Index and Sector markers. It details the composition of a sector, including the Gap 1, ID field, Gap 2, Data field, and Gap 3. The diagram shows two sectors: Physical sector 0 and Physical sector 1. Physical sector 0 is shown in full, while Physical sector 1 is partially shown. The diagram also shows the byte breakdown for each field in a sector, totaling 600 bytes per sector. Below the main diagram, two smaller tables show the byte breakdown for the ID field and the Data field, respectively.\n\n\n**Figure 6.4**\n   Winchester Disk Format (Seagate ST506)\n\n\n**Table 6.1**\n\nHead Motion | Platters\nFixed head (one per track) | Single platter\nMovable head (one per surface) | Multiple platter\nDisk Portability | Head Mechanism\nNonremovable disk | Contact (floppy)\nRemovable disk | Fixed gap\nSides | Aerodynamic gap (Winchester)\nSingle sided | \nDouble sided | \n\n\nFor most disks, the magnetizable coating is applied to both sides of the platter, which is then referred to as\n   **double sided**\n   . Some less expensive disk systems use\n   **single-sided**\n   disks.\n\n\nSome disk drives accommodate\n   **multiple platters**\n   stacked vertically a fraction of an inch apart. Multiple arms are provided (Figure 6.2). Multiple-platter disks employ a movable head, with one read-write head per platter surface. All of the heads are mechanically fixed so that all are at the same distance from the center of the disk and move together. Thus, at any time, all of the heads are positioned over tracks that are of equal distance from the center of the disk. The set of all the tracks in the same relative position on the platter is referred to as a\n   **cylinder**\n   . This is illustrated in Figure 6.2.\n\n\nFinally, the head mechanism provides a classification of disks into three types. Traditionally, the read-write head has been positioned a fixed distance above the platter, allowing an air gap. At the other extreme is a head mechanism that actually comes into physical contact with the medium during a read or write operation. This mechanism is used with the\n   **floppy disk**\n   , which is a small, flexible platter and the least expensive type of disk.\n\n\nTo understand the third type of disk, we need to comment on the relationship between data density and the size of the air gap. The head must generate or sense an electromagnetic field of sufficient magnitude to write and read properly. The narrower the head is, the closer it must be to the platter surface to function. A narrower head means narrower tracks and therefore greater data density, which is desirable. However, the closer the head is to the disk, the greater the risk of error from impurities or imperfections. To push the technology further, the Winchester disk was developed. Winchester heads are used in sealed drive assemblies that are almost free of contaminants. They are designed to operate closer to the disk's surface than conventional rigid disk heads, thus allowing greater data density. The head is actually an aerodynamic foil that rests lightly on the platter's surface when the disk is motionless. The air pressure generated by a spinning disk is enough to make the foil rise above the surface. The resulting noncontact system can be engineered to use narrower heads that operate closer to the platter's surface than conventional rigid disk heads.\n\n\nTable 6.2 gives disk parameters for typical contemporary high-performance disks.\n\n\n**Table 6.2**\n\nCharacteristics | Seagate Enterprise | Seagate Barracuda XT | Seagate Cheetah NS | Seagate Laptop HDD\nApplication | Enterprise | Desktop | Network-attached storage, application servers | Laptop\nCapacity | 6 TB | 3 TB | 600 GB | 2 TB\nAverage seek time | 4.16 ms | N/A | 3.9 ms read\n      \n      4.2 ms write | 13 ms\nSpindle speed | 7200 rpm | 7200 rpm | 10,075 rpm | 5400 rpm\nAverage latency | 4.16 ms | 4.16 ms | 2.98 | 5.6 ms\nMaximum sustained transfer rate | 216 MB/sec | 149 MB/sec | 97 MB/sec | 300 MB/sec\nBytes per sector | 512/4096 | 512 | 512 | 4096\nTracks per cylinder (number of platter surfaces) | 8 | 10 | 8 | 4\nCache | 128 MB | 64 MB | 16 MB | 8 MB\n\n\n\n\n**Disk Performance Parameters**\n\n\nThe actual details of disk I/O operation depend on the computer system, the operating system, and the nature of the I/O channel and disk controller hardware. A general timing diagram of disk I/O transfer is shown in Figure 6.5.\n\n\nWhen the disk drive is operating, the disk is rotating at constant speed. To read or write, the head must be positioned at the desired track and at the beginning of the desired sector on that track. Track selection involves moving the head in a movable-head system or electronically selecting one head on a fixed-head system. On a movable-head system, the time it takes to position the head at the track is known as\n   **seek time**\n   . In either case, once the track is selected, the disk controller waits until the appropriate sector rotates to line up with the head. The time it takes for the beginning of the sector to reach the head is known as\n   **rotational delay**\n   , or\n   *rotational latency*\n   . The sum of the seek time, if any, and the rotational delay equals the\n   **access time**\n   , which is the time it takes to get into position to read or write. Once the head is in position, the read or write operation is then performed as the sector moves under the head; this is the data transfer portion of the operation; the time required for the transfer is the\n   **transfer time**\n   .\n\n\nIn addition to the access time and transfer time, there are several queuing delays normally associated with a disk I/O operation. When a process issues an I/O\n\n\n\n\n![Timing diagram of a Disk I/O Transfer showing various stages and a 'Device busy' period.](images/image_0095.jpeg)\n\n\nThe diagram illustrates the timing of a disk I/O transfer. It shows a horizontal timeline with several vertical markers. The stages are labeled as follows:\n\n\n  * **Wait for device:**\n     Represented by a series of vertical tick marks at the beginning of the timeline.\n  * **Wait for channel:**\n     Represented by a series of vertical tick marks following the 'Wait for device' phase.\n  * **Seek:**\n     Represented by a dashed vertical line.\n  * **Rotational delay:**\n     Represented by a dashed vertical line.\n  * **Data transfer:**\n     Represented by a solid vertical line.\n\n\nA double-headed arrow below the timeline, labeled\n    **Device busy**\n    , spans from the end of the 'Wait for channel' phase to the end of the 'Data transfer' phase.\n\n\nTiming diagram of a Disk I/O Transfer showing various stages and a 'Device busy' period.\n\n\n**Figure 6.5**\nrequest, it must first wait in a queue for the device to be available. At that time, the device is assigned to the process. If the device shares a single I/O channel or a set of I/O channels with other disk drives, then there may be an additional wait for the channel to be available. At that point, the seek is performed to begin disk access.\n\n\nIn some high-end systems for servers, a technique known as rotational positional sensing (RPS) is used. This works as follows: When the seek command has been issued, the channel is released to handle other I/O operations. When the seek is completed, the device determines when the data will rotate under the head. As that sector approaches the head, the device tries to reestablish the communication path back to the host. If either the control unit or the channel is busy with another I/O, then the reconnection attempt fails and the device must rotate one whole revolution before it can attempt to reconnect, which is called an RPS miss. This is an extra delay element that must be added to the timeline of Figure 6.5.\n\n\n**SEEK TIME**\n   Seek time is the time required to move the disk arm to the required track. It turns out that this is a difficult quantity to pin down. The seek time consists of two key components: the initial startup time, and the time taken to traverse the tracks that have to be crossed once the access arm is up to speed. Unfortunately, the traversal time is not a linear function of the number of tracks, but includes a settling time (time after positioning the head over the target track until track identification is confirmed).\n\n\nMuch improvement comes from smaller and lighter disk components. Some years ago, a typical disk was 14 inches (36 cm) in diameter, whereas the most common size today is 3.5 inches (8.9 cm), reducing the distance that the arm has to travel. A typical average seek time on contemporary hard disks is under 10 ms.\n\n\n**ROTATIONAL DELAY**\n   Disks, other than floppy disks, rotate at speeds ranging from 3600 rpm (for handheld devices such as digital cameras) up to, as of this writing, 20,000 rpm; at this latter speed, there is one revolution per 3 ms. Thus, on the average, the rotational delay will be 1.5 ms.\n\n\n**TRANSFER TIME**\n   The transfer time to or from the disk depends on the rotation speed of the disk in the following fashion:\n\n\nT = \\frac{b}{rN}\n\n\nwhere\n\n\nT\n   \n   = transfer time\n\n\nb\n   \n   = number of bytes to be transferred\n\n\nN\n   \n   = number of bytes on a track\n\n\nr\n   \n   = rotation speed, in revolutions per second\n\n\nThus the total average read or write time\n   \n    T_{total}\n   \n   can be expressed as\n\n\nT_{total} = T_s + \\frac{1}{2r} + \\frac{b}{rN} \\quad (6.1)\n\n\nwhere\n   \n    T_s\n   \n   is the average seek time. Note that on a zoned drive, the number of bytes per track is variable, complicating the calculation.\n   \n    1\n\n\n1\n   \n   Compare the two preceding equations to Equation (4.1).\n\n\n**A TIMING COMPARISON**\n   With the foregoing parameters defined, let us look at two different I/O operations that illustrate the danger of relying on average values. Consider a disk with an advertised average seek time of 4 ms, rotation speed of 15,000 rpm, and 512-byte sectors with 500 sectors per track. Suppose that we wish to read a file consisting of 2500 sectors for a total of 1.28 Mbytes. We would like to estimate the total time for the transfer.\n\n\nFirst, let us assume that the file is stored as compactly as possible on the disk. That is, the file occupies all of the sectors on 5 adjacent tracks (\n   \n    5 \\text{ tracks} \\times 500 \\text{ sectors/track} = 2500 \\text{ sectors}\n   \n   ). This is known as\n   *sequential organization*\n   . Now, the time to read the first track is as follows:\n\n\n\nAverage seek | 4 ms\nAverage rotational delay | 2 ms\nRead 500 sectors | \\frac{4 \\text{ ms}}{10 \\text{ ms}}\n\n\nSuppose that the remaining tracks can now be read with essentially no seek time. That is, the I/O operation can keep up with the flow from the disk. Then, at most, we need to deal with rotational delay for the four remaining tracks. Thus each successive track is read in\n   \n    2 + 4 = 6 \\text{ ms}\n   \n   . To read the entire file,\n\n\n\\text{Total time} = 10 + (4 \\times 6) = 34 \\text{ ms} = 0.034 \\text{ seconds}\n\n\nNow let us calculate the time required to read the same data using random access rather than sequential access; that is, accesses to the sectors are distributed randomly over the disk. For each sector, we have\n\n\n\nAverage seek | 4 ms\nRotational delay | 2 ms\nRead 1 sectors | \\frac{0.008 \\text{ ms}}{6.008 \\text{ ms}}\n\n\n\\text{Total time} = 2500 \\times 6.008 = 15,020 \\text{ ms} = 15.02 \\text{ seconds}\n\n\nIt is clear that the order in which sectors are read from the disk has a tremendous effect on I/O performance. In the case of file access in which multiple sectors are read or written, we have some control over the way in which sectors of data are deployed. However, even in the case of a file access, in a multiprogramming environment, there will be I/O requests competing for the same disk. Thus, it is worthwhile to examine ways in which the performance of disk I/O can be improved over that achieved with purely random access to the disk. This leads to a consideration of disk scheduling algorithms, which is the province of the operating system and beyond the scope of this book (see [STAL15] for a discussion).\n\n\n\n\n![Online Interactive Simulation logo featuring a globe and the text 'www'.](images/image_0096.jpeg)\n\n\nOnline Interactive Simulation logo featuring a globe and the text 'www'."
        },
        {
          "name": "RAID",
          "content": "As discussed earlier, the rate in improvement in secondary storage performance has been considerably less than the rate for processors and main memory. This mismatch has made the disk storage system perhaps the main focus of concern in improving overall computer system performance.\n\n\nAs in other areas of computer performance, disk storage designers recognize that if one component can only be pushed so far, additional gains in performance are to be had by using multiple parallel components. In the case of disk storage, this leads to the development of arrays of disks that operate independently and in parallel. With multiple disks, separate I/O requests can be handled in parallel, as long as the data required reside on separate disks. Further, a single I/O request can be executed in parallel if the block of data to be accessed is distributed across multiple disks.\n\n\nWith the use of multiple disks, there is a wide variety of ways in which the data can be organized and in which redundancy can be added to improve reliability. This could make it difficult to develop database schemes that are usable on a number of platforms and operating systems. Fortunately, industry has agreed on a standardized scheme for multiple-disk database design, known as RAID (Redundant Array of Independent Disks). The RAID scheme consists of seven levels,\n   \n    2\n   \n   zero through six. These levels do not imply a hierarchical relationship but designate different design architectures that share three common characteristics:\n\n\n  * 1. RAID is a set of physical disk drives viewed by the operating system as a single logical drive.\n  * 2. Data are distributed across the physical drives of an array in a scheme known as striping, described subsequently.\n  * 3. Redundant disk capacity is used to store parity information, which guarantees data recoverability in case of a disk failure.\n\n\nThe details of the second and third characteristics differ for the different RAID levels. RAID 0 and RAID 1 do not support the third characteristic.\n\n\nThe term\n   *RAID*\n   was originally coined in a paper by a group of researchers at the University of California at Berkeley [PATT88].\n   \n    3\n   \n   The paper outlined various RAID configurations and applications and introduced the definitions of the RAID levels that are still used. The RAID strategy employs multiple disk drives and distributes data in such a way as to enable simultaneous access to data from multiple drives, thereby improving I/O performance and allowing easier incremental increases in capacity.\n\n\n2\n   \n   Additional levels have been defined by some researchers and some companies, but the seven levels described in this section are the ones universally agreed on.\n\n\n3\n   \n   In that paper, the acronym RAID stood for Redundant Array of Inexpensive Disks. The term\n   *inexpensive*\n   was used to contrast the small relatively inexpensive disks in the RAID array to the alternative, a single large expensive disk (SLED). The SLED is essentially a thing of the past, with similar disk technology being used for both RAID and non-RAID configurations. Accordingly, the industry has adopted the term\n   *independent*\n   to emphasize that the RAID array creates significant performance and reliability gains.\n\n\nThe unique contribution of the RAID proposal is to address effectively the need for redundancy. Although allowing multiple heads and actuators to operate simultaneously achieves higher I/O and transfer rates, the use of multiple devices increases the probability of failure. To compensate for this decreased reliability, RAID makes use of stored parity information that enables the recovery of data lost due to a disk failure.\n\n\nWe now examine each of the RAID levels. Table 6.3 provides a rough guide to the seven levels. In the table, I/O performance is shown both in terms of data transfer capacity, or ability to move data, and I/O request rate, or ability to satisfy I/O requests, since these RAID levels inherently perform differently relative to these two metrics. Each RAID level's strong point is highlighted by darker shading. Figure 6.6 illustrates the use of the seven RAID schemes to support a data capacity requiring four disks with no redundancy. The figures highlight the layout of user data and redundant data and indicates the relative storage requirements of the various levels. We refer to these figures throughout the following discussion. Of the seven RAID levels described, only four are commonly used: RAID levels 0, 1, 5, and 6.\n\n\n\n\n**RAID Level 0**\n\n\nRAID level 0 is not a true member of the RAID family because it does not include redundancy to improve performance. However, there are a few applications, such as some on supercomputers in which performance and capacity are primary concerns and low cost is more important than improved reliability.\n\n\nFor RAID 0, the user and system data are distributed across all of the disks in the array. This has a notable advantage over the use of a single large disk: If two different I/O requests are pending for two different blocks of data, then there is a good chance that the requested blocks are on different disks. Thus, the two requests can be issued in parallel, reducing the I/O queuing time.\n\n\nBut RAID 0, as with all of the RAID levels, goes further than simply distributing the data across a disk array: The data are\n   *striped*\n   across the available disks. This is best understood by considering Figure 6.7. All of the user and system data are viewed as being stored on a logical disk. The logical disk is divided into strips; these strips may be physical blocks, sectors, or some other unit. The strips are mapped round robin to consecutive physical disks in the RAID array. A set of logically consecutive strips that maps exactly one strip to each array member is referred to as a\n   **stripe**\n   . In an\n   \n    n\n   \n   -disk array, the first\n   \n    n\n   \n   logical strips are physically stored as the first strip on each of the\n   \n    n\n   \n   disks, forming the first stripe; the second\n   \n    n\n   \n   strips are distributed as the second strips on each disk; and so on. The advantage of this layout is that if a single I/O request consists of multiple logically contiguous strips, then up to\n   \n    n\n   \n   strips for that request can be handled in parallel, greatly reducing the I/O transfer time.\n\n\nFigure 6.7 indicates the use of array management software to map between logical and physical disk space. This software may execute either in the disk subsystem or in a host computer.\n\n\n**RAID 0 FOR HIGH DATA TRANSFER CAPACITY**\n   The performance of any of the RAID levels depends critically on the request patterns of the host system and on the layout of the data. These issues can be most clearly addressed in RAID 0, where the\n\n\n**Table 6.3**\n\nCategory | Level | Description | Disks Required | Data Availability | Large I/O Data Transfer Capacity | Small I/O Request Rate\nStriping | 0 | Nonredundant | N | Lower than single disk | Very high | Very high for both read and write\nMirroring | 1 | Mirrored | 2N | Higher than RAID 2, 3, 4, or 5; lower than RAID 6 | Higher than single disk for read; similar to single disk for write | Up to twice that of a single disk for read; similar to single disk for write\nParallel access | 2 | Redundant via Hamming code | N + m | Much higher than single disk; comparable to RAID 3, 4, or 5 | Highest of all listed alternatives | Approximately twice that of a single disk\n3 | Bit-interleaved parity | N + 1 | Much higher than single disk; comparable to RAID 2, 4, or 5 | Highest of all listed alternatives | Approximately twice that of a single disk\nIndependent access | 4 | Block-interleaved parity | N + 1 | Much higher than single disk; comparable to RAID 2, 3, or 5 | Similar to RAID 0 for read; significantly lower than single disk for write | Similar to RAID 0 for read; significantly lower than single disk for write\n5 | Block-interleaved distributed parity | N + 1 | Much higher than single disk; comparable to RAID 2, 3, or 4 | Similar to RAID 0 for read; lower than single disk for write | Similar to RAID 0 for read; generally lower than single disk for write\n6 | Block-interleaved dual distributed parity | N + 2 | Highest of all listed alternatives | Similar to RAID 0 for read; lower than RAID 5 for write | Similar to RAID 0 for read; significantly lower than RAID 5 for write\n\n\nNote:\n   \n    N\n   \n   = number of data disks;\n   \n    m\n   \n   proportional to\n   \n    \\log N\n\n\n\n\n![Diagram (a) RAID 0 (Nonredundant): Four disk cylinders. Disk 1: strip 0, strip 4, strip 8, strip 12. Disk 2: strip 1, strip 5, strip 9, strip 13. Disk 3: strip 2, strip 6, strip 10, strip 14. Disk 4: strip 3, strip 7, strip 11, strip 15.](images/image_0097.jpeg)\n\n\nDiagram (a) RAID 0 (Nonredundant): Four disk cylinders. Disk 1: strip 0, strip 4, strip 8, strip 12. Disk 2: strip 1, strip 5, strip 9, strip 13. Disk 3: strip 2, strip 6, strip 10, strip 14. Disk 4: strip 3, strip 7, strip 11, strip 15.\n\n\n(a) RAID 0 (Nonredundant)\n\n\n\n\n![Diagram (b) RAID 1 (Mirrored): Eight disk cylinders. Disk 1: strip 0, strip 4, strip 8, strip 12. Disk 2: strip 1, strip 5, strip 9, strip 13. Disk 3: strip 2, strip 6, strip 10, strip 14. Disk 4: strip 3, strip 7, strip 11, strip 15. Disk 5: strip 0, strip 4, strip 8, strip 12. Disk 6: strip 1, strip 5, strip 9, strip 13. Disk 7: strip 2, strip 6, strip 10, strip 14. Disk 8: strip 3, strip 7, strip 11, strip 15.](images/image_0098.jpeg)\n\n\nDiagram (b) RAID 1 (Mirrored): Eight disk cylinders. Disk 1: strip 0, strip 4, strip 8, strip 12. Disk 2: strip 1, strip 5, strip 9, strip 13. Disk 3: strip 2, strip 6, strip 10, strip 14. Disk 4: strip 3, strip 7, strip 11, strip 15. Disk 5: strip 0, strip 4, strip 8, strip 12. Disk 6: strip 1, strip 5, strip 9, strip 13. Disk 7: strip 2, strip 6, strip 10, strip 14. Disk 8: strip 3, strip 7, strip 11, strip 15.\n\n\n(b) RAID 1 (Mirrored)\n\n\n\n\n![Diagram (c) RAID 2 (Redundancy through Hamming code): Seven disk cylinders. Disk 1: b0. Disk 2: b1. Disk 3: b2. Disk 4: b3. Disk 5: f0(b). Disk 6: f1(b). Disk 7: f2(b).](images/image_0099.jpeg)\n\n\nDiagram (c) RAID 2 (Redundancy through Hamming code): Seven disk cylinders. Disk 1: b0. Disk 2: b1. Disk 3: b2. Disk 4: b3. Disk 5: f0(b). Disk 6: f1(b). Disk 7: f2(b).\n\n\n(c) RAID 2 (Redundancy through Hamming code)\n\n\n**Figure 6.6**\nimpact of redundancy does not interfere with the analysis. First, let us consider the use of RAID 0 to achieve a high data transfer rate. For applications to experience a high transfer rate, two requirements must be met. First, a high transfer capacity must exist along the entire path between host memory and the individual disk drives. This includes internal controller buses, host system I/O buses, I/O adapters, and host memory buses.\n\n\nThe second requirement is that the application must make I/O requests that drive the disk array efficiently. This requirement is met if the typical request is for large amounts of logically contiguous data, compared to the size of a strip. In this case, a single I/O request involves the parallel transfer of data from multiple disks, increasing the effective transfer rate compared to a single-disk transfer.\n\n\n**RAID 0 FOR HIGH I/O REQUEST RATE**\n   In a transaction-oriented environment, the user is typically more concerned with response time than with transfer rate. For an individual I/O request for a small amount of data, the I/O time is dominated by the motion of the disk heads (seek time) and the movement of the disk (rotational latency).\n\n\nIn a transaction environment, there may be hundreds of I/O requests per second. A disk array can provide high I/O execution rates by balancing the I/O load across multiple disks. Effective load balancing is achieved only if there are typically\n\n\n\n\n![Diagram (d) RAID 3 (Bit-interleaved parity). Five disk cylinders are shown. The first four contain data blocks b0, b1, b2, and b3 respectively. The fifth cylinder contains parity block P(b).](images/image_0100.jpeg)\n\n\nDiagram (d) RAID 3 (Bit-interleaved parity). Five disk cylinders are shown. The first four contain data blocks b0, b1, b2, and b3 respectively. The fifth cylinder contains parity block P(b).\n\n\n(d) RAID 3 (Bit-interleaved parity)\n\n\n\n\n![Diagram (e) RAID 4 (Block-level parity). Five disk cylinders are shown. The first four contain data blocks 0, 1, 2, and 3 respectively. The fifth cylinder contains parity blocks P(0-3), P(4-7), P(8-11), and P(12-15).](images/image_0101.jpeg)\n\n\nDiagram (e) RAID 4 (Block-level parity). Five disk cylinders are shown. The first four contain data blocks 0, 1, 2, and 3 respectively. The fifth cylinder contains parity blocks P(0-3), P(4-7), P(8-11), and P(12-15).\n\n\n(e) RAID 4 (Block-level parity)\n\n\n\n\n![Diagram (f) RAID 5 (Block-level distributed parity). Five disk cylinders are shown. The first four contain data blocks 0, 1, 2, and 3 respectively. The fifth cylinder contains parity blocks P(0-3), P(4-7), P(8-11), and P(12-15).](images/image_0102.jpeg)\n\n\nDiagram (f) RAID 5 (Block-level distributed parity). Five disk cylinders are shown. The first four contain data blocks 0, 1, 2, and 3 respectively. The fifth cylinder contains parity blocks P(0-3), P(4-7), P(8-11), and P(12-15).\n\n\n(f) RAID 5 (Block-level distributed parity)\n\n\n\n\n![Diagram (g) RAID 6 (Dual redundancy). Six disk cylinders are shown. The first four contain data blocks 0, 1, 2, and 3 respectively. The fifth cylinder contains parity blocks P(0-3), P(4-7), P(8-11), and P(12-15). The sixth cylinder contains parity blocks Q(0-3), Q(4-7), Q(8-11), and Q(12-15).](images/image_0103.jpeg)\n\n\nDiagram (g) RAID 6 (Dual redundancy). Six disk cylinders are shown. The first four contain data blocks 0, 1, 2, and 3 respectively. The fifth cylinder contains parity blocks P(0-3), P(4-7), P(8-11), and P(12-15). The sixth cylinder contains parity blocks Q(0-3), Q(4-7), Q(8-11), and Q(12-15).\n\n\n(g) RAID 6 (Dual redundancy)\n\n\n**Figure 6.6**\n*Continued*\nmultiple I/O requests outstanding. This, in turn, implies that there are multiple independent applications or a single transaction-oriented application that is capable of multiple asynchronous I/O requests. The performance will also be influenced by the strip size. If the strip size is relatively large, so that a single I/O request only involves a single disk access, then multiple waiting I/O requests can be handled in parallel, reducing the queuing time for each request.\n\n\n\n\n![Diagram illustrating Data Mapping for a RAID Level 0 Array. A Logical Disk on the left contains 16 strips (0-15). Four Physical disks (0-3) on the right each contain 4 strips. Array Management Software maps strips 0-3 to Physical disk 0, strips 4-7 to Physical disk 1, strips 8-11 to Physical disk 2, and strips 12-15 to Physical disk 3. Strips 8 and 12 are highlighted in green, as are the connections from the Logical Disk to Physical disk 0 and from Physical disk 0 to the Array Management Software.](images/image_0104.jpeg)\n\n\nThe diagram shows a RAID 0 configuration with four physical disks and one logical disk. The logical disk is a vertical stack of 16 strips labeled 0 through 15. The physical disks are arranged horizontally below it. Each physical disk contains 4 strips. The mapping is as follows:\n\n\n  * Physical disk 0: strips 0, 4, 8, 12\n  * Physical disk 1: strips 1, 5, 9, 13\n  * Physical disk 2: strips 2, 6, 10, 14\n  * Physical disk 3: strips 3, 7, 11, 15\n\n\nArray Management Software is shown as a central box. Solid green lines connect the logical disk to physical disk 0, and physical disk 0 to the software. Dashed green lines connect the software to physical disks 1, 2, and 3. Strips 8 and 12 are highlighted in green, as are the connections from the logical disk to physical disk 0 and from physical disk 0 to the software.\n\n\nDiagram illustrating Data Mapping for a RAID Level 0 Array. A Logical Disk on the left contains 16 strips (0-15). Four Physical disks (0-3) on the right each contain 4 strips. Array Management Software maps strips 0-3 to Physical disk 0, strips 4-7 to Physical disk 1, strips 8-11 to Physical disk 2, and strips 12-15 to Physical disk 3. Strips 8 and 12 are highlighted in green, as are the connections from the Logical Disk to Physical disk 0 and from Physical disk 0 to the Array Management Software.\n\n\n**Figure 6.7**\n   Data Mapping for a RAID Level 0 Array\n\n\n\n\n**RAID Level 1**\n\n\nRAID 1 differs from RAID levels 2 through 6 in the way in which redundancy is achieved. In these other RAID schemes, some form of parity calculation is used to introduce redundancy, whereas in RAID 1, redundancy is achieved by the simple expedient of duplicating all the data. As Figure 6.6b shows, data striping is used, as in RAID 0. But in this case, each logical strip is mapped to two separate physical disks so that every disk in the array has a mirror disk that contains the same data. RAID 1 can also be implemented without data striping, though this is less common.\n\n\nThere are a number of positive aspects to the RAID 1 organization:\n\n\n  * 1. A read request can be serviced by either of the two disks that contains the requested data, whichever one involves the minimum seek time plus rotational latency.\n  * 2. A write request requires that both corresponding strips be updated, but this can be done in parallel. Thus, the write performance is dictated by the slower of the two writes (i.e., the one that involves the larger seek time plus rotational latency). However, there is no “write penalty” with RAID 1. RAID levels 2 through 6 involve the use of parity bits. Therefore, when a single strip is updated, the array management software must first compute and update the parity bits as well as updating the actual strip in question.\n  * 3. Recovery from a failure is simple. When a drive fails, the data may still be accessed from the second drive.\n\n\nThe principal disadvantage of RAID 1 is the cost; it requires twice the disk space of the logical disk that it supports. Because of that, a RAID 1 configuration is likely to be limited to drives that store system software and data and other highly critical files. In these cases, RAID 1 provides real-time copy of all data so that in the event of a disk failure, all of the critical data are still immediately available.\n\n\nIn a transaction-oriented environment, RAID 1 can achieve high I/O request rates if the bulk of the requests are reads. In this situation, the performance of RAID 1 can approach double of that of RAID 0. However, if a substantial fraction of the I/O requests are write requests, then there may be no significant performance gain over RAID 0. RAID 1 may also provide improved performance over RAID 0 for data transfer intensive applications with a high percentage of reads. Improvement occurs if the application can split each read request so that both disk members participate.\n\n\n\n\n**RAID Level 2**\n\n\nRAID levels 2 and 3 make use of a parallel access technique. In a parallel access array, all member disks participate in the execution of every I/O request. Typically, the spindles of the individual drives are synchronized so that each disk head is in the same position on each disk at any given time.\n\n\nAs in the other RAID schemes, data striping is used. In the case of RAID 2 and 3, the strips are very small, often as small as a single byte or word. With RAID 2, an error-correcting code is calculated across corresponding bits on each data disk, and the bits of the code are stored in the corresponding bit positions on multiple parity disks. Typically, a Hamming code is used, which is able to correct single-bit errors and detect double-bit errors.\n\n\nAlthough RAID 2 requires fewer disks than RAID 1, it is still rather costly. The number of redundant disks is proportional to the log of the number of data disks. On a single read, all disks are simultaneously accessed. The requested data and the associated error-correcting code are delivered to the array controller. If there is a single-bit error, the controller can recognize and correct the error instantly, so that the read access time is not slowed. On a single write, all data disks and parity disks must be accessed for the write operation.\n\n\nRAID 2 would only be an effective choice in an environment in which many disk errors occur. Given the high reliability of individual disks and disk drives, RAID 2 is overkill and is not implemented.\n\n\n\n\n**RAID Level 3**\n\n\nRAID 3 is organized in a similar fashion to RAID 2. The difference is that RAID 3 requires only a single redundant disk, no matter how large the disk array. RAID 3 employs parallel access, with data distributed in small strips. Instead of an error-correcting code, a simple parity bit is computed for the set of individual bits in the same position on all of the data disks.\n\n\n**REDUNDANCY**\n   In the event of a drive failure, the parity drive is accessed and data is reconstructed from the remaining devices. Once the failed drive is replaced, the missing data can be restored on the new drive and operation resumed.\n\n\nData reconstruction is simple. Consider an array of five drives in which X0 through X3 contain data and X4 is the parity disk. The parity for the\n   \n    i\n   \n   th bit is calculated as follows:\n\n\nX4(i) = X3(i) \\oplus X2(i) \\oplus X1(i) \\oplus X0(i)\n\n\nwhere\n   \n    \\oplus\n   \n   is exclusive-OR function.\n\n\nSuppose that drive X1 has failed. If we add\n   \n    X4(i) \\oplus X1(i)\n   \n   to both sides of the preceding equation, we get\n\n\nX1(i) = X4(i) \\oplus X3(i) \\oplus X2(i) \\oplus X0(i)\n\n\nThus, the contents of each strip of data on X1 can be regenerated from the contents of the corresponding strips on the remaining disks in the array. This principle is true for RAID levels 3 through 6.\n\n\nIn the event of a disk failure, all of the data are still available in what is referred to as reduced mode. In this mode, for reads, the missing data are regenerated on the fly using the exclusive-OR calculation. When data are written to a reduced RAID 3 array, consistency of the parity must be maintained for later regeneration. Return to full operation requires that the failed disk be replaced and the entire contents of the failed disk be regenerated on the new disk.\n\n\n**PERFORMANCE**\n   Because data are striped in very small strips, RAID 3 can achieve very high data transfer rates. Any I/O request will involve the parallel transfer of data from all of the data disks. For large transfers, the performance improvement is especially noticeable. On the other hand, only one I/O request can be executed at a time. Thus, in a transaction-oriented environment, performance suffers.\n\n\n\n\n**RAID Level 4**\n\n\nRAID levels 4 through 6 make use of an independent access technique. In an independent access array, each member disk operates independently, so that separate I/O requests can be satisfied in parallel. Because of this, independent access arrays are more suitable for applications that require high I/O request rates and are relatively less suited for applications that require high data transfer rates.\n\n\nAs in the other RAID schemes, data striping is used. In the case of RAID 4 through 6, the strips are relatively large. With RAID 4, a bit-by-bit parity strip is calculated across corresponding strips on each data disk, and the parity bits are stored in the corresponding strip on the parity disk.\n\n\nRAID 4 involves a write penalty when an I/O write request of small size is performed. Each time that a write occurs, the array management software must update not only the user data but also the corresponding parity bits. Consider an array of five drives in which X0 through X3 contain data and X4 is the parity disk. Suppose that a write is performed that only involves a strip on disk X1. Initially, for each bit\n   \n    i\n   \n   , we have the following relationship:\n\n\nX4(i) = X3(i) \\oplus X2(i) \\oplus X1(i) \\oplus X0(i) \\quad (6.2)\n\n\nAfter the update, with potentially altered bits indicated by a prime symbol:\n\n\n\\begin{aligned} X4'(i) &= X3(i) \\oplus X2(i) \\oplus X1'(i) \\oplus X0(i) \\\\ &= X3(i) \\oplus X2(i) \\oplus X1'(i) \\oplus X0(i) \\oplus X1(i) \\oplus X1(i) \\\\ &= X3(i) \\oplus X2(i) \\oplus X1(i) \\oplus X0(i) \\oplus X1(i) \\oplus X1'(i) \\\\ &= X4(i) \\oplus X1(i) \\oplus X1'(i) \\end{aligned}\n\n\nThe preceding set of equations is derived as follows. The first line shows that a change in\n   \n    X1\n   \n   will also affect the parity disk\n   \n    X4\n   \n   . In the second line, we add the terms\n   \n    (\\oplus X1(i) \\oplus X1(i))\n   \n   . Because the exclusive-OR of any quantity with itself is 0, this does not affect the equation. However, it is a convenience that is used to create the third line, by reordering. Finally, Equation (6.2) is used to replace the first four terms by\n   \n    X4(i)\n   \n   .\n\n\nTo calculate the new parity, the array management software must read the old user strip and the old parity strip. Then it can update these two strips with the new data and the newly calculated parity. Thus, each strip write involves two reads and two writes.\n\n\nIn the case of a larger size I/O write that involves strips on all disk drives, parity is easily computed by calculation using only the new data bits. Thus, the parity drive can be updated in parallel with the data drives and there are no extra reads or writes.\n\n\nIn any case, every write operation must involve the parity disk, which therefore can become a bottleneck.\n\n\n\n\n**RAID Level 5**\n\n\nRAID 5 is organized in a similar fashion to RAID 4. The difference is that RAID 5 distributes the parity strips across all disks. A typical allocation is a round-robin scheme, as illustrated in Figure 6.6f. For an\n   \n    n\n   \n   -disk array, the parity strip is on a different disk for the first\n   \n    n\n   \n   stripes, and the pattern then repeats.\n\n\nThe distribution of parity strips across all drives avoids the potential I/O bottle-neck found in RAID 4.\n\n\n\n\n**RAID Level 6**\n\n\nRAID 6 was introduced in a subsequent paper by the Berkeley researchers [KATZ89]. In the RAID 6 scheme, two different parity calculations are carried out and stored in separate blocks on different disks. Thus, a RAID 6 array whose user data require\n   \n    N\n   \n   disks consists of\n   \n    N + 2\n   \n   disks.\n\n\nFigure 6.6g illustrates the scheme. P and Q are two different data check algorithms. One of the two is the exclusive-OR calculation used in RAID 4 and 5. But the other is an independent data check algorithm. This makes it possible to regenerate data even if two disks containing user data fail.\n\n\nThe advantage of RAID 6 is that it provides extremely high data availability. Three disks would have to fail within the MTTR (mean time to repair) interval to cause data to be lost. On the other hand, RAID 6 incurs a substantial write penalty, because each write affects two parity blocks. Performance benchmarks [EISC07] show a RAID 6 controller can suffer more than a 30% drop in overall write performance compared with a RAID 5 implementation. RAID 5 and RAID 6 read performance is comparable.\n\n\nTable 6.4 is a comparative summary of the seven levels."
        },
        {
          "name": "Solid State Drives",
          "content": "One of the most significant developments in computer architecture in recent years is the increasing use of solid state drives (SSDs) to complement or even replace\n   **hard disk drives (HDDs)**\n   , both as internal and external secondary memory. The term\n   *solid*\n\n\n**Table 6.4**\n\nLevel | Advantages | Disadvantages | Applications\n0 | I/O performance is greatly improved by spreading the I/O load across many channels and drives\n      \n      No parity calculation overhead is involved\n      \n      Very simple design\n      \n      Easy to implement | The failure of just one drive will result in all data in an array being lost | Video production and editing\n      \n      Image Editing\n      \n      Pre-press applications\n      \n      Any application requiring high bandwidth\n1 | 100% redundancy of data means no rebuild is necessary in case of a disk failure, just a copy to the replacement disk\n      \n      Under certain circumstances, RAID 1 can sustain multiple simultaneous drive failures\n      \n      Simplest RAID storage subsystem design | Highest disk overhead of all RAID types (100%)—inefficient | Accounting\n      \n      Payroll\n      \n      Financial\n      \n      Any application requiring very high availability\n2 | Extremely high data transfer rates possible\n      \n      The higher the data transfer rate required, the better the ratio of data disks to ECC disks\n      \n      Relatively simple controller design compared to RAID levels 3, 4, & 5 | Very high ratio of ECC disks to data disks with smaller word sizes—inefficient\n      \n      Entry level cost very high—requires very high transfer rate requirement to justify | No commercial implementations exist/not commercially viable\n3 | Very high read data transfer rate\n      \n      Very high write data transfer rate\n      \n      Disk failure has an insignificant impact on throughput\n      \n      Low ratio of ECC (parity) disks to data disks means high efficiency | Transaction rate equal to that of a single disk drive at best (if spindles are synchronized)\n      \n      Controller design is fairly complex | Video production and live streaming\n      \n      Image editing\n      \n      Video editing\n      \n      Prepress applications\n      \n      Any application requiring high throughput\n4 | Very high Read data transaction rate\n      \n      Low ratio of ECC (parity) disks to data disks means high efficiency | Quite complex controller design\n      \n      Worst write transaction rate and Write aggregate transfer rate\n      \n      Difficult and inefficient data rebuild in the event of disk failure | No commercial implementations exist/not commercially viable\n5 | Highest Read data transaction rate\n      \n      Low ratio of ECC (parity) disks to data disks means high efficiency\n      \n      Good aggregate transfer rate | Most complex controller design\n      \n      Difficult to rebuild in the event of a disk failure (as compared to RAID level 1) | File and application servers\n      \n      Database servers\n      \n      Web, e-mail, and news servers\n      \n      Intranet servers\n      \n      Most versatile RAID level\n6 | Provides for an extremely high data fault tolerance and can sustain multiple simultaneous drive failures | More complex controller design\n      \n      Controller overhead to compute parity addresses is extremely high | Perfect solution for mission critical applications\n\n\n*state*\n   refers to electronic circuitry built with semiconductors. An SSD is a memory device made with solid state components that can be used as a replacement to a hard disk drive. The SSDs now on the market and coming on line use NAND flash memory, which is described in Chapter 5.\n\n\n\n\n**SSD Compared to HDD**\n\n\nAs the cost of flash-based SSDs has dropped and the performance and bit density increased, SSDs have become increasingly competitive with HDDs. Table 6.5 shows typical measures of comparison at the time of this writing.\n\n\nSSDs have the following advantages over HDDs:\n\n\n  * ■\n    **High-performance input/output operations per second (IOPS):**\n    Significantly increases performance I/O subsystems.\n  * ■\n    **Durability:**\n    Less susceptible to physical shock and vibration.\n  * ■\n    **Longer lifespan:**\n    SSDs are not susceptible to mechanical wear.\n  * ■\n    **Lower power consumption:**\n    SSDs use considerably less power than comparable-size HDDs.\n  * ■\n    **Quieter and cooler running capabilities:**\n    Less space required, lower energy costs, and a greener enterprise.\n  * ■\n    **Lower access times and latency rates:**\n    Over 10 times faster than the spinning disks in an HDD.\n\n\nCurrently, HDDs enjoy a cost per bit advantage and a capacity advantage, but these differences are shrinking.\n\n\n\n\n**SSD Organization**\n\n\nFigure 6.8 illustrates a general view of the common architectural system component associated with any SSD system. On the host system, the operating system invokes file system software to access data on the disk. The file system, in turn, invokes I/O driver software. The I/O driver software provides host access to the particular SSD product. The interface component in Figure 6.8 refers to the physical and electrical interface between the host processor and the SSD peripheral device. If the device is an internal hard drive, a common interface is PCIe. For external devices, one common interface is USB.\n\n\n**Table 6.5**\n   Comparison of Solid State Drives and Disk Drives\n\n\n\n | NAND Flash Drives | Seagate Laptop Internal HDD\nFile copy/write speed | 200–550 Mbps | 50–120 Mbps\nPower draw/battery life | Less power draw, averages 2–3 watts, resulting in 30+ minute battery boost | More power draw, averages 6–7 watts and therefore uses more battery\nStorage capacity | Typically not larger than 512 GB for notebook size drives; 1 TB max for desktops | Typically around 500 GB and 2 TB max for notebook size drives; 4 TB max for desktops\nCost | Approx. $0.50 per GB for a 1-TB drive | Approx. $0.15 per GB for a 4-TB drive\n\n\n\n\n![Figure 6.8: Solid State Drive Architecture. The diagram shows a Host system connected to an SSD. The Host system contains Operating system software, File system software, I/O driver software, and an Interface. The SSD contains an Interface, Controller, Addressing, Data buffer/cache, Error correction, and multiple Flash memory components. A bidirectional arrow connects the Host system's Interface to the SSD's Interface.](images/image_0105.jpeg)\n\n\ngraph TD\n    subgraph Host system\n        OS[Operating system software]\n        FS[File system software]\n        IOD[I/O driver software]\n        InterfaceH[Interface]\n    end\n    subgraph SSD\n        InterfaceS[Interface]\n        Controller[Controller]\n        Addressing[Addressing]\n        DB[Data buffer/cache]\n        EC[Error correction]\n        FMC1[Flash memory components]\n        FMC2[Flash memory components]\n        FMC3[Flash memory components]\n        FMC4[Flash memory components]\n    end\n    InterfaceH <--> InterfaceS\n    OS --> IOD\n    IOD --> InterfaceH\n    InterfaceS --> Controller\n    Controller --> Addressing\n    Addressing --> DB\n    Addressing --> EC\n    DB --> FMC1\n    EC --> FMC1\n    DB --> FMC2\n    EC --> FMC2\n    DB --> FMC3\n    EC --> FMC3\n    DB --> FMC4\n    EC --> FMC4\n    \nFigure 6.8: Solid State Drive Architecture. The diagram shows a Host system connected to an SSD. The Host system contains Operating system software, File system software, I/O driver software, and an Interface. The SSD contains an Interface, Controller, Addressing, Data buffer/cache, Error correction, and multiple Flash memory components. A bidirectional arrow connects the Host system's Interface to the SSD's Interface.\n\n\n**Figure 6.8**\n   Solid State Drive Architecture\n\n\nIn addition to the interface to the host system, the SSD contains the following components:\n\n\n  * ■\n    **Controller:**\n    Provides SSD device level interfacing and firmware execution.\n  * ■\n    **Addressing:**\n    Logic that performs the selection function across the flash memory components.\n  * ■\n    **Data buffer/cache:**\n    High speed RAM memory components used for speed matching and to increased data throughput.\n\n\n  * ■\n    **Error correction:**\n    Logic for error detection and correction.\n  * ■\n    **Flash memory components:**\n    Individual NAND flash chips.\n\n\n\n\n**Practical Issues**\n\n\nThere are two practical issues peculiar to SSDs that are not faced by HDDs. First, SSD performance has a tendency to slow down as the device is used. To understand the reason for this, you need to know that files are stored on disk as a set of pages, typically 4 KB in length. These pages are not necessarily, and indeed not typically, stored as a contiguous set of pages on the disk. The reason for this arrangement is explained in our discussion of virtual memory in Chapter 8. However, flash memory is accessed in blocks, with a typical block size of 512 KB, so that there are typically 128 pages per block. Now consider what must be done to write a page onto a flash memory.\n\n\n  * 1. The entire block must be read from the flash memory and placed in a RAM buffer. Then the appropriate page in the RAM buffer is updated.\n  * 2. Before the block can be written back to flash memory, the entire block of flash memory must be erased—it is not possible to erase just one page of the flash memory.\n  * 3. The entire block from the buffer is now written back to the flash memory.\n\n\nNow, when a flash drive is relatively empty and a new file is created, the pages of that file are written on to the drive contiguously, so that one or only a few blocks are affected. However, over time, because of the way virtual memory works, files become fragmented, with pages scattered over multiple blocks. As the drive becomes more occupied, there is more fragmentation, so the writing of a new file can affect multiple blocks. Thus, the writing of multiple pages from one block becomes slower, the more fully occupied the disk is. Manufacturers have developed a variety of techniques to compensate for this property of flash memory, such as setting aside a substantial portion of the SSD as extra space for write operations (called over-provisioning), then to erase inactive pages during idle time used to defragment the disk. Another technique is the TRIM command, which allows an operating system to inform an SSD which blocks of data are no longer considered in use and can be wiped internally.\n   \n    4\n\n\nA second practical issue with flash memory drives is that a flash memory becomes unusable after a certain number of writes. As flash cells are stressed, they lose their ability to record and retain values. A typical limit is 100,000 writes [GSOE08]. Techniques for prolonging the life of an SSD drive include front-ending the flash with a cache to delay and group write operations, using wear-leveling algorithms that evenly distribute writes across block of cells, and sophisticated bad-block management techniques. In addition, vendors are deploying SSDs in RAID configurations to further reduce the probability of data loss. Most flash devices are also capable of estimating their own remaining lifetimes so systems can anticipate failure and take preemptive action.\n\n\n4\n   \n   While TRIM is frequently spelled in capital letters, it is not an acronym; it is merely a command name."
        },
        {
          "name": "Optical Memory",
          "content": "In 1983, one of the most successful consumer products of all time was introduced: the compact disk (CD) digital audio system. The CD is a nonerasable disk that can store more than 60 minutes of audio information on one side. The huge commercial success of the CD enabled the development of low-cost optical-disk storage technology that has revolutionized computer data storage. A variety of optical-disk systems have been introduced (Table 6.6). We briefly review each of these.\n\n\n\n\n**Compact Disk**\n\n\n**CD-ROM**\n   Both the audio CD and the\n   **CD-ROM**\n   (compact disk read-only memory) share a similar technology. The main difference is that CD-ROM players are more rugged and have error correction devices to ensure that data are properly transferred from disk to computer. Both types of disk are made the same way. The disk is formed from a resin, such as polycarbonate. Digitally recorded information (either music or computer data) is imprinted as a series of microscopic pits on the surface of the polycarbonate. This is done, first of all, with a finely focused, high-intensity laser to create a master disk. The master is used, in turn, to make a die to stamp out copies onto polycarbonate. The pitted surface is then coated with a highly reflective surface, usually aluminum or gold. This shiny surface is protected against dust and scratches by a top coat of clear acrylic. Finally, a label can be silkscreened onto the acrylic.\n\n\n**Table 6.6**\n   Optical Disk Products\n\n\n\nCD | Compact Disk. A nonerasable disk that stores digitized audio information. The standard system uses 12-cm disks and can record more than 60 minutes of uninterrupted playing time.\nCD-ROM | Compact Disk Read-Only Memory. A nonerasable disk used for storing computer data. The standard system uses 12-cm disks and can hold more than 650 Mbytes.\nCD-R | CD Recordable. Similar to a CD-ROM. The user can write to the disk only once.\nCD-RW | CD Rewritable. Similar to a CD-ROM. The user can erase and rewrite to the disk multiple times.\nDVD | Digital Versatile Disk. A technology for producing digitized, compressed representation of video information, as well as large volumes of other digital data. Both 8 and 12 cm diameters are used, with a double-sided capacity of up to 17 Gbytes. The basic DVD is read-only (DVD-ROM).\nDVD-R | DVD Recordable. Similar to a DVD-ROM. The user can write to the disk only once. Only one-sided disks can be used.\nDVD-RW | DVD Rewritable. Similar to a DVD-ROM. The user can erase and rewrite to the disk multiple times. Only one-sided disks can be used.\nBlu-ray DVD | High-definition video disk. Provides considerably greater data storage density than DVD, using a 405-nm (blue-violet) laser. A single layer on a single side can store 25 Gbytes.\n\n\nInformation is retrieved from a CD or CD-ROM by a low-powered laser housed in an optical-disk player, or drive unit. The laser shines through the clear polycarbonate while a motor spins the disk past it (Figure 6.9). The intensity of the reflected light of the laser changes as it encounters a\n   **pit**\n   . Specifically, if the laser beam falls on a pit, which has a somewhat rough surface, the light scatters and a low intensity is reflected back to the source. The areas between pits are called\n   **lands**\n   . A land is a smooth surface, which reflects back at higher intensity. The change between pits and lands is detected by a photosensor and converted into a digital signal. The sensor tests the surface at regular intervals. The beginning or end of a pit represents a 1; when no change in elevation occurs between intervals, a 0 is recorded.\n\n\nRecall that on a magnetic disk, information is recorded in concentric tracks. With the simplest constant angular velocity (CAV) system, the number of bits per track is constant. An increase in density is achieved with\n   **multiple zone recording**\n   , in which the surface is divided into a number of zones, with zones farther from the center containing more bits than zones closer to the center. Although this technique increases capacity, it is still not optimal.\n\n\nTo achieve greater capacity, CDs and CD-ROMs do not organize information on concentric tracks. Instead, the disk contains a single spiral track, beginning near the center and spiraling out to the outer edge of the disk. Sectors near the outside of the disk are the same length as those near the inside. Thus, information is packed evenly across the disk in segments of the same size and these are scanned at the same rate by rotating the disk at a variable speed. The pits are then read by the laser at a\n   **constant linear velocity (CLV)**\n   . The disk rotates more slowly for accesses near the outer edge than for those near the center. Thus, the capacity of a track and the rotational delay both increase for positions nearer the outer edge of the disk. The data capacity for a CD-ROM is about 680 MB.\n\n\nData on the CD-ROM are organized as a sequence of blocks. A typical block format is shown in Figure 6.10. It consists of the following fields:\n\n\n  * ■\n    **Sync:**\n    The sync field identifies the beginning of a block. It consists of a byte of all 0s, 10 bytes of all 1s, and a byte of all 0s.\n  * ■\n    **Header:**\n    The header contains the block address and the mode byte. Mode 0 specifies a blank data field; mode 1 specifies the use of an error-correcting\n\n\n\n\n![Diagram of a CD structure and laser operation. The CD is shown as a cross-section with layers: Protective acrylic (top), Label (middle), Polycarbonate plastic (bottom), and Aluminum (reflective layer). A spiral track is shown on the polycarbonate layer. A laser beam is shown passing through the layers and reflecting off the aluminum layer. The track is composed of 'Land' (flat) and 'Pit' (depressed) sections. Arrows indicate the laser's path and the reflection back to the receiver.](images/image_0106.jpeg)\n\n\nThe diagram illustrates the structure of a CD and the process of laser reading. It shows a cross-section of the disc with the following layers from top to bottom: Protective acrylic, Label, Polycarbonate plastic, and Aluminum. A spiral track is engraved on the polycarbonate layer. The track consists of flat areas called 'Land' and depressions called 'Pit'. A laser beam is shown passing through the layers and reflecting off the aluminum layer. The laser is positioned to read the track, and the reflected light is captured by a receiver. The diagram also shows the laser transmit/receive path at the bottom.\n\n\nDiagram of a CD structure and laser operation. The CD is shown as a cross-section with layers: Protective acrylic (top), Label (middle), Polycarbonate plastic (bottom), and Aluminum (reflective layer). A spiral track is shown on the polycarbonate layer. A laser beam is shown passing through the layers and reflecting off the aluminum layer. The track is composed of 'Land' (flat) and 'Pit' (depressed) sections. Arrows indicate the laser's path and the reflection back to the receiver.\n\n\n**Figure 6.9**\n   CD Operation\n\n\n\n\n![Figure 6.10: CD-ROM Block Format. A diagram showing the structure of a CD-ROM block. The top part is a table with columns: 00, FF ... FF, 00, MIN, SEC, Sector, Mode, Data, and Layered ECC. Below the table, horizontal arrows indicate the size of each section: 12 bytes SYNC, 4 bytes ID, 2048 bytes Data, and 288 bytes L-ECC. A long arrow at the bottom indicates the total size of 2352 bytes.](images/image_0107.jpeg)\n\n\n00 | FF ... FF | 00 | MIN | SEC | Sector | Mode | Data | Layered ECC\n\n\n12 bytes SYNC    4 bytes ID    2048 bytes Data    288 bytes L-ECC\n\n\n2352 bytes\n\n\nFigure 6.10: CD-ROM Block Format. A diagram showing the structure of a CD-ROM block. The top part is a table with columns: 00, FF ... FF, 00, MIN, SEC, Sector, Mode, Data, and Layered ECC. Below the table, horizontal arrows indicate the size of each section: 12 bytes SYNC, 4 bytes ID, 2048 bytes Data, and 288 bytes L-ECC. A long arrow at the bottom indicates the total size of 2352 bytes.\n\n\n**Figure 6.10**\n   CD-ROM Block Format\n\n\ncode and 2048 bytes of data; mode 2 specifies 2336 bytes of user data with no error-correcting code.\n\n\n  * ■\n    **Data:**\n    User data.\n  * ■\n    **Auxiliary:**\n    Additional user data in mode 2. In mode 1, this is a 288-byte error-correcting code.\n\n\nWith the use of CLV, random access becomes more difficult. Locating a specific address involves moving the head to the general area, adjusting the rotation speed and reading the address, and then making minor adjustments to find and access the specific sector.\n\n\nCD-ROM is appropriate for the distribution of large amounts of data to a large number of users. Because of the expense of the initial writing process, it is not appropriate for individualized applications. Compared with traditional magnetic disks, the CD-ROM has two advantages:\n\n\n  * ■ The optical disk together with the information stored on it can be mass replicated inexpensively—unlike a magnetic disk. The database on a magnetic disk has to be reproduced by copying one disk at a time using two disk drives.\n  * ■ The optical disk is removable, allowing the disk itself to be used for archival storage. Most magnetic disks are nonremovable. The information on nonremovable magnetic disks must first be copied to another storage medium before the disk drive/disk can be used to store new information.\n\n\nThe disadvantages of CD-ROM are as follows:\n\n\n  * ■ It is read-only and cannot be updated.\n  * ■ It has an access time much longer than that of a magnetic disk drive, as much as half a second.\n\n\n**CD RECORDABLE**\n   To accommodate applications in which only one or a small number of copies of a set of data is needed, the write-once read-many CD, known as the\n   **CD recordable (CD-R)**\n   , has been developed. For CD-R, a disk is prepared in such a way that it can be subsequently written once with a laser beam of modest-intensity. Thus, with a somewhat more expensive disk controller than for CD-ROM, the customer can write once as well as read the disk.\n\n\nThe CD-R medium is similar to but not identical to that of a CD or CD-ROM. For CDs and CD-ROMs, information is recorded by the pitting of the surface\n\n\nof the medium, which changes reflectivity. For a CD-R, the medium includes a dye layer. The dye is used to change reflectivity and is activated by a high-intensity laser. The resulting disk can be read on a CD-R drive or a CD-ROM drive.\n\n\nThe CD-R optical disk is attractive for archival storage of documents and files. It provides a permanent record of large volumes of user data.\n\n\n**CD REWRITABLE**\n   The\n   **CD-RW**\n   optical disk can be repeatedly written and overwritten, as with a magnetic disk. Although a number of approaches have been tried, the only pure optical approach that has proved attractive is called\n   **phase change**\n   . The phase change disk uses a material that has two significantly different reflectivities in two different phase states. There is an amorphous state, in which the molecules exhibit a random orientation that reflects light poorly; and a crystalline state, which has a smooth surface that reflects light well. A beam of laser light can change the material from one phase to the other. The primary disadvantage of phase change optical disks is that the material eventually and permanently loses its desirable properties. Current materials can be used for between 500,000 and 1,000,000 erase cycles.\n\n\nThe CD-RW has the obvious advantage over CD-ROM and CD-R that it can be rewritten and thus used as a true secondary storage. As such, it competes with magnetic disk. A key advantage of the optical disk is that the engineering tolerances for optical disks are much less severe than for high-capacity magnetic disks. Thus, they exhibit higher reliability and longer life.\n\n\n\n\n**Digital Versatile Disk**\n\n\nWith the capacious\n   **digital versatile disk (DVD)**\n   , the electronics industry has at last found an acceptable replacement for the analog VHS video tape. The DVD has replaced the videotape used in video cassette recorders (VCRs) and, more important for this discussion, replaced the CD-ROM in personal computers and servers. The DVD takes video into the digital age. It delivers movies with impressive picture quality, and it can be randomly accessed like audio CDs, which DVD machines can also play. Vast volumes of data can be crammed onto the disk, currently seven times as much as a CD-ROM. With DVD's huge storage capacity and vivid quality, PC games have become more realistic and educational software incorporates more video. Following in the wake of these developments has been a new crest of traffic over the Internet and corporate intranets, as this material is incorporated into Web sites.\n\n\nThe DVD's greater capacity is due to three differences from CDs (Figure 6.11):\n\n\n  * 1. Bits are packed more closely on a DVD. The spacing between loops of a spiral on a CD is\n    \n     1.6 \\mu\\text{m}\n    \n    and the minimum distance between pits along the spiral is\n    \n     0.834 \\mu\\text{m}\n    \n    .\n\n\nThe DVD uses a laser with shorter wavelength and achieves a loop spacing of\n   \n    0.74 \\mu\\text{m}\n   \n   and a minimum distance between pits of\n   \n    0.4 \\mu\\text{m}\n   \n   . The result of these two improvements is about a seven-fold increase in capacity, to about 4.7 GB.\n\n\n  * 2. The DVD employs a second layer of pits and lands on top of the first layer. A dual-layer DVD has a semireflective layer on top of the reflective layer, and by adjusting focus, the lasers in DVD drives can read each layer separately. This technique almost doubles the capacity of the disk, to about 8.5 GB. The lower reflectivity of the second layer limits its storage capacity so that a full doubling is not achieved.\n\n\n\n\n![Diagram (a) showing the cross-section of a CD-ROM. It consists of a polycarbonate substrate (plastic) with a reflective layer (aluminum) on top, covered by a protective layer (acrylic) and a label. A laser beam is shown focusing on the pits in front of the reflective layer. The total thickness is 1.2 mm.](images/image_0108.jpeg)\n\n\nLabel\n\n\nProtective layer (acrylic)\n\n\nReflective layer (aluminum)\n\n\nPolycarbonate substrate (plastic)\n\n\n1.2 mm thick\n\n\nLaser focuses on polycarbonate pits in front of reflective layer\n\n\nDiagram (a) showing the cross-section of a CD-ROM. It consists of a polycarbonate substrate (plastic) with a reflective layer (aluminum) on top, covered by a protective layer (acrylic) and a label. A laser beam is shown focusing on the pits in front of the reflective layer. The total thickness is 1.2 mm.\n\n\n(a) CD-ROM—Capacity 682 MB\n\n\n\n\n![Diagram (b) showing the cross-section of a double-sided, dual-layer DVD-ROM. It has two sides, each with a polycarbonate substrate, a semireflective layer, and a fully reflective layer. A laser beam is shown focusing on pits in one layer on one side at a time. The total thickness is 1.2 mm.](images/image_0109.jpeg)\n\n\nPolycarbonate substrate, side 2\n\n\nSemireflective layer, side 2\n\n\nPolycarbonate layer, side 2\n\n\nFully reflective layer, side 2\n\n\nFully reflective layer, side 1\n\n\nPolycarbonate layer, side 1\n\n\nSemireflective layer, side 1\n\n\nPolycarbonate substrate, side 1\n\n\n1.2 mm thick\n\n\nLaser focuses on pits in one layer on one side at a time. Disk must be flipped to read other side\n\n\nDiagram (b) showing the cross-section of a double-sided, dual-layer DVD-ROM. It has two sides, each with a polycarbonate substrate, a semireflective layer, and a fully reflective layer. A laser beam is shown focusing on pits in one layer on one side at a time. The total thickness is 1.2 mm.\n\n\n(b) DVD-ROM, double-sided, dual-layer—Capacity 17 GB\n\n\n**Figure 6.11**\n  * 3. The\n    **DVD-ROM**\n    can be two sided, whereas data are recorded on only one side of a CD. This brings total capacity up to 17 GB.\n\n\nAs with the CD, DVDs come in writeable as well as read-only versions (Table 6.6).\n\n\n\n\n**High-Definition Optical Disks**\n\n\nHigh-definition optical disks are designed to store high-definition videos and to provide significantly greater storage capacity compared to DVDs. The higher bit density is achieved by using a laser with a shorter wavelength, in the blue-violet range. The data pits, which constitute the digital 1s and 0s, are smaller on the high-definition optical disks compared to DVD because of the shorter laser wavelength.\n\n\nTwo competing disk formats and technologies initially competed for market acceptance: HD DVD and\n   **Blu-ray**\n   DVD. The Blu-ray scheme ultimately achieved market dominance. The HD DVD scheme can store 15 GB on a single layer on a single side. Blu-ray positions the data layer on the disk closer to the laser (shown on the right-hand side of each diagram in Figure 6.12). This enables a tighter focus and less distortion and thus smaller pits and tracks. Blu-ray can store 25 GB on a single layer. Three versions are available: read only (BD-ROM), recordable once (BD-R), and rerecordable (BD-RE).\n\n\n\n\n![Figure 6.12: Optical Memory Characteristics. This figure compares the physical characteristics of CD, DVD, and Blu-ray discs. The CD section shows a beam spot on a track with a pit and land, with a pit length of 2.11 μm and a laser wavelength of 780 nm. The DVD section shows a beam spot on a track with a pit length of 1.32 μm and a laser wavelength of 650 nm. The Blu-ray section shows a beam spot on a track with a pit length of 0.58 μm and a laser wavelength of 405 nm. Each section also includes a diagram of the laser pickup assembly with its height and width.](images/image_0110.jpeg)\n\n\nFigure 6.12 illustrates the optical memory characteristics for CD, DVD, and Blu-ray discs, showing the physical dimensions and laser wavelengths used for data reading and writing.\n\n\n\nDisc Type | Pit Length | Laser Wavelength | Pickup Assembly Height | Pickup Assembly Width\nCD | 2.11 \\mu\\text{m} | 780 \\text{ nm} | 1.2 \\mu\\text{m} | 0.1 \\mu\\text{m}\nDVD | 1.32 \\mu\\text{m} | 650 \\text{ nm} | 0.6 \\mu\\text{m} | 0.1 \\mu\\text{m}\nBlu-ray | 0.58 \\mu\\text{m} | 405 \\text{ nm} | 0.1 \\mu\\text{m} | 0.1 \\mu\\text{m}\n\n\nFigure 6.12: Optical Memory Characteristics. This figure compares the physical characteristics of CD, DVD, and Blu-ray discs. The CD section shows a beam spot on a track with a pit and land, with a pit length of 2.11 μm and a laser wavelength of 780 nm. The DVD section shows a beam spot on a track with a pit length of 1.32 μm and a laser wavelength of 650 nm. The Blu-ray section shows a beam spot on a track with a pit length of 0.58 μm and a laser wavelength of 405 nm. Each section also includes a diagram of the laser pickup assembly with its height and width.\n\n\nFigure 6.12 Optical Memory Characteristics"
        },
        {
          "name": "Magnetic Tape",
          "content": "Tape systems use the same reading and recording techniques as disk systems. The medium is flexible polyester (similar to that used in some clothing) tape coated with magnetizable material. The coating may consist of particles of pure metal in special binders or vapor-plated metal films. The tape and the tape drive are analogous to a home tape recorder system. Tape widths vary from 0.38 cm (0.15 inch) to 1.27 cm (0.5 inch). Tapes used to be packaged as open reels that have to be threaded through a second spindle for use. Today, virtually all tapes are housed in cartridges.\n\n\nData on the tape are structured as a number of parallel tracks running lengthwise. Earlier tape systems typically used nine tracks. This made it possible to store data one byte at a time, with an additional parity bit as the ninth track. This was followed by tape systems using 18 or 36 tracks, corresponding to a digital word or double word. The recording of data in this form is referred to as\n   **parallel recording**\n   . Most modern systems instead use\n   **serial recording**\n   , in which data are laid out as a sequence of bits along each track, as is done with magnetic disks. As with the disk, data are read and written in contiguous blocks, called\n   *physical records*\n   , on a tape. Blocks on the tape are separated by gaps referred to as\n   *interrecord gaps*\n   . As with the disk, the tape is formatted to assist in locating physical records.\n\n\nThe typical recording technique used in serial tapes is referred to as\n   **serpentine recording**\n   . In this technique, when data are being recorded, the first set of bits is recorded along the whole length of the tape. When the end of the tape is reached,\n\n\nthe heads are repositioned to record a new track, and the tape is again recorded on its whole length, this time in the opposite direction. That process continues, back and forth, until the tape is full (Figure 6.13a). To increase speed, the read-write head is capable of reading and writing a number of adjacent tracks simultaneously (typically two to eight tracks). Data are still recorded serially along individual tracks, but blocks in sequence are stored on adjacent tracks, as suggested by Figure 6.13b.\n\n\nA tape drive is a\n   *sequential-access*\n   device. If the tape head is positioned at record 1, then to read record\n   \n    N\n   \n   , it is necessary to read physical records 1 through\n   \n    N-1\n   \n   , one at a time. If the head is currently positioned beyond the desired record, it is necessary to rewind the tape a certain distance and begin reading forward. Unlike the disk, the tape is in motion only during a read or write operation.\n\n\nIn contrast to the tape, the disk drive is referred to as a\n   *direct-access*\n   device. A disk drive need not read all the sectors on a disk sequentially to get to the desired one. It must only wait for the intervening sectors within one track and can make successive accesses to any track.\n\n\nMagnetic tape was the first kind of secondary memory. It is still widely used as the lowest-cost, slowest-speed member of the memory hierarchy.\n\n\n\n\n![Diagram (a) showing serpentine reading and writing on a tape. Three tracks (Track 0, Track 1, Track 2) are shown with vertical lines representing records. The tape moves from bottom to top. Arrows indicate the direction of read-write: right for Track 2, left for Track 1, and right for Track 0. An arrow points to the 'Bottom edge of tape'.](images/image_0111.jpeg)\n\n\nDiagram (a) showing serpentine reading and writing on a tape. Three tracks (Track 0, Track 1, Track 2) are shown with vertical lines representing records. The tape moves from bottom to top. Arrows indicate the direction of read-write: right for Track 2, left for Track 1, and right for Track 0. An arrow points to the 'Bottom edge of tape'.\n\n\n(a) Serpentine reading and writing\n\n\n\n\n![Diagram (b) showing block layout for a system that reads-writes four tracks simultaneously. Four tracks (Track 0, Track 1, Track 2, Track 3) are shown with numbered blocks (1-20) arranged in a diagonal pattern. Track 0 has blocks 1, 5, 9, 13, 17. Track 1 has blocks 2, 6, 10, 14, 18. Track 2 has blocks 3, 7, 11, 15, 19. Track 3 has blocks 4, 8, 12, 16, 20. An arrow indicates the 'Direction of tape motion' from right to left.](images/image_0112.jpeg)\n\n\nDiagram (b) showing block layout for a system that reads-writes four tracks simultaneously. Four tracks (Track 0, Track 1, Track 2, Track 3) are shown with numbered blocks (1-20) arranged in a diagonal pattern. Track 0 has blocks 1, 5, 9, 13, 17. Track 1 has blocks 2, 6, 10, 14, 18. Track 2 has blocks 3, 7, 11, 15, 19. Track 3 has blocks 4, 8, 12, 16, 20. An arrow indicates the 'Direction of tape motion' from right to left.\n\n\n(b) Block layout for system that reads-writes four tracks simultaneously\n\n\n**Figure 6.13**\n   Typical Magnetic Tape Features\n\n\n**Table 6.7**\n\n | LTO-1 | LTO-2 | LTO-3 | LTO-4 | LTO-5 | LTO-6 | LTO-7 | LTO-8\nRelease date | 2000 | 2003 | 2005 | 2007 | 2010 | 2012 | TBA | TBA\nCompressed capacity | 200 GB | 400 GB | 800 GB | 1600 GB | 3.2 TB | 8 TB | 16 TB | 32 TB\nCompressed transfer rate | 40 MB/s | 80 MB/s | 160 MB/s | 240 MB/s | 280 MB/s | 400 MB/s | 788 MB/s | 1.18 GB/s\nLinear density (bits/mm) | 4880 | 7398 | 9638 | 13,250 | 15,142 | 15,143 |  | \nTape tracks | 384 | 512 | 704 | 896 | 1280 | 2176 |  | \nTape length (m) | 609 | 609 | 680 | 820 | 846 | 846 |  | \nTape width (cm) | 1.27 | 1.27 | 1.27 | 1.27 | 1.27 | 1.27 |  | \nWrite elements | 8 | 8 | 16 | 16 | 16 | 16 |  | \nWORM? | No | No | Yes | Yes | Yes | Yes | Yes | Yes\nEncryption Capable? | No | No | No | Yes | Yes | Yes | Yes | Yes\nPartitioning? | No | No | No | No | Yes | Yes | Yes | Yes\n\n\nThe dominant tape technology today is a cartridge system known as linear tape-open (LTO). LTO was developed in the late 1990s as an open-source alternative to the various proprietary systems on the market. Table 6.7 shows parameters for the various LTO generations. See Appendix J for details."
        }
      ]
    },
    {
      "name": "Input/Output",
      "sections": [
        {
          "name": "External Devices",
          "content": "I/O operations are accomplished through a wide assortment of external devices that provide a means of exchanging data between the external environment and the computer. An external device attaches to the computer by a link to an I/O module (Figure 7.1). The link is used to exchange control, status, and data between the I/O module and the external device. An external device connected to an I/O module is often referred to as a\n   *peripheral device*\n   or, simply, a\n   *peripheral*\n   .\n\n\nWe can broadly classify external devices into three categories:\n\n\n  * ■\n    **Human readable:**\n    Suitable for communicating with the computer user;\n  * ■\n    **Machine readable:**\n    Suitable for communicating with equipment;\n  * ■\n    **Communication:**\n    Suitable for communicating with remote devices.\n\n\nExamples of human-readable devices are video display terminals (VDTs) and printers. Examples of machine-readable devices are magnetic disk and tape systems, and sensors and actuators, such as are used in a robotics application. Note that we are viewing disk and tape systems as I/O devices in this chapter, whereas in Chapter 6 we viewed them as memory devices. From a functional point of view, these devices are part of the memory hierarchy, and their use is appropriately discussed in Chapter 6. From a structural point of view, these devices are controlled by I/O modules and are hence to be considered in this chapter.\n\n\nCommunication devices allow a computer to exchange data with a remote device, which may be a human-readable device, such as a terminal, a machine-readable device, or even another computer.\n\n\nIn very general terms, the nature of an external device is indicated in Figure 7.2. The interface to the I/O module is in the form of control, data, and status signals.\n   *Control signals*\n   determine the function that the device will perform, such as send data to the I/O module (INPUT or READ), accept data from the I/O module (OUTPUT or WRITE), report status, or perform some control function particular to the device (e.g., position a disk head).\n   *Data*\n   are in the form of a set of bits to be sent to or received from the I/O module.\n   *Status signals*\n   indicate the state of the device. Examples are READY/NOT-READY to show whether the device is ready for data transfer.\n\n\n*Control logic*\n   associated with the device controls the device's operation in response to direction from the I/O module. The\n   *transducer*\n   converts data from electrical to other forms of energy during output and from other forms to electrical during input. Typically, a buffer is associated with the transducer to temporarily hold data being transferred between the I/O module and the external environment. A buffer size of 8 to 16 bits is common for serial devices, whereas block-oriented devices such as disk drive controllers may have much larger buffers.\n\n\nThe interface between the I/O module and the external device will be examined in Section 7.7. The interface between the external device and the environment is beyond the scope of this book, but several brief examples are given here.\n\n\n\n\n**Keyboard/Monitor**\n\n\nThe most common means of computer/user interaction is a keyboard/monitor arrangement. The user provides input through the keyboard, the input is then transmitted to the computer and may also be displayed on the monitor. In addition, the monitor displays data provided by the computer.\n\n\n\n\n![Block Diagram of an External Device](images/image_0113.jpeg)\n\n\nThe diagram illustrates the internal structure and external interfaces of an external device. It consists of a large gray rectangular box representing the device's internal components. Inside this box, there are three main functional blocks: 'Control logic', 'Buffer', and 'Transducer'. The 'Control logic' block is on the left, the 'Buffer' block is in the center, and the 'Transducer' block is on the right. Arrows indicate the flow of information: 'Control signals from I/O module' enter the 'Control logic' block from the top. 'Status signals to I/O module' exit the 'Control logic' block to the top. 'Data bits to and from I/O module' enter and exit the 'Buffer' block from the top. The 'Control logic' block has an arrow pointing to the 'Buffer' block. The 'Buffer' block has an arrow pointing to the 'Transducer' block. 'Data (device-unique) to and from environment' enter and exit the 'Transducer' block from the bottom.\n\n\nBlock Diagram of an External Device\n\n\n**Figure 7.2**\n   Block Diagram of an External Device\n\n\nThe basic unit of exchange is the character. Associated with each character is a code, typically 7 or 8 bits in length. The most commonly used text code is the International Reference Alphabet (IRA).\n   \n    1\n   \n   Each character in this code is represented by a unique 7-bit binary code; thus, 128 different characters can be represented. Characters are of two types: printable and control. Printable characters are the alphabetic, numeric, and special characters that can be printed on paper or displayed on a screen. Some of the control characters have to do with controlling the printing or displaying of characters; an example is carriage return. Other control characters are concerned with communications procedures. See Appendix H for details.\n\n\nFor keyboard input, when the user depresses a key, this generates an electronic signal that is interpreted by the transducer in the keyboard and translated into the bit pattern of the corresponding IRA code. This bit pattern is then transmitted to the I/O module in the computer. At the computer, the text can be stored in the same IRA code. On output, IRA code characters are transmitted to an external device from the I/O module. The transducer at the device interprets this code and sends the required electronic signals to the output device either to display the indicated character or perform the requested control function.\n\n\n\n\n**Disk Drive**\n\n\nA disk drive contains electronics for exchanging data, control, and status signals with an I/O module plus the electronics for controlling the disk read/write mechanism. In a fixed-head disk, the transducer is capable of converting between the magnetic patterns on the moving disk surface and bits in the device's buffer (Figure 7.2). A moving-head disk must also be able to cause the disk arm to move radially in and out across the disk's surface."
        },
        {
          "name": "I/O Modules",
          "content": "**Module Function**\n\n\nThe major functions or requirements for an I/O module fall into the following categories:\n\n\n  * ■ Control and timing\n  * ■ Processor communication\n  * ■ Device communication\n  * ■ Data buffering\n  * ■ Error detection\n\n\nDuring any period of time, the processor may communicate with one or more external devices in unpredictable patterns, depending on the program's need for\n\n\n1\n   \n   IRA is defined in ITU-T Recommendation T.50 and was formerly known as International Alphabet Number 5 (IA5). The U.S. national version of IRA is referred to as the American Standard Code for Information Interchange (ASCII).\n\n\nI/O. The internal resources, such as main memory and the system bus, must be shared among a number of activities, including data I/O. Thus, the I/O function includes a\n   **control and timing**\n   requirement, to coordinate the flow of traffic between internal resources and external devices. For example, the control of the transfer of data from an external device to the processor might involve the following sequence of steps:\n\n\n  * 1. The processor interrogates the I/O module to check the status of the attached device.\n  * 2. The I/O module returns the device status.\n  * 3. If the device is operational and ready to transmit, the processor requests the transfer of data, by means of a command to the I/O module.\n  * 4. The I/O module obtains a unit of data (e.g., 8 or 16 bits) from the external device.\n  * 5. The data are transferred from the I/O module to the processor.\n\n\nIf the system employs a bus, then each of the interactions between the processor and the I/O module involves one or more bus arbitrations.\n\n\nThe preceding simplified scenario also illustrates that the I/O module must communicate with the processor and with the external device.\n   **Processor communication**\n   involves the following:\n\n\n  * ■\n    **Command decoding:**\n    The I/O module accepts commands from the processor, typically sent as signals on the control bus. For example, an I/O module for a disk drive might accept the following commands: READ SECTOR, WRITE SECTOR, SEEK track number, and SCAN record ID. The latter two commands each include a parameter that is sent on the data bus.\n  * ■\n    **Data:**\n    Data are exchanged between the processor and the I/O module over the data bus.\n  * ■\n    **Status reporting:**\n    Because peripherals are so slow, it is important to know the status of the I/O module. For example, if an I/O module is asked to send data to the processor (read), it may not be ready to do so because it is still working on the previous I/O command. This fact can be reported with a status signal. Common status signals are BUSY and READY. There may also be signals to report various error conditions.\n  * ■\n    **Address recognition:**\n    Just as each word of memory has an address, so does each I/O device. Thus, an I/O module must recognize one unique address for each peripheral it controls.\n\n\nOn the other side, the I/O module must be able to perform\n   **device communication**\n   . This communication involves commands, status information, and data (Figure 7.2).\n\n\nAn essential task of an I/O module is\n   **data buffering**\n   . The need for this function is apparent from Figure 2.1. Whereas the transfer rate into and out of main memory or the processor is quite high, the rate is orders of magnitude lower for many peripheral devices and covers a wide range. Data coming from main memory are sent to an I/O module in a rapid burst. The data are buffered in the I/O module and then sent to the peripheral device at its data rate. In the opposite direction, data are buffered so as not to tie up the memory in a slow transfer operation. Thus, the\n\n\nI/O module must be able to operate at both device and memory speeds. Similarly, if the I/O device operates at a rate higher than the memory access rate, then the I/O module performs the needed buffering operation.\n\n\nFinally, an I/O module is often responsible for\n   **error detection**\n   and for subsequently reporting errors to the processor. One class of errors includes mechanical and electrical malfunctions reported by the device (e.g., paper jam, bad disk track). Another class consists of unintentional changes to the bit pattern as it is transmitted from device to I/O module. Some form of error-detecting code is often used to detect transmission errors. A simple example is the use of a parity bit on each character of data. For example, the IRA character code occupies 7 bits of a byte. The eighth bit is set so that the total number of 1s in the byte is even (even parity) or odd (odd parity). When a byte is received, the I/O module checks the parity to determine whether an error has occurred.\n\n\n\n\n**I/O Module Structure**\n\n\nI/O modules vary considerably in complexity and the number of external devices that they control. We will attempt only a very general description here. (One specific device, the Intel 8255A, is described in Section 7.4.) Figure 7.3 provides a general block diagram of an I/O module. The module connects to the rest of the computer through a set of signal lines (e.g., system bus lines). Data transferred to and from the module are buffered in one or more data registers. There may also be one or more status registers that provide current status information. A status register may also function as a control register, to accept detailed control information from the processor. The logic within the module interacts with the processor via a set of control lines. The processor uses the control lines to issue commands\n\n\n\n\n![Block Diagram of an I/O Module](images/image_0114.jpeg)\n\n\nThe diagram illustrates the internal structure of an I/O module. It is a large rectangular block with two main external interfaces: \"Interface to system bus\" on the left and \"Interface to external device\" on the right. The \"Interface to system bus\" includes \"Data lines\" (bidirectional), \"Address lines\" (unidirectional from bus to module), and \"Control lines\" (unidirectional from bus to module). The \"Interface to external device\" includes \"Data\", \"Status\", and \"Control\" lines for each of two external devices, shown as \"External device interface logic\" blocks. Inside the module, \"Data registers\" and \"Status/Control registers\" are connected to the \"Data lines\". The \"I/O logic\" block is the central controller, connected to the \"Data registers\", \"Status/Control registers\", and both \"External device interface logic\" blocks. Vertical ellipses between the interface logic blocks indicate the possibility of multiple external devices.\n\n\nBlock Diagram of an I/O Module\n\n\n**Figure 7.3**\n   Block Diagram of an I/O Module\n\n\nto the I/O module. Some of the control lines may be used by the I/O module (e.g., for arbitration and status signals). The module must also be able to recognize and generate addresses associated with the devices it controls. Each I/O module has a unique address or, if it controls more than one external device, a unique set of addresses. Finally, the I/O module contains logic specific to the interface with each device that it controls.\n\n\nAn I/O module functions to allow the processor to view a wide range of devices in a simple-minded way. There is a spectrum of capabilities that may be provided. The I/O module may hide the details of timing, formats, and the electromechanics of an external device so that the processor can function in terms of simple read and write commands, and possibly open and close file commands. In its simplest form, the I/O module may still leave much of the work of controlling a device (e.g., rewind a tape) visible to the processor.\n\n\nAn I/O module that takes on most of the detailed processing burden, presenting a high-level interface to the processor, is usually referred to as an I/O channel or\n   *I/O processor*\n   . An I/O module that is quite primitive and requires detailed control is usually referred to as an\n   *I/O controller*\n   or\n   *device controller*\n   . I/O controllers are commonly seen on microcomputers, whereas I/O channels are used on mainframes.\n\n\nIn what follows, we will use the generic term\n   *I/O module*\n   when no confusion results and will use more specific terms where necessary."
        },
        {
          "name": "Programmed I/O",
          "content": "Three techniques are possible for I/O operations. With\n   *programmed I/O*\n   , data are exchanged between the processor and the I/O module. The processor executes a program that gives it direct control of the I/O operation, including sensing device status, sending a read or write command, and transferring the data. When the processor issues a command to the I/O module, it must wait until the I/O operation is complete. If the processor is faster than the I/O module, this is waste of processor time. With interrupt-driven I/O, the processor issues an\n   *I/O command*\n   , continues to execute other instructions, and is interrupted by the I/O module when the latter has completed its work. With both programmed and\n   *interrupt I/O*\n   , the processor is responsible for extracting data from main memory for output and storing data in main memory for input. The alternative is known as\n   **direct memory access (DMA)**\n   . In this mode, the I/O module and main memory exchange data directly, without processor involvement.\n\n\nTable 7.1 indicates the relationship among these three techniques. In this section, we explore programmed I/O. Interrupt I/O and DMA are explored in the following two sections, respectively.\n\n\n**Table 7.1**\n   I/O Techniques\n\n\n\n | No Interrupts | Use of Interrupts\nI/O-to-memory transfer through processor | Programmed I/O | Interrupt-driven I/O\nDirect I/O-to-memory transfer |  | Direct memory access (DMA)\n\n\n\n\n**Overview of Programmed I/O**\n\n\nWhen the processor is executing a program and encounters an instruction relating to I/O, it executes that instruction by issuing a command to the appropriate I/O module. With programmed I/O, the I/O module will perform the requested action and then set the appropriate bits in the I/O status register (Figure 7.3). The I/O module takes no further action to alert the processor. In particular, it does not interrupt the processor. Thus, it is the responsibility of the processor to periodically check the status of the I/O module until it finds that the operation is complete.\n\n\nTo explain the programmed I/O technique, we view it first from the point of view of the I/O commands issued by the processor to the I/O module, and then from the point of view of the I/O instructions executed by the processor.\n\n\n\n\n**I/O Commands**\n\n\nTo execute an I/O-related instruction, the processor issues an address, specifying the particular I/O module and external device, and an I/O command. There are four types of I/O commands that an I/O module may receive when it is addressed by a processor:\n\n\n  * ■\n    **Control:**\n    Used to activate a peripheral and tell it what to do. For example, a magnetic-tape unit may be instructed to rewind or to move forward one record. These commands are tailored to the particular type of peripheral device.\n  * ■\n    **Test:**\n    Used to test various status conditions associated with an\n    **I/O module**\n    and its peripherals. The processor will want to know that the peripheral of interest is powered on and available for use. It will also want to know if the most recent I/O operation is completed and if any errors occurred.\n  * ■\n    **Read:**\n    Causes the I/O module to obtain an item of data from the peripheral and place it in an internal buffer (depicted as a data register in Figure 7.3). The processor can then obtain the data item by requesting that the I/O module place it on the data bus.\n  * ■\n    **Write:**\n    Causes the I/O module to take an item of data (byte or word) from the data bus and subsequently transmit that data item to the peripheral.\n\n\nFigure 7.4a gives an example of the use of programmed I/O to read in a block of data from a peripheral device (e.g., a record from tape) into memory. Data are read in one word (e.g., 16 bits) at a time. For each word that is read in, the processor must remain in a status-checking cycle until it determines that the word is available in the I/O module's data register. This flowchart highlights the main disadvantage of this technique: it is a time-consuming process that keeps the processor busy needlessly.\n\n\n\n\n**I/O Instructions**\n\n\nWith programmed I/O, there is a close correspondence between the I/O-related instructions that the processor fetches from memory and the I/O commands that the processor issues to an I/O module to execute the instructions. That is, the instructions are easily mapped into I/O commands, and there is often a simple one-to-one relationship. The form of the instruction depends on the way in which external devices are addressed.\n\n\n\n\n![Figure 7.4: Three Techniques for Input of a Block of Data. (a) Programmed I/O: CPU issues a read command to the I/O module, reads its status, and if not ready, loops back. If ready, it reads a word from the I/O module and writes it into memory. (b) Interrupt-driven I/O: CPU issues a read command to the I/O module, reads its status, and if not ready, loops back. If ready, it reads a word from the I/O module and writes it into memory, then returns to the next instruction. (c) Direct memory access: CPU issues a block command to the I/O module, reads the DMA module's status, and if not ready, loops back. If ready, it proceeds to the next instruction.](images/image_0115.jpeg)\n\n\n(a) Programmed I/O\n\n\n(b) Interrupt-driven I/O\n\n\n(c) Direct memory access\n\n\nFigure 7.4: Three Techniques for Input of a Block of Data. (a) Programmed I/O: CPU issues a read command to the I/O module, reads its status, and if not ready, loops back. If ready, it reads a word from the I/O module and writes it into memory. (b) Interrupt-driven I/O: CPU issues a read command to the I/O module, reads its status, and if not ready, loops back. If ready, it reads a word from the I/O module and writes it into memory, then returns to the next instruction. (c) Direct memory access: CPU issues a block command to the I/O module, reads the DMA module's status, and if not ready, loops back. If ready, it proceeds to the next instruction.\n\n\n**Figure 7.4**\n   Three Techniques for Input of a Block of Data\n\n\nTypically, there will be many I/O devices connected through I/O modules to the system. Each device is given a unique identifier or address. When the processor issues an I/O command, the command contains the address of the desired device. Thus, each I/O module must interpret the address lines to determine if the command is for itself.\n\n\nWhen the processor, main memory, and I/O share a common bus, two modes of addressing are possible: memory mapped and isolated. With\n   **memory-mapped I/O**\n   , there is a single address space for memory locations and I/O devices. The processor treats the status and data registers of I/O modules as memory locations and uses the same machine instructions to access both memory and I/O devices. So, for example, with 10 address lines, a combined total of\n   \n    2^{10} = 1024\n   \n   memory locations and I/O addresses can be supported, in any combination.\n\n\nWith memory-mapped I/O, a single read line and a single write line are needed on the bus. Alternatively, the bus may be equipped with memory read and write plus input and output command lines. The command line specifies whether the address refers to a memory location or an I/O device. The full range of addresses may be available for both. Again, with 10 address lines, the system may now support both 1024 memory locations and 1024 I/O addresses. Because the address space for I/O is isolated from that for memory, this is referred to as\n   **isolated I/O**\n   .\n\n\nFigure 7.5 contrasts these two programmed I/O techniques. Figure 7.5a shows how the interface for a simple input device such as a terminal keyboard might appear to a programmer using memory-mapped I/O. Assume a 10-bit address, with a 512-bit memory (locations 0–511) and up to 512 I/O addresses (locations 512–1023). Two addresses are dedicated to keyboard input from a particular terminal. Address 516 refers to the data register and address 517 refers to the status register, which also functions as a control register for receiving processor commands. The program shown will read 1 byte of data from the keyboard into an accumulator register in the processor. Note that the processor loops until the data byte is available.\n\n\nWith isolated I/O (Figure 7.5b), the I/O ports are accessible only by special I/O commands, which activate the I/O command lines on the bus.\n\n\nFor most types of processors, there is a relatively large set of different instructions for referencing memory. If isolated I/O is used, there are only a few I/O instructions. Thus, an advantage of memory-mapped I/O is that this large repertoire of instructions can be used, allowing more efficient programming. A disadvantage is that valuable memory address space is used up. Both memory-mapped and isolated I/O are in common use.\n\n\n\n\n![Diagram of keyboard input registers 516 and 517. Register 516 is the 'Keyboard input data register'. Register 517 is the 'Keyboard input status and control register'. Both have 10-bit addresses (bits 7-0). Register 517 has two control signals: '1 = ready' (bit 0) and 'Set to 1 to start read' (bit 7).](images/image_0116.jpeg)\n\n\nThe diagram shows two 10-bit registers. Register 516 is labeled 'Keyboard input data register'. Register 517 is labeled 'Keyboard input status and control register'. Both registers have bit positions 7 through 0 above them. Register 517 has two arrows pointing to its bit 0 and bit 7. The arrow to bit 0 is labeled '1 = ready' and '0 = busy'. The arrow to bit 7 is labeled 'Set to 1 to start read'.\n\n\nDiagram of keyboard input registers 516 and 517. Register 516 is the 'Keyboard input data register'. Register 517 is the 'Keyboard input status and control register'. Both have 10-bit addresses (bits 7-0). Register 517 has two control signals: '1 = ready' (bit 0) and 'Set to 1 to start read' (bit 7).\n\n\n\nADDRESS | INSTRUCTION | OPERAND | COMMENT\n200 | Load AC | \"1\" | Load accumulator\n | Store AC | 517 | Initiate keyboard read\n202 | Load AC | 517 | Get status byte\n | Branch if Sign = 0 | 202 | Loop until ready\n | Load AC | 516 | Load data byte\n\n\n(a) Memory-mapped I/O\n\n\n\nADDRESS | INSTRUCTION | OPERAND | COMMENT\n200 | Load I/O | 5 | Initiate keyboard read\n201 | Test I/O | 5 | Check for completion\n | Branch Not Ready | 201 | Loop until complete\n | In | 5 | Load data byte\n\n\n(b) Isolated I/O\n\n\n**Figure 7.5**"
        },
        {
          "name": "Interrupt- Driven I/O",
          "content": "The problem with programmed I/O is that the processor has to wait a long time for the I/O module of concern to be ready for either reception or transmission of data. The processor, while waiting, must repeatedly interrogate the status of the I/O module. As a result, the level of the performance of the entire system is severely degraded.\n\n\nAn alternative is for the processor to issue an I/O command to a module and then go on to do some other useful work. The I/O module will then interrupt the processor to request service when it is ready to exchange data with the processor. The processor then executes the data transfer, as before, and then resumes its former processing.\n\n\nLet us consider how this works, first from the point of view of the I/O module. For input, the I/O module receives a READ command from the processor. The I/O module then proceeds to read data in from an associated peripheral. Once the data are in the module's data register, the module signals an interrupt to the processor over a control line. The module then waits until its data are requested by the processor. When the request is made, the module places its data on the data bus and is then ready for another I/O operation.\n\n\nFrom the processor's point of view, the action for input is as follows. The processor issues a READ command. It then goes off and does something else (e.g., the processor may be working on several different programs at the same time). At the end of each instruction cycle, the processor checks for interrupts (Figure 3.9). When the interrupt from the I/O module occurs, the processor saves the context (e.g., program counter and processor registers) of the current program and processes the interrupt. In this case, the processor reads the word of data from the I/O module and stores it in memory. It then restores the context of the program it was working on (or some other program) and resumes execution.\n\n\nFigure 7.4b shows the use of interrupt I/O for reading in a block of data. Compare this with Figure 7.4a. Interrupt I/O is more efficient than programmed I/O because it eliminates needless waiting. However, interrupt I/O still consumes a lot of processor time, because every word of data that goes from memory to I/O module or from I/O module to memory must pass through the processor.\n\n\n\n\n**Interrupt Processing**\n\n\nLet us consider the role of the processor in interrupt-driven I/O in more detail. The occurrence of an interrupt triggers a number of events, both in the processor hardware and in software. Figure 7.6 shows a typical sequence. When an I/O device completes an I/O operation, the following sequence of hardware events occurs:\n\n\n  * 1. The device issues an interrupt signal to the processor.\n  * 2. The processor finishes execution of the current instruction before responding to the interrupt, as indicated in Figure 3.9.\n  * 3. The processor tests for an interrupt, determines that there is one, and sends an acknowledgment signal to the device that issued the interrupt. The acknowledgment allows the device to remove its interrupt signal.\n\n\n\n\n![Flowchart of Simple Interrupt Processing showing Hardware and Software steps.](images/image_0117.jpeg)\n\n\nThe diagram illustrates the Simple Interrupt Processing flow, divided into Hardware and Software components.\n\n\n**Hardware**\n    (left side, indicated by a bracket above the steps):\n\n\n  * Device controller or other system hardware issues an interrupt\n  * Processor finishes execution of current instruction\n  * Processor signals acknowledgment of interrupt\n  * Processor pushes PSW and PC onto control stack\n  * Processor loads new PC value based on interrupt\n\n\n**Software**\n    (right side, indicated by a bracket above the steps):\n\n\n  * Save remainder of process state information\n  * Process interrupt\n  * Restore process state information\n  * Restore old PSW and PC\n\n\nA vertical line connects the end of the Hardware section (after loading the new PC) to the beginning of the Software section (saving process state information).\n\n\nFlowchart of Simple Interrupt Processing showing Hardware and Software steps.\n\n\n**Figure 7.6**\n   Simple Interrupt Processing\n\n\n  * 4. The processor now needs to prepare to transfer control to the interrupt routine. To begin, it needs to save information needed to resume the current program at the point of interrupt. The minimum information required is (a) the status of the processor, which is contained in a register called the\n    **program status word (PSW)**\n    ; and (b) the location of the next instruction to be executed, which is contained in the program counter. These can be pushed onto the system control stack.\n    \n     2\n  * 5. The processor now loads the program counter with the entry location of the interrupt-handling program that will respond to this interrupt. Depending on the computer architecture and operating system design, there may be a single program; one program for each type of interrupt; or one program for each device and each type of interrupt. If there is more than one interrupt-handling routine, the processor must determine which one to invoke. This information may have been included in the original interrupt signal, or the processor may have to issue a request to the device that issued the interrupt to get a response that contains the needed information.\n\n\n2\n   \n   See Appendix I for a discussion of stack operation.\n\n\nOnce the program counter has been loaded, the processor proceeds to the next instruction cycle, which begins with an instruction fetch. Because the instruction fetch is determined by the contents of the program counter, the result is that control is transferred to the interrupt-handler program. The execution of this program results in the following operations:\n\n\n  * 6. At this point, the program counter and PSW relating to the interrupted program have been saved on the system stack. However, there is other information that is considered part of the “state” of the executing program. In particular, the contents of the processor registers need to be saved, because these registers may be used by the interrupt handler. So, all of these values, plus any other state information, need to be saved. Typically, the interrupt handler will begin by saving the contents of all registers on the stack. Figure 7.7a shows a simple example. In this case, a user program is interrupted after the instruction at location\n    \n     N\n    \n    . The contents of all of the registers plus the address of the next instruction (\n    \n     N + 1\n    \n    ) are pushed onto the stack. The stack pointer is updated to point to the new top of stack, and the program counter is updated to point to the beginning of the interrupt service routine.\n  * 7. The interrupt handler next processes the interrupt. This includes an examination of status information relating to the I/O operation or other event that caused an interrupt. It may also involve sending additional commands or acknowledgments to the I/O device.\n  * 8. When interrupt processing is complete, the saved register values are retrieved from the stack and restored to the registers (e.g., see Figure 7.7b).\n  * 9. The final act is to restore the PSW and program counter values from the stack. As a result, the next instruction to be executed will be from the previously interrupted program.\n\n\nNote that it is important to save all the state information about the interrupted program for later resumption. This is because the interrupt is not a routine called from the program. Rather, the interrupt can occur at any time and therefore at any point in the execution of a user program. Its occurrence is unpredictable. Indeed, as we will see in the next chapter, the two programs may not have anything in common and may belong to two different users.\n\n\n\n\n**Design Issues**\n\n\nTwo design issues arise in implementing interrupt I/O. First, because there will almost invariably be multiple I/O modules, how does the processor determine which device issued the interrupt? And second, if multiple interrupts have occurred, how does the processor decide which one to process?\n\n\nLet us consider device identification first. Four general categories of techniques are in common use:\n\n\n  * ■ Multiple interrupt lines\n  * ■ Software poll\n  * ■ Daisy chain (hardware poll, vectored)\n  * ■ Bus arbitration (vectored)\n\n\n\n\n![Figure 7.7: Changes in Memory and Registers for an Interrupt. The diagram consists of two parts, (a) and (b), showing the state of Main Memory and the Processor during an interrupt and its return.](images/image_0118.jpeg)\n\n\n**(a) Interrupt occurs after instruction at location\n     \n      N**\n\n\n**Main Memory:**\n    A vertical stack of memory locations. From top to bottom, it contains: a stack of\n    \n     T-M\n    \n    locations labeled \"Control stack\", a location labeled\n    \n     T\n    \n    , a location labeled\n    \n     Y\n    \n    containing \"Start\", a location labeled\n    \n     Y+L\n    \n    containing \"Return\", and a stack of\n    \n     N+1\n    \n    locations labeled \"User's program\".\n\n\n**Processor:**\n    A block containing: \"Program counter\" with value\n    \n     N+1\n    \n    , \"General registers\" (a stack of three locations), and \"Stack pointer\" with value\n    \n     T\n    \n    .\n\n\n**(b) Return from interrupt**\n\n\n**Main Memory:**\n    A vertical stack of memory locations. From top to bottom, it contains: a stack of\n    \n     T-M\n    \n    locations labeled \"Control stack\", a location labeled\n    \n     T\n    \n    , a location labeled\n    \n     Y\n    \n    containing \"Start\", a location labeled\n    \n     Y+L\n    \n    containing \"Return\", and a stack of\n    \n     N+1\n    \n    locations labeled \"User's program\".\n\n\n**Processor:**\n    A block containing: \"Program counter\" with value\n    \n     Y+L\n    \n    , \"General registers\" (a stack of three locations), and \"Stack pointer\" with value\n    \n     T-M\n    \n    .\n\n\nFigure 7.7: Changes in Memory and Registers for an Interrupt. The diagram consists of two parts, (a) and (b), showing the state of Main Memory and the Processor during an interrupt and its return.\n\n\n**Figure 7.7**\n   Changes in Memory and Registers for an Interrupt\n\n\nThe most straightforward approach to the problem is to provide\n   **multiple interrupt lines**\n   between the processor and the I/O modules. However, it is impractical to dedicate more than a few bus lines or processor pins to interrupt lines. Consequently, even if multiple lines are used, it is likely that each line will have multiple I/O modules attached to it. Thus, one of the other three techniques must be used on each line.\n\n\nOne alternative is the\n   **software poll**\n   . When the processor detects an interrupt, it branches to an interrupt-service routine that polls each I/O module to determine which module caused the interrupt. The poll could be in the form of a separate command line (e.g., TESTI/O). In this case, the processor raises TESTI/O and places the address of a particular I/O module on the address lines. The I/O module responds positively if it set the interrupt. Alternatively, each I/O module could contain an addressable status register. The processor then reads the status register of each I/O module to identify the interrupting module. Once the correct module is identified, the processor branches to a device-service routine specific to that device.\n\n\nThe disadvantage of the software poll is that it is time consuming. A more efficient technique is to use a\n   **daisy chain**\n   , which provides, in effect, a hardware poll. An example of a daisy-chain configuration is shown in Figure 3.26. For interrupts, all I/O modules share a common interrupt request line. The interrupt acknowledge line is daisy chained through the modules. When the processor senses an interrupt, it sends out an interrupt acknowledge. This signal propagates through a series of I/O modules until it gets to a requesting module. The requesting module typically responds by placing a word on the data lines. This word is referred to as a\n   *vector*\n   and is either the address of the I/O module or some other unique identifier. In either case, the processor uses the vector as a pointer to the appropriate device-service routine. This avoids the need to execute a general interrupt-service routine first. This technique is called a\n   *vectored interrupt*\n   .\n\n\nThere is another technique that makes use of vectored interrupts, and that is\n   **bus arbitration**\n   . With bus arbitration, an I/O module must first gain control of the bus before it can raise the interrupt request line. Thus, only one module can raise the line at a time. When the processor detects the interrupt, it responds on the interrupt acknowledge line. The requesting module then places its vector on the data lines.\n\n\nThe aforementioned techniques serve to identify the requesting I/O module. They also provide a way of assigning priorities when more than one device is requesting interrupt service. With multiple lines, the processor just picks the interrupt line with the highest priority. With software polling, the order in which modules are polled determines their priority. Similarly, the order of modules on a daisy chain determines their priority. Finally, bus arbitration can employ a priority scheme, as discussed in Section 3.4.\n\n\nWe now turn to two examples of interrupt structures.\n\n\n\n\n**Intel 82C59A Interrupt Controller**\n\n\nThe Intel 80386 provides a single Interrupt Request (INTR) and a single Interrupt Acknowledge (INTA) line. To allow the 80386 to handle a variety of devices and priority structures, it is usually configured with an external interrupt arbiter, the 82C59A. External devices are connected to the 82C59A, which in turn connects to the 80386.\n\n\nFigure 7.8 shows the use of the 82C59A to connect multiple I/O modules for the 80386. A single 82C59A can handle up to eight modules. If control for more than eight modules is required, a cascade arrangement can be used to handle up to 64 modules.\n\n\nThe 82C59A's sole responsibility is the management of interrupts. It accepts interrupt requests from attached modules, determines which interrupt has the highest priority, and then signals the processor by raising the INTR line. The processor acknowledges via the INTA line. This prompts the 82C59A to place the appropriate vector information on the data bus. The processor can then proceed to process the interrupt and to communicate directly with the I/O module to read or write data.\n\n\nThe 82C59A is programmable. The 80386 determines the priority scheme to be used by setting a control word in the 82C59A. The following interrupt modes are possible:\n\n\n  * ■\n    **Fully nested:**\n    The interrupt requests are ordered in priority from 0 (IR0) through 7 (IR7).\n\n\n\n\n![Diagram showing the use of the 82C59A Interrupt Controller. It illustrates a master-slave configuration where multiple slave 82C59A controllers are connected to a single master 82C59A controller, which then sends an interrupt signal to an 80386 processor.](images/image_0119.jpeg)\n\n\nThe diagram illustrates the use of the 82C59A Interrupt Controller in a master-slave configuration. It shows three slave 82C59A interrupt controllers and one master 82C59A interrupt controller, all connected to an 80386 processor.\n\n\n  * **Slave 82C59A interrupt controller (top):**\n     Handles external devices 00 through 07. Its interrupt request lines (IR0-IR7) are connected to the master controller. Its interrupt output (INT) is connected to the master controller's IR1 line.\n  * **Slave 82C59A interrupt controller (middle):**\n     Handles external devices 08 through 15. Its interrupt request lines (IR0-IR7) are connected to the master controller. Its interrupt output (INT) is connected to the master controller's IR2 line.\n  * **Slave 82C59A interrupt controller (bottom):**\n     Handles external devices 56 through 63. Its interrupt request lines (IR0-IR7) are connected to the master controller. Its interrupt output (INT) is connected to the master controller's IR3 line.\n  * **Master 82C59A interrupt controller:**\n     Receives interrupt requests from the slave controllers (IR0-IR7) and sends a single interrupt signal (INTR) to the 80386 processor.\n  * **80386 processor:**\n     Receives the interrupt signal (INTR) from the master controller.\n\n\nDiagram showing the use of the 82C59A Interrupt Controller. It illustrates a master-slave configuration where multiple slave 82C59A controllers are connected to a single master 82C59A controller, which then sends an interrupt signal to an 80386 processor.\n\n\n**Figure 7.8**\n   Use of the 82C59A Interrupt Controller\n\n\n  * ■\n    **Rotating:**\n    In some applications a number of interrupting devices are of equal priority. In this mode a device, after being serviced, receives the lowest priority in the group.\n  * ■\n    **Special mask:**\n    This allows the processor to inhibit interrupts from certain devices.\n\n\n\n\n**The Intel 8255A Programmable Peripheral Interface**\n\n\nAs an example of an I/O module used for programmed I/O and interrupt-driven I/O, we consider the Intel 8255A Programmable Peripheral Interface. The 8255A is a single-chip, general-purpose I/O module originally designed for use with the Intel 80386 processor. It has since been cloned by other manufacturers and is a widely used peripheral controller chip. Its uses include as a controller for simple I/O devices for microprocessors and in embedded systems, including microcontroller systems.\n\n\n**ARCHITECTURE AND OPERATION**\n   Figure 7.9 shows a general block diagram plus the pin assignment for the 40-pin package in which it is housed. As shown on the pin layout, the 8255A includes the following lines:\n\n\n  * ■\n    **D0–D7:**\n    These are the data I/O lines for the device. All information read from and written to the 8255A occurs via these eight data lines.\n  * ■\n    **\\overline{CS}\n     \n     (Chip Select Input):**\n    If this line is a logical 0, the microprocessor can read and write to the 8255A.\n  * ■\n    **\\overline{RD}\n     \n     (Read Input):**\n    If this line is a logical 0 and the\n    \n     \\overline{CS}\n    \n    input is a logical 0, the 8255A data outputs are enabled onto the system data bus.\n  * ■\n    **\\overline{WR}\n     \n     (Write Input):**\n    If this input line is a logical 0 and the\n    \n     \\overline{CS}\n    \n    input is a logical 0, data are written to the 8255A from the system data bus.\n  * ■\n    **RESET:**\n    The 8255A is placed into its reset state if this input line is a logical 1. All peripheral ports are set to the input mode.\n\n\n\n\n![Figure 7.9: The Intel 8255A Programmable Peripheral Interface. (a) Block diagram showing internal structure: Power supplies (+5V, GND), Bi-directional data bus (D7-D0) connected to a Data bus buffer, Read/write control logic, Group A control, Group B control, Group A port A (8), Group A port C upper (4), Group B port C lower (4), and Group B port B (8). Internal connections include an 8-bit internal data bus and I/O lines (PA7-PA0, PC7-PC4, PC3-PC0, PB7-PB0). (b) Pin layout for a 40-pin DIP package, showing pin numbers 1-40 and their functions: PA3, PA2, PA1, PA0, RD, CS, GND, A1, A0, PC7, PC6, PC5, PC4, PC3, PC2, PC1, PC0, PB0, PB1, PB2, D0, D1, D2, D3, D4, D5, D6, D7, V, PB7, PB6, PB5, PB4, PB3.](images/image_0120.jpeg)\n\n\n(a) Block diagram\n\n\n(b) Pin layout\n\n\nFigure 7.9: The Intel 8255A Programmable Peripheral Interface. (a) Block diagram showing internal structure: Power supplies (+5V, GND), Bi-directional data bus (D7-D0) connected to a Data bus buffer, Read/write control logic, Group A control, Group B control, Group A port A (8), Group A port C upper (4), Group B port C lower (4), and Group B port B (8). Internal connections include an 8-bit internal data bus and I/O lines (PA7-PA0, PC7-PC4, PC3-PC0, PB7-PB0). (b) Pin layout for a 40-pin DIP package, showing pin numbers 1-40 and their functions: PA3, PA2, PA1, PA0, RD, CS, GND, A1, A0, PC7, PC6, PC5, PC4, PC3, PC2, PC1, PC0, PB0, PB1, PB2, D0, D1, D2, D3, D4, D5, D6, D7, V, PB7, PB6, PB5, PB4, PB3.\n\n\n**Figure 7.9**\n   The Intel 8255A Programmable Peripheral Interface\n\n\n  * ■\n    **PA0–PA7, PB0–PB7, PC0–PC7:**\n    These signal lines are used as 8-bit I/O ports. They can be connected to peripheral devices.\n  * ■\n    **A0, A1:**\n    The logical combination of these two input lines determine which internal register of the 8255A data are written to or read from.\n\n\nThe right side of the block diagram of Figure 7.9a is the external interface of the 8255A. The 24 I/O lines are divided into three 8-bit groups (A, B, C). Each group can function as an 8-bit I/O port, thus providing connection for three peripheral devices. In addition, group C is subdivided into 4-bit groups (\n   \n    C_A\n   \n   and\n   \n    C_B\n   \n   ), which may be used in conjunction with the A and B I/O ports. Configured in this manner, group C lines carry control and status signals.\n\n\nThe left side of the block diagram is the internal interface to the microprocessor system bus. It includes an 8-bit bidirectional data bus (D0 through D7), used to transfer data between the microprocessor and the I/O ports and to transfer control information.\n\n\nThe processor controls the 8255A by means of an 8-bit control register in the processor. The processor can set the value of the control register to specify a variety of operating modes and configurations. From the processor point of view, there is a control port, and the control register bits are set in the processor and then sent to the control port over lines D0–D7. The two address lines specify one of the three I/O ports or the control register, as follows:\n\n\n\nA1 | A2 | Selects\n0 | 0 | Port A\n0 | 1 | Port B\n1 | 0 | Port C\n1 | 1 | Control register\n\n\nThus, when the processor sets both A1 and A2 to 1, the 8255A interprets the 8-bit value on the data bus as a control word. When the processor transfers an 8-bit control word with line D7 set to 1 (Figure 7.10a), the control word is used to configure the operating mode of the 24 I/O lines. The three modes are:\n\n\n  * ■\n    **Mode 0:**\n    This is the basic I/O mode. The three groups of eight external lines function as three 8-bit I/O ports. Each port can be designated as input or output. Data may only be sent to a port if the port is defined as output, and data may only be read from a port if the port is set to input.\n  * ■\n    **Mode 1:**\n    In this mode, ports A and B can be configured as either input or output, and lines from port C serve as control lines for A and B. The control signals serve two principal purposes: “handshaking” and interrupt request. Handshaking is a simple timing mechanism. One control line is used by the sender as a DATA READY line, to indicate when the data are present on the I/O data lines. Another line is used by the receiver as an ACKNOWLEDGE, indicating that the data have been read and the data lines may be cleared. Another line may be designated as an INTERRUPT REQUEST line and tied back to the system bus.\n\n\n\n\n![Pin diagram of the 82C55A Programmable Peripheral Interface (PPI) showing port connections and control signals.](images/image_0121.jpeg)\n\n\nThe diagram illustrates the pin configuration and internal logic of the 82C55A Programmable Peripheral Interface (PPI). It shows the connections for Port A, Port B, and Port C, along with control signals and status flags.\n\n\n**Pin Connections:**\n\n\n  * **Group A (D7, D6, D5, D4, D3, D2, D1, D0):**\n  * **Port C (lower):**\n        D7, D6, D5, D4, D3, D2, D1, D0.\n  * **Port B:**\n        D7, D6, D5, D4, D3, D2, D1, D0.\n  * **Port C (upper):**\n        D7, D6, D5, D4, D3, D2, D1, D0.\n  * **Group B (D7, D6, D5, D4, D3, D2, D1, D0):**\n  * **Don't care:**\n        D7, D6, D5, D4, D3, D2, D1, D0.\n\n\n**Control and Status Signals:**\n\n\n  * **Mode set flag:**\n      D7 (Group A, Port C lower).\n  * **Bit set/reset flag:**\n      D7 (Group B, Don't care).\n  * **Bit set/reset:**\n      D0 (Group B, Don't care).\n\n\n**Port C (lower) Logic:**\n\n\n\nBit | D7 | D6 | D5 | D4 | D3 | D2 | D1 | D0 | Function\n0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | bit 0 of port C\n1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | bit 1 of port C\n2 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | bit 2 of port C\n3 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | bit 3 of port C\n4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | bit 4 of port C\n5 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | bit 5 of port C\n6 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | bit 6 of port C\n7 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | bit 7 of port C\n\n\n**Port C (upper) Logic:**\n\n\n\nBit | D7 | D6 | D5 | D4 | D3 | D2 | D1 | D0 | Function\n0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | bit 0 of port C\n1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | bit 1 of port C\n2 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | bit 2 of port C\n3 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | bit 3 of port C\n4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | bit 4 of port C\n5 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | bit 5 of port C\n6 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | bit 6 of port C\n7 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | bit 7 of port C\n\n\nPin diagram of the 82C55A Programmable Peripheral Interface (PPI) showing port connections and control signals.\n\n\n(a) Mode definition of the 8255 control register to configure the 8255\n\n\n(b) Bit definitions of the 8255 control register to modify single bits of port C\n\n\n**Figure 7.10**\n    The Intel 8255A Control Word\n\n\n  * * ■\n       **Mode 2:**\n       This is a bidirectional mode. In this mode, port A can be configured as either the input or output lines for bidirectional traffic on port B, with the port B lines providing the opposite direction. Again, port C lines are used for control signaling.\n\n\nWhen the processor sets D7 to 0 (Figure 7.10b), the control word is used to program the bit values of port C individually. This feature is rarely used.\n\n\n**KEYBOARD/DISPLAY EXAMPLE**\n    Because the 8255A is programmable via the control register, it can be used to control a variety of simple peripheral devices. Figure 7.11 illustrates its use to control a keyboard/display terminal. The keyboard provides 8 bits of input. Two of these bits, SHIFT and CONTROL, have special meaning to the keyboard-handling program executing in the processor. However, this interpretation is transparent to the 8255A, which simply accepts the 8 bits of data and presents them on the system data bus. Two handshaking control lines are provided for use with the keyboard.\n\n\nThe display is also linked by an 8-bit data port. Again, two of the bits have special meanings that are transparent to the 8255A. In addition to two handshaking lines, two lines provide additional control functions.\n\n\n\n\n![Diagram of a Keyboard/Display Interface to 8255A. The central component is an 82C55A Programmable Peripheral Interface (PPI) chip. It has three 8-bit ports: INPUT PORT (C3-A0 to C4-A7), OUTPUT PORT (C0-C7 to B0-B7), and CONTROL PORT (C2-C1 to C6-C0). The INPUT PORT is connected to a KEYBOARD device, which provides 8 data lines (R0-R7) and two control lines (Shift, Control). The OUTPUT PORT is connected to a DISPLAY device, which provides 6 status lines (S0-S5) and three control lines (Backspace, Clear, Blanking). The CONTROL PORT is connected to the DISPLAY device, which provides two control lines (Data ready Acknowledge, Clear line). Two interrupt request lines originate from the 82C55A: one from the top of the chip and one from the bottom.](images/image_0122.jpeg)\n\n\nDiagram of a Keyboard/Display Interface to 8255A. The central component is an 82C55A Programmable Peripheral Interface (PPI) chip. It has three 8-bit ports: INPUT PORT (C3-A0 to C4-A7), OUTPUT PORT (C0-C7 to B0-B7), and CONTROL PORT (C2-C1 to C6-C0). The INPUT PORT is connected to a KEYBOARD device, which provides 8 data lines (R0-R7) and two control lines (Shift, Control). The OUTPUT PORT is connected to a DISPLAY device, which provides 6 status lines (S0-S5) and three control lines (Backspace, Clear, Blanking). The CONTROL PORT is connected to the DISPLAY device, which provides two control lines (Data ready Acknowledge, Clear line). Two interrupt request lines originate from the 82C55A: one from the top of the chip and one from the bottom.\n\n\n**Figure 7.11**\n   Keyboard/Display Interface to 8255A"
        },
        {
          "name": "Direct Memory Access",
          "content": "**Drawbacks of Programmed and Interrupt-Driven I/O**\n\n\nInterrupt-driven I/O, though more efficient than simple programmed I/O, still requires the active intervention of the processor to transfer data between memory and an I/O module, and any data transfer must traverse a path through the processor. Thus, both these forms of I/O suffer from two inherent drawbacks:\n\n\n  * 1. The I/O transfer rate is limited by the speed with which the processor can test and service a device.\n\n\n  * 2. The processor is tied up in managing an I/O transfer; a number of instructions must be executed for each I/O transfer (e.g., Figure 7.5).\n\n\nThere is somewhat of a trade-off between these two drawbacks. Consider the transfer of a block of data. Using simple programmed I/O, the processor is dedicated to the task of I/O and can move data at a rather high rate, at the cost of doing nothing else. Interrupt I/O frees up the processor to some extent at the expense of the I/O transfer rate. Nevertheless, both methods have an adverse impact on both processor activity and I/O transfer rate.\n\n\nWhen large volumes of data are to be moved, a more efficient technique is required: direct memory access (DMA).\n\n\n\n\n**DMA Function**\n\n\nDMA involves an additional module on the system bus. The DMA module (Figure 7.12) is capable of mimicking the processor and, indeed, of taking over control of the system from the processor. It needs to do this to transfer data to and from memory over the system bus. For this purpose, the DMA module must use the bus only when the processor does not need it, or it must force the processor to suspend operation temporarily. The latter technique is more common and is referred to as\n   *cycle stealing*\n   , because the DMA module in effect steals a bus cycle.\n\n\nWhen the processor wishes to read or write a block of data, it issues a command to the DMA module, by sending to the DMA module the following information:\n\n\n  * ■ Whether a read or write is requested, using the read or write control line between the processor and the DMA module.\n  * ■ The address of the I/O device involved, communicated on the data lines.\n\n\n\n\n![Figure 7.12: Typical DMA Block Diagram. The diagram shows a vertical rectangular block representing the DMA module. Inside the block, from top to bottom, are four rectangular boxes: 'Data count', 'Data register', 'Address register', and 'Control logic'. To the left of the block, several lines connect to it: 'Data lines' (two lines, one entering and one exiting the block), 'Address lines' (one line entering the block), 'Request to DMA' (one line entering the block), 'Acknowledge from DMA' (one line exiting the block), 'Interrupt' (one line exiting the block), 'Read' (one line exiting the block), and 'Write' (one line exiting the block).](images/image_0123.jpeg)\n\n\nFigure 7.12: Typical DMA Block Diagram. The diagram shows a vertical rectangular block representing the DMA module. Inside the block, from top to bottom, are four rectangular boxes: 'Data count', 'Data register', 'Address register', and 'Control logic'. To the left of the block, several lines connect to it: 'Data lines' (two lines, one entering and one exiting the block), 'Address lines' (one line entering the block), 'Request to DMA' (one line entering the block), 'Acknowledge from DMA' (one line exiting the block), 'Interrupt' (one line exiting the block), 'Read' (one line exiting the block), and 'Write' (one line exiting the block).\n\n\nFigure 7.12 Typical DMA Block Diagram\n\n\n  * ■ The starting location in memory to read from or write to, communicated on the data lines and stored by the DMA module in its address register.\n  * ■ The number of words to be read or written, again communicated via the data lines and stored in the data count register.\n\n\nThe processor then continues with other work. It has delegated this I/O operation to the DMA module. The DMA module transfers the entire block of data, one word at a time, directly to or from memory, without going through the processor. When the transfer is complete, the DMA module sends an interrupt signal to the processor. Thus, the processor is involved only at the beginning and end of the transfer (Figure 7.4c).\n\n\nFigure 7.13 shows where in the instruction cycle the processor may be suspended. In each case, the processor is suspended just before it needs to use the bus. The DMA module then transfers one word and returns control to the processor. Note that this is not an interrupt; the processor does not save a context and do something else. Rather, the processor pauses for one bus cycle. The overall effect is to cause the processor to execute more slowly. Nevertheless, for a multiple-word I/O transfer, DMA is far more efficient than interrupt-driven or programmed I/O.\n\n\nThe DMA mechanism can be configured in a variety of ways. Some possibilities are shown in Figure 7.14. In the first example, all modules share the same system bus. The DMA module, acting as a surrogate processor, uses programmed I/O to exchange data between memory and an I/O module through the DMA module. This configuration, while it may be inexpensive, is clearly inefficient. As with processor-controlled programmed I/O, each transfer of a word consumes two bus cycles.\n\n\nThe number of required bus cycles can be cut substantially by integrating the DMA and I/O functions. As Figure 7.14b indicates, this means that there is a path between the DMA module and one or more I/O modules that does not include\n\n\n\n\n![Diagram showing DMA and Interrupt Breakpoints during an Instruction Cycle. The diagram illustrates the six stages of an instruction cycle: Fetch instruction, Decode instruction, Fetch operand, Execute instruction, Store result, and Process interrupt. DMA breakpoints are shown as arrows pointing to the boundaries between Fetch operand and Execute instruction, and between Store result and Process interrupt. An interrupt breakpoint is shown as an arrow pointing to the boundary between Store result and Process interrupt.](images/image_0124.jpeg)\n\n\nThe diagram illustrates the timing of an instruction cycle relative to DMA and interrupt breakpoints. The horizontal axis represents 'Time' with an arrow pointing to the right. The vertical axis represents the stages of the instruction cycle. A large bracket at the top spans the entire 'Instruction cycle'. Below this, six vertical lines divide the cycle into six 'Processor cycle' segments. The stages of the instruction cycle are listed below each segment: 'Fetch instruction', 'Decode instruction', 'Fetch operand', 'Execute instruction', 'Store result', and 'Process interrupt'. Arrows labeled 'DMA breakpoints' point to the boundaries between the 'Fetch operand' and 'Execute instruction' segments, and between the 'Store result' and 'Process interrupt' segments. An arrow labeled 'Interrupt breakpoint' points to the boundary between the 'Store result' and 'Process interrupt' segments.\n\n\nDiagram showing DMA and Interrupt Breakpoints during an Instruction Cycle. The diagram illustrates the six stages of an instruction cycle: Fetch instruction, Decode instruction, Fetch operand, Execute instruction, Store result, and Process interrupt. DMA breakpoints are shown as arrows pointing to the boundaries between Fetch operand and Execute instruction, and between Store result and Process interrupt. An interrupt breakpoint is shown as an arrow pointing to the boundary between Store result and Process interrupt.\n\n\n**Figure 7.13**\n   DMA and Interrupt Breakpoints during an Instruction Cycle\n\n\n\n\n![Figure 7.14: Alternative DMA Configurations. (a) Single-bus, detached DMA: All components (Processor, DMA, I/O, Memory) are connected to a single horizontal bus. (b) Single-bus, integrated DMA-I/O: The Processor and Memory are on the main bus, while the DMA and I/O modules are integrated into a single block connected to the main bus. (c) I/O bus: The Processor and Memory are on the System bus, while the DMA module is on the System bus and connects to an I/O bus that serves multiple I/O modules.](images/image_0125.jpeg)\n\n\n(a) Single-bus, detached DMA\n\n\n(b) Single-bus, integrated DMA-I/O\n\n\n(c) I/O bus\n\n\nFigure 7.14: Alternative DMA Configurations. (a) Single-bus, detached DMA: All components (Processor, DMA, I/O, Memory) are connected to a single horizontal bus. (b) Single-bus, integrated DMA-I/O: The Processor and Memory are on the main bus, while the DMA and I/O modules are integrated into a single block connected to the main bus. (c) I/O bus: The Processor and Memory are on the System bus, while the DMA module is on the System bus and connects to an I/O bus that serves multiple I/O modules.\n\n\n**Figure 7.14**\n   Alternative DMA Configurations\n\n\nthe system bus. The DMA logic may actually be a part of an I/O module, or it may be a separate module that controls one or more I/O modules. This concept can be taken one step further by connecting I/O modules to the DMA module using an I/O bus (Figure 7.14c). This reduces the number of I/O interfaces in the DMA module to one and provides for an easily expandable configuration. In both of these cases (Figures 7.14b and c), the system bus that the DMA module shares with the processor and memory is used by the DMA module only to exchange data with memory. The exchange of data between the DMA and I/O modules takes place off the system bus.\n\n\n\n\n**Intel 8237A DMA Controller**\n\n\nThe Intel 8237A DMA controller interfaces to the 80 × 86 family of processors and to DRAM memory to provide a DMA capability. Figure 7.15 indicates the location of the DMA module. When the DMA module needs to use the system buses (data, address, and control) to transfer data, it sends a signal called HOLD to the processor. The processor responds with the HLDA (hold acknowledge) signal, indicating that\n\n\n\n\n![Diagram showing the 8237 DMA chip interfacing with the CPU, Main memory, and Disk controller via system buses.](images/image_0126.jpeg)\n\n\nThe diagram illustrates the 8237 DMA chip's connection to the system buses. The CPU is represented by a large vertical rectangle on the left. The 8237 DMA chip is a central rectangle. The Main memory and Disk controller are represented by rectangles on the right. The system buses are labeled: Data bus (top), Address bus (middle), and Control bus (bottom, with IOR, IOW, MEMR, MEMW). The DMA chip has pins for HRQ (Hold Request) and HDLA (Hold Acknowledge) connected to the CPU. It also has pins for DREQ (DMA Request) and DACK (DMA Acknowledge) connected to the Main memory and Disk controller. The DMA chip is connected to the Data bus, Address bus, and Control bus.\n\n\nDiagram showing the 8237 DMA chip interfacing with the CPU, Main memory, and Disk controller via system buses.\n\n\nDACK = DMA acknowledge\n   \n\n   DREQ = DMA request\n   \n\n   HDLA = HOLD acknowledge\n   \n\n   HRQ = HOLD request\n\n\n**Figure 7.15**\n   8237 DMA Usage of System Bus\n\n\nthe DMA module can use the buses. For example, if the DMA module is to transfer a block of data from memory to disk, it will do the following:\n\n\n  * 1. The peripheral device (such as the disk controller) will request the service of DMA by pulling DREQ (DMA request) high.\n  * 2. The DMA will put a high on its HRQ (hold request), signaling the CPU through its HOLD pin that it needs to use the buses.\n  * 3. The CPU will finish the present bus cycle (not necessarily the present instruction) and respond to the DMA request by putting high on its HDLA (hold acknowledge), thus telling the 8237 DMA that it can go ahead and use the buses to perform its task. HOLD must remain active high as long as DMA is performing its task.\n  * 4. DMA will activate DACK (DMA acknowledge), which tells the peripheral device that it will start to transfer the data.\n  * 5. DMA starts to transfer the data from memory to peripheral by putting the address of the first byte of the block on the address bus and activating MEMR, thereby reading the byte from memory into the data bus; it then activates IOW to write it to the peripheral. Then DMA decrements the counter and increments the address pointer and repeats this process until the count reaches zero and the task is finished.\n  * 6. After the DMA has finished its job it will deactivate HRQ, signaling the CPU that it can regain control over its buses.\n\n\nWhile the DMA is using the buses to transfer data, the processor is idle. Similarly, when the processor is using the bus, the DMA is idle. The 8237 DMA is known as a\n   *fly-by*\n   DMA controller. This means that the data being moved from one location to another does not pass through the DMA chip and is not stored in the DMA chip. Therefore, the DMA can only transfer data between an I/O port and a memory address, and not between two I/O ports or two memory locations. However, as explained subsequently, the DMA chip can perform a memory-to-memory transfer via a register.\n\n\nThe 8237 contains four DMA channels that can be programmed independently, and any one of the channels may be active at any moment. These channels are numbered 0, 1, 2, and 3.\n\n\nThe 8237 has a set of five control/command registers to program and control DMA operation over one of its channels (Table 7.2):\n\n\n  * ■\n    **Command:**\n    The processor loads this register to control the operation of the DMA. D0 enables a memory-to-memory transfer, in which channel 0 is used to transfer a byte into an 8237 temporary register and channel 1 is used to transfer the byte from the register to memory. When memory-to-memory is enabled, D1 can be used to disable increment/decrement on channel 0 so that a fixed value can be written into a block of memory. D2 enables or disables DMA.\n  * ■\n    **Status:**\n    The processor reads this register to determine DMA status. Bits D0–D3 are used to indicate if channels 0–3 have reached their TC (terminal count). Bits D4–D7 are used by the processor to determine if any channel has a DMA request pending.\n  * ■\n    **Mode:**\n    The processor sets this register to determine the mode of operation of the DMA. Bits D0 and D1 are used to select a channel. The other bits select various operation modes for the selected channel. Bits D2 and D3 determine if the transfer is from an I/O device to memory (write) or from memory to I/O (read), or a verify operation. If D4 is set, then the memory address register and the count register are reloaded with their original values at the end of a DMA data transfer. Bits D6 and D7 determine the way in which the 8237 is used. In single mode, a single byte of data is transferred. Block and demand modes are used for a block transfer, with the demand mode allowing for premature ending of the transfer. Cascade mode allows multiple 8237s to be cascaded to expand the number of channels to more than 4.\n  * ■\n    **Single Mask:**\n    The processor sets this register. Bits D0 and D1 select the channel. Bit D2 clears or sets the mask bit for that channel. It is through this register that the DREQ input of a specific channel can be masked (disabled) or unmasked (enabled). While the command register can be used to disable the whole DMA chip, the single mask register allows the programmer to disable or enable a specific channel.\n  * ■\n    **All Mask:**\n    This register is similar to the single mask register except that all four channels can be masked or unmasked with one write operation.\n\n\nIn addition, the 8237A has eight data registers: one memory address register and one count register for each channel. The processor sets these registers to indicate the location of size of main memory to be affected by the transfers.\n\n\n**Table 7.2**\n\nBit | Command | Status | Mode | Single Mask | All Mask\nD0 | Memory-to-memory E/D | Channel 0 has reached TC | Channel select | Select channel mask bit | Clear/set channel 0 mask bit\nD1 | Channel 0 address hold E/D | Channel 1 has reached TC | Clear/set channel 1 mask bit\nD2 | Controller E/D | Channel 2 has reached TC | Verify/write/read transfer | Clear/set mask bit | Clear/set channel 2 mask bit\nD3 | Normal/compressed timing | Channel 3 has reached TC | Auto-initialization E/D | Not used | Clear/set channel 3 mask bit\nD4 | Fixed/rotating priority | Channel 0 request | Not used\nD5 | Late/extended write selection | Channel 0 request | Address increment/decrement select\nD6 | DREQ sense active high/low | Channel 0 request\nD7 | DACK sense active high/low | Channel 0 request | Demand/single/block/cascade mode select\n\n\nE/D = enable/disable\n\n\nTC = terminal count"
        },
        {
          "name": "Direct Cache Access",
          "content": "DMA has proved an effective means of enhancing performance of I/O with peripheral devices and network I/O traffic. However, for the dramatic increases in data rates for network I/O, DMA is not able to scale to meet the increased demand. This demand is coming primarily from the widespread deployment of 10-Gbps and 100-Gbps Ethernet switches to handle massive amounts of data transfer to and from database servers and other high-performance systems [STAL14a]. A secondary but increasingly important source of traffic comes from Wi-Fi in the gigabit range. Network Wi-Fi devices that handle 3.2 Gbps and 6.76 Gbps are becoming widely available and producing demand on enterprise systems [STAL14b].\n\n\nIn this section, we will show how enabling the I/O function to have direct access to the cache can enhance performance, a technique known as\n   **direct cache access (DCA)**\n   . Throughout this section, we are concerned only with the cache that is closest to main memory, referred to as the\n   **last-level cache**\n   . In some systems, this will be an L2 cache, in others an L3 cache.\n\n\nTo begin, we describe the way in which contemporary multicore systems use on-chip shared cache to enhance DMA performance. This approach involves enabling the DMA function to have direct access to the last-level cache. Next we examine cache-related performance issues that manifest when high-speed network traffic is processed. From there, we look at several different strategies for DCA that are designed to enhance network protocol processing performance. Finally, this section describes a DCA approach implemented by Intel, referred to as Direct Data I/O.\n\n\n\n\n**DMA Using Shared Last-Level Cache**\n\n\nAs was discussed in Chapter 1 (see Figure 1.2), contemporary multicore systems include both cache dedicated to each core and an additional level of shared cache, either L2 or L3. With the increasing size of available last-level cache, system designers have enhanced the DMA function so that the DMA controller has access to the shared cache in a manner similar to the cores. To clarify the interaction of DMA and cache, it will be useful to first describe a specific system architecture. For this purpose, the following is an overview of the Intel Xeon system.\n\n\n**XEON MULTICORE PROCESSOR**\n   Intel Xeon is Intel's high-end, high-performance processor family, used in servers, high-performance workstations, and supercomputers. Many of the members of the Xeon family use a ring interconnect system, as illustrated for the Xeon E5-2600/4600 in Figure 7.16.\n\n\nThe E5-2600/4600 can be configured with up to eight cores on a single chip. Each core has dedicated L1 and L2 caches. There is a shared L3 cache of up to 20 MB. The L3 cache is divided into slices, one associated with each core although each core can address the entire cache. Further, each slice has its own cache pipeline, so that requests can be sent in parallel to the slices.\n\n\nThe bidirectional high-speed ring interconnect links cores, last-level cache, PCIe, and integrated memory controller (IMC).\n\n\nIn essence, the ring operates as follows:\n\n\n  * 1. Each component that attaches to the bidirectional ring (QPI, PCIe, L3 cache, L2 cache) is considered a ring agent, and implements ring agent logic.\n  * 2. The ring agents cooperate via a distributed protocol to request and allocate access to the ring, in the form of time slots.\n  * 3. When an agent has data to send, it chooses the ring direction that results in the shortest path to the destination and transmits when a scheduling slot is available.\n\n\nThe ring architecture provides good performance and scales well for multiple cores, up to a point. For systems with a greater number of cores, multiple rings are used, with each ring supporting some of the cores.\n\n\n**DMA USE OF THE CACHE**\n   In traditional DMA operation, data are exchanged between main memory and an I/O device by means of the system interconnection structure, such as a bus, ring, or QPI point-to-point matrix. So, for example, if the Xeon E5-2600/4600 used a traditional DMA technique, output would proceed as follows. An I/O driver running on a core would send an I/O command to the I/O controller (labeled PCIe in Figure 7.16) with the location and size of the buffer in main memory containing the data to be transferred. The I/O controller issues a read request that is routed to the memory controller hub (MCH), which accesses the data on DDR3 memory and puts it on the system ring for delivery to the I/O controller. The L3 cache is not involved in this transaction and one or more off-chip memory reads are required. Similarly, for input, data arrive from the I/O controller and is delivered over the system ring to the MCH and written out to main memory. The MCH must also invalidate any L3 cache lines corresponding to the updated memory locations. In this case, one or more off-chip memory writes are required. Further, if an application wants to access the new data, a main memory read is required.\n\n\n\n\n![Diagram of the Xeon E5-2600/4600 Chip Architecture showing internal components and external interfaces.](images/image_0127.jpeg)\n\n\nThe diagram illustrates the internal architecture of a Xeon E5-2600/4600 chip, enclosed within a dashed-line boundary labeled \"Chip boundary\".\n\n\n**Internal Components:**\n\n\n  * **Processor Cores (8 total):**\n     Arranged in two columns of four cores each.\n       * **Left Column (Cores 0-3):**\n       Each core contains an L1 (64 KB) cache and an L2 (256 KB) cache.\n  * **Right Column (Cores 4-7):**\n       Each core contains an L1 (64 KB) cache, an L2 (256 KB) cache, and an L2 (256 KB) cache.\n  * **L3 Cache (8 total):**\n     Arranged in two columns of four L3 caches each, with each L3 cache labeled \"L3 Cache (2.5 MB)\".\n  * **Memory Controller Hub (MCH):**\n     A central component at the bottom of the chip.\n\n\n**External Interfaces and Connections:**\n\n\n  * **QPI (QuickPath Interconnect):**\n     Located at the top left, with a bidirectional arrow labeled \"To other processor chips\".\n  * **PCIe (Peripheral Component Interconnect Express):**\n     Located at the top right, with a bidirectional arrow labeled \"To I/O devices\".\n  * **DDR3 Memory:**\n     Located at the bottom, with a bidirectional arrow labeled \"To DDR3 memory\".\n\n\n**Internal Data Flow:**\n\n\n  * Each core is connected to its corresponding L3 cache via a vertical bus with a switch icon.\n  * The L3 caches are interconnected horizontally and vertically, forming a mesh-like structure.\n  * The Memory Controller Hub (MCH) is connected to the L3 caches via a horizontal bus with a switch icon.\n\n\nDiagram of the Xeon E5-2600/4600 Chip Architecture showing internal components and external interfaces.\n\n\n**Figure 7.16**\n   Xeon E5-2600/4600 Chip Architecture\n\n\nWith the availability of large amounts of last-level cache, a more efficient technique is possible, and is used by the Xeon E5-2600/4600. For output, when the I/O controller issues a read request, the MCH first checks to see if the data are in the L3 cache. This is likely to be the case, if an application has recently written data into the memory block to be output. In that case, the MCH directs data from the L3 cache to the I/O controller; no main memory accesses are needed. However, it also causes the data to be evicted from cache, that is, the act of reading by an I/O device\n\n\ncauses data to be evicted. Thus, the I/O operation proceeds efficiently because it does not require main memory access. But, if an application does need that data in the future, it must be read back into the L3 cache from main memory. The input operation on the Xeon E5-2600/4600 operates as described in the previous paragraph; the L3 cache is not involved. Thus, the performance improvement involves only output operations.\n\n\nA final point. Although the output transfer is directly from cache to the I/O controller, the term\n   *direct cache access*\n   is not used for this feature. Rather, the term is reserved for the I/O protocol application, as described in the remainder of this section.\n\n\n\n\n**Cache-Related Performance Issues**\n\n\nNetwork traffic is transmitted in the form of a sequence of protocol blocks, called packets or protocol data units. The lowest, or link, level protocol is typically Ethernet, so that each arriving and departing block of data consists of an Ethernet packet containing as payload the higher-level protocol packet. The higher-level protocols are usually the Internet Protocol (IP), operating on top of Ethernet, and the Transmission Control Protocol (TCP), operating on top of IP. Accordingly, the Ethernet payload consists of a block of data with a TCP header and an IP header. For outgoing data, Ethernet packets are formed in a peripheral component, such as an I/O controller or network interface controller (NIC). Similarly, for incoming traffic, the I/O controller strips off the Ethernet information and delivers the TCP/IP packet to the host CPU.\n\n\nFor both outgoing and incoming traffic, the core, main memory, and cache are all involved. In a DMA scheme, when an application wishes to transmit data, it places that data in an application-assigned buffer in main memory. The core transfers this to a system buffer in main memory and creates the necessary TCP and IP headers, which are also buffered in system memory. The packet is then picked up via DMA for transfer via the NIC. This activity engages not only main memory but also the cache. For incoming traffic, similar transfers between system and application buffers are required.\n\n\nWhen large volumes of protocol traffic are processed, two factors in this scenario degrade performance. First, the core consumes valuable clock cycles in copying data between system and application buffers. Second, because memory speeds have not kept up with CPU speeds, the core loses time waiting on memory reads and writes. In this traditional way of processing protocol traffic, the cache does not help because the data and protocol headers are constantly changing and thus the cache must constantly be updated.\n\n\nTo clarify the performance issue and to explain the benefit of DCA as a way of improving performance, let us look at the processing of protocol traffic in more detail for incoming traffic. In general terms, the following steps occur:\n\n\n  * 1.\n    **Packet arrives:**\n    The NIC receives an incoming Ethernet packet. The NIC processes and strips off the Ethernet control information. This includes doing an error detection calculation. The remaining TCP/IP packet is then transferred to the system's DMA module, which generally is part of the NIC. The NIC also creates a packet descriptor with information about the packet, such as its buffer location in memory.\n\n\n  * 2.\n    **DMA:**\n    The DMA module transfers data, including the packet descriptor, to main memory. It must also invalidate the corresponding cache lines, if any.\n  * 3.\n    **NIC interrupts host:**\n    After a number of packets have been transferred, the NIC issues an interrupt to the host processor.\n  * 4.\n    **Retrieve descriptors and headers:**\n    The core processes the interrupt, invoking an interrupt handling procedure, which reads the descriptor and header of the received packets.\n  * 5.\n    **Cache miss occurs:**\n    Because this is new data coming in, the cache lines corresponding to the system buffer containing the new data are invalidated. Thus, the core must stall to read the data from main memory into cache, and then to core registers.\n  * 6.\n    **Header is processed:**\n    The protocol software executes on the core to analyze the contents of the TCP and IP headers. This will likely include accessing a transport control block (TCB), which contains context information related to TCP. The TCB access may or may not trigger a cache miss, necessitating a main memory access.\n  * 7.\n    **Payload transferred:**\n    The data portion of the packet is transferred from the system buffer to the appropriate application buffer.\n\n\nA similar sequence of steps occurs for outgoing packet traffic, but there are some differences that affect how the cache is managed. For outgoing traffic, the following steps occur:\n\n\n  * 1.\n    **Packet transfer requested:**\n    When an application has a block of data to transfer to a remote system, it places the data in an application buffer and alerts the OS with some type of system call.\n  * 2.\n    **Packet created:**\n    The OS invokes a TCP/IP process to create the TCP/IP packet for transmission. The TCP/IP process accesses the TCB (which may involve a cache miss) and creates the appropriate headers. It also reads the data from the application buffer, and then places the completed packet (headers plus data) in a system buffer. Note that the data that is written into the system buffer also exists in the cache. The TCP/IP process also creates a packet descriptor that is placed in memory shared with the DMA module.\n  * 3.\n    **Output operation invoked:**\n    This uses a device driver program to signal the DMA module that output is ready for the NIC.\n  * 4.\n    **DMA transfer:**\n    The DMA module reads the packet descriptor, then a DMA transfer is performed from main memory or the last-level cache to the NIC. Note that DMA transfers invalidate the cache line in cache even in the case of a read (by the DMA module). If the line is modified, this causes a write back. The core does not do the invalidates. The invalidates happen when the DMA module reads the data.\n  * 5.\n    **NIC signals completion:**\n    After the transfer is complete, the NIC signals the driver on the core that originated the send signal.\n  * 6.\n    **Driver frees buffer:**\n    Once the driver receives the completion notice, it frees up the buffer space for reuse. The core must also invalidate the cache lines containing the buffer data.\n\n\nAs can be seen, network I/O involves a number of accesses to cache and main memory and the movement of data between an application buffer and a system buffer. The heavy involvement of main memory becomes a bottleneck, as both core and network performance outstrip gains in memory access times.\n\n\n\n\n**Direct Cache Access Strategies**\n\n\nSeveral strategies have been proposed for making more efficient use of caches for network I/O, with the general term\n   *direct cache access*\n   applied to all of these strategies.\n\n\nThe simplest strategy is one that was implemented as a prototype on a number of Intel Xeon processors between 2006 and 2010 [KUMA07, INTE08]. This form of DCA applies only to incoming network traffic. The DCA function in the memory controller sends a prefetch hint to the core as soon as the data are available in system memory. This enables the core to prefetch the data packet from the system buffer, thus avoiding cache misses and the associated waste of core cycles.\n\n\nWhile this simple form of DCA does provide some improvement, much more substantial gains can be realized by avoiding the system buffer in main memory altogether. For the specific function of protocol processing, note that the packet and packet descriptor information are accessed only once in the system buffer by the core. For incoming packets, the core reads the data from the buffer and transfers the packet payload to an application buffer. It has no need to access that data in the system buffer again. Similarly, for outgoing packets, once the core has placed the data in the system buffer, it has no need to access that data again. Suppose, therefore, that the I/O system were equipped not only with the capability of directly accessing main memory, but also of accessing the cache, both for input and output operations. Then it would be possible to use the last-level cache instead of the main memory to buffer packets and descriptors of incoming and outgoing packets.\n\n\nThis last approach, which is a true DCA, was proposed in [HUGG05]. It has also been described as\n   **cache injection**\n   [LEON06]. A version of this more complete form of DCA is implemented in Intel's Xeon processor line, referred to as\n   **Direct Data I/O**\n   [INTE12].\n\n\n\n\n**Direct Data I/O**\n\n\nIntel Direct Data I/O (DDIO) is implemented on all of the Xeon E5 family of processors. Its operation is best explained with a side-by-side comparison of transfers with and without DDIO.\n\n\n**PACKET INPUT**\n   First, we look at the case of a packet arriving at the NIC from the network. Figure 7.17a shows the steps involved for a DMA operation. The NIC initiates a memory write (1). Then the NIC invalidates the cache lines corresponding to the system buffer (2). Next, the DMA operation is performed, depositing the packet directly into main memory (3). Finally, after the appropriate core receives a DMA interrupt signal, the core can read the packet data from memory through the cache (4).\n\n\nBefore discussing the processing of an incoming packet using DDIO, we need to summarize the discussion of cache write policy from Chapter 4, and introduce a new technique. For the following discussion, there are issues relating to cache coherency that arise in a multiprocessor or multicore environment. These are discussed\n\n\n\n\n![Figure 7.17: Comparison of DMA and DDIO. The diagram consists of four sub-diagrams (a, b, c, d) showing data paths between cores, last-level cache, I/O controller, and main memory. (a) Normal DMA transfer to memory: Data flows from Core N to the Last-level cache, then to the I/O controller (arrow 2), and finally to Main memory (arrow 4). (b) DDIO transfer to cache: Data flows from Core N to the Last-level cache (arrow 3), then to the I/O controller (arrow 2), and finally to Main memory (arrow 3). (c) Normal DMA transfer to I/O: Data flows from Core N to the Last-level cache, then to the I/O controller (arrow 1), and finally to Main memory (arrow 3). (d) DDIO transfer to I/O: Data flows from Core N to the Last-level cache (arrow 1), then to the I/O controller (arrow 2), and finally to Main memory (arrow 3).](images/image_0128.jpeg)\n\n\nFigure 7.17: Comparison of DMA and DDIO. The diagram consists of four sub-diagrams (a, b, c, d) showing data paths between cores, last-level cache, I/O controller, and main memory. (a) Normal DMA transfer to memory: Data flows from Core N to the Last-level cache, then to the I/O controller (arrow 2), and finally to Main memory (arrow 4). (b) DDIO transfer to cache: Data flows from Core N to the Last-level cache (arrow 3), then to the I/O controller (arrow 2), and finally to Main memory (arrow 3). (c) Normal DMA transfer to I/O: Data flows from Core N to the Last-level cache, then to the I/O controller (arrow 1), and finally to Main memory (arrow 3). (d) DDIO transfer to I/O: Data flows from Core N to the Last-level cache (arrow 1), then to the I/O controller (arrow 2), and finally to Main memory (arrow 3).\n\n\n**Figure 7.17**\n   Comparison of DMA and DDIO\n\n\nin Chapter 17 but the details need not concern us here. Recall that there are two techniques for dealing with an update to a cache line:\n\n\n  * ■\n    **Write through:**\n    All write operations are made to main memory as well as to the cache, ensuring that main memory is always valid. Any other core-cache module can monitor traffic to main memory to maintain consistency within its own local cache.\n  * ■\n    **Write back:**\n    Updates are made only in the cache. When an update occurs, a dirty bit associated with the line is set. Then, when a block is replaced, it is written back to main memory if and only if the dirty bit is set.\n\n\nDDIO uses the write-back strategy in the L3 cache.\n\n\nA cache write operation may encounter a cache miss, which is dealt with by one of two strategies:\n\n\n  * ■\n    **Write allocate:**\n    The required line is loaded into the cache from main memory. Then, the line in the cache is updated by the write operation. This scheme is typically used with the write-back method.\n  * ■\n    **Non-write allocate:**\n    The block is modified directly in main memory. No change is made to the cache. This scheme is typically used with the write-through method.\n\n\nWith the above in mind, we can describe the DDIO strategy for inbound transfers initiated by the NIC.\n\n\n  * 1. If there is a cache hit, the cache line is updated, but not main memory; this is simply the write-back strategy for a cache hit. The Intel literature refers to this as\n    **write update**\n    .\n\n\n  * 2. If there is a cache miss, the write operation occurs to a line in the cache that will not be written back to main memory. Subsequent writes update the cache line, again with no reference to main memory or no future action that writes this data to main memory. The Intel documentation [INTE12] refers to this as\n    *write allocate*\n    , which unfortunately is not the same meaning for the term in the general cache literature.\n\n\nThe DDIO strategy is effective for a network protocol application because the incoming data need not be retained for future use. The protocol application is going to write the data to an application buffer, and there is no need to temporarily store it in a system buffer.\n\n\nFigure 7.17b shows the operation for DDIO input. The NIC initiates a memory write (1). Then the NIC invalidates the cache lines corresponding to the system buffer and deposits the incoming data in the cache (2). Finally, after the appropriate core receives a DCA interrupt signal, the core can read the packet data from the cache (3).\n\n\n**PACKET OUTPUT**\n   Figure 7.17c shows the steps involved for a DMA operation for outbound packet transmission. The TCP/IP protocol handler executing on the core reads data in from an application buffer and writes it out to a system buffer. These data access operations result in cache misses and cause data to be read from memory and into the L3 cache (1). When the NIC receives notification for starting a transmit operation, it reads the data from the L3 cache and transmits it (2). The cache access by the NIC causes the data to be evicted from the cache and written back to main memory (3).\n\n\nFigure 7.17d shows the steps involved for a DDIO operation for packet transmission. The TCP/IP protocol handler creates the packet to be transmitted and stores it in allocated space in the L3 cache (1), but not in main memory (2). The read operation initiated by the NIC is satisfied by data from the cache, without causing evictions to main memory.\n\n\nIt should be clear from these side-by-side comparisons that DDIO is more efficient than DMA for both incoming and outgoing packets and is therefore better able to keep up with the high packet traffic rate."
        },
        {
          "name": "I/O Channels and Processors",
          "content": "**The Evolution of the I/O Function**\n\n\nAs computer systems have evolved, there has been a pattern of increasing complexity and sophistication of individual components. Nowhere is this more evident than in the I/O function. We have already seen part of that evolution. The evolutionary steps can be summarized as follows:\n\n\n  * 1. The CPU directly controls a peripheral device. This is seen in simple microprocessor-controlled devices.\n  * 2. A controller or I/O module is added. The CPU uses programmed I/O without interrupts. With this step, the CPU becomes somewhat divorced from the specific details of external device interfaces.\n  * 3. The same configuration as in step 2 is used, but now interrupts are employed. The CPU need not spend time waiting for an I/O operation to be performed, thus increasing efficiency.\n\n\n  * 4. The I/O module is given direct access to memory via DMA. It can now move a block of data to or from memory without involving the CPU, except at the beginning and end of the transfer.\n  * 5. The I/O module is enhanced to become a processor in its own right, with a specialized instruction set tailored for I/O. The CPU directs the I/O processor to execute an I/O program in memory. The I/O processor fetches and executes these instructions without CPU intervention. This allows the CPU to specify a sequence of I/O activities and to be interrupted only when the entire sequence has been performed.\n  * 6. The I/O module has a local memory of its own and is, in fact, a computer in its own right. With this architecture, a large set of I/O devices can be controlled, with minimal CPU involvement. A common use for such an architecture has been to control communication with interactive terminals. The I/O processor takes care of most of the tasks involved in controlling the terminals.\n\n\nAs one proceeds along this evolutionary path, more and more of the I/O function is performed without CPU involvement. The CPU is increasingly relieved of I/O-related tasks, improving performance. With the last two steps (5–6), a major change occurs with the introduction of the concept of an I/O module capable of executing a program. For step 5, the I/O module is often referred to as an\n   *I/O channel*\n   . For step 6, the term\n   *I/O processor*\n   is often used. However, both terms are on occasion applied to both situations. In what follows, we will use the term\n   *I/O channel*\n   .\n\n\n\n\n**Characteristics of I/O Channels**\n\n\nThe I/O channel represents an extension of the DMA concept. An I/O channel has the ability to execute I/O instructions, which gives it complete control over I/O operations. In a computer system with such devices, the CPU does not execute I/O instructions. Such instructions are stored in main memory to be executed by a special-purpose processor in the I/O channel itself. Thus, the CPU initiates an I/O transfer by instructing the I/O channel to execute a program in memory. The program will specify the device or devices, the area or areas of memory for storage, priority, and actions to be taken for certain error conditions. The I/O channel follows these instructions and controls the data transfer.\n\n\nTwo types of I/O channels are common, as illustrated in Figure 7.18. A\n   *selector channel*\n   controls multiple high-speed devices and, at any one time, is dedicated to the transfer of data with one of those devices. Thus, the I/O channel selects one device and effects the data transfer. Each device, or a small set of devices, is handled by a\n   *controller*\n   , or I/O module, that is much like the I/O modules we have been discussing. Thus, the I/O channel serves in place of the CPU in controlling these I/O controllers. A\n   *multiplexor channel*\n   can handle I/O with multiple devices at the same time. For low-speed devices, a\n   *byte multiplexor*\n   accepts or transmits characters as fast as possible to multiple devices. For example, the resultant character stream from three devices with different rates and individual streams\n   \n    A_1A_2A_3A_4 \\dots\n   \n   ,\n   \n    B_1B_2B_3B_4 \\dots\n   \n   , and\n   \n    C_1C_2C_3C_4 \\dots\n   \n   might be\n   \n    A_1B_1C_1A_2C_2A_3B_2C_3A_4\n   \n   , and so on. For high-speed devices, a\n   *block multiplexor*\n   interleaves blocks of data from several devices.\n\n\n\n\n![Figure 7.18: I/O Channel Architecture. (a) Selector: A Selector channel receives 'Data and address channel to main memory' and 'Control signal path to CPU'. It connects to multiple I/O controllers, each with its own peripheral devices. (b) Multiplexor: A Multiplexor channel receives 'Data and address channel to main memory' and 'Control signal path to CPU'. It connects to multiple I/O controllers, each with its own peripheral devices.](images/image_0129.jpeg)\n\n\n(a) Selector\n\n\n(b) Multiplexor\n\n\nFigure 7.18: I/O Channel Architecture. (a) Selector: A Selector channel receives 'Data and address channel to main memory' and 'Control signal path to CPU'. It connects to multiple I/O controllers, each with its own peripheral devices. (b) Multiplexor: A Multiplexor channel receives 'Data and address channel to main memory' and 'Control signal path to CPU'. It connects to multiple I/O controllers, each with its own peripheral devices.\n\n\nFigure 7.18 I/O Channel Architecture"
        },
        {
          "name": "External Interconnection Standards",
          "content": "In this section, we provide a brief overview of the most widely used external interface standards to support I/O. Two of these, Thunderbolt and InfiniBand, are examined in detail in Appendix J.\n\n\n\n\n**Universal Serial Bus (USB)**\n\n\nUSB is widely used for peripheral connections. It is the default interface for slower-speed devices, such as keyboard and pointing devices, but is also commonly used for high-speed I/O, including printers, disk drives, and network adapters.\n\n\nUSB has gone through multiple generations. The first version, USB 1.0, defined a\n   *Low Speed*\n   data rate of 1.5 Mbps and a\n   *Full Speed*\n   rate of 12 Mbps. USB 2.0 provides a data rate of 480 Mbps. USB 3.0 includes a new, higher speed bus\n\n\ncalled\n   *SuperSpeed*\n   in parallel with the USB 2.0 bus. The signaling speed of SuperSpeed is 5 Gbps, but due to signaling overhead, the usable data rate is up to 4 Gbps. The most recent specification is USB 3.1, which includes a faster transfer mode called\n   *SuperSpeed+*\n   . This transfer mode achieves a signaling rate of 10 Gbps and a theoretical usable data rate of 9.7 Gbps.\n\n\nA USB system is controlled by a root host controller, which attaches to devices to create a local network with a hierarchical tree topology.\n\n\n\n\n**FireWire Serial Bus**\n\n\nFireWire was developed as an alternative to the small computer system interface (SCSI) to be used on smaller systems, such as personal computers, workstations, and servers. The objective was to meet the increasing demands for high I/O rates on these systems, while avoiding the bulky and expensive I/O channel technologies developed for mainframe and supercomputer systems. The result is the IEEE standard 1394, for a High Performance Serial Bus, commonly known as FireWire.\n\n\nFireWire uses a daisy-chain configuration, with up to 63 devices connected off a single port. Moreover, up to 1022 FireWire buses can be interconnected using bridges, enabling a system to support as many peripherals as required.\n\n\nFireWire provides for what is known as hot plugging, which makes it possible to connect and disconnect peripherals without having to power the computer system down or reconfigure the system. Also, FireWire provides for automatic configuration; it is not necessary manually to set device IDs or to be concerned with the relative position of devices. With FireWire, there are no terminations, and the system automatically performs a configuration function to assign addresses. A FireWire bus need not be a strict daisy chain. Rather, a tree-structured configuration is possible.\n\n\nAn important feature of the FireWire standard is that it specifies a set of three layers of protocols to standardize the way in which the host system interacts with the peripheral devices over the serial bus. The physical layer defines the transmission media that are permissible under FireWire and the electrical and signaling characteristics of each. Data rates from 25 Mbps to 3.2 Gbps are defined. The link layer describes the transmission of data in the packets. The transaction layer defines a request-response protocol that hides the lower-layer details of FireWire from applications.\n\n\n\n\n**Small Computer System Interface (SCSI)**\n\n\nSCSI is a once common standard for connecting peripheral devices (disks, modems, printers, etc.) to small and medium-sized computers. Although SCSI has evolved to higher data rates, it has lost popularity to such competitors as USB and FireWire in smaller systems. However, high-speed versions of SCSI remain popular for mass memory support on enterprise systems. For example, the IBM zEnterprise EC12 and other IBM mainframes offer support for SCSI, and a number of Seagate hard drive systems use SCSI.\n\n\nThe physical organization of SCSI is a shared bus, which can support up to 16 or 32 devices, depending on the generation of the standard. The bus provides for parallel transmission rather than serial, with a bus width of 16 bits on earlier generations and 32 bits on later generations. Speeds range from 5 Mbps on the original SCSI-1 specification to 160 Mbps on SCSI-3 U3.\n\n\n\n\n**Thunderbolt**\n\n\nThe most recent, and one of fastest, peripheral connection technology to become available for general-purpose use is Thunderbolt, developed by Intel with collaboration from Apple. One Thunderbolt cable can manage the work previously required of multiple cables. The technology combines data, video, audio, and power into a single high-speed connection for peripherals such as hard drives, RAID (Redundant Array of Independent Disks) arrays, video-capture boxes, and network interfaces. It provides up to 10 Gbps throughput in each direction and up to 10 watts of power to connected peripherals.\n\n\nThunderbolt is described in detail in Appendix J.\n\n\n\n\n**InfiniBand**\n\n\nInfiniBand is an I/O specification aimed at the high-end server market. The first version of the specification was released in early 2001 and has attracted numerous vendors. For example, IBM zEnterprise series of mainframes has relied heavily on InfiniBand for a number of years. The standard describes an architecture and specifications for data flow among processors and intelligent I/O devices. InfiniBand has become a popular interface for storage area networking and other large storage configurations. In essence, InfiniBand enables servers, remote storage, and other network devices to be attached in a central fabric of switches and links. The switch-based architecture can connect up to 64,000 servers, storage systems, and networking devices.\n\n\nInfiniband is described in detail in Appendix J.\n\n\n\n\n**PCI Express**\n\n\nPCI Express is a high-speed bus system for connecting peripherals of a wide variety of types and speeds. Chapter 3 discusses PCI Express in detail.\n\n\n\n\n**SATA**\n\n\nSerial ATA (Serial Advanced Technology Attachment) is an interface for disk storage systems. It provides data rates of up to 6 Gbps, with a maximum per device of 300 Mbps. SATA is widely used in desktop computers, and in industrial and embedded applications.\n\n\n\n\n**Ethernet**\n\n\nEthernet is the predominant wired networking technology, used in homes, offices, data centers, enterprises, and wide-area networks. As Ethernet has evolved to support data rates up to 100 Gbps and distances from a few meters to tens of km, it has become essential for supporting personal computers, workstations, servers, and massive data storage devices in organizations large and small.\n\n\nEthernet began as an experimental bus-based 3-Mbps system. With a bus system, all of the attached devices, such as PCs, connect to a common coaxial cable, much like residential cable TV systems. The first commercially-available Ethernet, and the first version of IEEE 802.3, were bus-based systems operating at 10 Mbps. As technology has advanced, Ethernet has moved from bus-based to switch-based, and the data rate has periodically increased by an order of magnitude. With\n\n\nswitch-based systems, there is a central switch, with all of the devices connected directly to the switch. Currently, Ethernet systems are available at speeds up to 100 Gbps. Here is a brief chronology.\n\n\n  * ■ 1983: 10 Mbps (megabit per second, million bits per second)\n  * ■ 1995: 100 Mbps\n  * ■ 1998: 1 Gbps (gigabit per second, billion bits per second)\n  * ■ 2003: 10 Gbps\n  * ■ 2010: 40 Gbps and 100 Gbps\n\n\n\n\n**Wi-Fi**\n\n\nWi-Fi is the predominant wireless Internet access technology, used in homes, offices, and public spaces. Wi-Fi in the home now connects computers, tablets, smart phones, and a host of electronic devices, such as video cameras, TVs, and thermostats. Wi-Fi in the enterprise has become an essential means of enhancing worker productivity and network effectiveness. And public Wi-Fi hotspots have expanded dramatically to provide free Internet access in most public places.\n\n\nAs the technology of antennas, wireless transmission techniques, and wireless protocol design has evolved, the IEEE 802.11 committee has been able to introduce standards for new versions of Wi-Fi at ever-higher speeds. Once the standard is issued, industry quickly develops the products. Here is a brief chronology, starting with the original standard, which was simply called IEEE 802.11, and showing the maximum data rate for each version:\n\n\n  * ■ 802.11 (1997): 2 Mbps (megabit per second, million bits per second)\n  * ■ 802.11a (1999): 54 Mbps\n  * ■ 802.11b (1999): 11 Mbps\n  * ■ 802.11n (1999): 600 Mbps\n  * ■ 802.11g (2003): 54 Mbps\n  * ■ 802.11ad (2012): 6.76 Gbps (billion bits per second)\n  * ■ 802.11ac (2014): 3.2 Gbps"
        },
        {
          "name": "IBM zEnterprise EC12 I/O Structure",
          "content": "The zEnterprise EC12 is IBM's latest mainframe computer offering (at the time of this writing). The system is based on the use of the zEC12 processor chip, which is a 5.5-GHz multicore chip with six cores. The zEC12 architecture can have a maximum of 101 processor chips for a total of 606 cores. In this section, we look at the I/O structure of the zEnterprise EC12.\n\n\n\n\n**Channel Structure**\n\n\nThe zEnterprise EC12 has a dedicated I/O subsystem that manages all I/O operations, completely off-loading this processing and memory burden from the main\n\n\n\n\n![Diagram of IBM zEC12 I/O Channel Subsystem Structure showing a hierarchy from partitions to channels.](images/image_0130.jpeg)\n\n\nThe diagram illustrates the hierarchical structure of the IBM zEC12 I/O subsystem. At the top level, the system is constrained by\n    \n     \\le 60\n    \n    partitions per system. These partitions are grouped into channel subsystems, with a constraint of\n    \n     \\le 15\n    \n    partitions per channel subsystem. Each partition is represented as a box containing 'Partition' and 'subchannels'. These partitions are connected to channel subsystems, which are represented as boxes containing 'Channel subsystem'. A group of four channel subsystems is indicated by a bracket and the label '4 channel subsystems'. Each channel subsystem is connected to a set of channels, represented as boxes containing 'Channel'. The entire structure is constrained by\n    \n     \\le 256\n    \n    channels per channel subsystem. The final overall constraint is\n    \n     \\le 1024\n    \n    partitions per system.\n\n\nDiagram of IBM zEC12 I/O Channel Subsystem Structure showing a hierarchy from partitions to channels.\n\n\n**Figure 7.19**\n   IBM zEC12 I/O Channel Subsystem Structure\n\n\nprocessors. Figure 7.21 shows the logical structure of the I/O subsystem. Of the 96 core processors, up to 4 of these can be dedicated for I/O use, creating 4\n   **channel subsystems (CSS)**\n   . Each CSS is made up of the following elements:\n\n\n  * ■\n    **System assist processor (SAP):**\n    The SAP is a core processor configured for I/O operation. Its role is to offload I/O operations and manage channels and the I/O operations queues. It relieves the other processors of all I/O tasks, allowing them to be dedicated to application logic.\n  * ■\n    **Hardware system area (HSA):**\n    The HSA is a reserved part of the system memory containing the I/O configuration. It is used by SAPs. A fixed amount of 32 GB is reserved, which is not part of the customer-purchased memory. This provides for greater configuration flexibility and higher availability by eliminating planned and unplanned outages.\n  * ■\n    **Logical partitions:**\n    A logical partition is a form of virtual machine, which is in essence, a logical processor defined at the operating system level.\n    \n     3\n    \n    Each CSS supports up to 16 logical partitions.\n\n\n3\n   \n   A virtual machine is an instance of an operating system along with one or more applications running in an isolated memory partition within the computer. It enables different operating systems to run in the same computer at the same time as well as prevents applications from interfering with each other. See [STAL12] for a discussion of virtual machines.\n\n\n  * ■\n    **Subchannels:**\n    A subchannel appears to a program as a logical device and contains the information required to perform an I/O operation. One subchannel exists for each I/O device addressable by the CSS. A subchannel is used by the channel subsystem code running on a partition to pass an I/O request to the channel subsystem. A subchannel is assigned for each device defined to the logical partition. Up to 196k subchannels are supported per CSS.\n  * ■\n    **Channel path:**\n    A channel path is a single interface between a channel subsystem and one or more control units, via a channel. Commands and data are sent across a channel path to perform I/O requests. Each CSS can have up to 256 channel paths.\n  * ■\n    **Channel:**\n    Channels are small processors that communicate with the I/O control units (CUs). They manage the data transfer between memory and the external devices.\n\n\nThis elaborate structure enables the mainframe to manage a massive number of I/O devices and communication links. All I/O processing is offloaded from the application and server processors, enhancing performance. The channel subsystem processors are somewhat general in configuration, enabling them to manage a wide variety of I/O duties and to keep up with evolving requirements. The channel processors are specifically programmed for the I/O control units to which they interface.\n\n\n\n\n**I/O System Organization**\n\n\nTo explain the I/O system organization, we need to first briefly explain the physical layout of the zEnterprise EC12. Figure 7.20 is a front view of the water-cooled version of the machine (there is also an air-cooled version). The system has the following characteristics:\n\n\n  * ■ Weight: 2430 kg (5358 lbs)\n  * ■ Width: 1.568 m (5.14 ft)\n  * ■ Depth: 1.69 m (6.13 ft)\n  * ■ Height: 2.015 m (6.6 ft)\n\n\nNot exactly a laptop.\n\n\nThe system consists of two large bays, called frames, that house the various components of the zEnterprise EC12. The right-hand A frame includes two large cages, plus room for cabling and other components. The upper cage is a processor cage, with four slots to house up to four processor books that are fully interconnected. Each book contains a multichip module (MCM), memory cards, and I/O cage connections. Each MCM is a board that houses six multicores chips and two storage control chips.\n\n\nThe lower cage in the A frame is an I/O cage, which contains I/O hardware, including multiplexors and channels. The I/O cage is a fixed unit installed by IBM to the customer specifications at the factory.\n\n\nThe left-hand Z frame contains internal batteries and power supplies and room for one or more support elements, which are used by a system manager for platform management. The Z frame also contains slots for two or more I/O drawers.\n\n\n\n\n![Figure 7.20: IBM zEC12 I/O Frames—Front View. This diagram shows the internal components of an IBM zEC12 I/O frame. Labels with arrows point to: Internal batteries (optional) at the top; Flexible service processor (FSP) controller cards in the upper middle section; Power supplies in the middle section; Support elements in the lower middle section; PCIe I/O drawer at the bottom; Processor books with memory HCA- and PCIe-fanout cards in the upper right section; InfiniBand and PCIe I/O interconnects in the middle right section; I/O cage carried forward in the lower right section; and N+1 water cooling units at the bottom right.](images/image_0131.jpeg)\n\n\nFigure 7.20: IBM zEC12 I/O Frames—Front View. This diagram shows the internal components of an IBM zEC12 I/O frame. Labels with arrows point to: Internal batteries (optional) at the top; Flexible service processor (FSP) controller cards in the upper middle section; Power supplies in the middle section; Support elements in the lower middle section; PCIe I/O drawer at the bottom; Processor books with memory HCA- and PCIe-fanout cards in the upper right section; InfiniBand and PCIe I/O interconnects in the middle right section; I/O cage carried forward in the lower right section; and N+1 water cooling units at the bottom right.\n\n\n**Figure 7.20**\n   IBM zEC12 I/O Frames—Front View\n\n\nAn I/O drawer contains similar components to an I/O cage. The differences are that the drawer is smaller and easily swapped in and out at the customer site to meet changing requirements.\n\n\nWith this background, we now show a typical configuration of the zEnterprise EC12 I/O system structure (Figure 7.21). Each zEC12 processor book supports two internal (i.e., internal to the A and Z frames) I/O infrastructures: InfiniBand for I/O cages and I/O drawers, and PCI Express (PCIe) for I/O drawers. These channel controllers are referred to as\n   **fanouts**\n   .\n\n\nThe InfiniBand connections from the processor book to the I/O cages and I/O drawers are via a Host Channel Adapter (HCA) fanout, which has InfiniBand links to InfiniBand multiplexors in the I/O cage or drawer. The InfiniBand multiplexors are used to interconnect servers, communications infrastructure equipment, storage, and embedded systems. In addition to using InfiniBand to interconnect systems, all of which use InfiniBand, the InfiniBand multiplexor supports other I/O technologies. ESCON (Enterprise Systems Connection) supports connectivity to disks, tapes, and printer devices using a proprietary fiber-based technology. Ethernet connections provide 1-Gbps and 10-Gbps connections to a variety of devices that support this popular local area network technology. One noteworthy use of Ethernet is to construct large server farms, particularly to interconnect blade servers with each other and with other mainframes.\n   \n    4\n\n\n4\n   \n   A blade server is a server architecture that houses multiple server modules (blades) in a single chassis. It is widely used in data centers to save space and improve system management. Either self-standing or rack mounted, the chassis provides the power supply, and each blade has its own CPU, memory, and hard disk.\n\n\n\n\n![Figure 7.21: IBM zEC12 I/O System Structure. The diagram shows four 'Book' units (Book 1 to Book 4) at the top, each containing Memory, PU (Processor Unit) blocks, SC1, SCO (I/O Controller), and PCIe (8x) or HCA2 (8x) interfaces. These connect to two types of I/O drawers below. The left drawer is a 'PCIe I/O Drawer' with PCIe switches connecting to Fibre Channel controllers and 10-Gbps Ethernet controllers. The right drawer is an 'I/O Cage & I/O Drawer' with InfiniBand and multiplexor interfaces connecting to ESCON Channels and 1-Gbps Ethernet Ports.](images/image_0132.jpeg)\n\n\nFigure 7.21: IBM zEC12 I/O System Structure. The diagram shows four 'Book' units (Book 1 to Book 4) at the top, each containing Memory, PU (Processor Unit) blocks, SC1, SCO (I/O Controller), and PCIe (8x) or HCA2 (8x) interfaces. These connect to two types of I/O drawers below. The left drawer is a 'PCIe I/O Drawer' with PCIe switches connecting to Fibre Channel controllers and 10-Gbps Ethernet controllers. The right drawer is an 'I/O Cage & I/O Drawer' with InfiniBand and multiplexor interfaces connecting to ESCON Channels and 1-Gbps Ethernet Ports.\n\n\n**Figure 7.21**\n   IBM zEC12 I/O System Structure\n\n\nThe PCIe connections from the processor book to the I/O drawers are via a PCIe fanout to PCIe switches. The PCIe switches can connect to a number of I/O device controllers. Typical examples for zEnterprise EC12 are 1-Gbps and 10-Gbps Ethernet and Fiber Channel.\n\n\nEach book contains a combination of up to 8 InfiniBand HCA and PCIe fanouts. Each fanout supports up to 32 connections, for a total maximum of 256 connections per processor book, each connection controlled by a channel processor."
        }
      ]
    },
    {
      "name": "Operating System Support",
      "sections": [
        {
          "name": "Operating System Overview",
          "content": "**Operating System Objectives and Functions**\n\n\nAn OS is a program that controls the execution of application programs and acts as an interface between applications and the computer hardware. It can be thought of as having two objectives:\n\n\n  * ■\n    **Convenience:**\n    An OS makes a computer more convenient to use.\n  * ■\n    **Efficiency:**\n    An OS allows the computer system resources to be used in an efficient manner.\n\n\nLet us examine these two aspects of an OS in turn.\n\n\n**THE OPERATING SYSTEM AS A USER/COMPUTER INTERFACE**\n   The hardware and software used in providing applications to a user can be viewed in a layered or hierarchical fashion, as depicted in Figure 8.1. The user of those applications, the end user, generally is not concerned with the computer's architecture. Thus the end user views a computer system in terms of an application. That application can be expressed in a programming language and is developed by an application programmer. To develop an application program as a set of processor instructions\n\n\n\n\n![Figure 8.1: Computer Hardware and Software Structure. A layered diagram showing the relationship between software and hardware. The top layer is 'Application programs'. Below it is 'Libraries/utilities'. Below that is 'Operating system'. These three are grouped as 'Software'. Below the operating system is 'Execution hardware'. Below that is 'System interconnect (bus)'. Below that is 'Memory translation'. These three are grouped as 'Hardware'. At the bottom are 'I/O devices and networking' and 'Main memory'. The 'Instruction set architecture' is indicated by a horizontal line separating the software layers from the execution hardware layer.](images/image_0133.jpeg)\n\n\nFigure 8.1: Computer Hardware and Software Structure. A layered diagram showing the relationship between software and hardware. The top layer is 'Application programs'. Below it is 'Libraries/utilities'. Below that is 'Operating system'. These three are grouped as 'Software'. Below the operating system is 'Execution hardware'. Below that is 'System interconnect (bus)'. Below that is 'Memory translation'. These three are grouped as 'Hardware'. At the bottom are 'I/O devices and networking' and 'Main memory'. The 'Instruction set architecture' is indicated by a horizontal line separating the software layers from the execution hardware layer.\n\n\n**Figure 8.1**\n   Computer Hardware and Software Structure\n\n\nthat is completely responsible for controlling the computer hardware would be an overwhelmingly complex task. To ease this task, a set of system programs is provided. Some of these programs are referred to as\n   **utilities**\n   . These implement frequently used functions that assist in program creation, the management of files, and the control of I/O devices. A programmer makes use of these facilities in developing an application, and the application, while it is running, invokes the utilities to perform certain functions. The most important system program is the OS. The OS masks the details of the hardware from the programmer and provides the programmer with a convenient interface for using the system. It acts as mediator, making it easier for the programmer and for application programs to access and use those facilities and services.\n\n\nBriefly, the OS typically provides services in the following areas:\n\n\n  * ■\n    **Program creation:**\n    The OS provides a variety of facilities and services, such as editors and debuggers, to assist the programmer in creating programs. Typically, these services are in the form of\n    **utility**\n    programs that are not actually part of the OS but are accessible through the OS.\n  * ■\n    **Program execution:**\n    A number of steps need to be performed to execute a program. Instructions and data must be loaded into main memory, I/O devices and files must be initialized, and other resources must be prepared. The OS handles all of this for the user.\n  * ■\n    **Access to I/O devices:**\n    Each I/O device requires its own specific set of instructions or control signals for operation. The OS takes care of the details so that the programmer can think in terms of simple reads and writes.\n  * ■\n    **Controlled access to files:**\n    In the case of files, control must include an understanding of not only the nature of the I/O device (disk drive, tape drive) but also the file format on the storage medium. Again, the OS worries about the details. Further, in the case of a system with multiple simultaneous users, the OS can provide protection mechanisms to control access to the files.\n\n\n  * ■\n    **System access:**\n    In the case of a shared or public system, the OS controls access to the system as a whole and to specific system resources. The access function must provide protection of resources and data from unauthorized users and must resolve conflicts for resource contention.\n  * ■\n    **Error detection and response:**\n    A variety of errors can occur while a computer system is running. These include internal and external hardware errors, such as a memory error, or a device failure or malfunction; and various software errors, such as arithmetic overflow, attempt to access forbidden memory location, and inability of the OS to grant the request of an application. In each case, the OS must make the response that clears the error condition with the least impact on running applications. The response may range from ending the program that caused the error, to retrying the operation, to simply reporting the error to the application.\n  * ■\n    **Accounting:**\n    A good OS collects usage statistics for various resources and monitors performance parameters such as response time. On any system, this information is useful in anticipating the need for future enhancements and in tuning the system to improve performance. On a multiuser system, the information can be used for billing purposes. Figure 8.1 also indicates three key interfaces in a typical computer system:\n      * ■\n      **Instruction set architecture (ISA):**\n      The ISA defines the repertoire of machine language instructions that a computer can follow. This interface is the boundary between hardware and software. Note that both application programs and utilities may access the ISA directly. For these programs, a subset of the instruction repertoire is available (user ISA). The OS has access to additional machine language instructions that deal with managing system resources (system ISA).\n  * ■\n      **Application binary interface (ABI):**\n      The ABI defines a standard for binary portability across programs. The ABI defines the system call interface to the operating system and the hardware resources and services available in a system through the user ISA.\n  * ■\n      **Application programming interface (API):**\n      The API gives a program access to the hardware resources and services available in a system through the user ISA supplemented with\n      **high-level language (HLL)**\n      library calls. Any system calls are usually performed through libraries. Using an API enables application software to be ported easily, through recompilation, to other systems that support the same API.\n\n\n**THE OPERATING SYSTEM AS RESOURCE MANAGER**\n   A computer is a set of resources for the movement, storage, and processing of data and for the control of these functions. The OS is responsible for managing these resources.\n\n\nCan we say that the OS controls the movement, storage, and processing of data? From one point of view, the answer is yes: By managing the computer's resources, the OS is in control of the computer's basic functions. But this control is exercised in a curious way. Normally, we think of a control mechanism as something external to that which is controlled, or at least as something that is a distinct and separate part of that which is controlled. (For example, a residential heating system\n\n\nis controlled by a thermostat, which is completely distinct from the heat-generation and heat-distribution apparatus.) This is not the case with the OS, which as a control mechanism is unusual in two respects:\n\n\n  * ■ The OS functions in the same way as ordinary computer software; that is, it is a program executed by the processor.\n  * ■ The OS frequently relinquishes control and must depend on the processor to allow it to regain control.\n\n\nLike other computer programs, the OS provides instructions for the processor. The key difference is in the intent of the program. The OS directs the processor in the use of the other system resources and in the timing of its execution of other programs. But in order for the processor to do any of these things, it must cease executing the OS program and execute other programs. Thus, the OS relinquishes control for the processor to do some “useful” work and then resumes control long enough to prepare the processor to do the next piece of work. The mechanisms involved in all this should become clear as the chapter proceeds.\n\n\nFigure 8.2 suggests the main resources that are managed by the OS. A portion of the OS is in main memory. This includes the\n   **kernel**\n   , or\n   **nucleus**\n   , which contains the most frequently used functions in the OS and, at a given time, other portions of the OS currently in use. The remainder of main memory contains user programs and data. The allocation of this resource (main memory) is controlled jointly by the OS and memory-management hardware in the processor, as we will see. The OS decides when an I/O device can be used by a program in execution, and controls access to and\n\n\n\n\n![Diagram illustrating the Operating System as a Resource Manager. The diagram shows a 'Computer system' box containing 'Memory' and 'I/O controller' components. 'Memory' contains 'Operating system software', 'Programs and data', and 'Processor' blocks. 'I/O controller' blocks are connected to 'I/O devices' (Printers, keyboards, digital camera, etc.). A dashed line connects an 'I/O controller' to a 'Storage' circle, which contains 'OS', 'Programs', and 'Data'.](images/image_0134.jpeg)\n\n\nThe diagram illustrates the Operating System as a Resource Manager. It shows a 'Computer system' box containing 'Memory' and 'I/O controller' components. 'Memory' contains 'Operating system software', 'Programs and data', and 'Processor' blocks. 'I/O controller' blocks are connected to 'I/O devices' (Printers, keyboards, digital camera, etc.). A dashed line connects an 'I/O controller' to a 'Storage' circle, which contains 'OS', 'Programs', and 'Data'.\n\n\nDiagram illustrating the Operating System as a Resource Manager. The diagram shows a 'Computer system' box containing 'Memory' and 'I/O controller' components. 'Memory' contains 'Operating system software', 'Programs and data', and 'Processor' blocks. 'I/O controller' blocks are connected to 'I/O devices' (Printers, keyboards, digital camera, etc.). A dashed line connects an 'I/O controller' to a 'Storage' circle, which contains 'OS', 'Programs', and 'Data'.\n\n\n**Figure 8.2**\n   The Operating System as Resource Manager\n\n\nuse of files. The processor itself is a resource, and the OS must determine how much processor time is to be devoted to the execution of a particular user program. In the case of a multiple-processor system, this decision must span all of the processors.\n\n\n\n\n**Types of Operating Systems**\n\n\nCertain key characteristics serve to differentiate various types of operating systems. The characteristics fall along two independent dimensions. The first dimension specifies whether the system is batch or interactive. In an\n   **interactive**\n   system, the user/programmer interacts directly with the computer, usually through a keyboard/display terminal, to request the execution of a job or to perform a transaction. Furthermore, the user may, depending on the nature of the application, communicate with the computer during the execution of the job. A\n   **batch system**\n   is the opposite of interactive. The user's program is batched together with programs from other users and submitted by a computer operator. After the program is completed, results are printed out for the user. Pure batch systems are rare today, however, it will be useful to the description of contemporary operating systems to briefly examine batch systems.\n\n\nAn independent dimension specifies whether the system employs\n   **multiprogramming**\n   or not. With multiprogramming, the attempt is made to keep the processor as busy as possible, by having it work on more than one program at a time. Several programs are loaded into memory, and the processor switches rapidly among them. The alternative is a\n   **uniprogramming**\n   system that works only one program at a time.\n\n\n**EARLY SYSTEMS**\n   With the earliest computers, from the late 1940s to the mid-1950s, the programmer interacted directly with the computer hardware; there was no OS. These processors were run from a console, consisting of display lights, toggle switches, some form of input device, and a printer. Programs in processor code were loaded via the input device (e.g., a card reader). If an error halted the program, the error condition was indicated by the lights. The programmer could proceed to examine registers and main memory to determine the cause of the error. If the program proceeded to a normal completion, the output appeared on the printer.\n\n\nThese early systems presented two main problems:\n\n\n  * ■\n    **Scheduling:**\n    Most installations used a sign-up sheet to reserve processor time. Typically, a user could sign up for a block of time in multiples of a half hour or so. A user might sign up for an hour and finish in 45 minutes; this would result in wasted computer idle time. On the other hand, the user might run into problems, not finish in the allotted time, and be forced to stop before resolving the problem.\n  * ■\n    **Setup time:**\n    A single program, called a\n    **job**\n    , could involve loading the compiler plus the high-level language program (source program) into memory, saving the compiled program (object program), and then loading and linking together the object program and common functions. Each of these steps could involve mounting or dismounting tapes, or setting up card decks. If an error occurred, the hapless user typically had to go back to the beginning of the setup sequence. Thus a considerable amount of time was spent just in setting up the program to run.\n\n\nThis mode of operation could be termed serial processing, reflecting the fact that users have access to the computer in series. Over time, various system software tools were developed to attempt to make serial processing more efficient. These include libraries of common functions, linkers, loaders, debuggers, and I/O driver routines that were available as common software for all users.\n\n\n**SIMPLE BATCH SYSTEMS**\n   Early processors were very expensive, and therefore it was important to maximize processor utilization. The wasted time due to scheduling and setup time was unacceptable.\n\n\nTo improve utilization, simple batch operating systems were developed. With such a system, also called a\n   **monitor**\n   , the user no longer has direct access to the processor. Rather, the user submits the job on cards or tape to a computer operator, who\n   *batchs*\n   the jobs together sequentially and places the entire batch on an input device, for use by the monitor.\n\n\nTo understand how this scheme works, let us look at it from two points of view: that of the monitor and that of the processor. From the point of view of the monitor, the monitor controls the sequence of events. For this to be so, much of the monitor must always be in main memory and available for execution (Figure 8.3). That portion is referred to as the\n   **resident monitor**\n   . The rest of the monitor consists of utilities and common functions that are loaded as subroutines to the user program at the beginning of any job that requires them. The monitor reads in jobs one at a time from the input device (typically a card reader or magnetic tape drive). As it is read in, the current job is placed in the user program area, and control is passed to this job. When the job is completed, it returns control to the monitor, which immediately reads in the next job. The results of each job are printed out for delivery to the user.\n\n\n\n\n![Figure 8.3: Memory Layout for a Resident Monitor. The diagram shows a vertical stack of memory segments. A bracket on the left labels the top four segments as 'Monitor'. These segments are: 'Interrupt processing', 'Device drivers', 'Job sequencing', and 'Control language interpreter'. A horizontal arrow labeled 'Boundary' points to the right, indicating the start of the 'User program area' segment, which is the bottom segment of the stack.](images/image_0135.jpeg)\n\n\nFigure 8.3: Memory Layout for a Resident Monitor. The diagram shows a vertical stack of memory segments. A bracket on the left labels the top four segments as 'Monitor'. These segments are: 'Interrupt processing', 'Device drivers', 'Job sequencing', and 'Control language interpreter'. A horizontal arrow labeled 'Boundary' points to the right, indicating the start of the 'User program area' segment, which is the bottom segment of the stack.\n\n\n**Figure 8.3**\n   Memory Layout for a Resident Monitor\n\n\nNow consider this sequence from the point of view of the processor. At a certain point in time, the processor is executing instructions from the portion of main memory containing the monitor. These instructions cause the next job to be read in to another portion of main memory. Once a job has been read in, the processor will encounter in the monitor a branch instruction that instructs the processor to continue execution at the start of the user program. The processor will then execute the instruction in the user's program until it encounters an ending or error condition. Either event causes the processor to fetch its next instruction from the monitor program. Thus the phrase \"control is passed to a job\" simply means that the processor is now fetching and executing instructions in a user program, and \"control is returned to the monitor\" means that the processor is now fetching and executing instructions from the monitor program.\n\n\nIt should be clear that the monitor handles the scheduling problem. A batch of jobs is queued up, and jobs are executed as rapidly as possible, with no intervening idle time.\n\n\nHow about the job setup time? The monitor handles this as well. With each job, instructions are included in a\n   **job control language (JCL)**\n   . This is a special type of programming language used to provide instructions to the monitor. A simple example is that of a user submitting a program written in FORTRAN plus some data to be used by the program. Each FORTRAN instruction and each item of data is on a separate punched card or a separate record on tape. In addition to FORTRAN and data lines, the job includes job control instructions, which are denoted by the beginning \"$\". The overall format of the job looks like this:\n\n\n$JOB\n$FTN\n:\n} FORTRAN instructions\n$LOAD\n$RUN\n:\n} Data\n$END\n\nTo execute this job, the monitor reads the $FTN line and loads the appropriate compiler from its mass storage (usually tape). The compiler translates the user's program into object code, which is stored in memory or mass storage. If it is stored in memory, the operation is referred to as \"compile, load, and go.\" If it is stored on tape, then the $LOAD instruction is required. This instruction is read by the monitor, which regains control after the compile operation. The monitor invokes the loader, which loads the object program into memory in place of the compiler and transfers control to it. In this manner, a large segment of main memory can be shared among different subsystems, although only one such subsystem could be resident and executing at a time.\n\n\nWe see that the monitor, or batch OS, is simply a computer program. It relies on the ability of the processor to fetch instructions from various portions of main\n\n\nmemory in order to seize and relinquish control alternately. Certain other hardware features are also desirable:\n\n\n  * ■\n    **Memory protection:**\n    While the user program is executing, it must not alter the memory area containing the monitor. If such an attempt is made, the processor hardware should detect an error and transfer control to the monitor. The monitor would then abort the job, print out an error message, and load the next job.\n  * ■\n    **Timer:**\n    A timer is used to prevent a single job from monopolizing the system. The timer is set at the beginning of each job. If the timer expires, an interrupt occurs, and control returns to the monitor.\n  * ■\n    **Privileged instructions:**\n    Certain instructions are designated privileged and can be executed only by the monitor. If the processor encounters such an instruction while executing a user program, an error interrupt occurs. Among the privileged instructions are I/O instructions, so that the monitor retains control of all I/O devices. This prevents, for example, a user program from accidentally reading job control instructions from the next job. If a user program wishes to perform I/O, it must request that the monitor perform the operation for it. If a privileged instruction is encountered by the processor while it is executing a user program, the processor hardware considers this an error and transfers control to the monitor.\n  * ■\n    **Interrupts:**\n    Early computer models did not have this capability. This feature gives the OS more flexibility in relinquishing control to and regaining control from user programs.\n\n\nProcessor time alternates between execution of user programs and execution of the monitor. There have been two sacrifices: Some main memory is now given over to the monitor and some processor time is consumed by the monitor. Both of these are forms of overhead. Even with this overhead, the simple batch system improves utilization of the computer.\n\n\n**MULTIPROGRAMMED BATCH SYSTEMS**\n   Even with the automatic job sequencing provided by a simple batch OS, the processor is often idle. The problem is that I/O devices are slow compared to the processor. Figure 8.4 details a representative calculation. The calculation concerns a program that processes a file of records and performs, on average, 100 processor instructions per record. In this example the computer spends over 96% of its time waiting for I/O devices to finish transferring data! Figure 8.5a illustrates this situation. The processor spends a certain amount of\n\n\n\nRead one record from file | 15\n      \n       \\mu\n      \n      s\nExecute 100 instructions | 1\n      \n       \\mu\n      \n      s\nWrite one record to file | 15\n      \n       \\mu\n      \n      s\nTOTAL | 31\n       \n        \\mu\n       \n       s\nPercent CPU utilization =\n      \n       \\frac{1}{31} = 0.032 = 3.2\\%\n\n\nFigure 8.4 System Utilization Example\n\n\n\n\n![Figure 8.5: Multiprogramming Example. The figure consists of three horizontal timelines labeled (a), (b), and (c). Each timeline shows the execution of one or more programs over time. A horizontal arrow at the bottom of each timeline is labeled 'Time'.](images/image_0136.jpeg)\n\n\n(a) Uniprogramming\n\n\nProgram A: Run, Wait, Run, Wait\n\n\n(b) Multiprogramming with two programs\n\n\nProgram A: Run, Wait, Run, Wait\n\n\nProgram B: Wait, Run, Wait, Run, Wait\n\n\nCombined: Run A, Run B, Wait, Run A, Run B, Wait\n\n\n(c) Multiprogramming with three programs\n\n\nProgram A: Run, Wait, Run, Wait\n\n\nProgram B: Wait, Run, Wait, Run, Wait\n\n\nProgram C: Wait, Run, Wait, Run, Wait\n\n\nCombined: Run A, Run B, Run C, Wait, Run A, Run B, Run C, Wait\n\n\nFigure 8.5: Multiprogramming Example. The figure consists of three horizontal timelines labeled (a), (b), and (c). Each timeline shows the execution of one or more programs over time. A horizontal arrow at the bottom of each timeline is labeled 'Time'.\n\n\n**Figure 8.5**\n   Multiprogramming Example\n\n\ntime executing, until it reaches an I/O instruction. It must then wait until that I/O instruction concludes before proceeding.\n\n\nThis inefficiency is not necessary. We know that there must be enough memory to hold the OS (resident monitor) and one user program. Suppose that there is room for the OS and two user programs. Now, when one job needs to wait for I/O, the processor can switch to the other job, which likely is not waiting for I/O (Figure 8.5b). Furthermore, we might expand memory to hold three, four, or more programs and switch among all of them (Figure 8.5c). This technique is known as\n   **multiprogramming**\n   , or\n   **multitasking**\n   .\n   \n    1\n   \n   It is the central theme of modern operating systems.\n\n\n1\n   \n   The term\n   *multitasking*\n   is sometimes reserved to mean multiple tasks within the same program that may be handled concurrently by the OS, in contrast to\n   *multiprogramming*\n   , which would refer to multiple processes from multiple programs. However, it is more common to equate the terms\n   *multitasking*\n   and\n   *multiprogramming*\n   , as is done in most standards dictionaries (e.g., IEEE Std 100-1992,\n   *The New IEEE Standard Dictionary of Electrical and Electronics Terms*\n   ).\n\n\n**EXAMPLE 8.1**\n   This example illustrates the benefit of multiprogramming. Consider a computer with 250 Mbytes of available memory (not used by the OS), a disk, a terminal, and a printer. Three programs, JOB1, JOB2, and JOB3, are submitted for execution at the same time, with the attributes listed in Table 8.1. We assume minimal processor requirements for JOB1 and JOB2 and continuous disk and printer use by JOB3. For a simple batch environment, these jobs will be executed in sequence. Thus, JOB1 completes in 5 minutes. JOB2 must wait until the 5 minutes is over and then completes 15 minutes after that. JOB3 begins after 20 minutes and completes at 30 minutes from the time it was initially submitted. The average resource utilization, throughput, and response times are shown in the uniprogramming column of Table 8.2. Device-by-device utilization is illustrated in Figure 8.6a. It is evident that there is gross underutilization for all resources when averaged over the required 30-minute time period.\n\n\nNow suppose that the jobs are run concurrently under a multiprogramming OS. Because there is little resource contention between the jobs, all three can run in nearly minimum time while coexisting with the others in the computer (assuming that JOB2 and JOB3 are allotted enough processor time to keep their input and output operations active). JOB1 will still require 5 minutes to complete but at the end of that time, JOB2 will be one-third finished, and JOB3 will be half finished. All three jobs will have finished within 15 minutes. The improvement is evident when examining the multiprogramming column of Table 8.2, obtained from the histogram shown in Figure 8.6b.\n\n\nAs with a simple batch system, a multiprogramming batch system must rely on certain computer hardware features. The most notable additional feature that is useful for multiprogramming is the hardware that supports I/O interrupts\n\n\n**Table 8.1**\n   Sample Program Execution Attributes\n\n\n\n | JOB1 | JOB2 | JOB3\nType of job | Heavy compute | Heavy I/O | Heavy I/O\nDuration (min) | 5 | 15 | 10\nMemory required (M) | 50 | 100 | 80\nNeed disk? | No | No | Yes\nNeed terminal? | No | Yes | No\nNeed printer? | No | No | Yes\n\n\n**Table 8.2**\n   Effects of Multiprogramming on Resource Utilization\n\n\n\n | Uniprogramming | Multiprogramming\nProcessor use (%) | 20 | 40\nMemory use (%) | 33 | 67\nDisk use (%) | 33 | 67\nPrinter use (%) | 33 | 67\nElapsed time (min) | 30 | 15\nThroughput rate (jobs/hr) | 6 | 12\nMean response time (min) | 18 | 10\n\n\n\n\n![Figure 8.6: Utilization Histograms comparing Uniprogramming and Multiprogramming. The figure consists of two side-by-side bar charts. The left chart, labeled (a) Uniprogramming, shows a single job (JOB1) running from 0 to 5 minutes, then idle until 10 minutes, then running again from 10 to 25 minutes, and finally idle until 30 minutes. The right chart, labeled (b) Multiprogramming, shows three jobs (JOB1, JOB2, JOB3) running concurrently. JOB1 runs from 0 to 5 minutes, JOB2 from 5 to 15 minutes, and JOB3 from 15 to 25 minutes. Both charts track the utilization of CPU, Memory, Disk, Terminal, and Printer over a 30-minute period. The CPU utilization is 100% during job execution and 0% during idle time. Memory utilization is 100% during job execution and 0% during idle time. Disk, Terminal, and Printer utilization are 100% during job execution and 0% during idle time. The Job history section at the bottom shows the duration of each job: JOB1 (0-5), JOB2 (5-15), and JOB3 (15-25).](images/image_0137.jpeg)\n\n\nFigure 8.6: Utilization Histograms comparing Uniprogramming and Multiprogramming. The figure consists of two side-by-side bar charts. The left chart, labeled (a) Uniprogramming, shows a single job (JOB1) running from 0 to 5 minutes, then idle until 10 minutes, then running again from 10 to 25 minutes, and finally idle until 30 minutes. The right chart, labeled (b) Multiprogramming, shows three jobs (JOB1, JOB2, JOB3) running concurrently. JOB1 runs from 0 to 5 minutes, JOB2 from 5 to 15 minutes, and JOB3 from 15 to 25 minutes. Both charts track the utilization of CPU, Memory, Disk, Terminal, and Printer over a 30-minute period. The CPU utilization is 100% during job execution and 0% during idle time. Memory utilization is 100% during job execution and 0% during idle time. Disk, Terminal, and Printer utilization are 100% during job execution and 0% during idle time. The Job history section at the bottom shows the duration of each job: JOB1 (0-5), JOB2 (5-15), and JOB3 (15-25).\n\n\n**Figure 8.6**\n   Utilization Histograms\n\n\nand DMA. With interrupt-driven I/O or DMA, the processor can issue an I/O command for one job and proceed with the execution of another job while the I/O is carried out by the device controller. When the I/O operation is complete, the processor is interrupted and control is passed to an interrupt-handling program in the OS. The OS will then pass control to another job.\n\n\nMultiprogramming operating systems are fairly sophisticated compared to single-program, or\n   **uniprogramming**\n   , systems. To have several jobs ready to run, the jobs must be kept in main memory, requiring some form of\n   **memory management**\n   . In addition, if several jobs are ready to run, the processor must decide which one to run, which requires some algorithm for scheduling. These concepts are discussed later in this chapter.\n\n\n**TIME-SHARING SYSTEMS**\n   With the use of multiprogramming, batch processing can be quite efficient. However, for many jobs, it is desirable to provide a mode in which the user interacts directly with the computer. Indeed, for some jobs, such as transaction processing, an interactive mode is essential.\n\n\nToday, the requirement for an interactive computing facility can be, and often is, met by the use of a dedicated microcomputer. That option was not available in the 1960s, when most computers were big and costly. Instead, time sharing was developed.\n\n\nJust as multiprogramming allows the processor to handle multiple batch jobs at a time, multiprogramming can be used to handle multiple interactive jobs. In this latter case, the technique is referred to as time sharing, because the processor's time is shared among multiple users. In a\n   **time-sharing system**\n   , multiple users\n\n\n**Table 8.3**\n\n | Batch Multiprogramming | Time Sharing\nPrincipal objective | Maximize processor use | Minimize response time\nSource of directives to operating system | Job control language commands provided with the job | Commands entered at the terminal\n\n\nsimultaneously access the system through terminals, with the OS interleaving the execution of each user program in a short burst or quantum of computation. Thus, if there are\n   \n    n\n   \n   users actively requesting service at one time, each user will only see on the average\n   \n    1/n\n   \n   of the effective computer speed, not counting OS overhead. However, given the relatively slow human reaction time, the response time on a properly designed system should be comparable to that on a dedicated computer.\n\n\nBoth batch multiprogramming and time sharing use multiprogramming. The key differences are listed in Table 8.3."
        },
        {
          "name": "Scheduling",
          "content": "The key to multiprogramming is scheduling. In fact, four types of scheduling are typically involved (Table 8.4). We will explore these presently. But first, we introduce the concept of\n   **process**\n   . This term was first used by the designers of the Multics OS in the 1960s. It is a somewhat more general term than\n   *job*\n   . Many definitions have been given for the term\n   *process*\n   , including\n\n\n  * ■ A program in execution\n  * ■ The “animated spirit” of a program\n  * ■ That entity to which a processor is assigned\n\n\nThis concept should become clearer as we proceed.\n\n\n\n\n**Long-Term Scheduling**\n\n\nThe long-term scheduler determines which programs are admitted to the system for processing. Thus, it controls the degree of multiprogramming (number of processes in memory). Once admitted, a job or user program becomes a process and is added to the queue for the short-term scheduler. In some systems, a newly created process begins in a swapped-out condition, in which case it is added to a queue for the medium-term scheduler.\n\n\n**Table 8.4**\n\nLong-term scheduling | The decision to add to the pool of processes to be executed.\nMedium-term scheduling | The decision to add to the number of processes that are partially or fully in main memory.\nShort-term scheduling | The decision as to which available process will be executed by the processor.\nI/O scheduling | The decision as to which process’s pending I/O request shall be handled by an available I/O device.\n\n\nIn a batch system, or for the batch portion of a general-purpose OS, newly submitted jobs are routed to disk and held in a batch queue. The long-term scheduler creates processes from the queue when it can. There are two decisions involved here. First, the scheduler must decide that the OS can take on one or more additional processes. Second, the scheduler must decide which job or jobs to accept and turn into processes. The criteria used may include priority, expected execution time, and I/O requirements.\n\n\nFor interactive programs in a time-sharing system, a process request is generated when a user attempts to connect to the system. Time-sharing users are not simply queued up and kept waiting until the system can accept them. Rather, the OS will accept all authorized comers until the system is saturated, using some predefined measure of saturation. At that point, a connection request is met with a message indicating that the system is full and the user should try again later.\n\n\n\n\n**Medium-Term Scheduling**\n\n\nMedium-term scheduling is part of the swapping function, described in Section 8.3. Typically, the swapping-in decision is based on the need to manage the degree of multiprogramming. On a system that does not use virtual memory, memory management is also an issue. Thus, the swapping-in decision will consider the memory requirements of the swapped-out processes.\n\n\n\n\n**Short-Term Scheduling**\n\n\nThe long-term scheduler executes relatively infrequently and makes the coarse-grained decision of whether or not to take on a new process, and which one to take. The short-term scheduler, also known as the\n   **dispatcher**\n   , executes frequently and makes the fine-grained decision of which job to execute next.\n\n\n**PROCESS STATES**\n   To understand the operation of the short-term scheduler, we need to consider the concept of a\n   **process state**\n   . During the lifetime of a process, its status will change a number of times. Its status at any point in time is referred to as a\n   *state*\n   . The term\n   *state*\n   is used because it connotes that certain information exists that defines the status at that point. At minimum, there are five defined states for a process (Figure 8.7):\n\n\n  * ■\n    **New:**\n    A program is admitted by the high-level scheduler but is not yet ready to execute. The OS will initialize the process, moving it to the ready state.\n\n\n\n\n![Figure 8.7: Five-State Process Model. A state transition diagram showing five states: New, Ready, Running, Blocked, and Exit. Transitions are: New to Ready (Admit), Ready to Running (Dispatch), Running to Ready (Timeout), Running to Blocked (Event wait), Blocked to Ready (Event occurs), Running to Exit (Release).](images/image_0138.jpeg)\n\n\ngraph LR\n    New((New)) -- Admit --> Ready((Ready))\n    Ready -- Dispatch --> Running((Running))\n    Running -- Timeout --> Ready\n    Running -- \"Event wait\" --> Blocked((Blocked))\n    Blocked -- \"Event occurs\" --> Ready\n    Running -- Release --> Exit((Exit))\n  \nFigure 8.7: Five-State Process Model. A state transition diagram showing five states: New, Ready, Running, Blocked, and Exit. Transitions are: New to Ready (Admit), Ready to Running (Dispatch), Running to Ready (Timeout), Running to Blocked (Event wait), Blocked to Ready (Event occurs), Running to Exit (Release).\n\n\n**Figure 8.7**\n   Five-State Process Model\n\n\n  * ■\n    **Ready:**\n    The process is ready to execute and is awaiting access to the processor.\n  * ■\n    **Running:**\n    The process is being executed by the processor.\n  * ■\n    **Waiting:**\n    The process is suspended from execution waiting for some system resource, such as I/O.\n  * ■\n    **Halted:**\n    The process has terminated and will be destroyed by the OS.\n\n\nFor each process in the system, the OS must maintain information indicating the state of the process and other information necessary for process execution. For this purpose, each process is represented in the OS by a\n   **process control block**\n   (Figure 8.8), which typically contains:\n\n\n  * ■\n    **Identifier:**\n    Each current process has a unique identifier.\n  * ■\n    **State:**\n    The current state of the process (new, ready, and so on).\n  * ■\n    **Priority:**\n    Relative priority level.\n  * ■\n    **Program counter:**\n    The address of the next instruction in the program to be executed.\n  * ■\n    **Memory pointers:**\n    The starting and ending locations of the process in memory.\n  * ■\n    **Context data:**\n    These are data that are present in registers in the processor while the process is executing, and they will be discussed in Part Three. For now, it is enough to say that these data represent the “context” of the process. The context data plus the program counter are saved when the process leaves the running state. They are retrieved by the processor when it resumes execution of the process.\n\n\n\n\n![Diagram of a Process Control Block (PCB) structure, showing a vertical stack of fields: Identifier, State, Priority, Program counter, Memory pointers, Context data, I/O status information, Accounting information, and an ellipsis.](images/image_0139.jpeg)\n\n\nIdentifier\nState\nPriority\nProgram counter\nMemory pointers\nContext data\nI/O status information\nAccounting information\n⋮\n\n\nDiagram of a Process Control Block (PCB) structure, showing a vertical stack of fields: Identifier, State, Priority, Program counter, Memory pointers, Context data, I/O status information, Accounting information, and an ellipsis.\n\n\n**Figure 8.8**\n   Process Control Block\n\n\n  * ■\n    **I/O status information:**\n    Includes outstanding I/O requests, I/O devices (e.g., tape drives) assigned to this process, a list of files assigned to the process, and so on.\n  * ■\n    **Accounting information:**\n    May include the amount of processor time and clock time used, time limits, account numbers, and so on.\n\n\nWhen the scheduler accepts a new job or user request for execution, it creates a blank process control block and places the associated process in the new state. After the system has properly filled in the process control block, the process is transferred to the ready state.\n\n\n**SCHEDULING TECHNIQUES**\n   To understand how the OS manages the scheduling of the various jobs in memory, let us begin by considering the simple example in Figure 8.9. The figure shows how main memory is partitioned at a given point in time. The kernel of the OS is, of course, always resident. In addition, there are a number of active processes, including\n   **A**\n   and\n   **B**\n   , each of which is allocated a portion of memory.\n\n\n\n\n![Figure 8.9: Scheduling Example. Three vertical panels (a), (b), and (c) show memory partitions. Each panel has a top section for the 'Operating system' containing 'Service handler', 'Interrupt handler', and 'Scheduler'. Below this are partitions for processes 'A' and 'B', and 'Other partitions'. Panel (a) shows process A as 'Running' and process B as 'Ready'. Panel (b) shows process A as 'Waiting' and process B as 'Ready'. Panel (c) shows process A as 'Waiting' and process B as 'Running'. In all panels, the 'Scheduler' and 'Service handler' are marked 'In control'.](images/image_0140.jpeg)\n\n\nThe diagram illustrates three states of memory partitioning for processes A and B, along with the operating system components.\n\n\n**Common Structure:**\n\n\n  * **Operating system:**\n     Contains the Service handler, Interrupt handler, and Scheduler.\n  * **Process A:**\n     Labeled \"A\" and contains a dashed circle labeled \"In control\".\n  * **Process B:**\n     Labeled \"B\".\n  * **Other partitions:**\n     A large bottom section.\n\n\n**State (a):**\n\n\n  * Process A is labeled \"Running\".\n  * Process B is labeled \"Ready\".\n  * The Scheduler and Service handler are marked \"In control\".\n\n\n**State (b):**\n\n\n  * Process A is labeled \"Waiting\".\n  * Process B is labeled \"Ready\".\n  * The Scheduler and Service handler are marked \"In control\".\n\n\n**State (c):**\n\n\n  * Process A is labeled \"Waiting\".\n  * Process B is labeled \"Running\".\n  * The Scheduler and Service handler are marked \"In control\".\n\n\nFigure 8.9: Scheduling Example. Three vertical panels (a), (b), and (c) show memory partitions. Each panel has a top section for the 'Operating system' containing 'Service handler', 'Interrupt handler', and 'Scheduler'. Below this are partitions for processes 'A' and 'B', and 'Other partitions'. Panel (a) shows process A as 'Running' and process B as 'Ready'. Panel (b) shows process A as 'Waiting' and process B as 'Ready'. Panel (c) shows process A as 'Waiting' and process B as 'Running'. In all panels, the 'Scheduler' and 'Service handler' are marked 'In control'.\n\n\n**Figure 8.9**\n   Scheduling Example\n\n\nWe begin at a point in time when process\n   **A**\n   is running. The processor is executing instructions from the program contained in\n   **A**\n   's memory partition. At some later point in time, the processor ceases to execute instructions in\n   **A**\n   and begins executing instructions in the OS area. This will happen for one of three reasons:\n\n\n  * 1. Process\n    **A**\n    issues a service call (e.g., an I/O request) to the OS. Execution of\n    **A**\n    is suspended until this call is satisfied by the OS.\n  * 2. Process\n    **A**\n    causes an\n    *interrupt*\n    . An interrupt is a hardware-generated signal to the processor. When this signal is detected, the processor ceases to execute\n    **A**\n    and transfers to the interrupt handler in the OS. A variety of events related to\n    **A**\n    will cause an interrupt. One example is an error, such as attempting to execute a privileged instruction. Another example is a timeout; to prevent any one process from monopolizing the processor, each process is only granted the processor for a short period at a time.\n  * 3. Some event unrelated to process\n    **A**\n    that requires attention causes an interrupt. An example is the completion of an I/O operation.\n\n\nIn any case, the result is the following. The processor saves the current context data and the program counter for\n   **A**\n   in\n   **A**\n   's process control block and then begins executing in the OS. The OS may perform some work, such as initiating an I/O operation. Then the short-term-scheduler portion of the OS decides which process should be executed next. In this example,\n   **B**\n   is chosen. The OS instructs the processor to restore\n   **B**\n   's context data and proceed with the execution of\n   **B**\n   where it left off.\n\n\nThis simple example highlights the basic functioning of the short-term scheduler. Figure 8.10 shows the major elements of the OS involved in the multiprogramming and scheduling of processes. The OS receives control of the processor at the\n\n\n\n\n![Diagram of the Operating System structure for multiprogramming. The OS is represented as a large box containing several components. On the left, 'Service call from process' and 'Interrupt from process' and 'Interrupt from I/O' arrows point into the 'Service call handler (code)' and 'Interrupt handler (code)' respectively. To the right of these handlers are three vertical stacks of boxes representing 'Long-term queue', 'Short-term queue', and 'I/O queues'. Below these queues is a 'Short-term scheduler (code)' box. An arrow points from the scheduler down to the text 'Pass control to process'.](images/image_0141.jpeg)\n\n\nThe diagram illustrates the internal structure of the Operating System (OS) for multiprogramming. The OS is depicted as a large rectangular container. Inside this container, there are two main entry points: a 'Service call handler (code)' and an 'Interrupt handler (code)'. External inputs, such as 'Service call from process', 'Interrupt from process', and 'Interrupt from I/O', feed into these handlers. To the right of the handlers, there are three vertical columns of small rectangular boxes representing process queues: the 'Long-term queue', the 'Short-term queue', and 'I/O queues'. Below these queues is a 'Short-term scheduler (code)' block. A downward-pointing arrow from the scheduler leads to the final step: 'Pass control to process'.\n\n\nDiagram of the Operating System structure for multiprogramming. The OS is represented as a large box containing several components. On the left, 'Service call from process' and 'Interrupt from process' and 'Interrupt from I/O' arrows point into the 'Service call handler (code)' and 'Interrupt handler (code)' respectively. To the right of these handlers are three vertical stacks of boxes representing 'Long-term queue', 'Short-term queue', and 'I/O queues'. Below these queues is a 'Short-term scheduler (code)' box. An arrow points from the scheduler down to the text 'Pass control to process'.\n\n\n**Figure 8.10**\n   Key Elements of an Operating System for Multiprogramming\n\n\ninterrupt handler if an interrupt occurs and at the service-call handler if a service call occurs. Once the interrupt or service call is handled, the short-term scheduler is invoked to select a process for execution.\n\n\nTo do its job, the OS maintains a number of queues. Each queue is simply a waiting list of processes waiting for some resource. The\n   **long-term queue**\n   is a list of jobs waiting to use the system. As conditions permit, the high-level scheduler will allocate memory and create a process for one of the waiting items. The\n   **short-term queue**\n   consists of all processes in the ready state. Any one of these processes could use the processor next. It is up to the short-term scheduler to pick one. Generally, this is done with a round-robin algorithm, giving each process some time in turn. Priority levels may also be used. Finally, there is an\n   **I/O queue**\n   for each I/O device. More than one process may request the use of the same I/O device. All processes waiting to use each device are lined up in that device's queue.\n\n\nFigure 8.11 suggests how processes progress through the computer under the control of the OS. Each process request (batch job, user-defined interactive job) is placed in the long-term queue. As resources become available, a process request becomes a process and is then placed in the ready state and put in the short-term queue. The processor alternates between executing OS instructions and executing user processes. While the OS is in control, it decides which process in the short-term queue should be executed next. When the OS has finished its immediate tasks, it turns the processor over to the chosen process.\n\n\nAs was mentioned earlier, a process being executed may be suspended for a variety of reasons. If it is suspended because the process requests I/O, then it\n\n\n\n\n![Queuing Diagram Representation of Processor Scheduling](images/image_0142.jpeg)\n\n\nThe diagram illustrates the flow of processes through various queues and the processor. It starts with an 'Admit' arrow pointing into a 'Long-term queue' represented by a horizontal bar with six segments. An arrow from the Long-term queue points to a 'Short-term queue', also represented by a horizontal bar with six segments. An arrow from the Short-term queue points into a 'Processor' block, which is depicted as a 3D rectangular prism. An arrow labeled 'End' points out from the Processor block. From the Short-term queue, a vertical line branches out to the right, leading to a series of 'I/O queues'. These are labeled 'I/O 1 occurs', 'I/O 2 occurs', and 'I/O n occurs', with an ellipsis between them. Each I/O queue is represented by a horizontal bar with six segments. Arrows point from these I/O queues back to the Short-term queue, indicating that processes are moved back to the ready state after I/O operations. A long feedback arrow at the bottom of the diagram points from the I/O queues back to the Short-term queue, representing the overall cycle of process execution and I/O waiting.\n\n\nQueuing Diagram Representation of Processor Scheduling\n\n\n**Figure 8.11**\n   Queuing Diagram Representation of Processor Scheduling\n\n\nis placed in the appropriate I/O queue. If it is suspended because of a timeout or because the OS must attend to pressing business, then it is placed in the ready state and put into the short-term queue.\n\n\nFinally, we mention that the OS also manages the I/O queues. When an I/O operation is completed, the OS removes the satisfied process from that I/O queue and places it in the short-term queue. It then selects another waiting process (if any) and signals for the I/O device to satisfy that process's request."
        },
        {
          "name": "Memory Management",
          "content": "In a uniprogramming system, main memory is divided into two parts: one part for the OS (resident monitor) and one part for the program currently being executed. In a multiprogramming system, the “user” part of memory is subdivided to accommodate multiple processes. The task of subdivision is carried out dynamically by the OS and is known as\n   **memory management**\n   .\n\n\nEffective memory management is vital in a multiprogramming system. If only a few processes are in memory, then for much of the time all of the processes will be waiting for I/O and the processor will be idle. Thus, memory needs to be allocated efficiently to pack as many processes into memory as possible.\n\n\n\n\n**Swapping**\n\n\nReferring back to Figure 8.11, we have discussed three types of queues: the long-term queue of requests for new processes, the short-term queue of processes ready to use the processor, and the various I/O queues of processes that are not ready to use the processor. Recall that the reason for this elaborate machinery is that I/O activities are much slower than computation and therefore the processor in a uniprogramming system is idle most of the time.\n\n\nBut the arrangement in Figure 8.11 does not entirely solve the problem. It is true that, in this case, memory holds multiple processes and that the processor can move to another process when one process is waiting. But the processor is so much faster than I/O that it will be common for\n   *all*\n   the processes in memory to be waiting on I/O. Thus, even with multiprogramming, a processor could be idle most of the time.\n\n\nWhat to do? Main memory could be expanded, and so be able to accommodate more processes. But there are two flaws in this approach. First, main memory is expensive, even today. Second, the appetite of programs for memory has grown as fast as the cost of memory has dropped. So larger memory results in larger processes, not more processes.\n\n\nAnother solution is\n   **swapping**\n   , depicted in Figure 8.12. We have a long-term queue of process requests, typically stored on disk. These are brought in, one at a time, as space becomes available. As processes are completed, they are moved out of main memory. Now the situation will arise that none of the processes in memory are in the ready state (e.g., all are waiting on an I/O operation). Rather than remain idle, the processor\n   *swaps*\n   one of these processes back out to disk into an\n   *intermediate queue*\n   . This is a queue of existing processes that have been temporarily\n\n\n\n\n![Figure 8.12: The Use of Swapping. (a) Simple job scheduling: Disk storage contains a Long-term queue. An arrow points from the Long-term queue to Main memory, which contains an Operating system. An arrow points from Main memory to Completed jobs and user sessions. (b) Swapping: Disk storage contains an Intermediate queue and a Long-term queue. An arrow points from the Intermediate queue to Main memory. An arrow points from Main memory to the Intermediate queue. An arrow points from the Long-term queue to Main memory. An arrow points from Main memory to Completed jobs and user sessions.](images/image_0143.jpeg)\n\n\n(a) Simple job scheduling\n\n\n(b) Swapping\n\n\nFigure 8.12: The Use of Swapping. (a) Simple job scheduling: Disk storage contains a Long-term queue. An arrow points from the Long-term queue to Main memory, which contains an Operating system. An arrow points from Main memory to Completed jobs and user sessions. (b) Swapping: Disk storage contains an Intermediate queue and a Long-term queue. An arrow points from the Intermediate queue to Main memory. An arrow points from Main memory to the Intermediate queue. An arrow points from the Long-term queue to Main memory. An arrow points from Main memory to Completed jobs and user sessions.\n\n\n**Figure 8.12**\n   The Use of Swapping\n\n\nkicked out of memory. The OS then brings in another process from the intermediate queue, or it honors a new process request from the long-term queue. Execution then continues with the newly arrived process.\n\n\nSwapping, however, is an I/O operation, and therefore there is the potential for making the problem worse, not better. But because disk I/O is generally the fastest I/O on a system (e.g., compared with tape or printer I/O), swapping will usually enhance performance. A more sophisticated scheme, involving virtual memory, improves performance over simple swapping. This will be discussed shortly. But first, we must prepare the ground by explaining partitioning and paging.\n\n\n\n\n**Partitioning**\n\n\nThe simplest scheme for partitioning available memory is to use\n   *fixed-size partitions*\n   , as shown in Figure 8.13. Note that, although the partitions are of fixed size, they need not be of equal size. When a process is brought into memory, it is placed in the smallest available partition that will hold it.\n\n\nEven with the use of unequal fixed-size partitions, there will be wasted memory. In most cases, a process will not require exactly as much memory as provided\n\n\n\n\n![Figure 8.13: Example of Fixed Partitioning of a 64-Mbyte Memory. (a) Equal-size partitions: 8M OS, 8M, 8M, 8M, 8M, 8M, 8M. (b) Unequal-size partitions: 8M OS, 2M, 4M, 6M, 8M, 8M, 12M, 16M.](images/image_0144.jpeg)\n\n\nFigure 8.13 illustrates two methods of fixed partitioning for a 64-Mbyte memory. (a) Equal-size partitions: The memory is divided into 8 equal 8M partitions, with the top partition reserved for the operating system (OS). (b) Unequal-size partitions: The memory is divided into partitions of varying sizes: 8M for the OS, followed by 2M, 4M, 6M, 8M, 8M, 12M, and 16M partitions.\n\n\nFigure 8.13: Example of Fixed Partitioning of a 64-Mbyte Memory. (a) Equal-size partitions: 8M OS, 8M, 8M, 8M, 8M, 8M, 8M. (b) Unequal-size partitions: 8M OS, 2M, 4M, 6M, 8M, 8M, 12M, 16M.\n\n\n**Figure 8.13**\n   Example of Fixed Partitioning of a 64-Mbyte Memory\n\n\nby the partition. For example, a process that requires 3M bytes of memory would be placed in the 4M partition of Figure 8.13b, wasting 1M that could be used by another process.\n\n\nA more efficient approach is to use\n   *variable-size partitions*\n   . When a process is brought into memory, it is allocated exactly as much memory as it requires and no more.\n\n\n**EXAMPLE 8.2**\n   An example, using 64 Mbytes of main memory, is shown in Figure 8.14. Initially, main memory is empty, except for the OS (a). The first three processes are loaded in, starting where the OS ends and occupying just enough space for each process (b, c, d). This leaves a “hole” at the end of memory that is too small for a fourth process. At some point, none of the processes in memory is ready. The OS swaps out process 2 (e), which leaves sufficient room to load a new process, process 4 (f). Because process 4 is smaller than process 2, another small hole is created. Later, a point is reached at which none of the processes in main memory is ready, but process 2, in the ready-suspend state, is available. Because there is insufficient room in memory for process 2, the OS swaps process 1 out (g) and swaps process 2 back in (h).\n\n\nAs this example shows, this method starts out well, but eventually it leads to a situation in which there are a lot of small holes in memory. As time goes on, memory becomes more and more fragmented, and memory utilization declines. One technique for overcoming this problem is\n   **compaction**\n   : From time to time, the OS shifts the processes in memory to place all the free memory together in one block. This is a time-consuming procedure, wasteful of processor time.\n\n\nBefore we consider ways of dealing with the shortcomings of partitioning, we must clear up one loose end. Consider Figure 8.14; it should be obvious that a process is not likely to be loaded into the same place in main memory each time it is swapped in. Furthermore, if compaction is used, a process may be shifted while in main memory. A process in memory consists of instructions plus data. The instructions will contain addresses for memory locations of two types:\n\n\n  * ■ Addresses of data items\n  * ■ Addresses of instructions, used for branching instructions\n\n\n\n\n![Figure 8.14: The Effect of Dynamic Partitioning. The diagram shows eight memory configurations (a-h) illustrating the fragmentation and compaction of processes in main memory.](images/image_0145.jpeg)\n\n\nFigure 8.14 illustrates the effect of dynamic partitioning through eight memory configurations (a-h). Each configuration shows a vertical memory bar divided into segments for the Operating System (OS) and various processes. The OS segment is consistently at the top of each bar.\n\n\n  * **(a)**\n     Initial state: OS (8M) at the top, followed by a large free block (56M).\n  * **(b)**\n     Process 1 (20M) is loaded into the first partition.\n  * **(c)**\n     Process 2 (14M) is loaded into the next partition, leaving a 22M hole.\n  * **(d)**\n     Process 3 (18M) is loaded, leaving a 4M hole at the bottom.\n  * **(e)**\n     Compaction: All processes are moved to the bottom, and the OS is moved to the top, consolidating the 4M hole at the bottom.\n  * **(f)**\n     Process 4 (8M) is loaded into the first partition.\n  * **(g)**\n     Process 4 (8M) is swapped out, leaving a 6M hole.\n  * **(h)**\n     Compaction: All processes are moved to the bottom, and the OS is moved to the top, consolidating the 6M hole at the bottom.\n\n\nFigure 8.14: The Effect of Dynamic Partitioning. The diagram shows eight memory configurations (a-h) illustrating the fragmentation and compaction of processes in main memory.\n\n\n**Figure 8.14**\n   The Effect of Dynamic Partitioning\n\n\nBut these addresses are not fixed. They will change each time a process is swapped in. To solve this problem, a distinction is made between logical addresses and physical addresses. A\n   **logical address**\n   is expressed as a location relative to the beginning of the program. Instructions in the program contain only logical addresses. A\n   **physical address**\n   is an actual location in main memory. When the processor executes a process, it automatically converts from logical to physical address by adding the current starting location of the process, called its\n   **base address**\n   , to each logical address. This is another example of a processor hardware feature designed to meet an OS requirement. The exact nature of this hardware feature depends on the memory management strategy in use. We will see several examples later in this chapter.\n\n\n\n\n**Paging**\n\n\nBoth unequal fixed-size and variable-size partitions are inefficient in the use of memory. Suppose, however, that memory is partitioned into equal fixed-size chunks that are relatively small, and that each process is also divided into small fixed-size chunks of some size. Then the chunks of a program, known as\n   **pages**\n   , could be assigned to available chunks of memory, known as\n   **frames**\n   , or page frames. At most, then, the wasted space in memory for that process is a fraction of the last page.\n\n\nFigure 8.15 shows an example of the use of pages and frames. At a given point in time, some of the frames in memory are in use and some are free. The list of free frames is maintained by the OS. Process A, stored on disk, consists of four pages.\n\n\n\n\n![Figure 8.15: Allocation of Free Frames. (a) Before: Process A (4 pages) is on disk. Main memory has 10 frames: 13, 14, 15, 16, 17, 18, 19, 20. Frames 13-15 are free, 16-20 are in use. Free frame list: 13, 14, 15, 18, 20. (b) After: Process A is loaded. Frame 13 contains Page 1 of A, frame 14 contains Page 2 of A, frame 15 contains Page 3 of A. Frame 16 is still in use. Free frame list: 20. Process A page table maps logical pages to physical frames: Page 0 to frame 18, Page 1 to frame 13, Page 2 to frame 14, Page 3 to frame 15.](images/image_0146.jpeg)\n\n\n**(a) Before**\n\n\n**Process A**\n    (disk): Page 0, Page 1, Page 2, Page 3\n\n\n**Main memory**\n    (frames 13-20):\n\n\n  * 13: Free\n  * 14: Free\n  * 15: Free\n  * 16: In use\n  * 17: In use\n  * 18: In use\n  * 19: In use\n  * 20: In use\n\n\n**Free frame list:**\n    13, 14, 15, 18, 20\n\n\n**(b) After**\n\n\n**Process A**\n    (disk): Page 0, Page 1, Page 2, Page 3\n\n\n**Main memory**\n    (frames 13-20):\n\n\n  * 13: Page 1 of A\n  * 14: Page 2 of A\n  * 15: Page 3 of A\n  * 16: In use\n  * 17: In use\n  * 18: Page 0 of A\n  * 19: In use\n  * 20: In use\n\n\n**Free frame list:**\n    20\n\n\n**Process A page table:**\n\n\n\nPage 0 | 18\nPage 1 | 13\nPage 2 | 14\nPage 3 | 15\n\n\nFigure 8.15: Allocation of Free Frames. (a) Before: Process A (4 pages) is on disk. Main memory has 10 frames: 13, 14, 15, 16, 17, 18, 19, 20. Frames 13-15 are free, 16-20 are in use. Free frame list: 13, 14, 15, 18, 20. (b) After: Process A is loaded. Frame 13 contains Page 1 of A, frame 14 contains Page 2 of A, frame 15 contains Page 3 of A. Frame 16 is still in use. Free frame list: 20. Process A page table maps logical pages to physical frames: Page 0 to frame 18, Page 1 to frame 13, Page 2 to frame 14, Page 3 to frame 15.\n\n\n**Figure 8.15**\n   Allocation of Free Frames\n\n\nWhen it comes time to load this process, the OS finds four free frames and loads the four pages of the process\n   **A**\n   into the four frames.\n\n\nNow suppose, as in this example, that there are not sufficient unused contiguous frames to hold the process. Does this prevent the OS from loading\n   **A**\n   ? The answer is no, because we can once again use the concept of logical address. A simple base address will no longer suffice. Rather, the OS maintains a\n   **page table**\n   for each process. The page table shows the frame location for each page of the process. Within the program, each logical address consists of a page number and a relative address within the page. Recall that in the case of simple partitioning, a logical address is the location of a word relative to the beginning of the program; the processor translates that into a physical address. With paging, the logical-to-physical address translation is still done by processor hardware. The processor must know how to access the page table of the current process. Presented with a logical address (page number, relative address), the processor uses the page table to produce a physical address (frame number, relative address). An example is shown in Figure 8.16.\n\n\nThis approach solves the problems raised earlier. Main memory is divided into many small equal-size frames. Each process is divided into frame-size pages: smaller processes require fewer pages, larger processes require more. When a process is brought in, its pages are loaded into available frames, and a page table is set up.\n\n\n\n\n![Diagram illustrating Logical and Physical Addresses. A Logical address (1, 30) is translated via a Process A page table to a Physical address (13, 30), which maps to Page 1 of A in Main memory.](images/image_0147.jpeg)\n\n\nThe diagram illustrates the translation of logical addresses to physical addresses using a page table. It shows three main components: a Logical address, a Process A page table, and Main memory.\n\n\n**Logical address:**\n    A box containing the values 1 and 30. Arrows point to these values with labels: \"Page number\" and \"Relative address within page\".\n\n\n**Process A page table:**\n    A vertical stack of four boxes containing the values 18, 13, 14, and 15. An arrow from the Logical address points to the box containing 13. An arrow from the box containing 13 points to a Physical address box.\n\n\n**Physical address:**\n    A box containing the values 13 and 30. Arrows point to these values with labels: \"Frame number\" and \"Relative address within frame\".\n\n\n**Main memory:**\n    A vertical stack of boxes representing frames. The top four frames are labeled \"Page 1 of A\", \"Page 2 of A\", \"Page 3 of A\", and \"Page 0 of A\". To the right of these labels are the numbers 13, 14, 15, and 16 respectively. An arrow from the Physical address box points to the frame labeled \"Page 1 of A\".\n\n\nDiagram illustrating Logical and Physical Addresses. A Logical address (1, 30) is translated via a Process A page table to a Physical address (13, 30), which maps to Page 1 of A in Main memory.\n\n\n**Figure 8.16**\n   Logical and Physical Addresses\n\n\n\n\n**Virtual Memory**\n\n\n**DEMAND PAGING**\n   With the use of paging, truly effective multiprogramming systems came into being. Furthermore, the simple tactic of breaking a process up into pages led to the development of another important concept: virtual memory.\n\n\nTo understand virtual memory, we must add a refinement to the paging scheme just discussed. That refinement is\n   **demand paging**\n   , which simply means that each page of a process is brought in only when it is needed, that is, on demand.\n\n\nConsider a large process, consisting of a long program plus a number of arrays of data. Over any short period of time, execution may be confined to a small section of the program (e.g., a subroutine), and perhaps only one or two arrays of data are being used. This is the principle of locality, which we introduced in Appendix 4A. It would clearly be wasteful to load in dozens of pages for that process when only a few pages will be used before the program is suspended. We can make better use of memory by loading in just a few pages. Then, if the program branches to an instruction on a page not in main memory, or if the program references data on a page not in memory, a\n   **page fault**\n   is triggered. This tells the OS to bring in the desired page.\n\n\nThus, at any one time, only a few pages of any given process are in memory, and therefore more processes can be maintained in memory. Furthermore, time is saved because unused pages are not swapped in and out of memory. However, the OS must be clever about how it manages this scheme. When it brings one page in, it must throw another page out; this is known as\n   **page replacement**\n   . If it throws out a page just before it is about to be used, then it will just have to go get that page again almost immediately. Too much of this leads to a condition known as\n   **thrashing**\n   : the processor spends most of its time swapping pages rather than executing instructions. The avoidance of thrashing was a major research area in the 1970s and led to a variety of complex but effective algorithms. In essence, the OS tries to guess, based on recent history, which pages are least likely to be used in the near future.\n\n\n\n\n![Online Interactive Simulator logo featuring a globe and the text 'Online Interactive Simulator' and 'www'.](images/image_0148.jpeg)\n\n\nOnline Interactive Simulator logo featuring a globe and the text 'Online Interactive Simulator' and 'www'.\n\n\n\n\n**Page Replacement Algorithm Simulators**\n\n\nA discussion of page replacement algorithms is beyond the scope of this chapter. A potentially effective technique is least recently used (LRU), the same algorithm discussed in Chapter 4 for cache replacement. In practice, LRU is difficult to implement for a virtual memory paging scheme. Several alternative approaches that seek to approximate the performance of LRU are in use; see Appendix K for details.\n\n\nWith demand paging, it is not necessary to load an entire process into main memory. This fact has a remarkable consequence:\n   *It is possible for a process to be larger than all of main memory*\n   . One of the most fundamental restrictions in programming has been lifted. Without demand paging, a programmer must be acutely aware of how much memory is available. If the program being written is too large, the programmer must devise ways to structure the program into pieces that can be\n\n\nloaded one at a time. With demand paging, that job is left to the OS and the hardware. As far as the programmer is concerned, he or she is dealing with a huge memory, the size associated with disk storage.\n\n\nBecause a process executes only in main memory, that memory is referred to as\n   **real memory**\n   . But a programmer or user perceives a much larger memory—that which is allocated on the disk. This latter is therefore referred to as\n   **virtual memory**\n   . Virtual memory allows for very effective multiprogramming and relieves the user of the unnecessarily tight constraints of main memory.\n\n\n**PAGE TABLE STRUCTURE**\n   The basic mechanism for reading a word from memory involves the translation of a virtual, or logical, address, consisting of page number and offset, into a physical address, consisting of frame number and offset, using a page table. Because the page table is of variable length, depending on the size of the process, we cannot expect to hold it in registers. Instead, it must be in main memory to be accessed. Figure 8.16 suggests a hardware implementation of this scheme. When a particular process is running, a register holds the starting address of the page table for that process. The page number of a virtual address is used to index that table and look up the corresponding frame number. This is combined with the offset portion of the virtual address to produce the desired real address.\n\n\nIn most systems, there is one page table per process. But each process can occupy huge amounts of virtual memory. For example, in the VAX architecture, each process can have up to\n   \n    2^{31} = 2\n   \n   Gbytes of virtual memory. Using\n   \n    2^9 = 512\n   \n   – byte pages, that means that as many as\n   \n    2^{22}\n   \n   page table entries are required\n   *per process*\n   . Clearly, the amount of memory devoted to page tables alone could be unacceptably high. To overcome this problem, most virtual memory schemes store page tables in virtual memory rather than real memory. This means that page tables are subject to paging just as other pages are. When a process is running, at least a part of its page table must be in main memory, including the page table entry of the currently executing page. Some processors make use of a two-level scheme to organize large page tables. In this scheme, there is a page directory, in which each entry points to a page table. Thus, if the length of the page directory is\n   \n    X\n   \n   , and if the maximum length of a page table is\n   \n    Y\n   \n   , then a process can consist of up to\n   \n    X \\times Y\n   \n   pages. Typically, the maximum length of a page table is restricted to be equal to one page. We will see an example of this two-level approach when we consider the Intel x86 later in this chapter.\n\n\nAn alternative approach to the use of one- or two-level page tables is the use of an inverted page table structure (Figure 8.17). Variations on this approach are used on the PowerPC, UltraSPARC, and the IA-64 architecture. An implementation of the Mach OS on the RT-PC also uses this technique.\n\n\nIn this approach, the page number portion of a virtual address is mapped into a hash value using a simple hashing function.\n   \n    2\n   \n   The hash value is a pointer to the inverted page table, which contains the page table entries. There is one entry in the\n\n\n2\n   \n   A hash function maps numbers in the range 0 through\n   \n    M\n   \n   into numbers in the range 0 through\n   \n    N\n   \n   , where\n   \n    M > N\n   \n   . The output of the hash function is used as an index into the hash table. Since more than one input maps into the same output, it is possible for an input item to map to a hash table entry that is already occupied. In that case, the new item must\n   *overflow*\n   into another hash table location. Typically, the new item is placed in the first succeeding empty space, and a pointer from the original location is provided to chain the entries together. See Appendix L for more information on hash functions.\n\n\n\n\n![Diagram of the Inverted Page Table Structure. A virtual address of n bits is split into Page # and Offset. The Page # is passed to a Hash function, which outputs m bits. This m-bit index is used to access an Inverted page table, which has 2^m entries. Each entry contains a Page #, a Process ID, and a Chain pointer. The entry at index i is highlighted. The Chain pointer from entry i points to entry j. The Frame # from entry j is combined with the original Offset to form the Real address (m bits).](images/image_0149.jpeg)\n\n\nThe diagram illustrates the Inverted Page Table Structure. A Virtual address of\n    \n     n\n    \n    bits is split into Page # and Offset. The Page # is passed to a Hash function, which outputs\n    \n     m\n    \n    bits. This\n    \n     m\n    \n    -bit index is used to access an Inverted page table, which has\n    \n     2^m\n    \n    entries. Each entry in the table contains a Page #, a Process ID, and a Chain pointer. The entry at index\n    \n     i\n    \n    is highlighted. The Chain pointer from entry\n    \n     i\n    \n    points to entry\n    \n     j\n    \n    . The Frame # from entry\n    \n     j\n    \n    is combined with the original Offset to form the Real address (\n    \n     m\n    \n    bits).\n\n\nDiagram of the Inverted Page Table Structure. A virtual address of n bits is split into Page # and Offset. The Page # is passed to a Hash function, which outputs m bits. This m-bit index is used to access an Inverted page table, which has 2^m entries. Each entry contains a Page #, a Process ID, and a Chain pointer. The entry at index i is highlighted. The Chain pointer from entry i points to entry j. The Frame # from entry j is combined with the original Offset to form the Real address (m bits).\n\n\n**Figure 8.17**\n   Inverted Page Table Structure\n\n\ninverted page table for each real memory page frame rather than one per virtual page. Thus a fixed proportion of real memory is required for the tables regardless of the number of processes or virtual pages supported. Because more than one virtual address may map into the same hash table entry, a chaining technique is used for managing the overflow. The hashing technique results in chains that are typically short—between one and two entries. The page table’s structure is called\n   *inverted*\n   because it indexes page table entries by frame number rather than by virtual page number.\n\n\n\n\n**Translation Lookaside Buffer**\n\n\nIn principle, then, every virtual memory reference can cause two physical memory accesses: one to fetch the appropriate page table entry, and one to fetch the desired data. Thus, a straightforward virtual memory scheme would have the effect of doubling the memory access time. To overcome this problem, most virtual memory schemes make use of a special cache for page table entries, usually called a\n   **translation lookaside buffer (TLB)**\n   . This cache functions in the same way as a memory cache and contains those page table entries that have been most recently used. Figure 8.18 is a flowchart that shows the use of the TLB. By the principle of locality, most virtual memory references will be to locations in recently used pages. Therefore, most references will involve page table entries in the cache. Studies of the VAX TLB have shown that this scheme can significantly improve performance [CLAR85, SATY81].\n\n\n\n\n![Flowchart illustrating the operation of Paging and Translation Lookaside Buffer (TLB).](images/image_0150.jpeg)\n\n\ngraph TD\n    Start([Start]) --> CPU_TLB[CPU checks the TLB]\n    CPU_TLB --> TLB_Q{Page table entry in TLB?}\n    TLB_Q -- Yes --> CPU_PHA[CPU generates physical address]\n    TLB_Q -- No --> Access_PT[Access page table]\n    Access_PT --> Mem_Q{Page in main memory?}\n    Mem_Q -- Yes --> Update_TLB[Update TLB]\n    Update_TLB --> CPU_PHA\n    Mem_Q -- No --> OS[OS instructs CPU to read the page from disk]\n    OS --> I/O[CPU activates I/O hardware]\n    I/O --> Transfer[Page transferred from disk to main memory]\n    Transfer --> Mem_Full_Q{Memory full?}\n    Mem_Full_Q -- Yes --> Replace[Perform page replacement]\n    Replace --> Mem_Full_Q\n    Mem_Full_Q -- No --> Updated[Page tables updated]\n    Updated --> CPU_PHA\n    CPU_PHA --> Fault[Return to faulted instruction]\n    Fault --> CPU_TLB\n  \nThe flowchart illustrates the operation of Paging and Translation Lookaside Buffer (TLB). It begins with a 'Start' node, leading to 'CPU checks the TLB'. A decision is made: 'Page table entry in TLB?'. If 'Yes', the 'CPU generates physical address'. If 'No', it proceeds to 'Access page table'. Another decision is made: 'Page in main memory?'. If 'Yes', it proceeds to 'Update TLB', then to 'CPU generates physical address'. If 'No', it enters a 'Page fault handling routine' (indicated by a dashed box). This routine includes 'OS instructs CPU to read the page from disk', 'CPU activates I/O hardware', and 'Page transferred from disk to main memory'. A final decision is made: 'Memory full?'. If 'Yes', it enters a 'Perform page replacement' loop. If 'No', it proceeds to 'Page tables updated', then to 'CPU generates physical address'. Finally, it loops back to 'Return to faulted instruction'.\n\n\nFlowchart illustrating the operation of Paging and Translation Lookaside Buffer (TLB).\n\n\n**Figure 8.18**\n   Operation of Paging and Translation Lookaside Buffer (TLB)\n\n\nNote that the virtual memory mechanism must interact with the cache system (not the TLB cache, but the main memory cache). This is illustrated in Figure 8.19. A virtual address will generally be in the form of a page number, offset. First, the memory system consults the TLB to see if the matching page table entry is present. If it is, the real (physical) address is generated by combining the frame number with the offset. If not, the entry is accessed from a page table. Once the real address is generated, which is in the form of a tag and a remainder, the cache is consulted to see if the block containing that word is present (see Figure 4.5). If so, it is returned to the processor. If not, the word is retrieved from main memory.\n\n\nThe reader should be able to appreciate the complexity of the processor hardware involved in a single memory reference. The virtual address is translated into a real address. This involves reference to a page table, which may be in the TLB, in\n\n\n\n\n![Diagram illustrating the Translation Lookaside Buffer (TLB) and Cache Operation. The diagram is divided into two main sections: 'TLB operation' and 'Cache operation'. In the 'TLB operation' section, a 'Virtual address' is split into 'Page #' and 'Offset'. The 'Page #' is sent to a 'TLB' block. If there is a 'TLB hit', the resulting 'Real address' is sent to a 'Page table' block and then to an adder (+). If there is a 'TLB miss', the 'Page #' is sent to the 'Page table' block, and the resulting 'Real address' is sent to the adder. The 'Offset' is also sent to the adder. The output of the adder is the 'Real address', which is split into 'Tag' and 'Remainder'. In the 'Cache operation' section, the 'Tag' is compared with the 'Cache' block. If there is a 'Hit', the 'Value' is returned. If there is a 'Miss', the 'Remainder' is sent to 'Main memory' to fetch the 'Value', which is then stored in the 'Cache' block. The 'Value' is also returned from the 'Main memory' block.](images/image_0151.jpeg)\n\n\nDiagram illustrating the Translation Lookaside Buffer (TLB) and Cache Operation. The diagram is divided into two main sections: 'TLB operation' and 'Cache operation'. In the 'TLB operation' section, a 'Virtual address' is split into 'Page #' and 'Offset'. The 'Page #' is sent to a 'TLB' block. If there is a 'TLB hit', the resulting 'Real address' is sent to a 'Page table' block and then to an adder (+). If there is a 'TLB miss', the 'Page #' is sent to the 'Page table' block, and the resulting 'Real address' is sent to the adder. The 'Offset' is also sent to the adder. The output of the adder is the 'Real address', which is split into 'Tag' and 'Remainder'. In the 'Cache operation' section, the 'Tag' is compared with the 'Cache' block. If there is a 'Hit', the 'Value' is returned. If there is a 'Miss', the 'Remainder' is sent to 'Main memory' to fetch the 'Value', which is then stored in the 'Cache' block. The 'Value' is also returned from the 'Main memory' block.\n\n\n**Figure 8.19**\n   Translation Lookaside Buffer and Cache Operation\n\n\nmain memory, or on disk. The referenced word may be in cache, in main memory, or on disk. In the latter case, the page containing the word must be loaded into main memory and its block loaded into the cache. In addition, the page table entry for that page must be updated.\n\n\n\n\n**Segmentation**\n\n\nThere is another way in which addressable memory can be subdivided, known as\n   *segmentation*\n   . Whereas paging is invisible to the programmer and serves the purpose of providing the programmer with a larger address space, segmentation is usually visible to the programmer and is provided as a convenience for organizing programs and data and as a means for associating privilege and protection attributes with instructions and data.\n\n\nSegmentation allows the programmer to view memory as consisting of multiple address spaces or segments. Segments are of variable, indeed dynamic, size. Typically, the programmer or the OS will assign programs and data to different segments. There may be a number of program segments for various types of programs as well as a number of data segments. Each segment may be assigned access and usage rights. Memory references consist of a (segment number, offset) form of address.\n\n\nThis organization has a number of advantages to the programmer over a non-segmented address space:\n\n\n  * 1. It simplifies the handling of growing data structures. If the programmer does not know ahead of time how large a particular data structure will become, it is not necessary to guess. The data structure can be assigned its own segment, and the OS will expand or shrink the segment as needed.\n  * 2. It allows programs to be altered and recompiled independently without requiring that an entire set of programs be relinked and reloaded. Again, this is accomplished using multiple segments.\n  * 3. It lends itself to sharing among processes. A programmer can place a utility program or a useful table of data in a segment that can be addressed by other processes.\n  * 4. It lends itself to protection. Because a segment can be constructed to contain a well-defined set of programs or data, the programmer or a system administrator can assign access privileges in a convenient fashion.\n\n\nThese advantages are not available with paging, which is invisible to the programmer. On the other hand, we have seen that paging provides for an efficient form of memory management. To combine the advantages of both, some systems are equipped with the hardware and OS software to provide both."
        },
        {
          "name": "Intel x86 Memory Management",
          "content": "Since the introduction of the 32-bit architecture, microprocessors have evolved sophisticated memory management schemes that build on the lessons learned with medium- and large-scale systems. In many cases, the microprocessor versions are superior to their larger-system antecedents. Because the schemes were developed by the microprocessor hardware vendor and may be employed with a variety of operating systems, they tend to be quite general purpose. A representative example is the scheme used on the Intel x86 architecture.\n\n\n\n\n**Address Spaces**\n\n\nThe x86 includes hardware for both segmentation and paging. Both mechanisms can be disabled, allowing the user to choose from four distinct views of memory:\n\n\n  * ■\n    **Unsegmented unpaged memory:**\n    In this case, the virtual address is the same as the physical address. This is useful, for example, in low-complexity, high-performance controller applications.\n  * ■\n    **Unsegmented paged memory:**\n    Here memory is viewed as a paged linear address space. Protection and management of memory is done via paging. This is favored by some operating systems (e.g., Berkeley UNIX).\n  * ■\n    **Segmented unpaged memory:**\n    Here memory is viewed as a collection of logical address spaces. The advantage of this view over a paged approach is that it affords protection down to the level of a single byte, if necessary. Furthermore, unlike paging, it guarantees that the translation table needed (the segment table) is on-chip when the segment is in memory. Hence, segmented unpaged memory results in predictable access times.\n\n\n  * ■\n    **Segmented paged memory:**\n    Segmentation is used to define logical memory partitions subject to access control, and paging is used to manage the allocation of memory within the partitions. Operating systems such as UNIX System V favor this view.\n\n\n\n\n**Segmentation**\n\n\nWhen segmentation is used, each virtual address (called a logical address in the x86 documentation) consists of a 16-bit segment reference and a 32-bit offset. Two bits of the segment reference deal with the protection mechanism, leaving 14 bits for specifying a particular segment. Thus, with unsegmented memory, the user's virtual memory is\n   \n    2^{32} = 4\n   \n   Gbytes. With segmented memory, the total virtual memory space as seen by a user is\n   \n    2^{46} = 64\n   \n   terabytes (Tbytes). The physical address space employs a 32-bit address for a maximum of 4 Gbytes.\n\n\nThe amount of virtual memory can actually be larger than the 64 Tbytes. This is because the processor's interpretation of a virtual address depends on which process is currently active. Virtual address space is divided into two parts. One-half of the virtual address space (\n   \n    8\\text{K segments} \\times 4\\text{ Gbytes}\n   \n   ) is global, shared by all processes; the remainder is local and is distinct for each process.\n\n\nAssociated with each segment are two forms of protection: privilege level and access attribute. There are four privilege levels, from most protected (level 0) to least protected (level 3). The privilege level associated with a data segment is its “classification”; the privilege level associated with a program segment is its “clearance.” An executing program may only access data segments for which its clearance level is lower than (more privileged) or equal to (same privilege) the privilege level of the data segment.\n\n\nThe hardware does not dictate how these privilege levels are to be used; this depends on the OS design and implementation. It was intended that privilege level 1 would be used for most of the OS, and level 0 would be used for that small portion of the OS devoted to memory management, protection, and access control. This leaves two levels for applications. In many systems, applications will reside at level 3, with level 2 being unused. Specialized application subsystems that must be protected because they implement their own security mechanisms are good candidates for level 2. Some examples are database management systems, office automation systems, and software engineering environments.\n\n\nIn addition to regulating access to data segments, the privilege mechanism limits the use of certain instructions. Some instructions, such as those dealing with memory-management registers, can only be executed in level 0. I/O instructions can only be executed up to a certain level that is designated by the OS; typically, this will be level 1.\n\n\nThe access attribute of a data segment specifies whether read/write or read-only accesses are permitted. For program segments, the access attribute specifies read/execute or read-only access.\n\n\nThe address translation mechanism for segmentation involves mapping a virtual address into what is referred to as a linear address (Figure 8.20b). A virtual address consists of the 32-bit offset and a 16-bit segment selector (Figure 8.20a). An instruction fetching or storing an operand specifies the offset and a register containing the segment selector. The segment selector consists of the following fields:\n\n\n  * ■\n    **Table Indicator (TI):**\n    Indicates whether the global segment table or a local segment table should be used for translation.\n\n\n\n15 |  | 3 | 2 | 1 | 0\nIndex | T | RPL\n | I | \n\n\nTI = Table indicator\n\n\nRPL = Requestor privilege level\n\n\n(a) Segment selector\n\n\n\n31 | 22 | 21 | 12 | 11 | 0\nDirectory | Table | Offset\n\n\n(b) Linear address\n\n\n\n31 | 24 | 23 | 22 | 20 | 19 | 16 | 15 | 14 | 13 | 12 | 11 | 8 | 7 | 0\nBase 31...24 | G | D | / | L | A | Segment |  |  |  |  |  |  | \n | B |  |  |  | V | limit | P | DPL | S | Type | Base 23...16\nBase 15...0 | Segment limit 15...0\n\n\nAVL = Available for use by system software\n\n\nBase = Segment base address\n\n\nD/B = Default operation size\n\n\nDPL = Descriptor privilege size\n\n\nG = Granularity\n\n\nL = 64-bit code segment\n   \n\n   (64-bit mode only)\n\n\nP = Segment present\n\n\nType = Segment type\n\n\nS = Descriptor type\n\n\n(c) Segment descriptor (segment table entry)\n\n\n\n31 | 12 | 11 | 9 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nPage frame address 31...12 | AVL | P | S | 0 | A | P | P | U | R | P\n |  |  |  |  |  | C | W | S | W | P\n |  |  |  |  |  | D | T |  |  | \n\n\nAVL = Available for systems programmer use\n\n\nP = Page size\n\n\nA = Accessed\n\n\nPCD = Cache disable\n\n\nPWT = Write through\n\n\nUS = User/supervisor\n\n\nRW = Read-write\n\n\nP = Present\n\n\n■ = Reserved\n\n\n(d) Page directory entry\n\n\n\n31 | 12 | 11 | 9 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nPage frame address 31...12 | AVL |  | D | A | P | P | U | R | P\n |  |  |  |  | C | W | S | W | P\n |  |  |  |  | D | T |  |  | \n\n\nD = Dirty\n\n\n(e) Page table entry\n\n\n**Figure 8.20**\n   Intel x86 Memory Management Formats\n\n\n  * ■\n    **Segment Number:**\n    The number of the segment. This serves as an index into the segment table.\n  * ■\n    **Requested Privilege Level (RPL):**\n    The privilege level requested for this access.\n\n\nEach entry in a segment table consists of 64 bits, as shown in Figure 8.20c. The fields are defined in Table 8.5.\n\n\n**Table 8.5**\n\n\n**Segment Descriptor (Segment Table Entry)**\n\n\n\n\n**Base**\n\n\nDefines the starting address of the segment within the 4-Gbyte linear address space.\n\n\n\n\n**D/B bit**\n\n\nIn a code segment, this is the D bit and indicates whether operands and addressing modes are 16 or 32 bits.\n\n\n\n\n**Descriptor Privilege Level (DPL)**\n\n\nSpecifies the privilege level of the segment referred to by this segment descriptor.\n\n\n\n\n**Granularity bit (G)**\n\n\nIndicates whether the Limit field is to be interpreted in units by one byte or 4 Kbytes.\n\n\n\n\n**Limit**\n\n\nDefines the size of the segment. The processor interprets the limit field in one of two ways, depending on the granularity bit: in units of one byte, up to a segment size limit of 1 Mbyte, or in units of 4 Kbytes, up to a segment size limit of 4 Gbytes.\n\n\n\n\n**S bit**\n\n\nDetermines whether a given segment is a system segment or a code or data segment.\n\n\n\n\n**Segment Present bit (P)**\n\n\nUsed for nonpaged systems. It indicates whether the segment is present in main memory. For paged systems, this bit is always set to 1.\n\n\n\n\n**Type**\n\n\nDistinguishes between various kinds of segments and indicates the access attributes.\n\n\n\n\n**Page Directory Entry and Page Table Entry**\n\n\n\n\n**Accessed bit (A)**\n\n\nThis bit is set to 1 by the processor in both levels of page tables when a read or write operation to the corresponding page occurs.\n\n\n\n\n**Dirty bit (D)**\n\n\nThis bit is set to 1 by the processor when a write operation to the corresponding page occurs.\n\n\n\n\n**Page Frame Address**\n\n\nProvides the physical address of the page in memory if the present bit is set. Since page frames are aligned on 4K boundaries, the bottom 12 bits are 0, and only the top 20 bits are included in the entry. In a page directory, the address is that of a page table.\n\n\n\n\n**Page Cache Disable bit (PCD)**\n\n\nIndicates whether data from page may be cached.\n\n\n\n\n**Page Size bit (PS)**\n\n\nIndicates whether page size is 4 Kbyte or 4 Mbyte.\n\n\n\n\n**Page Write Through bit (PWT)**\n\n\nIndicates whether write-through or write-back caching policy will be used for data in the corresponding page.\n\n\n\n\n**Present bit (P)**\n\n\nIndicates whether the page table or page is in main memory.\n\n\n\n\n**Read/Write bit (RW)**\n\n\nFor user-level pages, indicates whether the page is read-only access or read/write access for user-level programs.\n\n\n\n\n**User/Supervisor bit (US)**\n\n\nIndicates whether the page is available only to the operating system (supervisor level) or is available to both operating system and applications (user level).\n\n\n\n\n**Paging**\n\n\nSegmentation is an optional feature and may be disabled. When segmentation is in use, addresses used in programs are virtual addresses and are converted into linear addresses, as just described. When segmentation is not in use, linear addresses are used in programs. In either case, the following step is to convert that linear address into a real 32-bit address.\n\n\nTo understand the structure of the linear address, you need to know that the x86 paging mechanism is actually a two-level table lookup operation. The first level is a page directory, which contains up to 1024 entries. This splits the 4-Gbyte linear memory space into 1024 page groups, each with its own page table, and each 4 Mbytes in length. Each page table contains up to 1024 entries; each entry corresponds to a single 4-Kbyte page. Memory management has the option of using one page directory for all processes, one page directory for each process, or some combination of the two. The page directory for the current task is always in main memory. Page tables may be in virtual memory.\n\n\nFigure 8.20 shows the formats of entries in page directories and page tables, and the fields are defined in Table 8.5. Note that access control mechanisms can be provided on a page or page group basis.\n\n\nThe x86 also makes use of a translation lookaside buffer. The buffer can hold 32 page table entries. Each time that the page directory is changed, the buffer is cleared.\n\n\nFigure 8.21 illustrates the combination of segmentation and paging mechanisms. For clarity, the translation lookaside buffer and memory cache mechanisms are not shown.\n\n\n\n\n![Diagram illustrating the Intel x86 Memory Address Translation Mechanisms, showing the flow from Logical address to Physical address space through Segmentation and Paging.](images/image_0152.jpeg)\n\n\nThe diagram illustrates the Intel x86 Memory Address Translation Mechanisms, showing the flow from a Logical address to a Physical address space through Segmentation and Paging.\n\n\n**Logical address**\n    is split into\n    **Segment selector**\n    and\n    **Offset**\n    .\n\n\nThe\n    **Segment selector**\n    points to the\n    **Global descriptor table (GDT)**\n    , which contains a\n    **Segment descriptor**\n    . The\n    **Segment descriptor**\n    provides the\n    **Segment base address**\n    .\n\n\nThe\n    **Segment base address**\n    and\n    **Offset**\n    are combined to form the\n    **Linear address space**\n    . The\n    **Linear address space**\n    is divided into\n    **Segment**\n    and\n    **Lin. Addr.**\n    (Linear Address) regions.\n\n\nThe\n    **Lin. Addr.**\n    is used for\n    **Paging**\n    . It is split into\n    **Dir**\n    (Directory),\n    **Table**\n    , and\n    **Offset**\n    .\n\n\nThe\n    **Dir**\n    points to the\n    **Page directory**\n    , which contains an\n    **Entry**\n    . The\n    **Entry**\n    points to the\n    **Page table**\n    , which also contains an\n    **Entry**\n    . The\n    **Entry**\n    in the\n    **Page table**\n    points to the\n    **Page**\n    in the\n    **Physical address space**\n    .\n\n\nThe\n    **Page**\n    contains the\n    **Phy. Addr.**\n    (Physical Address).\n\n\nA horizontal line at the bottom indicates the transition between\n    **Segmentation**\n    and\n    **Paging**\n    .\n\n\nDiagram illustrating the Intel x86 Memory Address Translation Mechanisms, showing the flow from Logical address to Physical address space through Segmentation and Paging.\n\n\n**Figure 8.21**\n   Intel x86 Memory Address Translation Mechanisms\n\n\nFinally, the x86 includes a new extension not found on the earlier 80386 or 80486, the provision for two page sizes. If the PSE (page size extension) bit in control register 4 is set to 1, then the paging unit permits the OS programmer to define a page as either 4 Kbyte or 4 Mbyte in size.\n\n\nWhen 4-Mbyte pages are used, there is only one level of table lookup for pages. When the hardware accesses the page directory, the page directory entry (Figure 8.20d) has the PS bit set to 1. In this case, bits 9 through 21 are ignored and bits 22 through 31 define the base address for a 4-Mbyte page in memory. Thus, there is a single page table.\n\n\nThe use of 4-Mbyte pages reduces the memory-management storage requirements for large main memories. With 4-Kbyte pages, a full 4-Gbyte main memory requires about 4 Mbytes of memory just for the page tables. With 4-Mbyte pages, a single table, 4 Kbytes in length, is sufficient for page memory management."
        },
        {
          "name": "Arm Memory Management",
          "content": "ARM provides a versatile virtual memory system architecture that can be tailored to the needs of the embedded system designer.\n\n\n\n\n**Memory System Organization**\n\n\nFigure 8.22 provides an overview of the memory management hardware in the ARM for virtual memory. The virtual memory translation hardware uses one or two levels of tables for translation from virtual to physical addresses, as explained subsequently. The translation lookaside buffer (TLB) is a cache of recent page table entries. If an entry is available in the TLB, then the TLB directly sends a physical address to main memory for a read or write operation. As explained in Chapter 4, data is exchanged\n\n\n\n\n![Figure 8.22: ARM Memory System Overview. This block diagram illustrates the flow of virtual and physical addresses between the ARM core, MMU, TLB, VMT hardware, and Main memory, including the Cache and write buffer and Cache line fetch hardware.](images/image_0153.jpeg)\n\n\nThe diagram shows the following components and their interactions:\n\n\n  * **Memory-management unit (MMU)**\n     (dashed box):\n       * **Access control hardware**\n       : Receives\n       **Abort**\n       signal from the ARM core and sends\n       **Access bits, domain**\n       to the TLB.\n  * **TLB**\n       (Translation Lookaside Buffer): Receives\n       **Access bits, domain**\n       from the Access control hardware and\n       **Virtual address**\n       from the ARM core. It sends\n       **Physical address**\n       to the Main memory and\n       **Control bits**\n       to the Cache and write buffer. It also sends\n       **Access bits, domain**\n       to the Virtual memory translation hardware.\n  * **Virtual memory translation hardware**\n       : Receives\n       **Virtual address**\n       from the ARM core and\n       **Access bits, domain**\n       from the TLB. It sends\n       **Physical address**\n       to the Main memory.\n  * **ARM core**\n     : Sends\n     **Virtual address**\n     to the TLB and the Virtual memory translation hardware. It receives\n     **Abort**\n     from the Access control hardware and\n     **Control bits**\n     from the TLB.\n  * **Main memory**\n     : Receives\n     **Physical address**\n     from the TLB and the Virtual memory translation hardware. It interacts with the Cache and write buffer and the Cache line fetch hardware.\n  * **Cache and write buffer**\n     : Receives\n     **Virtual address**\n     from the ARM core and\n     **Control bits**\n     from the TLB. It sends\n     **Physical address**\n     to the Main memory and\n     **Virtual address**\n     back to the ARM core.\n  * **Cache line fetch hardware**\n     : Receives\n     **Physical address**\n     from the Main memory and sends data back to the Main memory.\n\n\nFigure 8.22: ARM Memory System Overview. This block diagram illustrates the flow of virtual and physical addresses between the ARM core, MMU, TLB, VMT hardware, and Main memory, including the Cache and write buffer and Cache line fetch hardware.\n\n\nFigure 8.22 ARM Memory System Overview\n\n\nbetween the processor and main memory via the cache. If a logical cache organization is used (Figure 4.7a), then the ARM supplies that address directly to the cache as well as supplying it to the TLB when a cache miss occurs. If a physical cache organization is used (Figure 4.7b), then the TLB must supply the physical address to the cache.\n\n\nEntries in the translation tables also include access control bits, which determine whether a given process may access a given portion of memory. If access is denied, access control hardware supplies an abort signal to the ARM processor.\n\n\n\n\n**Virtual Memory Address Translation**\n\n\nThe ARM supports memory access based on either sections or pages:\n\n\n  * ■\n    **Supersections (optional):**\n    Consist of 16-MB blocks of main memory.\n  * ■\n    **Sections:**\n    Consist of 1-MB blocks of main memory.\n  * ■\n    **Large pages:**\n    Consist of 64-kB blocks of main memory.\n  * ■\n    **Small pages:**\n    Consist of 4-kB blocks of main memory.\n\n\nSections and supersections are supported to allow mapping of a large region of memory while using only a single entry in the TLB. Additional access control mechanisms are extended within small pages to 1kB subpages, and within large pages to 16kB subpages. The translation table held in main memory has two levels:\n\n\n  * ■\n    **Level 1 table:**\n    Holds level 1 descriptors that contain the base address and translation properties for a Section and Supersection; and translation properties and pointers to a level 2 table for a large page or a small page.\n  * ■\n    **Level 2 table:**\n    Holds level 2 descriptors that contain the base address and translation properties for a Small page or a Large page. A level 2 table requires 1 kB of memory.\n\n\nThe memory-management unit (MMU) translates virtual addresses generated by the processor into physical addresses to access main memory, and also derives and checks the access permission. Translations occur as the result of a TLB miss, and start with a first-level fetch. A section-mapped access only requires a first-level fetch, whereas a page-mapped access also requires a second-level fetch.\n\n\nFigure 8.23 shows the two-level address translation process for small pages. There is a single level 1 (L1) page table with 4K 32-bit entries. Each L1 entry points to a level 2 (L2) page table with 256 32-bit entries. Each of the L2 entry points to a 4-kB page in main memory. The 32-bit virtual address is interpreted as follows: The most significant 12 bits are an index into the L1 page table. The next 8 bits are an index into the relevant L2 page table. The least significant 12 bits index a byte in the relevant page in main memory.\n\n\nA similar two-page lookup procedure is used for large pages. For sections and supersection, only the L1 page table lookup is required.\n\n\n\n\n**Memory-Management Formats**\n\n\nTo get a better understanding of the ARM memory management scheme, we consider the key formats, as shown in Figure 8.24. The control bits shown in this figure are defined in Table 8.6.\n\n\n\n\n![Diagram of ARM Virtual Memory Address Translation for Small Pages. A 32-bit virtual address is split into L1 index (bits 31-19), L2 index (bits 19-11), and Page index (bits 10-0). The L1 index points to an entry in the Level 1 (L1) page table (4096 entries, 4KB). The L2 index points to an entry in the Level 2 (L2) page table (256 entries, 4KB). The Page index points to a 4KB page in Main memory. The L1 entry contains an L2 PT base address (bits 31-11) and a 2-bit field (bits 10-0). The L2 entry contains a page base address (bits 31-12) and a 2-bit field (bits 11-0).](images/image_0154.jpeg)\n\n\nThe diagram illustrates the ARM Virtual Memory Address Translation for Small Pages. A 32-bit virtual address is divided into three fields: L1 index (bits 31-19), L2 index (bits 19-11), and Page index (bits 10-0). The L1 index is used to access the Level 1 (L1) page table, which has 4096 entries. The L2 index is used to access the Level 2 (L2) page table, which has 256 entries. The Page index is used to access a 4KB page in Main memory. The L1 page table entry contains an L2 PT base address (bits 31-11) and a 2-bit field (bits 10-0). The L2 page table entry contains a page base address (bits 31-12) and a 2-bit field (bits 11-0). The Page index is used to access a 4KB page in Main memory.\n\n\nDiagram of ARM Virtual Memory Address Translation for Small Pages. A 32-bit virtual address is split into L1 index (bits 31-19), L2 index (bits 19-11), and Page index (bits 10-0). The L1 index points to an entry in the Level 1 (L1) page table (4096 entries, 4KB). The L2 index points to an entry in the Level 2 (L2) page table (256 entries, 4KB). The Page index points to a 4KB page in Main memory. The L1 entry contains an L2 PT base address (bits 31-11) and a 2-bit field (bits 10-0). The L2 entry contains a page base address (bits 31-12) and a 2-bit field (bits 11-0).\n\n\n**Figure 8.23**\n   ARM Virtual Memory Address Translation for Small Pages\n\n\nFor the L1 table, each entry is a descriptor of how its associated 1-MB virtual address range is mapped. Each entry has one of four alternative formats:\n\n\n  * ■\n    **Bits [1:0] = 00:**\n    The associated virtual addresses are unmapped, and attempts to access them generate a translation fault.\n  * ■\n    **Bits [1:0] = 01:**\n    The entry gives the physical address of an L2 page table, which specifies how the associated virtual address range is mapped.\n  * ■\n    **Bits [1:0] = 01: and bit 19 = 0:**\n    The entry is a section descriptor for its associated virtual addresses.\n  * ■\n    **Bits [1:0] = 01: and bit 19 = 1:**\n    The entry is a supersection descriptor for its associated virtual addresses.\n\n\nEntries with bits [1:0] = 11 are reserved.\n\n\nFor memory structured into pages, a two-level page table access is required. Bits [31:10] of the L1 page entry contain a pointer to a L2 page table. For small pages, the L2 entry contains a 20-bit pointer to the base address of a 4-kB page in main memory.\n\n\nFor large pages, the structure is more complex. As with virtual addresses for small pages, a virtual address for a large page structure includes a 12-bit index into\n\n\n\n | 31 | 24 23 | 20 19 | 14 | 12 11 10 9 8 | 5 | 4 | 3 | 2 | 1 | 0\nFault | IGN | 0 | 0\nPage table | Coarse page table base address | P | Domain | SBZ | 0 | 1\nSection | Section base address | S | B | 0 | n | G | S | AP | X | TEX | AP | P | Domain | X | N | C | B | 1 | 0\nSupersection | Supersection base address | Base address [35:32] | S | B | Z | 1 | n | G | S | AP | X | TEX | AP | P | Base address [39:36] | X | N | C | B | 1 | 0\n\n\n(a) Alternative first-level descriptor formats\n\n\n\n | 31 | 16 15 14 | 12 11 10 9 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nFault | IGN | 0 | 0\nSmall page | Small page base address | n | G | S | AP | X | TEX | AP | C | B | 1 | X | N\nLarge page | Large page base address | X | N | TEX | n | G | S | AP | X | SBZ | AP | C | B | 0 | 1\n\n\n(b) Alternative second-level descriptor formats\n\n\n\nSupersection | 31 | 24 23 | 20 19 | 0\n | Level 1 table index | Supersection index\nSection | 31 | 20 19 | 0\n | Level 1 table index | Section index\nSmall page | 31 | 20 19 | 12 11 | 0\n | Level 1 table index | Level 2 table index | Page index\nLarge page | 31 | 20 19 | 16 15 | 12 11 | 0\n | Level 1 table index | Level 2 table index | Page index\n\n\n(c) Virtual memory address formats\n\n\n**Figure 8.24**\n   ARM Memory-Management Formats\n\n\nthe level one table and an 8-bit index into the L2 table. For the 64-kB large pages, the page index portion of the virtual address must be 16 bits. To accommodate all of these bits in a 32-bit format, there is a 4-bit overlap between the page index field and the L2 table index field. ARM accommodates this overlap by requiring that each page table entry in a L2 page table that supports large pages be replicated 16 times. In effect, the size of the L2 page table is reduced from 256 entries to 16 entries, if all of the entries refer to large pages. However, a given L2 page can service a mixture of large and small pages, hence the need for the replication for large page entries.\n\n\n**Table 8.6**\n\n\n**Access Permission (AP), Access Permission Extension (APX)**\n\n\nThese bits control access to the corresponding memory region. If an access is made to an area of memory without the required permissions, a Permission Fault is raised.\n\n\n\n\n**Bufferable (B) bit**\n\n\nDetermines, with the TEX bits, how the write buffer is used for cacheable memory.\n\n\n\n\n**Cacheable (C) bit**\n\n\nDetermines whether this memory region can be mapped through the cache.\n\n\n\n\n**Domain**\n\n\nCollection of memory regions. Access control can be applied on the basis of domain.\n\n\n\n\n**not Global (nG)**\n\n\nDetermines whether the translation should be marked as global (0), or process specific (1).\n\n\n\n\n**Shared (S)**\n\n\nDetermines whether the translation is for not-shared (0), or shared (1) memory.\n\n\n\n\n**SBZ**\n\n\nShould be zero.\n\n\n\n\n**Type Extension (TEX)**\n\n\nThese bits, together with the B and C bits, control accesses to the caches, how the write buffer is used, and if the memory region is shareable and therefore must be kept coherent.\n\n\n\n\n**Execute Never (XN)**\n\n\nDetermines whether the region is executable (0) or not executable (1).\n\n\nFor memory structured into sections or supersections, a one-level page table access is required. For sections, bits [31:20] of the L1 entry contain a 12-bit pointer to the base of the 1-MB section in main memory.\n\n\nFor supersections, bits [31:24] of the L1 entry contain an 8-bit pointer to the base of the 16-MB section in main memory. As with large pages, a page table entry replication is required. In the case of supersections, the L1 table index portion of the virtual address overlaps by 4 bits with the supersection index portion of the virtual address. Therefore, 16 identical L1 page table entries are required.\n\n\nThe range of physical address space can be expanded by up to eight additional address bits (bits [23:20] and [8:5]). The number of additional bits is implementation dependent. These additional bits can be interpreted as extending the size of physical memory by as much as a factor of\n   \n    2^8 = 256\n   \n   . Thus, physical memory may in fact be as much as 256 times as large as the memory space available to each individual process.\n\n\n\n\n**Access Control**\n\n\nThe AP access control bits in each table entry control access to a region of memory by a given process. A region of memory can be designated as no access, read only, or read-write. Further, the region can be designated as privileged access only, reserved for use by the OS and not by applications.\n\n\nARM also employs the concept of a domain, which is a collection of sections and/or pages that have particular access permissions. The ARM architecture\n\n\nsupports 16 domains. The domain feature allows multiple processes to use the same translation tables while maintaining some protection from each other.\n\n\nEach page table entry and TLB entry contains a field that specifies which domain the entry is in. A 2-bit field in the Domain Access Control Register controls access to each domain. Each field allows the access to an entire domain to be enabled and disabled very quickly, so that whole memory areas can be swapped in and out of virtual memory very efficiently. Two kinds of domain access are supported:\n\n\n  * ■\n    **Clients:**\n    Users of domains (execute programs and access data) that must observe the access permissions of the individual sections and/or pages that make up that domain.\n  * ■\n    **Managers:**\n    Control the behavior of the domain (the current sections and pages in the domain, and the domain access), and bypass the access permissions for table entries in that domain.\n\n\nOne program can be a client of some domains, and a manager of some other domains, and have no access to the remaining domains. This allows very flexible memory protection for programs that access different memory resources."
        }
      ]
    },
    {
      "name": "Number Systems",
      "sections": [
        {
          "name": "The Decimal System",
          "content": "In everyday life we use a system based on decimal digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) to represent numbers, and refer to the system as the decimal system. Consider what the number 83 means. It means eight tens plus three:\n\n\n83 = (8 \\times 10) + 3\n\n\nThe number 4728 means four thousands, seven hundreds, two tens, plus eight:\n\n\n4728 = (4 \\times 1000) + (7 \\times 100) + (2 \\times 10) + 8\n\n\nThe decimal system is said to have a\n   **base**\n   , or\n   **radix**\n   , of 10. This means that each digit in the number is multiplied by 10 raised to a power corresponding to that digit's position:\n\n\n\\begin{aligned} 83 &= (8 \\times 10^1) + (3 \\times 10^0) \\\\ 4728 &= (4 \\times 10^3) + (7 \\times 10^2) + (2 \\times 10^1) + (8 \\times 10^0) \\end{aligned}\n\n\nThe same principle holds for decimal fractions, but negative powers of 10 are used. Thus, the decimal fraction 0.256 stands for 2 tenths plus 5 hundredths plus 6 thousandths:\n\n\n0.256 = (2 \\times 10^{-1}) + (5 \\times 10^{-2}) + (6 \\times 10^{-3})\n\n\nA number with both an integer and fractional part has digits raised to both positive and negative powers of 10:\n\n\n\\begin{aligned} 442.256 &= (4 \\times 10^2) + (4 \\times 10^1) + (2 \\times 10^0) + (2 \\times 10^{-1}) + (5 \\times 10^{-2}) \\\\ &\\quad + (6 \\times 10^{-3}) \\end{aligned}\n\n\nIn any number, the leftmost digit is referred to as the\n   **most significant digit**\n   , because it carries the highest value. The rightmost digit is called the\n   **least significant digit**\n   . In the preceding decimal number, the 4 on the left is the most significant digit and the 6 on the right is the least significant digit.\n\n\nTable 9.1 shows the relationship between each digit position and the value assigned to that position. Each position is weighted 10 times the value of the position to the right and one-tenth the value of the position to the left. Thus, positions represent successive powers of 10. If we number the positions as indicated in Table 9.1, then position\n   \n    i\n   \n   is weighted by the value\n   \n    10^i\n   \n   .\n\n\n**Table 9.1**\n\n4 | 7 | 2 | 2 | 5 | 6\n100s | 10s | 1s | tenths | hundredths | thousandths\n10^2 | 10^1 | 10^0 | 10^{-1} | 10^{-2} | 10^{-3}\nposition 2 | position 1 | position 0 | position -1 | position -2 | position -3\n\n\nIn general, for the decimal representation of\n   \n    X = \\{\\dots d_2 d_1 d_0 . d_{-1} d_{-2} d_{-3} \\dots\\}\n   \n   , the value of\n   \n    X\n   \n   is\n\n\nX = \\sum_{i} (d_i \\times 10^i) \\quad (9.1)\n\n\nOne other observation is worth making. Consider the number 509 and ask how many tens are in the number. Because there is a 0 in the tens position, you might be tempted to say there are no tens. But there are in fact 50 tens. What the 0 in the tens position means is that there are no tens left over that cannot be lumped into the hundreds, or thousands, and so on. Therefore, because each position holds only the leftover numbers that cannot be lumped into higher positions, each digit position needs to have a value of no greater than nine. Nine is the maximum value that a position can hold before it flips over into the next higher position."
        },
        {
          "name": "Positional Number Systems",
          "content": "In a positional number system, each number is represented by a string of digits in which each digit position\n   \n    i\n   \n   has an associated weight\n   \n    r^i\n   \n   , where\n   \n    r\n   \n   is the radix, or base, of the number system. The general form of a number in such a system with radix\n   \n    r\n   \n   is\n\n\n(\\dots a_3 a_2 a_1 a_0 . a_{-1} a_{-2} a_{-3} \\dots)_r\n\n\nwhere the value of any digit\n   \n    a_i\n   \n   is an integer in the range\n   \n    0 \\le a_i < r\n   \n   . The dot between\n   \n    a_0\n   \n   and\n   \n    a_{-1}\n   \n   is called the\n   **radix point**\n   . The number is defined to have the value\n\n\n\\begin{aligned} & \\dots + a_3 r^3 + a_2 r^2 + a_1 r^1 + a_0 r^0 + a_{-1} r^{-1} + a_{-2} r^{-2} + a_{-3} r^{-3} + \\dots \\\\ &= \\sum_{i} (a_i \\times r^i) \\quad (9.2) \\end{aligned}\n\n\nThe decimal system, then, is a special case of a positional number system with radix 10 and with digits in the range 0 through 9.\n\n\nAs an example of another positional system, consider the system with base 7. Table 9.2 shows the weighting value for positions -1 through 4. In each position, the digit value ranges from 0 through 6.\n\n\n**Table 9.2**\n\nPosition | 4 | 3 | 2 | 1 | 0 | -1\nValue in Exponential Form | 7^4 | 7^3 | 7^2 | 7^1 | 7^0 | 7^{-1}\nDecimal Value | 2401 | 343 | 49 | 7 | 1 | 1/7"
        },
        {
          "name": "The Binary System",
          "content": "In the decimal system, 10 different digits are used to represent numbers with a base of 10. In the binary system, we have only two digits, 1 and 0. Thus, numbers in the binary system are represented to base 2.\n\n\nTo avoid confusion, we will sometimes put a subscript on a number to indicate its base. For example,\n   \n    83_{10}\n   \n   and\n   \n    4728_{10}\n   \n   are numbers represented in decimal notation or, more briefly, decimal numbers. The digits 1 and 0 in binary notation have the same meaning as in decimal notation:\n\n\n0_2 = 0_{10}\n\n\n1_2 = 1_{10}\n\n\nTo represent larger numbers, as with decimal notation, each digit in a binary number has a value depending on its position:\n\n\n10_2 = (1 \\times 2^1) + (0 \\times 2^0) = 2_{10}\n\n\n11_2 = (1 \\times 2^1) + (1 \\times 2^0) = 3_{10}\n\n\n100_2 = (1 \\times 2^2) + (0 \\times 2^1) + (0 \\times 2^0) = 4_{10}\n\n\nand so on. Again, fractional values are represented with negative powers of the radix:\n\n\n1001.101 = 2^3 + 2^0 + 2^{-1} + 2^{-3} = 9.625_{10}\n\n\nIn general, for the binary representation of\n   \n    Y = \\{\\dots b_2 b_1 b_0 . b_{-1} b_{-2} b_{-3} \\dots\\}\n   \n   , the value of\n   \n    Y\n   \n   is\n\n\nY = \\sum_{i} (b_i \\times 2^i) \\quad (9.3)"
        },
        {
          "name": "Converting Between Binary and Decimal",
          "content": "It is a simple matter to convert a number from binary notation to decimal notation. In fact, we showed several examples in the previous subsection. All that is required is to multiply each binary digit by the appropriate power of 2 and add the results.\n\n\nTo convert from decimal to binary, the integer and fractional parts are handled separately.\n\n\n\n\n**Integers**\n\n\nFor the integer part, recall that in binary notation, an integer represented by\n\n\nb_{m-1} b_{m-2} \\dots b_2 b_1 b_0 \\quad b_i = 0 \\text{ or } 1\n\n\nhas the value\n\n\n(b_{m-1} \\times 2^{m-1}) + (b_{m-2} \\times 2^{m-2}) + \\dots + (b_1 \\times 2^1) + b_0\n\n\nSuppose it is required to convert a decimal integer\n   \n    N\n   \n   into binary form. If we divide\n   \n    N\n   \n   by 2, in the decimal system, and obtain a quotient\n   \n    N_1\n   \n   and a remainder\n   \n    R_0\n   \n   , we may write\n\n\nN = 2 \\times N_1 + R_0 \\quad R_0 = 0 \\text{ or } 1\n\n\nNext, we divide the quotient\n   \n    N_1\n   \n   by 2. Assume that the new quotient is\n   \n    N_2\n   \n   and the new remainder\n   \n    R_1\n   \n   . Then\n\n\nN_1 = 2 \\times N_2 + R_1 \\quad R_1 = 0 \\text{ or } 1\n\n\nso that\n\n\nN = 2(2N_2 + R_1) + R_0 = (N_2 \\times 2^2) + (R_1 \\times 2^1) + R_0\n\n\nIf next\n\n\nN_2 = 2N_3 + R_2\n\n\nwe have\n\n\nN = (N_3 \\times 2^3) + (R_2 \\times 2^2) + (R_1 \\times 2^1) + R_0\n\n\nBecause\n   \n    N > N_1 > N_2 \\dots\n   \n   , continuing this sequence will eventually produce a quotient\n   \n    N_{m-1} = 1\n   \n   (except for the decimal integers 0 and 1, whose binary equivalents are 0 and 1, respectively) and a remainder\n   \n    R_{m-2}\n   \n   , which is 0 or 1. Then\n\n\nN = (1 \\times 2^{m-1}) + (R_{m-2} \\times 2^{m-2}) + \\dots + (R_2 \\times 2^2) + (R_1 \\times 2^1) + R_0\n\n\nwhich is the binary form of\n   \n    N\n   \n   . Hence, we convert from base 10 to base 2 by repeated divisions by 2. The remainders and the final quotient, 1, give us, in order of increasing significance, the binary digits of\n   \n    N\n   \n   . Figure 9.1 shows two examples.\n\n\n\n\n**Fractions**\n\n\nFor the fractional part, recall that in binary notation, a number with a value between 0 and 1 is represented by\n\n\n0.b_{-1}b_{-2}b_{-3} \\dots \\quad b_i = 0 \\text{ or } 1\n\n\nand has the value\n\n\n(b_{-1} \\times 2^{-1}) + (b_{-2} \\times 2^{-2}) + (b_{-3} \\times 2^{-3}) \\dots\n\n\nThis can be rewritten as\n\n\n2^{-1} \\times (b_{-1} + 2^{-1} \\times (b_{-2} + 2^{-1} \\times (b_{-3} + \\dots)))\n\n\nThis expression suggests a technique for conversion. Suppose we want to convert the number\n   \n    F\n   \n   (\n   \n    0 < F < 1\n   \n   ) from decimal to binary notation. We know that\n   \n    F\n   \n   can be expressed in the form\n\n\nF = 2^{-1} \\times (b_{-1} + 2^{-1} \\times (b_{-2} + 2^{-1} \\times (b_{-3} + \\dots)))\n\n\nIf we multiply\n   \n    F\n   \n   by 2, we obtain,\n\n\n2 \\times F = b_{-1} + 2^{-1} \\times (b_{-2} + 2^{-1} \\times (b_{-3} + \\dots))\n\n\n\n\n![](images/image_0155.jpeg)\n\n\n| Quotient | Remainder\n\\frac{11}{2} = | 5 | 1\n\\frac{5}{2} = | 2 | 1\n\\frac{2}{2} = | 1 | 0\n\\frac{1}{2} = | 0 | 1\n\n\nDiagram (a) shows the conversion of 11\n    \n     10\n    \n    to binary. The remainders (1, 1, 0, 1) are collected from bottom to top to form the binary number 1011\n    \n     2\n    \n    , which equals 11\n    \n     10\n    \n    .\n\n\n\n   10\n  \n\n\n![](images/image_0156.jpeg)\n\n\n| Quotient | Remainder\n\\frac{21}{2} = | 10 | 1\n\\frac{10}{2} = | 5 | 0\n\\frac{5}{2} = | 2 | 1\n\\frac{2}{2} = | 1 | 0\n\\frac{1}{2} = | 0 | 1\n\n\nDiagram (b) shows the conversion of 21\n    \n     10\n    \n    to binary. The remainders (1, 0, 1, 0, 1) are collected from bottom to top to form the binary number 10101\n    \n     2\n    \n    , which equals 21\n    \n     10\n    \n    .\n\n\n\n   10\n  \n**Figure 9.1**\nFrom this equation, we see that the integer part of\n   \n    (2 \\times F)\n   \n   , which must be either 0 or 1 because\n   \n    0 < F < 1\n   \n   , is simply\n   \n    b_{-1}\n   \n   . So we can say\n   \n    (2 \\times F) = b_{-1} + F_1\n   \n   , where\n   \n    0 < F_1 < 1\n   \n   and where\n\n\nF_1 = 2^{-1} \\times (b_{-2} + 2^{-1} \\times (b_{-3} + 2^{-1} \\times (b_{-4} + \\dots)))\n\n\nTo find\n   \n    b_{-2}\n   \n   , we repeat the process. Therefore, the conversion algorithm involves repeated multiplication by 2. At each step, the fractional part of the number from the previous step is multiplied by 2. The digit to the left of the decimal point in the product will be 0 or 1 and contributes to the binary representation, starting with the most significant digit. The fractional part of the product is used as the multiplicand in the next step. Figure 9.2 shows two examples.\n\n\nThis process is not necessarily exact; that is, a decimal fraction with a finite number of digits may require a binary fraction with an infinite number of digits. In such cases, the conversion algorithm is usually halted after a prespecified number of steps, depending on the desired accuracy.\n\n\n\n\n![Diagram (a) showing the conversion of 0.81 from decimal to binary. It lists six multiplication steps by 2, extracting the integer part (1, 1, 0, 0, 1, 1) to form the binary fraction 0.110011_2. Arrows connect each integer part to its corresponding bit in the final binary result.](images/image_0157.jpeg)\n\n\nProduct | Integer Part | \n0.81 \\times 2 = 1.62 | 1 | 0.110011_2\n0.62 \\times 2 = 1.24 | 1\n0.24 \\times 2 = 0.48 | 0\n0.48 \\times 2 = 0.96 | 0\n0.96 \\times 2 = 1.92 | 1\n0.92 \\times 2 = 1.84 | 1\n\n\nDiagram (a) showing the conversion of 0.81 from decimal to binary. It lists six multiplication steps by 2, extracting the integer part (1, 1, 0, 0, 1, 1) to form the binary fraction 0.110011_2. Arrows connect each integer part to its corresponding bit in the final binary result.\n\n\n\n   0.81_{10} = 0.110011_2\n  \n\n\n![Diagram (b) showing the conversion of 0.25 from decimal to binary. It lists two multiplication steps by 2, extracting the integer parts (0, 1) to form the binary fraction 0.01_2. Arrows connect each integer part to its corresponding bit in the final binary result.](images/image_0158.jpeg)\n\n\nProduct | Integer Part | \n0.25 \\times 2 = 0.5 | 0 | 0.01_2\n0.5 \\times 2 = 1.0 | 1\n\n\nDiagram (b) showing the conversion of 0.25 from decimal to binary. It lists two multiplication steps by 2, extracting the integer parts (0, 1) to form the binary fraction 0.01_2. Arrows connect each integer part to its corresponding bit in the final binary result.\n\n\n\n   0.25_{10} = 0.01_2\n  \n**Figure 9.2**"
        },
        {
          "name": "Hexadecimal Notation",
          "content": "Because of the inherent binary nature of digital computer components, all forms of data within computers are represented by various binary codes. However, no matter how convenient the binary system is for computers, it is exceedingly cumbersome for human beings. Consequently, most computer professionals who must spend time working with the actual raw data in the computer prefer a more compact notation.\n\n\nWhat notation to use? One possibility is the decimal notation. This is certainly more compact than binary notation, but it is awkward because of the tediousness of converting between base 2 and base 10.\n\n\nInstead, a notation known as hexadecimal has been adopted. Binary digits are grouped into sets of four bits, called a\n   **nibble**\n   . Each possible combination of four binary digits is given a symbol, as follows:\n\n\n\n0000 = 0 | 0100 = 4 | 1000 = 8 | 1100 = C\n0001 = 1 | 0101 = 5 | 1001 = 9 | 1101 = D\n0010 = 2 | 0110 = 6 | 1010 = A | 1110 = E\n0011 = 3 | 0111 = 7 | 1011 = B | 1111 = F\n\n\nBecause 16 symbols are used, the notation is called\n   **hexadecimal**\n   , and the 16 symbols are the\n   **hexadecimal digits**\n   .\n\n\nA sequence of hexadecimal digits can be thought of as representing an integer in base 16 (Table 9.3). Thus,\n\n\n\\begin{aligned} 2C_{16} &= (2_{16} \\times 16^1) + (C_{16} \\times 16^0) \\\\ &= (2_{10} \\times 16^1) + (12_{10} \\times 16^0) = 44 \\end{aligned}\n\n\nThus, viewing hexadecimal numbers as numbers in the positional number system with base 16, we have\n\n\nZ = \\sum_{i} (h_i \\times 16^i) \\quad (9.4)\n\n\nwhere 16 is the base and each hexadecimal digit\n   \n    h_i\n   \n   is in the decimal range\n   \n    0 \\le h_i < 15\n   \n   , equivalent to the hexadecimal range\n   \n    0 \\le h_i \\le F\n   \n   .\n\n\n**Table 9.3**\n   Decimal, Binary, and Hexadecimal\n\n\n\nDecimal (base 10) | Binary (base 2) | Hexadecimal (base 16)\n0 | 0000 | 0\n1 | 0001 | 1\n2 | 0010 | 2\n3 | 0011 | 3\n4 | 0100 | 4\n5 | 0101 | 5\n6 | 0110 | 6\n7 | 0111 | 7\n8 | 1000 | 8\n9 | 1001 | 9\n10 | 1010 | A\n11 | 1011 | B\n12 | 1100 | C\n13 | 1101 | D\n14 | 1110 | E\n15 | 1111 | F\n16 | 0001 0000 | 10\n17 | 0001 0001 | 11\n18 | 0001 0010 | 12\n31 | 0001 1111 | 1F\n100 | 0110 0100 | 64\n255 | 1111 1111 | FF\n256 | 0001 0000 0000 | 100\n\n\nHexadecimal notation is not only used for representing integers but also used as a concise notation for representing any sequence of binary digits, whether they represent text, numbers, or some other type of data. The reasons for using hexadecimal notation are as follows:\n\n\n  * 1. It is more compact than binary notation.\n  * 2. In most computers, binary data occupy some multiple of 4 bits, and hence some multiple of a single hexadecimal digit.\n  * 3. It is extremely easy to convert between binary and hexadecimal notation.\n\n\nAs an example of the last point, consider the binary string 110111100001. This is equivalent to\n\n\n\\begin{array}{cccc} 1101 & 1110 & 0001 & = DE1_{16} \\\\ D & E & 1 & \\end{array}\n\n\nThis process is performed so naturally that an experienced programmer can mentally convert visual representations of binary data to their hexadecimal equivalent without written effort."
        }
      ]
    },
    {
      "name": "Computer Arithmetic",
      "sections": [
        {
          "name": "The Arithmetic and Logic Unit",
          "content": "The ALU is that part of the computer that actually performs arithmetic and logical operations on data. All of the other elements of the computer system—control unit, registers, memory, I/O—are there mainly to bring data into the ALU for it to process and then to take the results back out. We have, in a sense, reached the core or essence of a computer when we consider the ALU.\n\n\nAn ALU and indeed, all electronic components in the computer, are based on the use of simple digital logic devices that can store binary digits and perform simple Boolean logic operations.\n\n\nFigure 10.1 indicates, in general terms, how the ALU is interconnected with the rest of the processor. Operands for arithmetic and logic operations are presented to the ALU in registers, and the results of an operation are stored in registers. These registers are temporary storage locations within the processor that are connected by signal paths to the ALU (e.g., see Figure 2.3). The ALU may also set flags as the result of an operation. For example, an overflow flag is set to 1 if the result of a computation exceeds the length of the register into which it is to be stored.\n\n\n\n\n![Diagram of ALU Inputs and Outputs. A central 3D block labeled 'ALU' has four input arrows on the left: 'Control signals' (two lines with dots), 'Operand registers' (one thick line), and 'Flags' (two lines with dots) on the right side of the block. There is also a 'Result registers' output arrow on the right side of the block.](images/image_0159.jpeg)\n\n\nDiagram of ALU Inputs and Outputs. A central 3D block labeled 'ALU' has four input arrows on the left: 'Control signals' (two lines with dots), 'Operand registers' (one thick line), and 'Flags' (two lines with dots) on the right side of the block. There is also a 'Result registers' output arrow on the right side of the block.\n\n\n**Figure 10.1**\n   ALU Inputs and Outputs\n\n\nThe flag values are also stored in registers within the processor. The processor provides signals that control the operation of the ALU and the movement of the data into and out of the ALU."
        },
        {
          "name": "Integer Representation",
          "content": "In the binary number system,\n   \n    1\n   \n   arbitrary numbers can be represented with just the digits zero and one, the minus sign (for negative numbers), and the period, or\n   **radix point**\n   (for numbers with a fractional component).\n\n\n-1101.0101_2 = -13.3125_{10}\n\n\nFor purposes of computer storage and processing, however, we do not have the benefit of special symbols for the minus sign and radix point. Only binary digits (0 and 1) may be used to represent numbers. If we are limited to nonnegative integers, the representation is straightforward.\n\n\nAn 8-bit word can represent the numbers from 0 to 255, such as\n\n\n00000000 = 0\n\n\n00000001 = 1\n\n\n00101001 = 41\n\n\n10000000 = 128\n\n\n11111111 = 255\n\n\nIn general, if an\n   \n    n\n   \n   -bit sequence of binary digits\n   \n    a_{n-1}a_{n-2} \\dots a_1a_0\n   \n   is interpreted as an unsigned integer\n   \n    A\n   \n   , its value is\n\n\nA = \\sum_{i=0}^{n-1} 2^i a_i\n\n\n1\n   \n   See Chapter 9 for a basic refresher on number systems (decimal, binary, hexadecimal).\n\n\n\n\n**Sign-Magnitude Representation**\n\n\nThere are several alternative conventions used to represent negative as well as positive integers, all of which involve treating the most significant (leftmost) bit in the word as a sign bit. If the sign bit is 0, the number is positive; if the sign bit is 1, the number is negative.\n\n\nThe simplest form of representation that employs a sign bit is the sign-magnitude representation. In an\n   \n    n\n   \n   -bit word, the rightmost\n   \n    n - 1\n   \n   bits hold the magnitude of the integer.\n\n\n\n+18 | = | 00010010\n-18 | = | 10010010 | (sign magnitude)\n\n\nThe general case can be expressed as follows:\n\n\n**Sign Magnitude**\n\n\n\n    A = \\begin{cases} \\sum_{i=0}^{n-2} 2^i a_i & \\text{if } a_{n-1} = 0 \\\\ -\\sum_{i=0}^{n-2} 2^i a_i & \\text{if } a_{n-1} = 1 \\end{cases} \\quad (10.1)\n   \nThere are several drawbacks to sign-magnitude representation. One is that addition and subtraction require a consideration of both the signs of the numbers and their relative magnitudes to carry out the required operation. This should become clear in the discussion in Section 10.3. Another drawback is that there are two representations of 0:\n\n\n\n+ 0\n     \n      10 | = | 00000000\n- 0\n     \n      10 | = | 10000000 | (sign magnitude)\n\n\nThis is inconvenient because it is slightly more difficult to test for 0 (an operation performed frequently on computers) than if there were a single representation.\n\n\nBecause of these drawbacks, sign-magnitude representation is rarely used in implementing the integer portion of the ALU. Instead, the most common scheme is twos complement representation.\n   \n    2\n\n\n\n\n**Twos Complement Representation**\n\n\nLike sign magnitude, twos complement representation uses the most significant bit as a sign bit, making it easy to test whether an integer is positive or negative. It differs from the use of the sign-magnitude representation in the way that the other bits are interpreted. Table 10.1 highlights key characteristics of twos complement representation and arithmetic, which are elaborated in this section and the next.\n\n\nMost treatments of twos complement representation focus on the rules for producing negative numbers, with no formal proof that the scheme is valid. Instead,\n\n\n2\n   \n   In the literature, the terms\n   *two's complement*\n   or\n   *2's complement*\n   are often used. Here we follow the practice used in standards documents and omit the apostrophe (e.g., IEEE Std 100-1992,\n   *The New IEEE Standard Dictionary of Electrical and Electronics Terms*\n   ).\n\n\n**Table 10.1**\n\nRange | -2^{n-1}\n     \n     through\n     \n      2^{n-1} - 1\nNumber of Representations of Zero | One\nNegation | Take the Boolean complement of each bit of the corresponding positive number, then add 1 to the resulting bit pattern viewed as an unsigned integer.\nExpansion of Bit Length | Add additional bit positions to the left and fill in with the value of the original sign bit.\nOverflow Rule | If two numbers with the same sign (both positive or both negative) are added, then overflow occurs if and only if the result has the opposite sign.\nSubtraction Rule | To subtract\n     \n      B\n     \n     from\n     \n      A\n     \n     , take the twos complement of\n     \n      B\n     \n     and add it to\n     \n      A\n     \n     .\n\n\nour presentation of twos complement integers in this section and in Section 10.3 is based on [DATT93], which suggests that twos complement representation is best understood by defining it in terms of a weighted sum of bits, as we did previously for unsigned and sign-magnitude representations. The advantage of this treatment is that it does not leave any lingering doubt that the rules for arithmetic operations in twos complement notation may not work for some special cases.\n\n\nConsider an\n   \n    n\n   \n   -bit integer,\n   \n    A\n   \n   , in twos complement representation. If\n   \n    A\n   \n   is positive, then the sign bit,\n   \n    a_{n-1}\n   \n   , is zero. The remaining bits represent the magnitude of the number in the same fashion as for sign magnitude:\n\n\nA = \\sum_{i=0}^{n-2} 2^i a_i \\quad \\text{for } A \\ge 0\n\n\nThe number zero is identified as positive and therefore has a 0 sign bit and a magnitude of all 0s. We can see that the range of positive integers that may be represented is from 0 (all of the magnitude bits are 0) through\n   \n    2^{n-1} - 1\n   \n   (all of the magnitude bits are 1). Any larger number would require more bits.\n\n\nNow, for a negative number\n   \n    A\n   \n   (\n   \n    A < 0\n   \n   ), the sign bit,\n   \n    a_{n-1}\n   \n   , is one. The remaining\n   \n    n - 1\n   \n   bits can take on any one of\n   \n    2^{n-1}\n   \n   values. Therefore, the range of negative integers that can be represented is from\n   \n    -1\n   \n   to\n   \n    -2^{n-1}\n   \n   . We would like to assign the bit values to negative integers in such a way that arithmetic can be handled in a straightforward fashion, similar to unsigned integer arithmetic. In unsigned integer representation, to compute the value of an integer from the bit representation, the weight of the most significant bit is\n   \n    +2^{n-1}\n   \n   . For a representation with a sign bit, it turns out that the desired arithmetic properties are achieved, as we will see in Section 10.3, if the weight of the most significant bit is\n   \n    -2^{n-1}\n   \n   . This is the convention used in twos complement representation, yielding the following expression for negative numbers:\n\n\n\\mathbf{Twos\\ Complement} \\quad A = -2^{n-1}a_{n-1} + \\sum_{i=0}^{n-2} 2^i a_i \\quad (10.2)\n\n\nEquation (10.2) defines the twos complement representation for both positive and negative numbers. For\n   \n    a_{n-1} = 0\n   \n   , the term\n   \n    -2^{n-1}a_{n-1} = 0\n   \n   and the equation defines\n\n\n**Table 10.2**\n\nDecimal Representation | Sign-Magnitude Representation | Twos Complement Representation | Biased Representation\n+8 | — | — | 1111\n+7 | 0111 | 0111 | 1110\n+6 | 0110 | 0110 | 1101\n+5 | 0101 | 0101 | 1100\n+4 | 0100 | 0100 | 1011\n+3 | 0011 | 0011 | 1010\n+2 | 0010 | 0010 | 1001\n+1 | 0001 | 0001 | 1000\n+0 | 0000 | 0000 | 0111\n-0 | 1000 | — | —\n-1 | 1001 | 1111 | 0110\n-2 | 1010 | 1110 | 0101\n-3 | 1011 | 1101 | 0100\n-4 | 1100 | 1100 | 0011\n-5 | 1101 | 1011 | 0010\n-6 | 1110 | 1010 | 0001\n-7 | 1111 | 1001 | 0000\n-8 | — | 1000 | —\n\n\na nonnegative integer. When\n   \n    a_{n-1} = 1\n   \n   , the term\n   \n    2^{n-1}\n   \n   is subtracted from the summation term, yielding a negative integer.\n\n\nTable 10.2 compares the sign-magnitude and twos complement representations for 4-bit integers. Although twos complement is an awkward representation from the human point of view, we will see that it facilitates the most important arithmetic operations, addition and subtraction. For this reason, it is almost universally used as the processor representation for integers.\n\n\nA useful illustration of the nature of twos complement representation is a value box, in which the value on the far right in the box is 1 (\n   \n    2^0\n   \n   ) and each succeeding position to the left is double in value, until the leftmost position, which is negated. As you can see in Figure 10.2a, the most negative twos complement number that can be represented is\n   \n    -2^{n-1}\n   \n   ; if any of the bits other than the sign bit is one, it adds a positive amount to the number. Also, it is clear that a negative number must have a 1 at its leftmost position and a positive number must have a 0 in that position. Thus, the largest positive number is a 0 followed by all 1s, which equals\n   \n    2^{n-1} - 1\n   \n   .\n\n\nThe rest of Figure 10.2 illustrates the use of the value box to convert from twos complement to decimal and from decimal to twos complement.\n\n\n\n\n**Range Extension**\n\n\nIt is sometimes desirable to take an\n   \n    n\n   \n   -bit integer and store it in\n   \n    m\n   \n   bits, where\n   \n    m > n\n   \n   . This expansion of bit length is referred to as\n   **range extension**\n   , because the range of numbers that can be expressed is extended by increasing the bit length.\n\n\n\n-128 | 64 | 32 | 16 | 8 | 4 | 2 | 1\n\n\n(a) An eight-position twos complement value box\n\n\n\n-128 | 64 | 32 | 16 | 8 | 4 | 2 | 1\n1 | 0 | 0 | 0 | 0 | 0 | 1 | 1\n\n\n-128 \\quad +2 \\quad +1 = -125\n\n\n(b) Convert binary 10000011 to decimal\n\n\n\n-128 | 64 | 32 | 16 | 8 | 4 | 2 | 1\n1 | 0 | 0 | 0 | 1 | 0 | 0 | 0\n\n\n-120 = -128 \\quad +8\n\n\n(c) Convert decimal -120 to binary\n\n\n**Figure 10.2**\nIn sign-magnitude notation, this is easily accomplished: simply move the sign bit to the new leftmost position and fill in with zeros.\n\n\n\n+18 = | 00010010 | (sign magnitude, 8 bits)\n+18 = | 0000000000000010 | (sign magnitude, 16 bits)\n-18 = | 10010010 | (sign magnitude, 8 bits)\n-18 = | 1000000000000010 | (sign magnitude, 16 bits)\n\n\nThis procedure will not work for twos complement negative integers. Using the same example,\n\n\n\n+18 = | 00010010 | (twos complement, 8 bits)\n+18 = | 0000000000000010 | (twos complement, 16 bits)\n-18 = | 11101110 | (twos complement, 8 bits)\n-32,658 = | 1000000001101110 | (twos complement, 16 bits)\n\n\nThe next to last line is easily seen using the value box of Figure 10.2. The last line can be verified using Equation (10.2) or a 16-bit value box.\n\n\nInstead, the rule for twos complement integers is to move the sign bit to the new leftmost position and fill in with copies of the sign bit. For positive numbers, fill in with zeros, and for negative numbers, fill in with ones. This is called sign extension.\n\n\n\n-18 = | 11101110 | (twos complement, 8 bits)\n-18 = | 111111111101110 | (twos complement, 16 bits)\n\n\nTo see why this rule works, let us again consider an\n   \n    n\n   \n   -bit sequence of binary digits\n   \n    a_{n-1}a_{n-2} \\dots a_1a_0\n   \n   interpreted as a twos complement integer\n   \n    A\n   \n   , so that its value is\n\n\nA = -2^{n-1}a_{n-1} + \\sum_{i=0}^{n-2} 2^i a_i\n\n\nIf\n   \n    A\n   \n   is a positive number, the rule clearly works. Now, if\n   \n    A\n   \n   is negative and we want to construct an\n   \n    m\n   \n   -bit representation, with\n   \n    m > n\n   \n   . Then\n\n\nA = -2^{m-1}a_{m-1} + \\sum_{i=0}^{m-2} 2^i a_i\n\n\nThe two values must be equal:\n\n\n\\begin{aligned} -2^{m-1} + \\sum_{i=0}^{m-2} 2^i a_i &= -2^{n-1} + \\sum_{i=0}^{n-2} 2^i a_i \\\\ -2^{m-1} + \\sum_{i=n-1}^{m-2} 2^i a_i &= -2^{n-1} \\\\ -2^{n-1} + \\sum_{i=n-1}^{m-2} 2^i a_i &= 2^{m-1} \\\\ 1 + \\sum_{i=0}^{n-2} 2^i + \\sum_{i=n-1}^{m-2} 2^i a_i &= 1 + \\sum_{i=0}^{m-2} 2^i \\\\ \\sum_{i=n-1}^{m-2} 2^i a_i &= \\sum_{i=n-1}^{m-2} 2^i \\\\ \\Rightarrow a_{m-2} &= \\dots = a_{n-2} = a_{n-2} = 1 \\end{aligned}\n\n\nIn going from the first to the second equation, we require that the least significant\n   \n    n - 1\n   \n   bits do not change between the two representations. Then we get to the next to last equation, which is only true if all of the bits in positions\n   \n    n - 1\n   \n   through\n   \n    m - 2\n   \n   are 1. Therefore, the sign-extension rule works. The reader may find the rule easier to grasp after studying the discussion on twos complement negation at the beginning of Section 10.3.\n\n\n\n\n**Fixed-Point Representation**\n\n\nFinally, we mention that the representations discussed in this section are sometimes referred to as fixed point. This is because the radix point (binary point) is fixed and assumed to be to the right of the rightmost digit. The programmer can use the same representation for binary fractions by scaling the numbers so that the binary point is implicitly positioned at some other location."
        },
        {
          "name": "Integer Arithmetic",
          "content": "This section examines common arithmetic functions on numbers in twos complement representation.\n\n\n\n\n**Negation**\n\n\nIn sign-magnitude representation, the rule for forming the negation of an integer is simple: invert the sign bit. In twos complement notation, the negation of an integer can be formed with the following rules:\n\n\n  * 1. Take the Boolean complement of each bit of the integer (including the sign bit). That is, set each 1 to 0 and each 0 to 1.\n  * 2. Treating the result as an unsigned binary integer, add 1.\n\n\nThis two-step process is referred to as the\n   **twos complement operation**\n   , or the taking of the twos complement of an integer.\n\n\n\\begin{array}{rcl}\n    +18 & = & 00010010 \\quad (\\text{twos complement}) \\\\\n    \\text{bitwise complement} & = & 11101101 \\\\\n    & & \\underline{+ \\quad 1} \\\\\n    & & 11101110 = -18\n    \\end{array}\n\n\nAs expected, the negative of the negative of that number is itself:\n\n\n\\begin{array}{rcl}\n    -18 & = & 11101110 \\quad (\\text{twos complement}) \\\\\n    \\text{bitwise complement} & = & 00010001 \\\\\n    & & \\underline{+ \\quad 1} \\\\\n    & & 00010010 = +18\n    \\end{array}\n\n\nWe can demonstrate the validity of the operation just described using the definition of the twos complement representation in Equation (10.2). Again, interpret an\n   \n    n\n   \n   -bit sequence of binary digits\n   \n    a_{n-1}a_{n-2} \\dots a_1a_0\n   \n   as a twos complement integer\n   \n    A\n   \n   , so that its value is\n\n\nA = -2^{n-1}a_{n-1} + \\sum_{i=0}^{n-2} 2^i a_i\n\n\nNow form the bitwise complement,\n   \n    \\overline{a_{n-1}a_{n-2} \\dots a_0}\n   \n   , and, treating this as an unsigned integer, add 1. Finally, interpret the resulting\n   \n    n\n   \n   -bit sequence of binary digits as a twos complement integer\n   \n    B\n   \n   , so that its value is\n\n\nB = -2^{n-1}\\overline{a_{n-1}} + 1 + \\sum_{i=0}^{n-2} 2^i \\overline{a_i}\n\n\nNow, we want\n   \n    A = -B\n   \n   , which means\n   \n    A + B = 0\n   \n   . This is easily shown to be true:\n\n\n\\begin{aligned}\n    A + B &= -(a_{n-1} + \\overline{a_{n-1}})2^{n-1} + 1 + \\left( \\sum_{i=0}^{n-2} 2^i (a_i + \\overline{a_i}) \\right) \\\\\n    &= -2^{n-1} + 1 + \\left( \\sum_{i=0}^{n-2} 2^i \\right) \\\\\n    &= -2^{n-1} + 1 + (2^{n-1} - 1) \\\\\n    &= -2^{n-1} + 2^{n-1} = 0\n    \\end{aligned}\n\n\nThe preceding derivation assumes that we can first treat the bitwise complement of\n   \n    A\n   \n   as an unsigned integer for the purpose of adding 1, and then treat the result as a twos complement integer. There are two special cases to consider. First, consider\n   \n    A = 0\n   \n   . In that case, for an 8-bit representation:\n\n\n\\begin{array}{rcl}\n    0 & = & 00000000 \\quad (\\text{twos complement}) \\\\\n    \\text{bitwise complement} & = & 11111111 \\\\\n    & & \\underline{+ \\quad 1} \\\\\n    & & 100000000 = 0\n    \\end{array}\n\n\nThere is a\n   *carry*\n   out of the most significant bit position, which is ignored. The result is that the negation of 0 is 0, as it should be.\n\n\nThe second special case is more of a problem. If we take the negation of the bit pattern of 1 followed by\n   \n    n - 1\n   \n   zeros, we get back the same number. For example, for 8-bit words,\n\n\n\\begin{array}{rcl}\n    +128 & = & 10000000 \\quad (\\text{twos complement}) \\\\\n    \\text{bitwise complement} & = & 01111111 \\\\\n    & & \\underline{+ \\quad 1} \\\\\n    & & 10000000 = -128\n    \\end{array}\n\n\nSome such anomaly is unavoidable. The number of different bit patterns in an\n   \n    n\n   \n   -bit word is\n   \n    2n\n   \n   , which is an even number. We wish to represent positive and negative integers and 0. If an equal number of positive and negative integers are represented (sign magnitude), then there are two representations for 0. If there is only one representation of 0 (twos complement), then there must be an unequal number of negative and positive numbers represented. In the case of twos complement, for an\n   \n    n\n   \n   -bit length, there is a representation for\n   \n    -2^{n-1}\n   \n   but not for\n   \n    +2^{n-1}\n   \n   .\n\n\n\n\n**Addition and Subtraction**\n\n\nAddition in twos complement is illustrated in Figure 10.3. Addition proceeds as if the two numbers were unsigned integers. The first four examples illustrate successful operations. If the result of the operation is positive, we get a positive number in twos complement form, which is the same as in unsigned-integer form. If the result of the operation is negative, we get a negative number in twos complement form. Note that, in some instances, there is a carry bit beyond the end of the word (indicated by shading), which is ignored.\n\n\nOn any addition, the result may be larger than can be held in the word size being used. This condition is called\n   **overflow**\n   . When overflow occurs, the ALU must signal this fact so that no attempt is made to use the result. To detect overflow, the following rule is observed:\n\n\n**OVERFLOW RULE:**\n   If two numbers are added, and they are both positive or both negative, then overflow occurs if and only if the result has the opposite sign.\n\n\n\n\\begin{array}{r} 1001 = -7 \\\\ +0101 = 5 \\\\ \\hline 1110 = -2 \\end{array}\n      \n\n       (a)\n       \n        (-7) + (+5) | \\begin{array}{r} 1100 = -4 \\\\ +0100 = 4 \\\\ \\hline 10000 = 0 \\end{array}\n      \n\n       (b)\n       \n        (-4) + (+4)\n\\begin{array}{r} 0011 = 3 \\\\ +0100 = 4 \\\\ \\hline 0111 = 7 \\end{array}\n      \n\n       (c)\n       \n        (+3) + (+4) | \\begin{array}{r} 1100 = -4 \\\\ +1111 = -1 \\\\ \\hline 11011 = -5 \\end{array}\n      \n\n       (d)\n       \n        (-4) + (-1)\n\\begin{array}{r} 0101 = 5 \\\\ +0100 = 4 \\\\ \\hline 1001 = \\text{Overflow} \\end{array}\n      \n\n       (e)\n       \n        (+5) + (+4) | \\begin{array}{r} 1001 = -7 \\\\ +1010 = -6 \\\\ \\hline 10011 = \\text{Overflow} \\end{array}\n      \n\n       (f)\n       \n        (-7) + (-6)\n\n\n**Figure 10.3**\n   Addition of Numbers in Twos Complement Representation\n\n\nFigures 10.3e and f show examples of overflow. Note that overflow can occur whether or not there is a carry.\n\n\nSubtraction is easily handled with the following rule:\n\n\n**SUBTRACTION RULE:**\n   To subtract one number (subtrahend) from another (minuend), take the twos complement (negation) of the subtrahend and add it to the minuend.\n\n\nThus, subtraction is achieved using addition, as illustrated in Figure 10.4. The last two examples demonstrate that the overflow rule still applies.\n\n\n\n\\begin{array}{r} 0010 = 2 \\\\ +1001 = -7 \\\\ \\hline 1011 = -5 \\end{array}\n      \n\n       (a)\n       \n        M = 2 = 0010\n       \n\n\n        S = 7 = 0111\n       \n\n\n        -S = 1001 | \\begin{array}{r} 0101 = 5 \\\\ +1110 = -2 \\\\ \\hline 10011 = 3 \\end{array}\n      \n\n       (b)\n       \n        M = 5 = 0101\n       \n\n\n        S = 2 = 0010\n       \n\n\n        -S = 1110\n\\begin{array}{r} 1011 = -5 \\\\ +1110 = -2 \\\\ \\hline 11001 = -7 \\end{array}\n      \n\n       (c)\n       \n        M = -5 = 1011\n       \n\n\n        S = 2 = 0010\n       \n\n\n        -S = 1110 | \\begin{array}{r} 0101 = 5 \\\\ +0010 = 2 \\\\ \\hline 0111 = 7 \\end{array}\n      \n\n       (d)\n       \n        M = 5 = 0101\n       \n\n\n        S = -2 = 1110\n       \n\n\n        -S = 0010\n\\begin{array}{r} 0111 = 7 \\\\ +0111 = 7 \\\\ \\hline 1110 = \\text{Overflow} \\end{array}\n      \n\n       (e)\n       \n        M = 7 = 0111\n       \n\n\n        S = -7 = 1001\n       \n\n\n        -S = 0111 | \\begin{array}{r} 1010 = -6 \\\\ +1100 = -4 \\\\ \\hline 10110 = \\text{Overflow} \\end{array}\n      \n\n       (f)\n       \n        M = -6 = 1010\n       \n\n\n        S = 4 = 0100\n       \n\n\n        -S = 1100\n\n\n**Figure 10.4**\n   Subtraction of Numbers in Twos Complement Representation (\n   \n    M - S\n   \n   )\n\n\n\n\n![Figure 10.5: Geometric Depiction of Twos Complement Integers. (a) 4-bit numbers: A circle with 16 points representing 4-bit binary numbers from 0000 to 1111. The circle is divided into two halves by a dashed horizontal line. The top half contains positive numbers (0001 to 0111) and the bottom half contains negative numbers (1001 to 1111). A number line below shows integers from -9 to 9. Arrows indicate 'Subtraction of positive numbers' (clockwise) and 'Addition of positive numbers' (counterclockwise). (b) n-bit numbers: A circle with points representing n-bit binary numbers. The top half contains positive numbers (000...0 to 011...1) and the bottom half contains negative numbers (111...1 to 100...0). A number line below shows integers from -2^(n-1) to 2^(n-1)-1. Arrows indicate 'Subtraction of positive numbers' (clockwise) and 'Addition of positive numbers' (counterclockwise).](images/image_0160.jpeg)\n\n\n(a) 4-bit numbers\n\n\n(b)\n    \n     n\n    \n    -bit numbers\n\n\nFigure 10.5: Geometric Depiction of Twos Complement Integers. (a) 4-bit numbers: A circle with 16 points representing 4-bit binary numbers from 0000 to 1111. The circle is divided into two halves by a dashed horizontal line. The top half contains positive numbers (0001 to 0111) and the bottom half contains negative numbers (1001 to 1111). A number line below shows integers from -9 to 9. Arrows indicate 'Subtraction of positive numbers' (clockwise) and 'Addition of positive numbers' (counterclockwise). (b) n-bit numbers: A circle with points representing n-bit binary numbers. The top half contains positive numbers (000...0 to 011...1) and the bottom half contains negative numbers (111...1 to 100...0). A number line below shows integers from -2^(n-1) to 2^(n-1)-1. Arrows indicate 'Subtraction of positive numbers' (clockwise) and 'Addition of positive numbers' (counterclockwise).\n\n\n**Figure 10.5**\n   Geometric Depiction of Twos Complement Integers\n\n\nSome insight into twos complement addition and subtraction can be gained by looking at a geometric depiction [BENH92], as shown in Figure 10.5. The circle in the upper half of each part of the figure is formed by selecting the appropriate segment of the number line and joining the endpoints. Note that when the numbers are laid out on a circle, the twos complement of any number is horizontally opposite that number (indicated by dashed horizontal lines). Starting at any number on the circle, we can add positive\n   \n    k\n   \n   (or subtract negative\n   \n    k\n   \n   ) to that number by moving\n   \n    k\n   \n   positions clockwise, and we can subtract positive\n   \n    k\n   \n   (or add negative\n   \n    k\n   \n   ) from that number by moving\n   \n    k\n   \n   positions counterclockwise. If an arithmetic operation results in traversal of the point where the endpoints are joined, an incorrect answer is given (overflow).\n\n\n**ALL OF**\n   the examples of Figures 10.3 and 10.4 are easily traced in the circle of Figure 10.5.\n\n\nFigure 10.6 suggests the data paths and hardware elements needed to accomplish addition and subtraction. The central element is a binary adder, which is presented two numbers for addition and produces a sum and an overflow indication. The binary adder treats the two numbers as unsigned integers. (A logic implementation of an adder is given in Chapter 11.) For addition, the two numbers are presented to the adder from two registers, designated in this case as\n   **A**\n   and\n   **B**\n   registers. The result may be stored in one of these registers or in a third. The overflow indication is stored in a 1-bit overflow flag (0 = no overflow; 1 = overflow). For subtraction, the subtrahend (\n   **B**\n   register) is passed through a twos completer so that its twos complement is presented to the adder. Note that Figure 10.6 only shows the\n\n\n\n\n![Block Diagram of Hardware for Addition and Subtraction. The diagram shows a B Register connected to a Complementer. The output of the Complementer and the B Register are connected to a Switch (SW). The output of the SW and the A Register are connected to an Adder. The Adder outputs an Overflow bit (OF) and its result is fed back to the A Register.](images/image_0161.jpeg)\n\n\ngraph TD\n      BR[B Register] --> C[Complementer]\n      C --> SW[SW]\n      BR --> SW\n      SW --> A[Adder]\n      AR[A Register] --> A\n      A --> OF[Overflow bit]\n      A --> AR\n  \nBlock Diagram of Hardware for Addition and Subtraction. The diagram shows a B Register connected to a Complementer. The output of the Complementer and the B Register are connected to a Switch (SW). The output of the SW and the A Register are connected to an Adder. The Adder outputs an Overflow bit (OF) and its result is fed back to the A Register.\n\n\nOF = Overflow bit\n   \n\n   SW = Switch (select addition or subtraction)\n\n\n**Figure 10.6**\n   Block Diagram of Hardware for Addition and Subtraction\n\n\ndata paths. Control signals are needed to control whether or not the complementer is used, depending on whether the operation is addition or subtraction.\n\n\n\n\n**Multiplication**\n\n\nCompared with addition and subtraction, multiplication is a complex operation, whether performed in hardware or software. A wide variety of algorithms have been used in various computers. The purpose of this subsection is to give the reader some feel for the type of approach typically taken. We begin with the simpler problem of multiplying two unsigned (nonnegative) integers, and then we look at one of the most common techniques for multiplication of numbers in twos complement representation.\n\n\n**UNSIGNED INTEGERS**\n   Figure 10.7 illustrates the multiplication of unsigned binary integers, as might be carried out using paper and pencil. Several important observations can be made:\n\n\n  * 1. Multiplication involves the generation of partial products, one for each digit in the multiplier. These partial products are then summed to produce the final product.\n\n\n\n\n![](images/image_0162.jpeg)\n\n\n1011 | Multiplicand (11)\n× 1101 | Multiplier (13)\n1011 | Partial products\n0000\n1011\n1011\n10001111 | Product (143)\n\n\n**Figure 10.7**\n   Multiplication of Unsigned Binary Integers\n\n\n  * 2. The partial products are easily defined. When the multiplier bit is 0, the partial product is 0. When the multiplier is 1, the partial product is the multiplicand.\n  * 3. The total product is produced by summing the partial products. For this operation, each successive partial product is shifted one position to the left relative to the preceding partial product.\n  * 4. The multiplication of two\n    \n     n\n    \n    -bit binary integers results in a product of up to\n    \n     2n\n    \n    bits in length (e.g.,\n    \n     11 \\times 11 = 1001\n    \n    ).\n\n\nCompared with the pencil-and-paper approach, there are several things we can do to make computerized multiplication more efficient. First, we can perform a running addition on the partial products rather than waiting until the end. This eliminates the need for storage of all the partial products; fewer registers are needed. Second, we can save some time on the generation of partial products. For each 1 on the multiplier, an add and a shift operation are required; but for each 0, only a shift is required.\n\n\nFigure 10.8a shows a possible implementation employing these measures. The multiplier and multiplicand are loaded into two registers (Q and M). A third\n\n\n\n\n![Block diagram of a hardware implementation of unsigned binary multiplication. The diagram shows a 'Multiplicand' register (M_{n-1} ... M_0) connected to an 'n-bit adder'. The 'n-bit adder' is also connected to an 'n-bit register' (A_{n-1} ... A_0). The 'n-bit register' is connected to a 'Shift and add control logic' block. The 'Shift and add control logic' block is connected to the 'n-bit adder' and a 'Multiplier' register (Q_{n-1} ... Q_0). The 'Multiplier' register is connected to the 'Shift and add control logic' block. A feedback loop connects the output of the 'n-bit adder' back to the input of the 'n-bit register'. The 'Shift and add control logic' block controls the 'Shift right' operation on the 'Multiplier' register.](images/image_0163.jpeg)\n\n\nBlock diagram of a hardware implementation of unsigned binary multiplication. The diagram shows a 'Multiplicand' register (M_{n-1} ... M_0) connected to an 'n-bit adder'. The 'n-bit adder' is also connected to an 'n-bit register' (A_{n-1} ... A_0). The 'n-bit register' is connected to a 'Shift and add control logic' block. The 'Shift and add control logic' block is connected to the 'n-bit adder' and a 'Multiplier' register (Q_{n-1} ... Q_0). The 'Multiplier' register is connected to the 'Shift and add control logic' block. A feedback loop connects the output of the 'n-bit adder' back to the input of the 'n-bit register'. The 'Shift and add control logic' block controls the 'Shift right' operation on the 'Multiplier' register.\n\n\n(a) Block diagram\n\n\n\nC | A | Q | M | \n0 | 0000 | 1101 | 1011 | Initial values\n0 | 1011 | 1101 | 1011 | Add\n      \n      }\n      \n       First cycle\n0 | 0101 | 1110 | 1011 | Shift\n      \n      }\n0 | 0010 | 1111 | 1011 | Shift\n      \n      }\n      \n       Second cycle\n0 | 1101 | 1111 | 1011 | Add\n      \n      }\n      \n       Third cycle\n0 | 0110 | 1111 | 1011 | Shift\n      \n      }\n1 | 0001 | 1111 | 1011 | Add\n      \n      }\n      \n       Fourth cycle\n0 | 1000 | 1111 | 1011 | Shift\n      \n      }\n\n\n(b) Example from Figure 10.7 (product in A, Q)\n\n\n**Figure 10.8**\nregister, the A register, is also needed and is initially set to 0. There is also a 1-bit C register, initialized to 0, which holds a potential carry bit resulting from addition.\n\n\nThe operation of the multiplier is as follows. Control logic reads the bits of the multiplier one at a time. If\n   \n    Q_0\n   \n   is 1, then the multiplicand is added to the A register and the result is stored in the A register, with the C bit used for overflow. Then all of the bits of the C, A, and Q registers are shifted to the right one bit, so that the C bit goes into\n   \n    A_{n-1}\n   \n   ,\n   \n    A_0\n   \n   goes into\n   \n    Q_{n-1}\n   \n   , and\n   \n    Q_0\n   \n   is lost. If\n   \n    Q_0\n   \n   is 0, then no addition is performed, just the shift. This process is repeated for each bit of the original multiplier. The resulting\n   \n    2n\n   \n   -bit product is contained in the A and Q registers. A flowchart of the operation is shown in Figure 10.9, and an example is given in Figure 10.8b. Note that on the second cycle, when the multiplier bit is 0, there is no add operation.\n\n\n**TWOS COMPLEMENT MULTIPLICATION**\n   We have seen that addition and subtraction can be performed on numbers in twos complement notation by treating them as unsigned integers. Consider\n\n\n\\begin{array}{r} 1001 \\\\ + 0011 \\\\ \\hline 1100 \\end{array}\n\n\nIf these numbers are considered to be unsigned integers, then we are adding 9 (1001) plus 3 (0011) to get 12 (1100). As twos complement integers, we are adding\n   \n    -7\n   \n   (1001) to 3 (0011) to get\n   \n    -4\n   \n   (1100).\n\n\n\n\n![Flowchart for Unsigned Binary Multiplication](images/image_0164.jpeg)\n\n\ngraph TD\n    Start([START]) --> Init[C, A ← 0\nM ← Multiplicand\nQ ← Multiplier\nCount ← n]\n    Init --> Q0{Q₀ = 1?}\n    Q0 -- Yes --> Add[C, A ← A + M]\n    Q0 -- No --> Shift[Shift right C, A, Q\nCount ← Count - 1]\n    Add --> Shift\n    Shift --> Count0{Count = 0?}\n    Count0 -- No --> Q0\n    Count0 -- Yes --> End([END])\n    End --> Product[Product\nin A, Q]\n  \nThe flowchart for Unsigned Binary Multiplication starts with an oval labeled 'START'. An arrow points down to a rectangular process block containing: 'C, A ← 0', 'M ← Multiplicand', 'Q ← Multiplier', and 'Count ← n'. An arrow points down from this block to a diamond decision block labeled 'Q₀ = 1?'. From the 'Yes' branch of this decision, an arrow points right to a rectangular process block labeled 'C, A ← A + M'. From the 'No' branch, an arrow points down to a rectangular process block labeled 'Shift right C, A, Q' and 'Count ← Count - 1'. Both the 'C, A ← A + M' block and the 'Shift right C, A, Q' block have arrows pointing down to the same rectangular process block. From this block, an arrow points down to a second diamond decision block labeled 'Count = 0?'. From the 'No' branch of this decision, an arrow points left and then up to the 'Q₀ = 1?' decision block, creating a loop. From the 'Yes' branch, an arrow points right to an oval labeled 'END'. An arrow points from the 'END' oval to a label 'Product in A, Q'.\n\n\nFlowchart for Unsigned Binary Multiplication\n\n\n**Figure 10.9**\n   Flowchart for Unsigned Binary Multiplication\n\n\n\n1011 | \n× 1101 | \n00001011 | 1011 × 1 × 2\n     \n      0\n00000000 | 1011 × 0 × 2\n     \n      1\n00101100 | 1011 × 1 × 2\n     \n      2\n01011000 | 1011 × 1 × 2\n     \n      3\n10001111 | \n\n\n**Figure 10.10**\n   Multiplication of Two Unsigned 4-Bit Integers Yielding an 8-Bit Result\n\n\nUnfortunately, this simple scheme will not work for multiplication. To see this, consider again Figure 10.7. We multiplied 11 (1011) by 13 (1101) to get 143 (10001111). If we interpret these as twos complement numbers, we have\n   \n    -5(1011)\n   \n   times\n   \n    -3(1101)\n   \n   equals\n   \n    -113(10001111)\n   \n   . This example demonstrates that straightforward multiplication will not work if both the multiplicand and multiplier are negative. In fact, it will not work if either the multiplicand or the multiplier is negative. To justify this statement, we need to go back to Figure 10.7 and explain what is being done in terms of operations with powers of 2. Recall that any unsigned binary number can be expressed as a sum of powers of 2. Thus,\n\n\n1101 = 1 \\times 2^3 + 1 \\times 2^2 + 0 \\times 2^1 + 1 \\times 2^0 = 2^3 + 2^2 + 2^0\n\n\nFurther, the multiplication of a binary number by\n   \n    2^n\n   \n   is accomplished by shifting that number to the left\n   \n    n\n   \n   bits. With this in mind, Figure 10.10 recasts Figure 10.7 to make the generation of partial products by multiplication explicit. The only difference in Figure 10.10 is that it recognizes that the partial products should be viewed as\n   \n    2n\n   \n   -bit numbers generated from the\n   \n    n\n   \n   -bit multiplicand.\n\n\nThus, as an unsigned integer, the 4-bit multiplicand 1011 is stored in an 8-bit word as 00001011. Each partial product (other than that for\n   \n    2^0\n   \n   ) consists of this number shifted to the left, with the unoccupied positions on the right filled with zeros (e.g., a shift to the left of two places yields 00101100).\n\n\nNow we can demonstrate that straightforward multiplication will not work if the multiplicand is negative. The problem is that each contribution of the negative multiplicand as a partial product must be a negative number on a\n   \n    2n\n   \n   -bit field; the sign bits of the partial products must line up. This is demonstrated in Figure 10.11, which shows that multiplication of 1001 by 0011. If these are treated as unsigned integers, the multiplication of\n   \n    9 \\times 3 = 27\n   \n   proceeds simply. However, if 1001 is interpreted\n\n\n\n1001 (9)\n     \n     × 0011 (3)\n     \n     —————\n     \n     00001001 1001 × 2\n     \n      0\n     \n\n     00010010 1001 × 2\n     \n      1\n     \n\n     00011011 (27) | 1001 (-7)\n     \n     × 0011 (3)\n     \n     —————\n     \n     11111001 (-7) × 2\n     \n      0\n     \n     = (-7)\n     \n     11110010 (-7) × 2\n     \n      1\n     \n     = (-14)\n     \n     11101011 (-21)\n\n\n(a) Unsigned integers\n\n\n(b) Twos complement integers\n\n\n**Figure 10.11**\n   Comparison of Multiplication of Unsigned and Twos Complement Integers\n\n\nas the twos complement value\n   \n    -7\n   \n   , then each partial product must be a negative twos complement number of\n   \n    2n\n   \n   (8) bits, as shown in Figure 10.11b. Note that this is accomplished by padding out each partial product to the left with binary 1s.\n\n\nIf the multiplier is negative, straightforward multiplication also will not work. The reason is that the bits of the multiplier no longer correspond to the shifts or multiplications that must take place. For example, the 4-bit decimal number\n   \n    -3\n   \n   is written 1101 in twos complement. If we simply took partial products based on each bit position, we would have the following correspondence:\n\n\n1101 \\leftrightarrow -(1 \\times 2^3 + 1 \\times 2^2 + 0 \\times 2^1 + 1 \\times 2^0) = -(2^3 + 2^2 + 2^0)\n\n\nIn fact, what is desired is\n   \n    -(2^1 + 2^0)\n   \n   . So this multiplier cannot be used directly in the manner we have been describing.\n\n\nThere are a number of ways out of this dilemma. One would be to convert both multiplier and multiplicand to positive numbers, perform the multiplication, and then take the twos complement of the result if and only if the sign of the two original numbers differed. Implementers have preferred to use techniques that do not require this final transformation step. One of the most common of these is Booth's algorithm [BOOT51]. This algorithm also has the benefit of speeding up the multiplication process, relative to a more straightforward approach.\n\n\nBooth's algorithm is depicted in Figure 10.12 and can be described as follows. As before, the multiplier and multiplicand are placed in the Q and M registers,\n\n\n\n\n![Flowchart of Booth's Algorithm for Twos Complement Multiplication. The process starts with initialization: A <- 0, Q_{-1} <- 0, M <- Multiplicand, Q <- Multiplier, and Count <- n. It then enters a loop where it checks the current bits Q_0 and Q_{-1}. If Q_0, Q_{-1} = 10, it performs A <- A - M. If Q_0, Q_{-1} = 01, it performs A <- A + M. If Q_0, Q_{-1} = 11 or 00, it does nothing. After the operation, it performs an arithmetic right shift on A, Q, and Q_{-1}, and decrements Count. The loop continues until Count = 0, at which point the algorithm ends.](images/image_0165.jpeg)\n\n\ngraph TD\n    START([START]) --> Init[A < 0, Q_{-1} < 0\nM < Multiplicand\nQ < Multiplier\nCount < n]\n    Init --> Decision{Q_0, Q_{-1}}\n    Decision -- \"= 10\" --> Sub[A < A - M]\n    Decision -- \"= 01\" --> Add[A < A + M]\n    Decision -- \"= 11\" --> Shift[Arithmetic shift\nRight: A, Q, Q_{-1}\nCount < Count - 1]\n    Decision -- \"= 00\" --> Shift\n    Sub --> Shift\n    Add --> Shift\n    Shift --> Decision2{Count = 0?}\n    Decision2 -- \"No\" --> Decision\n    Decision2 -- \"Yes\" --> END([END])\n  \nFlowchart of Booth's Algorithm for Twos Complement Multiplication. The process starts with initialization: A <- 0, Q_{-1} <- 0, M <- Multiplicand, Q <- Multiplier, and Count <- n. It then enters a loop where it checks the current bits Q_0 and Q_{-1}. If Q_0, Q_{-1} = 10, it performs A <- A - M. If Q_0, Q_{-1} = 01, it performs A <- A + M. If Q_0, Q_{-1} = 11 or 00, it does nothing. After the operation, it performs an arithmetic right shift on A, Q, and Q_{-1}, and decrements Count. The loop continues until Count = 0, at which point the algorithm ends.\n\n\n**Figure 10.12**\n   Booth's Algorithm for Twos Complement Multiplication\n\n\n\nA | Q | Q\n      \n       -1 | M | \n0000 | 0011 | 0 | 0111 | Initial values\n1001 | 0011 | 0 | 0111 | First cycle\n1100 | 1001 | 1 | 0111\n1110 | 0100 | 1 | 0111 | Second cycle\n0101 | 0100 | 1 | 0111\n0010 | 1010 | 0 | 0111 | Third cycle\n0001 | 0101 | 0 | 0111\n\n\n**Figure 10.13**\n   Example of Booth's Algorithm (\n   \n    7 \\times 3\n   \n   )\n\n\nrespectively. There is also a 1-bit register placed logically to the right of the least significant bit (\n   \n    Q_0\n   \n   ) of the Q register and designated\n   \n    Q_{-1}\n   \n   ; its use is explained shortly. The results of the multiplication will appear in the A and Q registers. A and\n   \n    Q_{-1}\n   \n   are initialized to 0. As before, control logic scans the bits of the multiplier one at a time. Now, as each bit is examined, the bit to its right is also examined. If the two bits are the same (1–1 or 0–0), then all of the bits of the A, Q, and\n   \n    Q_{-1}\n   \n   registers are shifted to the right 1 bit. If the two bits differ, then the multiplicand is added to or subtracted from the A register, depending on whether the two bits are 0–1 or 1–0. Following the addition or subtraction, the right shift occurs. In either case, the right shift is such that the leftmost bit of A, namely\n   \n    A_{n-1}\n   \n   , not only is shifted into\n   \n    A_{n-2}\n   \n   , but also remains in\n   \n    A_{n-1}\n   \n   . This is required to preserve the sign of the number in A and Q. It is known as an\n   **arithmetic shift**\n   , because it preserves the sign bit.\n\n\nFigure 10.13 shows the sequence of events in Booth's algorithm for the multiplication of 7 by 3. More compactly, the same operation is depicted in Figure 10.14a. The rest of Figure 10.14 gives other examples of the algorithm. As can be seen, it works with any combination of positive and negative numbers. Note also the efficiency of the algorithm. Blocks of 1s or 0s are skipped over, with an average of only one addition or subtraction per block.\n\n\n\n\\begin{array}{r} 0111 \\\\ \\times 0011 \\\\ \\hline 1111001 \\\\ 0000000 \\\\ 000111 \\\\ \\hline 00010101 \\end{array} \\quad \\begin{array}{l} (0) \\\\ 1-0 \\\\ 1-1 \\\\ 0-1 \\\\ (21) \\end{array} | \\begin{array}{r} 0111 \\\\ \\times 1101 \\\\ \\hline 1111001 \\\\ 0000111 \\\\ 111001 \\\\ \\hline 11101011 \\end{array} \\quad \\begin{array}{l} (0) \\\\ 1-0 \\\\ 0-1 \\\\ 1-0 \\\\ (-21) \\end{array}\n(a)\n      \n       (7) \\times (3) = (21) | (b)\n      \n       (7) \\times (-3) = (-21)\n\\begin{array}{r} 1001 \\\\ \\times 0011 \\\\ \\hline 00000111 \\\\ 0000000 \\\\ 111001 \\\\ \\hline 11101011 \\end{array} \\quad \\begin{array}{l} (0) \\\\ 1-0 \\\\ 1-1 \\\\ 0-1 \\\\ (-21) \\end{array} | \\begin{array}{r} 1001 \\\\ \\times 1101 \\\\ \\hline 00000111 \\\\ 1111001 \\\\ 000111 \\\\ \\hline 00010101 \\end{array} \\quad \\begin{array}{l} (0) \\\\ 1-0 \\\\ 0-1 \\\\ 1-0 \\\\ (21) \\end{array}\n(c)\n      \n       (-7) \\times (3) = (-21) | (d)\n      \n       (-7) \\times (-3) = (21)\n\n\n**Figure 10.14**\n   Examples Using Booth's Algorithm\n\n\nWhy does Booth's algorithm work? Consider first the case of a positive multiplier. In particular, consider a positive multiplier consisting of one block of 1s surrounded by 0s (e.g., 00011110). As we know, multiplication can be achieved by adding appropriately shifted copies of the multiplicand:\n\n\n\\begin{aligned} M \\times (00011110) &= M \\times (2^4 + 2^3 + 2^2 + 2^1) \\\\ &= M \\times (16 + 8 + 4 + 2) \\\\ &= M \\times 30 \\end{aligned}\n\n\nThe number of such operations can be reduced to two if we observe that\n\n\n2^n + 2^{n-1} + \\dots + 2^{n-K} = 2^{n+1} - 2^{n-K} \\quad (10.3)\n\n\n\\begin{aligned} M \\times (00011110) &= M \\times (2^5 - 2^1) \\\\ &= M \\times (32 - 2) \\\\ &= M \\times 30 \\end{aligned}\n\n\nSo the product can be generated by one addition and one subtraction of the multiplicand. This scheme extends to any number of blocks of 1s in a multiplier, including the case in which a single 1 is treated as a block.\n\n\n\\begin{aligned} M \\times (01111010) &= M \\times (2^6 + 2^5 + 2^4 + 2^3 + 2^1) \\\\ &= M \\times (2^7 - 2^3 + 2^2 - 2^1) \\end{aligned}\n\n\nBooth's algorithm conforms to this scheme by performing a subtraction when the first 1 of the block is encountered (1-0) and an addition when the end of the block is encountered (0-1).\n\n\nTo show that the same scheme works for a negative multiplier, we need to observe the following. Let\n   \n    X\n   \n   be a negative number in twos complement notation:\n\n\n\\text{Representation of } X = \\{1x_{n-2}x_{n-3} \\dots x_1x_0\\}\n\n\nThen the value of\n   \n    X\n   \n   can be expressed as follows:\n\n\nX = -2^{n-1} + (x_{n-2} \\times 2^{n-2}) + (x_{n-3} \\times 2^{n-3}) + \\dots + (x_1 \\times 2^1) + (x_0 \\times 2^0) \\quad (10.4)\n\n\nThe reader can verify this by applying the algorithm to the numbers in Table 10.2.\n\n\nThe leftmost bit of\n   \n    X\n   \n   is 1, because\n   \n    X\n   \n   is negative. Assume that the leftmost 0 is in the\n   \n    k\n   \n   th position. Thus,\n   \n    X\n   \n   is of the form\n\n\n\\text{Representation of } X = \\{111 \\dots 10x_{k-1}x_{k-2} \\dots x_1x_0\\} \\quad (10.5)\n\n\nThen the value of\n   \n    X\n   \n   is\n\n\nX = -2^{n-1} + 2^{n-2} + \\dots + 2^{k+1} + (x_{k-1} \\times 2^{k-1}) + \\dots + (x_0 \\times 2^0) \\quad (10.6)\n\n\nFrom Equation (10.3), we can say that\n\n\n2^{n-2} + 2^{n-3} + \\dots + 2^{k-1} = 2^{n-1} - 2^{k-1}\n\n\nRearranging\n\n\n-2^{n-1} + 2^{n-2} + 2^{n-3} + \\dots + 2^{k+1} = -2^{k+1} \\quad (10.7)\n\n\nSubstituting Equation (10.7) into Equation (10.6), we have\n\n\nX = -2^{k+1} + (x_{k-1} \\times 2^{k-1}) + \\dots + (x_0 \\times 2^0) \\quad (10.8)\n\n\nAt last we can return to Booth's algorithm. Remembering the representation of\n   \n    X\n   \n   [Equation (10.5)], it is clear that all of the bits from\n   \n    x_0\n   \n   up to the leftmost 0 are handled properly because they produce all of the terms in Equation (10.8) but\n   \n    (-2^{k+1})\n   \n   and thus are in the proper form. As the algorithm scans over the leftmost 0 and encounters the next 1 (\n   \n    2^{k+1}\n   \n   ), a 1-0 transition occurs and a subtraction takes place\n   \n    (-2^{k+1})\n   \n   . This is the remaining term in Equation (10.8).\n\n\nAs an example, consider the multiplication of some multiplicand by\n   \n    (-6)\n   \n   . In two's complement representation, using an 8-bit word,\n   \n    (-6)\n   \n   is represented as 11111010. By Equation (10.4), we know that\n\n\n-6 = -2^7 + 2^6 + 2^5 + 2^4 + 2^3 + 2^1\n\n\nwhich the reader can easily verify. Thus,\n\n\nM \\times (11111010) = M \\times (-2^7 + 2^6 + 2^5 + 2^4 + 2^3 + 2^1)\n\n\nUsing Equation (10.7),\n\n\nM \\times (11111010) = M \\times (-2^3 + 2^1)\n\n\nwhich the reader can verify is still\n   \n    M \\times (-6)\n   \n   . Finally, following our earlier line of reasoning,\n\n\nM \\times (11111010) = M \\times (-2^3 + 2^2 - 2^1)\n\n\nWe can see that Booth's algorithm conforms to this scheme. It performs a subtraction when the first 1 is encountered (10), an addition when (01) is encountered, and finally another subtraction when the first 1 of the next block of 1s is encountered. Thus, Booth's algorithm performs fewer additions and subtractions than a more straightforward algorithm.\n\n\n\n\n**Division**\n\n\nDivision is somewhat more complex than multiplication but is based on the same general principles. As before, the basis for the algorithm is the paper-and-pencil approach, and the operation involves repetitive shifting and addition or subtraction.\n\n\nFigure 10.15 shows an example of the long division of unsigned binary integers. It is instructive to describe the process in detail. First, the bits of the dividend are examined from left to right, until the set of bits examined represents a number greater than or equal to the divisor; this is referred to as the divisor being able to divide the number. Until this event occurs, 0s are placed in the quotient from left to right. When the event occurs, a 1 is placed in the quotient and the divisor is subtracted from the partial dividend. The result is referred to as a\n   *partial remainder*\n   .\n\n\n\n\n![Figure 10.15: Example of Division of Unsigned Binary Integers. The diagram shows the long division of 10010011 (Dividend) by 1011 (Divisor). The Quotient is 00001101 and the Remainder is 100. The process is shown in steps with partial remainders: 1001, 0011, 00111, and 001111.](images/image_0166.jpeg)\n\n\nFigure 10.15 illustrates the division of the unsigned binary dividend 10010011 by the divisor 1011. The quotient is 00001101 and the remainder is 100. The process is shown in steps with partial remainders: 1001, 0011, 00111, and 001111.\n\n\nFigure 10.15: Example of Division of Unsigned Binary Integers. The diagram shows the long division of 10010011 (Dividend) by 1011 (Divisor). The Quotient is 00001101 and the Remainder is 100. The process is shown in steps with partial remainders: 1001, 0011, 00111, and 001111.\n\n\n**Figure 10.15**\n   Example of Division of Unsigned Binary Integers\n\n\nFrom this point on, the division follows a cyclic pattern. At each cycle, additional bits from the dividend are appended to the partial remainder until the result is greater than or equal to the divisor. As before, the divisor is subtracted from this number to produce a new partial remainder. The process continues until all the bits of the dividend are exhausted.\n\n\nFigure 10.16 shows a machine algorithm that corresponds to the long division process. The divisor is placed in the M register, the dividend in the Q register. At\n\n\n\n\n![Figure 10.16: Flowchart for Unsigned Binary Division. The flowchart starts with initialization (A=0, M=Divisor, Q=Dividend, Count=n). It then enters a loop: Shift left A, Q; A = A - M. If A < 0, then Q0 = 0 and A = A + M. If A >= 0, then Q0 = 1. Then Count = Count - 1. If Count = 0, then END (Quotient in Q, Remainder in A).](images/image_0167.jpeg)\n\n\ngraph TD\n    START([START]) --> Init[A ← 0\nM ← Divisor\nQ ← Dividend\nCount ← n]\n    Init --> Shift[Shift left\nA, Q]\n    Shift --> Sub[A ← A - M]\n    Sub --> Cond{A < 0?}\n    Cond -- No --> SetQ1[Q0 ← 1]\n    Cond -- Yes --> SetQ0[Q0 ← 0\nA ← A + M]\n    SetQ1 --> DecCount[Count ← Count - 1]\n    SetQ0 --> DecCount\n    DecCount --> Cond2{Count = 0?}\n    Cond2 -- No --> Shift\n    Cond2 -- Yes --> END([END])\n    END --> Result[Quotient in Q\nRemainder in A]\n  \nFigure 10.16: Flowchart for Unsigned Binary Division. The flowchart starts with initialization (A=0, M=Divisor, Q=Dividend, Count=n). It then enters a loop: Shift left A, Q; A = A - M. If A < 0, then Q0 = 0 and A = A + M. If A >= 0, then Q0 = 1. Then Count = Count - 1. If Count = 0, then END (Quotient in Q, Remainder in A).\n\n\n**Figure 10.16**\n   Flowchart for Unsigned Binary Division\n\n\n\nA | Q | \n0000 | 0111 | Initial value\n0000\n      \n\n       1101\n      \n\n      1101\n      \n      0000 | 1110 | Shift\n      \n      Use twos complement of 0011 for subtraction\n      \n      Subtract\n      \n      Restore, set\n      \n       Q_0 = 0\n0001\n      \n\n       1101\n      \n\n      1110\n      \n      0001 | 1100 | Shift\n      \n      Subtract\n      \n      Restore, set\n      \n       Q_0 = 0\n0011\n      \n\n       1101\n      \n\n      0000 | 1000 | Shift\n      \n      Subtract, set\n      \n       Q_0 = 1\n0001\n      \n\n       1101\n      \n\n      1110\n      \n      0001 | 0010 | Shift\n      \n      Subtract\n      \n      Restore, set\n      \n       Q_0 = 0\n\n\n**Figure 10.17**\n   Example of Restoring Twos Complement Division (7/3)\n\n\neach step, the A and Q registers together are shifted to the left 1 bit. M is subtracted from A to determine whether A divides the partial remainder.\n   \n    3\n   \n   If it does, then\n   \n    Q_0\n   \n   gets a 1 bit. Otherwise,\n   \n    Q_0\n   \n   gets a 0 bit and M must be added back to A to restore the previous value. The count is then decremented, and the process continues for\n   \n    n\n   \n   steps. At the end, the quotient is in the Q register and the remainder is in the A register.\n\n\nThis process can, with some difficulty, be extended to negative numbers. We give here one approach for twos complement numbers. An example of this approach is shown in Figure 10.17.\n\n\nThe algorithm assumes that the divisor\n   \n    V\n   \n   and the dividend\n   \n    D\n   \n   are positive and that\n   \n    |V| < |D|\n   \n   . If\n   \n    |V| = |D|\n   \n   , then the quotient\n   \n    Q = 1\n   \n   and the remainder\n   \n    R = 0\n   \n   . If\n   \n    |V| > |D|\n   \n   , then\n   \n    Q = 0\n   \n   and\n   \n    R = D\n   \n   . The algorithm can be summarized as follows:\n\n\n  * 1. Load the twos complement of the divisor into the M register; that is, the M register contains the negative of the divisor. Load the dividend into the A, Q registers. The dividend must be expressed as a\n    \n     2n\n    \n    -bit positive number. Thus, for example, the 4-bit 0111 becomes 00000111.\n  * 2. Shift A, Q left 1 bit position.\n  * 3. Perform\n    \n     A \\leftarrow A - M\n    \n    . This operation subtracts the divisor from the contents of A.\n  * 4.\n      * a. If the result is nonnegative (most significant bit of A = 0), then set\n      \n       Q_0 \\leftarrow 1\n      \n      .\n  * b. If the result is negative (most significant bit of A = 1), then set\n      \n       Q_0 \\leftarrow 0\n      \n      and restore the previous value of A.\n  * 5. Repeat steps 2 through 4 as many times as there are bit positions in Q.\n  * 6. The remainder is in A and the quotient is in Q.\n\n\n3\n   \n   This is subtraction of unsigned integers. A result that requires a borrow out of the most significant bit is a negative result.\n\n\nTo deal with negative numbers, we recognize that the remainder is defined by\n\n\nD = Q \\times V + R\n\n\nThat is, the remainder is the value of\n   \n    R\n   \n   needed for the preceding equation to be valid. Consider the following examples of integer division with all possible combinations of signs of\n   \n    D\n   \n   and\n   \n    V\n   \n   :\n\n\n\\begin{array}{llll} D = 7 & V = 3 & \\Rightarrow & Q = 2 \\quad R = 1 \\\\ D = 7 & V = -3 & \\Rightarrow & Q = -2 \\quad R = 1 \\\\ D = -7 & V = 3 & \\Rightarrow & Q = -2 \\quad R = -1 \\\\ D = -7 & V = -3 & \\Rightarrow & Q = 2 \\quad R = -1 \\end{array}\n\n\nThe reader will note from Figure 10.17 that\n   \n    (-7)/(3)\n   \n   and\n   \n    (7)/(-3)\n   \n   produce different remainders. We see that the magnitudes of\n   \n    Q\n   \n   and\n   \n    R\n   \n   are unaffected by the input signs and that the signs of\n   \n    Q\n   \n   and\n   \n    R\n   \n   are easily derivable from the signs of\n   \n    D\n   \n   and\n   \n    V\n   \n   . Specifically,\n   \n    \\text{sign}(R) = \\text{sign}(D)\n   \n   and\n   \n    \\text{sign}(Q) = \\text{sign}(D) \\times \\text{sign}(V)\n   \n   . Hence, one way to do twos complement division is to convert the operands into unsigned values and, at the end, to account for the signs by complementation where needed. This is the method of choice for the restoring division algorithm [PARH10]."
        },
        {
          "name": "Floating-Point Representation",
          "content": "**Principles**\n\n\nWith a fixed-point notation (e.g., twos complement) it is possible to represent a range of positive and negative integers centered on or near 0. By assuming a fixed binary or radix point, this format allows the representation of numbers with a fractional component as well.\n\n\nThis approach has limitations. Very large numbers cannot be represented, nor can very small fractions. Furthermore, the fractional part of the quotient in a division of two large numbers could be lost.\n\n\nFor decimal numbers, we get around this limitation by using scientific notation. Thus, 976,000,000,000,000 can be represented as\n   \n    9.76 \\times 10^{14}\n   \n   , and 0.0000000000000976 can be represented as\n   \n    9.76 \\times 10^{-14}\n   \n   . What we have done, in effect, is dynamically to slide the decimal point to a convenient location and use the exponent of 10 to keep track of that decimal point. This allows a range of very large and very small numbers to be represented with only a few digits.\n\n\nThis same approach can be taken with binary numbers. We can represent a number in the form\n\n\n\\pm S \\times B^{\\pm E}\n\n\nThis number can be stored in a binary word with three fields:\n\n\n  * ■ Sign: plus or minus\n  * ■ Significand\n    \n     S\n  * ■ Exponent\n    \n     E\n\n\n\n\n![Diagram of a 32-bit floating-point format. It shows a 32-bit word divided into three fields: a 1-bit 'Sign of significand' field, an 8-bit 'Biased exponent' field, and a 23-bit 'Significand' field. Arrows indicate the bit lengths for each field.](images/image_0168.jpeg)\n\n\nDiagram of a 32-bit floating-point format. It shows a 32-bit word divided into three fields: a 1-bit 'Sign of significand' field, an 8-bit 'Biased exponent' field, and a 23-bit 'Significand' field. Arrows indicate the bit lengths for each field.\n\n\n(a) Format\n\n\n\n1.1010001 \\times 2^{10100} | = 0 10010011 101000100000000000000000 | =\n      \n       1.6328125 \\times 2^{20}\n-1.1010001 \\times 2^{10100} | = 1 10010011 101000100000000000000000 | =\n      \n       -1.6328125 \\times 2^{20}\n1.1010001 \\times 2^{-10100} | = 0 01101011 101000100000000000000000 | =\n      \n       1.6328125 \\times 2^{-20}\n-1.1010001 \\times 2^{-10100} | = 1 01101011 101000100000000000000000 | =\n      \n       -1.6328125 \\times 2^{-20}\n\n\n(b) Examples\n\n\n**Figure 10.18**\nThe\n   **base**\n\n    B\n   \n   is implicit and need not be stored because it is the same for all numbers. Typically, it is assumed that the radix point is to the right of the leftmost, or most significant, bit of the significand. That is, there is one bit to the left of the radix point.\n\n\nThe principles used in representing binary floating-point numbers are best explained with an example. Figure 10.18a shows a typical 32-bit floating-point format. The leftmost bit stores the\n   **sign**\n   of the number (0 = positive, 1 = negative). The\n   **exponent**\n   value is stored in the next 8 bits. The representation used is known as a\n   **biased representation**\n   . A fixed value, called the bias, is subtracted from the field to get the true exponent value. Typically, the bias equals\n   \n    (2^{k-1} - 1)\n   \n   , where\n   \n    k\n   \n   is the number of bits in the binary exponent. In this case, the 8-bit field yields the numbers 0 through 255. With a bias of 127 (\n   \n    2^7 - 1\n   \n   ), the true exponent values are in the range\n   \n    -127\n   \n   to\n   \n    +128\n   \n   . In this example, the base is assumed to be 2.\n\n\nTable 10.2 shows the biased representation for 4-bit integers. Note that when the bits of a biased representation are treated as unsigned integers, the relative magnitudes of the numbers do not change. For example, in both biased and unsigned representations, the largest number is 1111 and the smallest number is 0000. This is not true of sign-magnitude or twos complement representation. An advantage of biased representation is that nonnegative floating-point numbers can be treated as integers for comparison purposes.\n\n\nThe final portion of the word (23 bits in this case) is the\n   **significand**\n   .\n   \n    4\n\n\nAny floating-point number can be expressed in many ways.\n\n\nThe following are equivalent, where the significand is expressed in binary form:\n\n\n\\begin{aligned} 0.110 \\times 2^5 \\\\ 110 \\times 2^2 \\\\ 0.0110 \\times 2^6 \\end{aligned}\n\n\nTo simplify operations on floating-point numbers, it is typically required that they be normalized. A\n   **normal number**\n   is one in which the most significant digit of the\n\n\n4\n   \n   The term\n   **mantissa**\n   , sometimes used instead of\n   *significand*\n   , is considered obsolete.\n   *Mantissa*\n   also means “the fractional part of a logarithm,” so is best avoided in this context.\n\n\nsignificand is nonzero. For base 2 representation, a normal number is therefore one in which the most significant bit of the significand is one. As was mentioned, the typical convention is that there is one bit to the left of the radix point. Thus, a normal nonzero number is one in the form\n\n\n\\pm 1.bbb \\dots b \\times 2^{\\pm E}\n\n\nwhere\n   \n    b\n   \n   is either binary digit (0 or 1). Because the most significant bit is always one, it is unnecessary to store this bit; rather, it is implicit. Thus, the 23-bit field is used to store a 24-bit significand with a value in the half open interval\n   \n    [1, 2)\n   \n   . Given a number that is not normal, the number may be normalized by shifting the radix point to the right of the leftmost 1 bit and adjusting the exponent accordingly.\n\n\nFigure 10.18b gives some examples of numbers stored in this format. For each example, on the left is the binary number; in the center is the corresponding bit pattern; on the right is the decimal value. Note the following features:\n\n\n  * ■ The sign is stored in the first bit of the word.\n  * ■ The first bit of the true significand is always 1 and need not be stored in the significand field.\n  * ■ The value 127 is added to the true exponent to be stored in the exponent field.\n  * ■ The base is 2.\n\n\nFor comparison, Figure 10.19 indicates the range of numbers that can be represented in a 32-bit word. Using twos complement integer representation, all of the integers from\n   \n    -2^{31}\n   \n   to\n   \n    2^{31} - 1\n   \n   can be represented, for a total of\n   \n    2^{32}\n   \n   different numbers. With the example floating-point format of Figure 10.18, the following ranges of numbers are possible:\n\n\n  * ■ Negative numbers between\n    \n     -(2 - 2^{-23}) \\times 2^{128}\n    \n    and\n    \n     -2^{-127}\n  * ■ Positive numbers between\n    \n     2^{-127}\n    \n    and\n    \n     (2 - 2^{-23}) \\times 2^{128}\n\n\n\n\n![Figure 10.19: Expressible Numbers in Typical 32-Bit Formats. (a) Twos complement integers: A number line from -2^31 to 2^31 - 1 with a bracket labeled 'Expressible integers' covering the entire range. (b) Floating-point numbers: A number line with regions for 'Negative overflow', 'Expressible negative numbers', 'Zero', 'Expressible positive numbers', and 'Positive overflow'. The 'Expressible negative numbers' region starts at -2^127 and ends at -(2 - 2^-23) * 2^128. The 'Expressible positive numbers' region starts at 2^-127 and ends at (2 - 2^-23) * 2^128. The 'Zero' point is at 0.](images/image_0169.jpeg)\n\n\nExpressible integers\n\n\nNumber line\n\n\n-2^{31}\n    \n    0\n    \n     2^{31} - 1\n\n\n(a) Twos complement integers\n\n\nNegative underflow Positive underflow\n\n\nExpressible negative numbers Zero Expressible positive numbers Positive overflow\n\n\nNumber line\n\n\n-(2 - 2^{-23}) \\times 2^{128}\n    \n\n     -2^{-127}\n    \n    0\n    \n     2^{-127}\n    \n\n     (2 - 2^{-23}) \\times 2^{128}\n\n\n(b) Floating-point numbers\n\n\nFigure 10.19: Expressible Numbers in Typical 32-Bit Formats. (a) Twos complement integers: A number line from -2^31 to 2^31 - 1 with a bracket labeled 'Expressible integers' covering the entire range. (b) Floating-point numbers: A number line with regions for 'Negative overflow', 'Expressible negative numbers', 'Zero', 'Expressible positive numbers', and 'Positive overflow'. The 'Expressible negative numbers' region starts at -2^127 and ends at -(2 - 2^-23) * 2^128. The 'Expressible positive numbers' region starts at 2^-127 and ends at (2 - 2^-23) * 2^128. The 'Zero' point is at 0.\n\n\n**Figure 10.19**\n   Expressible Numbers in Typical 32-Bit Formats\n\n\nFive regions on the number line are not included in these ranges:\n\n\n  * ■ Negative numbers less than\n    \n     -(2 - 2^{-23}) \\times 2^{128}\n    \n    , called\n    **negative overflow**\n  * ■ Negative numbers greater than\n    \n     2^{-127}\n    \n    , called\n    **negative underflow**\n  * ■ Zero\n  * ■ Positive numbers less than\n    \n     2^{-127}\n    \n    , called\n    **positive underflow**\n  * ■ Positive numbers greater than\n    \n     (2 - 2^{-23}) \\times 2^{128}\n    \n    , called\n    **positive overflow**\n\n\nThe representation as presented will not accommodate a value of 0. However, as we shall see, actual floating-point representations include a special bit pattern to designate zero. Overflow occurs when an arithmetic operation results in an absolute value greater than can be expressed with an exponent of 128 (e.g.,\n   \n    2^{120} \\times 2^{100} = 2^{220}\n   \n   ). Underflow occurs when the fractional magnitude is too small (e.g.,\n   \n    2^{-120} \\times 2^{-100} = 2^{-220}\n   \n   ). Underflow is a less serious problem because the result can generally be satisfactorily approximated by 0.\n\n\nIt is important to note that we are not representing more individual values with floating-point notation. The maximum number of different values that can be represented with 32 bits is still\n   \n    2^{32}\n   \n   . What we have done is to spread those numbers out in two ranges, one positive and one negative. In practice, most floating-point numbers that one would wish to represent are represented only approximately. However, for moderate sized integers, the representation is exact.\n\n\nAlso, note that the numbers represented in floating-point notation are not spaced evenly along the number line, as are fixed-point numbers. The possible values get closer together near the origin and farther apart as you move away, as shown in Figure 10.20. This is one of the trade-offs of floating-point math: Many calculations produce results that are not exact and have to be rounded to the nearest value that the notation can represent.\n\n\nIn the type of format depicted in Figure 10.18, there is a trade-off between range and precision. The example shows 8 bits devoted to the exponent and 23 to the significand. If we increase the number of bits in the exponent, we expand the range of expressible numbers. But because only a fixed number of different values can be expressed, we have reduced the density of those numbers and therefore the precision. The only way to increase both range and precision is to use more bits. Thus, most computers offer, at least, single-precision numbers and double-precision numbers. For example, a processor could support a single-precision format of 64 bits, and a double-precision format of 128 bits.\n\n\nSo there is a trade-off between the number of bits in the exponent and the number of bits in the significand. But it is even more complicated than that. The implied base of the exponent need not be 2. The IBM S/390 architecture, for example, uses a base of 16 [ANDE67b]. The format consists of a 7-bit exponent and a 24-bit significand.\n\n\n\n\n![Figure 10.20: Density of Floating-Point Numbers. A number line with points -n, 0, n, 2n, and 4n. The interval from -n to 0 has many closely spaced tick marks, while the interval from 0 to 4n has fewer, more widely spaced tick marks, illustrating that floating-point numbers are more densely packed near zero.](images/image_0170.jpeg)\n\n\nFigure 10.20: Density of Floating-Point Numbers. A number line with points -n, 0, n, 2n, and 4n. The interval from -n to 0 has many closely spaced tick marks, while the interval from 0 to 4n has fewer, more widely spaced tick marks, illustrating that floating-point numbers are more densely packed near zero.\n\n\n**Figure 10.20**\n   Density of Floating-Point Numbers\n\n\nIn the IBM base-16 format,\n\n\n0.11010001 \\times 2^{10100} = 0.11010001 \\times 16^{101}\n\n\nand the exponent is stored to represent 5 rather than 20.\n\n\nThe advantage of using a larger exponent is that a greater range can be achieved for the same number of exponent bits. But remember, we have not increased the number of different values that can be represented. Thus, for a fixed format, a larger exponent base gives a greater range at the expense of less precision.\n\n\n\n\n**IEEE Standard for Binary Floating-Point Representation**\n\n\nThe most important floating-point representation is defined in IEEE Standard 754, adopted in 1985 and revised in 2008. This standard was developed to facilitate the portability of programs from one processor to another and to encourage the development of sophisticated, numerically oriented programs. The standard has been widely adopted and is used on virtually all contemporary processors and arithmetic coprocessors. IEEE 754-2008 covers both binary and decimal floating-point representations. In this chapter, we deal only with binary representations.\n\n\nIEEE 754-2008 defines the following different types of floating-point formats:\n\n\n  * ■\n    **Arithmetic format:**\n    All the mandatory operations defined by the standard are supported by the format. The format may be used to represent floating-point operands or results for the operations described in the standard.\n  * ■\n    **Basic format:**\n    This format covers five floating-point representations, three binary and two decimal, whose encodings are specified by the standard, and which can be used for arithmetic. At least one of the basic formats is implemented in any conforming implementation.\n  * ■\n    **Interchange format:**\n    A fully specified, fixed-length binary encoding that allows data interchange between different platforms and that can be used for storage.\n\n\nThe three basic binary formats have bit lengths of 32, 64, and 128 bits, with exponents of 8, 11, and 15 bits, respectively (Figure 10.21). Table 10.3 summarizes the characteristics of the three formats. The two basic decimal formats have bit lengths of 64 and 128 bits. All of the basic formats are also arithmetic format types (can be used for arithmetic operations) and interchange format types (platform independent).\n\n\nSeveral other formats are specified in the standard. The binary16 format is only an interchange format and is intended for storage of values when higher precision is not required. The binary{k} format and the decimal{k} format are interchange formats with total length\n   \n    k\n   \n   bits and with defined lengths for the significand and exponent. The format must be a multiple of 32 bits; thus formats are defined for\n   \n    k = 160, 192\n   \n   , and so on. These two families of formats are also arithmetic formats.\n\n\nIn addition, the standard defines\n   **extended precision formats**\n   , which extend a supported basic format by providing additional bits in the exponent (extended range) and in the significand (extended precision). The exact format\n\n\n\n\n![Diagram illustrating the IEEE 754 floating-point formats: (a) Binary32, (b) Binary64, and (c) Binary128. Each format is shown as a horizontal bar divided into fields. (a) Binary32: 8 bits for Sign bit and Biased exponent, 23 bits for Trailing significant field. (b) Binary64: 11 bits for Sign bit and Biased exponent, 52 bits for Trailing significant field. (c) Binary128: 15 bits for Sign bit and Biased exponent, 112 bits for Trailing significant field.](images/image_0171.jpeg)\n\n\n(a) Binary32 format\n\n\n(b) Binary64 format\n\n\n(c) Binary128 format\n\n\nDiagram illustrating the IEEE 754 floating-point formats: (a) Binary32, (b) Binary64, and (c) Binary128. Each format is shown as a horizontal bar divided into fields. (a) Binary32: 8 bits for Sign bit and Biased exponent, 23 bits for Trailing significant field. (b) Binary64: 11 bits for Sign bit and Biased exponent, 52 bits for Trailing significant field. (c) Binary128: 15 bits for Sign bit and Biased exponent, 112 bits for Trailing significant field.\n\n\n**Figure 10.21**\nis implementation dependent, but the standard places certain constraints on the length of the exponent and significand. These formats are arithmetic format types but not interchange format types. The extended formats are to be used for intermediate calculations. With their greater precision, the extended formats lessen the\n\n\n**Table 10.3**\n\nParameter | Format\nBinary32 | Binary64 | Binary128\nStorage width (bits) | 32 | 64 | 128\nExponent width (bits) | 8 | 11 | 15\nExponent bias | 127 | 1023 | 16383\nMaximum exponent | 127 | 1023 | 16383\nMinimum exponent | -126 | -1022 | -16382\nApprox normal number range (base 10) | 10^{-38}, 10^{+38} | 10^{-308}, 10^{+308} | 10^{-4932}, 10^{+4932}\nTrailing significant width (bits)* | 23 | 52 | 112\nNumber of exponents | 254 | 2046 | 32766\nNumber of fractions | 2^{23} | 2^{52} | 2^{112}\nNumber of values | 1.98 \\times 2^{31} | 1.99 \\times 2^{63} | 1.99 \\times 2^{128}\nSmallest positive normal number | 2^{-126} | 2^{-1022} | 2^{-16362}\nLargest positive normal number | 2^{128} - 2^{104} | 2^{1024} - 2^{971} | 2^{16384} - 2^{16271}\nSmallest subnormal magnitude | 2^{-149} | 2^{-1074} | 2^{-16494}\n\n\nNote: * Not including implied bit and not including sign bit.\n\n\nchance of a final result that has been contaminated by excessive roundoff error; with their greater range, they also lessen the chance of an intermediate overflow aborting a computation whose final result would have been representable in a basic format. An additional motivation for the extended format is that it affords some of the benefits of a larger basic format without incurring the time penalty usually associated with higher precision.\n\n\nFinally, IEEE 754-2008 defines an\n   **extendable precision format**\n   as a format with a precision and range that are defined under user control. Again, these formats may be used for intermediate calculations, but the standard places no constraint on format or length.\n\n\nTable 10.4 shows the relationship between defined formats and format types.\n\n\nNot all bit patterns in the IEEE formats are interpreted in the usual way; instead, some bit patterns are used to represent special values. Table 10.5 indicates the values assigned to various bit patterns. The exponent values of all zeros (0 bits) and all ones (1 bits) define special values. The following classes of numbers are represented:\n\n\n  * ■ For exponent values in the range of 1 through 254 for 32-bit format, 1 through 2046 for 64-bit format, and 1 through 16382, normal nonzero floating-point numbers are represented. The exponent is biased, so that the range of exponents is\n    \n     -126\n    \n    through\n    \n     +127\n    \n    for 32-bit format, and so on. A normal number requires a 1 bit to the left of the binary point; this bit is implied, giving an effective 24-bit, 53-bit, or 113-bit significand. Because one of the bits is implied, the corresponding field in the binary format is referred to as the\n    **trailing significand field**\n    .\n  * ■ An exponent of zero together with a fraction of zero represents positive or negative zero, depending on the sign bit. As was mentioned, it is useful to have an exact value of 0 represented.\n\n\n**Table 10.4**\n   IEEE Formats\n\n\n\nFormat | Format Type\nArithmetic Format | Basic Format | Interchange Format\nbinary16 |  |  | X\nbinary32 | X | X | X\nbinary64 | X | X | X\nbinary128 | X | X | X\nbinary{k}\n      \n      (\n      \n       k = n \\times 32\n      \n      for\n      \n       n > 4\n      \n      ) | X |  | X\ndecimal64 | X | X | X\ndecimal128 | X | X | X\ndecimal{k}\n      \n      (\n      \n       k = n \\times 32\n      \n      for\n      \n       n > 4\n      \n      ) | X |  | X\nextended precision | X |  | \nextendable precision | X |  | \n\n\n**Table 10.5**\n\n\n**(a) binary32 format**\n\n\n\n | Sign | Biased Exponent | Fraction | Value\npositive zero | 0 | 0 | 0 | 0\nnegative zero | 1 | 0 | 0 | -0\nplus infinity | 0 | all 1s | 0 | \\infty\nminus infinity | 1 | all 1s | 0 | -\\infty\nquiet NaN | 0 or 1 | all 1s | \\neq 0\n      \n      ; first bit = 1 | qNaN\nsignaling NaN | 0 or 1 | all 1s | \\neq 0\n      \n      ; first bit = 0 | sNaN\npositive normal nonzero | 0 | 0 < e < 225 | f | 2^{e-127}(1.f)\nnegative normal nonzero | 1 | 0 < e < 225 | f | -2^{e-127}(1.f)\npositive subnormal | 0 | 0 | f \\neq 0 | 2^{e-126}(0.f)\nnegative subnormal | 1 | 0 | f \\neq 0 | -2^{e-126}(0.f)\n\n\n\n\n**(b) binary64 format**\n\n\n\n | Sign | Biased Exponent | Fraction | Value\npositive zero | 0 | 0 | 0 | 0\nnegative zero | 1 | 0 | 0 | -0\nplus infinity | 0 | all 1s | 0 | \\infty\nminus infinity | 1 | all 1s | 0 | -\\infty\nquiet NaN | 0 or 1 | all 1s | \\neq 0\n      \n      ; first bit = 1 | qNaN\nsignaling NaN | 0 or 1 | all 1s | \\neq 0\n      \n      ; first bit = 0 | sNaN\npositive normal nonzero | 0 | 0 < e < 2047 | f | 2^{e-1023}(1.f)\nnegative normal nonzero | 1 | 0 < e < 2047 | f | -2^{e-1023}(1.f)\npositive subnormal | 0 | 0 | f \\neq 0 | 2^{e-1022}(0.f)\nnegative subnormal | 1 | 0 | f \\neq 0 | -2^{e-1022}(0.f)\n\n\n\n\n**(c) binary128 format**\n\n\n\n | Sign | Biased Exponent | Fraction | Value\npositive zero | 0 | 0 | 0 | 0\nnegative zero | 1 | 0 | 0 | -0\nplus infinity | 0 | all 1s | 0 | \\infty\nminus infinity | 1 | all 1s | 0 | -\\infty\nquiet NaN | 0 or 1 | all 1s | \\neq 0\n      \n      ; first bit = 1 | qNaN\nsignaling NaN | 0 or 1 | all 1s | \\neq 0\n      \n      ; first bit = 0 | sNaN\npositive normal nonzero | 0 | all 1s | f | 2^{e-16383}(1.f)\nnegative normal nonzero | 1 | all 1s | f | -2^{e-16383}(1.f)\npositive subnormal | 0 | 0 | f \\neq 0 | 2^{e-16383}(0.f)\nnegative subnormal | 1 | 0 | f \\neq 0 | -2^{e-16383}(0.f)\n\n\n  * ■ An exponent of all ones together with a fraction of zero represents positive or negative infinity, depending on the sign bit. It is also useful to have a representation of infinity. This leaves it up to the user to decide whether to treat overflow as an error condition or to carry the value\n    \n     \\infty\n    \n    and proceed with whatever program is being executed.\n  * ■ An exponent of zero together with a nonzero fraction represents a subnormal number. In this case, the bit to the left of the binary point is zero and the true exponent is\n    \n     -126\n    \n    or\n    \n     -1022\n    \n    . The number is positive or negative depending on the sign bit.\n  * ■ An exponent of all ones together with a nonzero fraction is given the value NaN, which means\n    *Not a Number*\n    , and is used to signal various exception conditions.\n\n\nThe significance of subnormal numbers and NaNs is discussed in Section 10.5."
        },
        {
          "name": "Floating-Point Arithmetic",
          "content": "Table 10.6 summarizes the basic operations for floating-point arithmetic. For addition and subtraction, it is necessary to ensure that both operands have the same exponent value. This may require shifting the radix point on one of the operands to achieve alignment. Multiplication and division are more straightforward.\n\n\nA floating-point operation may produce one of these conditions:\n\n\n  * ■\n    **Exponent overflow:**\n    A positive exponent exceeds the maximum possible exponent value. In some systems, this may be designated as\n    \n     +\\infty\n    \n    or\n    \n     -\\infty\n    \n    .\n  * ■\n    **Exponent underflow:**\n    A negative exponent is less than the minimum possible exponent value (e.g.,\n    \n     -200\n    \n    is less than\n    \n     -127\n    \n    ). This means that the number is too small to be represented, and it may be reported as 0.\n\n\n**Table 10.6**\n   Floating-Point Numbers and Arithmetic Operations\n\n\n\nFloating-Point Numbers | Arithmetic Operations\nX = X_S \\times B^{X_E}\n      \n\n\n       Y = Y_S \\times B^{Y_E} | X + Y = (X_S \\times B^{X_E - Y_E} + Y_S) \\times B^{Y_E}\n      \n\n\n       X - Y = (X_S \\times B^{X_E - Y_E} - Y_S) \\times B^{Y_E}\n      \n\n\n       X \\times Y = (X_S \\times Y_S) \\times B^{X_E + Y_E}\n      \n\n\n       \\frac{X}{Y} = \\left( \\frac{X_S}{Y_S} \\right) \\times B^{X_E - Y_E}\n\n\nExamples:\n\n\nX = 0.3 \\times 10^2 = 30\n\n\nY = 0.2 \\times 10^3 = 200\n\n\nX + Y = (0.3 \\times 10^{2-3} + 0.2) \\times 10^3 = 0.23 \\times 10^3 = 230\n\n\nX - Y = (0.3 \\times 10^{2-3} - 0.2) \\times 10^3 = (-0.17) \\times 10^3 = -170\n\n\nX \\times Y = (0.3 \\times 0.2) \\times 10^{2+3} = 0.06 \\times 10^5 = 6000\n\n\nX \\div Y = (0.3 \\div 0.2) \\times 10^{2-3} = 1.5 \\times 10^{-1} = 0.15\n\n\n  * ■\n    **Significand underflow:**\n    In the process of aligning significands, digits may flow off the right end of the significand. As we will discuss, some form of rounding is required.\n  * ■\n    **Significand overflow:**\n    The addition of two significands of the same sign may result in a carry out of the most significant bit. This can be fixed by realignment, as we will explain.\n\n\n\n\n**Addition and Subtraction**\n\n\nIn floating-point arithmetic, addition and subtraction are more complex than multiplication and division. This is because of the need for alignment. There are four basic phases of the algorithm for addition and subtraction:\n\n\n  * 1. Check for zeros.\n  * 2. Align the significands.\n  * 3. Add or subtract the significands.\n  * 4. Normalize the result.\n\n\nA typical flowchart is shown in Figure 10.22. A step-by-step narrative highlights the main functions required for floating-point addition and subtraction. We assume a format similar to those of Figure 10.21. For the addition or subtraction operation, the two operands must be transferred to registers that will be used by the ALU. If the floating-point format includes an implicit significand bit, that bit must be made explicit for the operation.\n\n\n**Phase 1. Zero check:**\n   Because addition and subtraction are identical except for a sign change, the process begins by changing the sign of the subtrahend if it is a subtract operation. Next, if either operand is 0, the other is reported as the result.\n\n\n**Phase 2. Significand alignment:**\n   The next phase is to manipulate the numbers so that the two exponents are equal.\n\n\nTo see the need for aligning exponents, consider the following decimal addition:\n\n\n(123 \\times 10^0) + (456 \\times 10^{-2})\n\n\nClearly, we cannot just add the significands. The digits must first be set into equivalent positions, that is, the 4 of the second number must be aligned with the 3 of the first. Under these conditions, the two exponents will be equal, which is the mathematical condition under which two numbers in this form can be added. Thus,\n\n\n(123 \\times 10^0) + (456 \\times 10^{-2}) = (123 \\times 10^0) + (4.56 \\times 10^0) = 127.56 \\times 10^0\n\n\nAlignment may be achieved by shifting either the smaller number to the right (increasing its exponent) or shifting the larger number to the left. Because either operation may result in the loss of digits, it is the smaller number that is shifted; any digits that are lost are therefore of relatively small significance. The alignment\n\n\n\n\n![Flowchart for Floating-Point Addition and Subtraction (Z ← X ± Y).](images/image_0172.jpeg)\n\n\ngraph TD\n    ADD([ADD]) --> X0{\"X = 0?\"}\n    SUBTRACT([SUBTRACT]) --> ChangeSign[Change sign of Y]\n    ChangeSign --> X0\n    X0 -- Yes --> ZY[Z ← Y]\n    X0 -- No --> Y0{\"Y = 0?\"}\n    Y0 -- Yes --> ZX[Z ← X]\n    Y0 -- No --> Exponents{\"Exponents equal?\"}\n    ZY --> RETURN1([RETURN])\n    ZX --> RETURN1\n    Exponents -- Yes --> AddSignificands[Add signed significands]\n    Exponents -- No --> IncrementExponent[Increment smaller exponent]\n    AddSignificands --> Signif0{\"Significand = 0?\"}\n    Signif0 -- Yes --> Z0[Z ← 0]\n    Z0 --> RETURN2([RETURN])\n    Signif0 -- No --> SignifOverflow{\"Significand overflow?\"}\n    IncrementExponent --> ShiftRight[Shift significant right]\n    ShiftRight --> Signif0_2{\"Significand = 0?\"}\n    SignifOverflow -- Yes --> ShiftRight2[Shift significant right]\n    SignifOverflow -- No --> DecrementExponent[Decrement exponent]\n    Signif0_2 -- Yes --> PutOther[Put other number in Z]\n    Signif0_2 -- No --> ExponentOverflow{\"Exponent overflow?\"}\n    PutOther --> RETURN3([RETURN])\n    ShiftRight2 --> IncrementExponent2[Increment exponent]\n    IncrementExponent2 --> ExponentOverflow\n    DecrementExponent --> Underflow{\"Exponent underflow?\"}\n    Underflow -- Yes --> ReportUnderflow[Report underflow]\n    Underflow -- No --> Exponents\n    ExponentOverflow -- Yes --> ReportOverflow[Report overflow]\n    ExponentOverflow -- No --> Exponents\n    ReportOverflow --> RETURN4([RETURN])\n    ReportUnderflow --> RETURN5([RETURN])\n    Exponents --> Normalized{\"Results normalized?\"}\n    Normalized -- Yes --> Round[Round result]\n    Round --> RETURN6([RETURN])\n    Normalized -- No --> DecrementExponent\n  \nThe flowchart illustrates the algorithm for floating-point addition and subtraction, calculating\n    \n     Z \\leftarrow X \\pm Y\n    \n    . It starts with an 'ADD' operation, which branches into two main paths: one for 'SUBTRACT' and one for 'ADD'. The 'SUBTRACT' path first changes the sign of Y, then proceeds to the main logic. The main logic begins by checking if X is zero. If yes, Z is set to Y and the process returns. If X is not zero, it checks if Y is zero. If yes, Z is set to X and the process returns. If neither X nor Y is zero, the exponents are compared. If they are equal, the signed significands are added. If the result is zero, Z is set to 0 and the process returns. If the result is non-zero, it checks for overflow in the significand. If overflow occurs, the process returns. If no overflow, it checks if the result is normalized. If normalized, it rounds the result and returns. If not normalized, it shifts the significand left and decrements the exponent. It then checks for underflow in the exponent. If underflow occurs, it reports underflow and returns. If no underflow, it loops back to check if the results are normalized. If the exponents are not equal, the smaller exponent is incremented, the significand is shifted right, and the process loops back to check if the significand is zero. If the significand is zero, the other number is put into Z and the process returns. If the significand is non-zero, it checks for overflow in the significand. If overflow occurs, the process returns. If no overflow, it checks if the exponent is overflowing. If it is, the process reports overflow and returns. If the exponent is not overflowing, it loops back to check if the exponents are equal.\n\n\nFlowchart for Floating-Point Addition and Subtraction (Z ← X ± Y).\n\n\n**Figure 10.22**\n   Floating-Point Addition and Subtraction (\n   \n    Z \\leftarrow X \\pm Y\n   \n   )\n\n\nis achieved by repeatedly shifting the magnitude portion of the significand right 1 digit and incrementing the exponent until the two exponents are equal. (Note that if the implied base is 16, a shift of 1 digit is a shift of 4 bits.) If this process results in a 0 value for the significand, then the other number is reported as the result. Thus, if two numbers have exponents that differ significantly, the lesser number is lost.\n\n\n**Phase 3. Addition:**\n   Next, the two significands are added together, taking into account their signs. Because the signs may differ, the result may be 0. There is also the possibility of significand overflow by 1 digit. If so, the significand of the result is shifted right and the exponent is incremented. An exponent overflow could occur as a result; this would be reported and the operation halted.\n\n\n**Phase 4. Normalization:**\n   The final phase normalizes the result. Normalization consists of shifting significand digits left until the most significant digit (bit, or 4 bits for base-16 exponent) is nonzero. Each shift causes a decrement of the exponent and thus could cause an exponent underflow. Finally, the result must be rounded off and then reported. We defer a discussion of rounding until after a discussion of multiplication and division.\n\n\n\n\n**Multiplication and Division**\n\n\nFloating-point multiplication and division are much simpler processes than addition and subtraction, as the following discussion indicates.\n\n\nWe first consider multiplication, illustrated in Figure 10.23. First, if either operand is 0, 0 is reported as the result. The next step is to add the exponents. If the exponents are stored in biased form, the exponent sum would have doubled the bias. Thus, the bias value must be subtracted from the sum. The result could be either an exponent overflow or underflow, which would be reported, ending the algorithm.\n\n\nIf the exponent of the product is within the proper range, the next step is to multiply the significands, taking into account their signs. The multiplication is performed in the same way as for integers. In this case, we are dealing with a sign-magnitude representation, but the details are similar to those for twos complement representation. The product will be double the length of the multiplier and multiplicand. The extra bits will be lost during rounding.\n\n\nAfter the product is calculated, the result is then normalized and rounded, as was done for addition and subtraction. Note that normalization could result in exponent underflow.\n\n\nFinally, let us consider the flowchart for division depicted in Figure 10.24. Again, the first step is testing for 0. If the divisor is 0, an error report is issued, or the result is set to infinity, depending on the implementation. A dividend of 0 results in 0. Next, the divisor exponent is subtracted from the dividend exponent. This removes the bias, which must be added back in. Tests are then made for exponent underflow or overflow.\n\n\nThe next step is to divide the significands. This is followed with the usual normalization and rounding.\n\n\n\n\n![Flowchart for Floating-Point Multiplication (Z ← X ± Y).](images/image_0173.jpeg)\n\n\ngraph TD\n    Start([MULTIPLY]) --> X0{X = 0?}\n    X0 -- Yes --> Z0[Z ← 0]\n    X0 -- No --> Y0{Y = 0?}\n    Y0 -- Yes --> Z0\n    Y0 -- No --> AddExponents[Add exponents]\n    AddExponents --> SubtractBias[Subtract bias]\n    SubtractBias --> Overflow{Exponent overflow?}\n    Overflow -- Yes --> ReportOverflow[Report overflow]\n    Overflow -- No --> Underflow{Exponent underflow?}\n    Underflow -- Yes --> ReportUnderflow[Report underflow]\n    Underflow -- No --> MultiplySignificands[Multiply significands]\n    MultiplySignificands --> Normalize[Normalize]\n    Normalize --> Round[Round]\n    Round --> Return([RETURN])\n    ReportOverflow --> Return\n    ReportUnderflow --> Return\n  \nThe flowchart illustrates the process of floating-point multiplication. It begins with a 'MULTIPLY' start node. The first decision is whether\n    \n     X = 0\n    \n    . If yes, the result\n    \n     Z\n    \n    is set to 0 and the process returns. If\n    \n     X \\neq 0\n    \n    , the next decision is whether\n    \n     Y = 0\n    \n    . If yes,\n    \n     Z\n    \n    is set to 0 and the process returns. If\n    \n     Y \\neq 0\n    \n    , the exponents of\n    \n     X\n    \n    and\n    \n     Y\n    \n    are added, and then the bias is subtracted from the result. The next decision is whether the resulting exponent overflows. If it does, an 'overflow' error is reported and the process returns. If it does not overflow, the next decision is whether the exponent underflows. If it does, an 'underflow' error is reported and the process returns. If it does not underflow, the significands of\n    \n     X\n    \n    and\n    \n     Y\n    \n    are multiplied, the result is normalized, and finally rounded to produce the final result\n    \n     Z\n    \n    .\n\n\nFlowchart for Floating-Point Multiplication (Z ← X ± Y).\n\n\n**Figure 10.23**\n   Floating-Point Multiplication (\n   \n    Z \\leftarrow X \\pm Y\n   \n   )\n\n\n\n\n**Precision Considerations**\n\n\n**GUARD BITS**\n   We mentioned that, prior to a floating-point operation, the exponent and significand of each operand are loaded into ALU registers. In the case of the significand, the length of the register is almost always greater than the length of the significand plus an implied bit. The register contains additional bits, called guard bits, which are used to pad out the right end of the significand with 0s.\n\n\nThe reason for the use of guard bits is illustrated in Figure 10.25. Consider numbers in the IEEE format, which has a 24-bit significand, including an implied 1 bit to the left of the binary point. Two numbers that are very close in value are\n   \n    x = 1.00 \\cdots 00 \\times 2^1\n   \n   and\n   \n    y = 1.11 \\cdots 11 \\times 2^0\n   \n   . If the smaller number is to be subtracted from the larger, it must be shifted right 1 bit to align the exponents. This is shown in Figure 10.25a. In the process,\n   \n    y\n   \n   loses 1 bit of significance; the result is\n   \n    2^{-22}\n   \n   . The same operation is repeated in\n\n\n\n\n![Flowchart for Floating-Point Division (Z ← X/Y).](images/image_0174.jpeg)\n\n\ngraph TD\n    DIVIDE([DIVIDE]) --> X0{X = 0?}\n    X0 -- Yes --> Z0[Z ← 0]\n    X0 -- No --> Y0{Y = 0?}\n    Y0 -- Yes --> ZInf[Z ← ∞]\n    Y0 -- No --> Sub[Subtract exponents]\n    Sub --> Add[Add bias]\n    Add --> Overflow{Exponent overflow?}\n    Overflow -- Yes --> ReportOverflow1[Report overflow]\n    Overflow -- No --> Underflow{Exponent underflow?}\n    Underflow -- Yes --> ReportUnderflow1[Report underflow]\n    Underflow -- No --> Divide[Divide significands]\n    Divide --> Normalize[Normalize]\n    Normalize --> Round[Round]\n    Round --> RETURN([RETURN])\n    ReportOverflow1 --> RETURN\n    ReportUnderflow1 --> RETURN\n    Z0 --> RETURN\n    ZInf --> RETURN\n  \nThe flowchart illustrates the algorithm for floating-point division. It starts with the 'DIVIDE' operation. If\n    \n     X = 0\n    \n    , the result\n    \n     Z\n    \n    is set to 0 and the process returns. If\n    \n     Y = 0\n    \n    , the result\n    \n     Z\n    \n    is set to infinity and the process returns. Otherwise, the exponents of\n    \n     X\n    \n    and\n    \n     Y\n    \n    are subtracted, and a bias is added to the result. If the resulting exponent overflows, an overflow error is reported and the process returns. If it underflows, an underflow error is reported and the process returns. If neither occurs, the significands of\n    \n     X\n    \n    and\n    \n     Y\n    \n    are divided, the result is normalized, and finally rounded to produce the final result\n    \n     Z\n    \n    .\n\n\nFlowchart for Floating-Point Division (Z ← X/Y).\n\n\n**Figure 10.24**\n   Floating-Point Division (\n   \n    Z \\leftarrow X/Y\n   \n   )\n\n\n\nx = 1.000\\dots.00 \\times 2^1\n     \n\n\n      \\underline{-y} = \\underline{0.111\\dots.11} \\times 2^1\n     \n\n\n      z = 0.000\\dots.01 \\times 2^1\n     \n\n\n      = 1.000\\dots.00 \\times 2^{-22} | x = .100000 \\times 16^1\n     \n\n\n      \\underline{-y} = \\underline{.0FFFFF} \\times 16^1\n     \n\n\n      z = .000001 \\times 16^1\n     \n\n\n      = .100000 \\times 16^{-4}\n\n\n(a) Binary example, without guard bits\n\n\n(c) Hexadecimal example, without guard bits\n\n\n\nx = 1.000\\dots.00 \\ 0000 \\times 2^1\n     \n\n\n      \\underline{-y} = \\underline{0.111\\dots.11} \\ 1000 \\times 2^1\n     \n\n\n      z = 0.000\\dots.00 \\ 1000 \\times 2^1\n     \n\n\n      = 1.000\\dots.00 \\ 0000 \\times 2^{-23} | x = .100000 \\ 00 \\times 16^1\n     \n\n\n      \\underline{-y} = \\underline{.0FFFFF} \\ F0 \\times 16^1\n     \n\n\n      z = .000000 \\ 10 \\times 16^1\n     \n\n\n      = .100000 \\ 00 \\times 16^{-5}\n\n\n(b) Binary example, with guard bits\n\n\n(d) Hexadecimal example, with guard bits\n\n\n**Figure 10.25**\n   The Use of Guard Bits\n\n\npart (b) with the addition of guard bits. Now the least significant bit is not lost due to alignment, and the result is\n   \n    2^{-23}\n   \n   , a difference of a factor of 2 from the previous answer. When the radix is 16, the loss of precision can be greater. As Figures 10.25c and (d) show, the difference can be a factor of 16.\n\n\n**ROUNDING**\n   Another detail that affects the precision of the result is the rounding policy. The result of any operation on the significands is generally stored in a longer register. When the result is put back into the floating-point format, the extra bits must be eliminated in such a way as to produce a result that is close to the exact result. This process is called\n   **rounding**\n   .\n\n\nA number of techniques have been explored for performing rounding. In fact, the IEEE standard lists four alternative approaches:\n\n\n  * ■\n    **Round to nearest:**\n    The result is rounded to the nearest representable number.\n  * ■\n    **Round toward\n     \n      +\\infty\n     \n     :**\n    The result is rounded up toward plus infinity.\n  * ■\n    **Round toward\n     \n      -\\infty\n     \n     :**\n    The result is rounded down toward negative infinity.\n  * ■\n    **Round toward 0:**\n    The result is rounded toward zero.\n\n\nLet us consider each of these policies in turn.\n   **Round to nearest**\n   is the default rounding mode listed in the standard and is defined as follows: The representable value nearest to the infinitely precise result shall be delivered.\n\n\nIf the extra bits, beyond the 23 bits that can be stored, are 10010, then the extra bits amount to more than one-half of the last representable bit position. In this case, the correct answer is to add binary 1 to the last representable bit, rounding up to the next representable number. Now consider that the extra bits are 01111. In this case, the extra bits amount to less than one-half of the last representable bit position. The correct answer is simply to drop the extra bits (truncate), which has the effect of rounding down to the next representable number.\n\n\nThe standard also addresses the special case of extra bits of the form 10000.... Here the result is exactly halfway between the two possible representable values. One possible technique here would be to always truncate, as this would be the simplest operation. However, the difficulty with this simple approach is that it introduces a small but cumulative bias into a sequence of computations. What is required is an unbiased method of rounding. One possible approach would be to round up or down on the basis of a random number so that, on average, the result would be unbiased. The argument against this approach is that it does not produce predictable, deterministic results. The approach taken by the IEEE standard is to force the result to be even: If the result of a computation is exactly midway between two representable numbers, the value is rounded up if the last representable bit is currently 1 and not rounded up if it is currently 0.\n\n\nThe next two options,\n   **rounding to plus**\n   and\n   **minus infinity**\n   , are useful in implementing a technique known as interval arithmetic. Interval arithmetic provides an efficient method for monitoring and controlling errors in floating-point computations by producing two values for each result. The two values correspond to the lower and upper endpoints of an interval that contains the true result. The width of the interval, which is the difference between the upper and lower endpoints, indicates the accuracy of the result. If the endpoints of an interval are not representable, then the interval endpoints are rounded down and up, respectively. Although the width of the interval may vary according to implementation, many algorithms have been designed to produce narrow intervals. If the range between the upper and lower bounds is sufficiently narrow, then a sufficiently accurate result has been obtained. If not, at least we know this and can perform additional analysis.\n\n\nThe final technique specified in the standard is\n   **round toward zero**\n   . This is, in fact, simple truncation: The extra bits are ignored. This is certainly the simplest technique. However, the result is that the magnitude of the truncated value is always less than or equal to the more precise original value, introducing a consistent bias toward zero in the operation. This is a serious bias because it affects every operation for which there are nonzero extra bits.\n\n\n\n\n**IEEE Standard for Binary Floating-Point Arithmetic**\n\n\nIEEE 754 goes beyond the simple definition of a format to lay down specific practices and procedures so that floating-point arithmetic produces uniform, predictable results independent of the hardware platform. One aspect of this has already been discussed, namely rounding. This subsection looks at three other topics: infinity, NaNs, and subnormal numbers.\n\n\n**INFINITY**\n   Infinity arithmetic is treated as the limiting case of real arithmetic, with the infinity values given the following interpretation:\n\n\n-\\infty < (\\text{every finite number}) < +\\infty\n\n\nWith the exception of the special cases discussed subsequently, any arithmetic operation involving infinity yields the obvious result.\n\n\nFor example:\n\n\n\n5 + (+\\infty) = +\\infty | 5 \\div (+\\infty) = +0\n5 - (+\\infty) = -\\infty | (+\\infty) + (+\\infty) = +\\infty\n5 + (-\\infty) = -\\infty | (-\\infty) + (-\\infty) = -\\infty\n5 - (-\\infty) = +\\infty | (-\\infty) - (+\\infty) = -\\infty\n5 \\times (+\\infty) = +\\infty | (+\\infty) - (-\\infty) = +\\infty\n\n\n**QUIET AND SIGNALING NaNs**\n   A NaN is a symbolic entity encoded in floating-point format, of which there are two types: signaling and quiet. A signaling NaN signals an invalid operation exception whenever it appears as an operand. Signaling\n\n\n**Table 10.7**\n\nOperation | Quiet NaN Produced By\nAny | Any operation on a signaling NaN\nAdd or subtract | Magnitude subtraction of infinities:\n      \n\n       (+\\infty) + (-\\infty)\n      \n\n\n       (-\\infty) + (+\\infty)\n      \n\n\n       (+\\infty) - (+\\infty)\n      \n\n\n       (-\\infty) - (-\\infty)\nMultiply | 0 \\times \\infty\nDivision | \\frac{0}{0}\n      \n      or\n      \n       \\frac{\\infty}{\\infty}\nRemainder | x \\text{ REM } 0\n      \n      or\n      \n       \\infty \\text{ REM } y\nSquare root | \\sqrt{x}\n      \n      , where\n      \n       x < 0\n\n\nNaNs afford values for uninitialized variables and arithmetic-like enhancements that are not the subject of the standard. A quiet NaN propagates through almost every arithmetic operation without signaling an exception. Table 10.7 indicates operations that will produce a quiet NaN.\n\n\nNote that both types of NaNs have the same general format (Table 10.4): an exponent of all ones and a nonzero fraction. The actual bit pattern of the nonzero fraction is implementation dependent; the fraction values can be used to distinguish quiet NaNs from signaling NaNs and to specify particular exception conditions.\n\n\n**SUBNORMAL NUMBERS**\n   Subnormal numbers are included in IEEE 754 to handle cases of exponent underflow. When the exponent of the result becomes too small (a negative exponent with too large a magnitude), the result is subnormalized by right shifting the fraction and incrementing the exponent for each shift until the exponent is within a representable range.\n\n\nFigure 10.26 illustrates the effect of including subnormal numbers. The representable numbers can be grouped into intervals of the form\n   \n    [2^n, 2^{n+1}]\n   \n   . Within\n\n\n\n\n![Figure 10.26(a): A number line showing the gaps between representable numbers in a 32-bit format without subnormal numbers. The line has tick marks at 0, 2^-126, 2^-125, 2^-124, and 2^-123. A large gap is indicated between 2^-126 and 2^-125.](images/image_0175.jpeg)\n\n\nFigure 10.26(a): A number line showing the gaps between representable numbers in a 32-bit format without subnormal numbers. The line has tick marks at 0, 2^-126, 2^-125, 2^-124, and 2^-123. A large gap is indicated between 2^-126 and 2^-125.\n\n\n(a) 32-bit format without subnormal numbers\n\n\n\n\n![Figure 10.26(b): A number line showing the uniform spacing of representable numbers in a 32-bit format with subnormal numbers. The line has tick marks at 0, 2^-126, 2^-125, 2^-124, and 2^-123. The spacing between tick marks is uniform throughout the range.](images/image_0176.jpeg)\n\n\nFigure 10.26(b): A number line showing the uniform spacing of representable numbers in a 32-bit format with subnormal numbers. The line has tick marks at 0, 2^-126, 2^-125, 2^-124, and 2^-123. The spacing between tick marks is uniform throughout the range.\n\n\n(b) 32-bit format with subnormal numbers\n\n\n**Figure 10.26**\neach such interval, the exponent portion of the number remains constant while the fraction varies, producing a uniform spacing of representable numbers within the interval. As we get closer to zero, each successive interval is half the width of the preceding interval but contains the same number of representable numbers. Hence the density of representable numbers increases as we approach zero. However, if only normal numbers are used, there is a gap between the smallest normal number and 0. In the case of the 32-bit IEEE 754 format, there are\n   \n    2^{23}\n   \n   representable numbers in each interval, and the smallest representable positive number is\n   \n    2^{-126}\n   \n   . With the addition of subnormal numbers, an additional\n   \n    2^{23} - 1\n   \n   numbers are uniformly added between 0 and\n   \n    2^{-126}\n   \n   .\n\n\nThe use of subnormal numbers is referred to as\n   *gradual underflow*\n   [COON81]. Without subnormal numbers, the gap between the smallest representable nonzero number and zero is much wider than the gap between the smallest representable nonzero number and the next larger number. Gradual underflow fills in that gap and reduces the impact of exponent underflow to a level comparable with roundoff among the normal numbers."
        }
      ]
    },
    {
      "name": "Digital Logic",
      "sections": [
        {
          "name": "Boolean Algebra",
          "content": "The digital circuitry in digital computers and other digital systems is designed, and its behavior is analyzed, with the use of a mathematical discipline known as\n   **Boolean algebra**\n   . The name is in honor of an English mathematician George Boole, who proposed the basic principles of this algebra in 1854 in his treatise,\n   *An Investigation of the Laws of Thought on Which to Found the Mathematical Theories of Logic and Probabilities*\n   . In 1938, Claude Shannon, a research assistant in the Electrical Engineering Department at M.I.T., suggested that Boolean algebra could be used to solve problems in relay-switching circuit design [SHAN38].\n   \n    1\n   \n   Shannon's techniques were subsequently used in the analysis and design of electronic digital circuits. Boolean algebra turns out to be a convenient tool in two areas:\n\n\n  * ■\n    **Analysis:**\n    It is an economical way of describing the function of digital circuitry.\n  * ■\n    **Design:**\n    Given a desired function, Boolean algebra can be applied to develop a simplified implementation of that function.\n\n\nAs with any algebra, Boolean algebra makes use of variables and operations. In this case, the variables and operations are logical variables and operations. Thus, a variable may take on the value 1 (TRUE) or 0 (FALSE). The basic logical\n\n\n1\n   \n   The paper is available at\n   box.com/COA10e\n   .\n\n\noperations are AND, OR, and NOT, which are symbolically represented by dot, plus sign, and overbar:\n   \n    2\n\n\nA \\text{ AND } B = A \\cdot B\n\n\nA \\text{ OR } B = A + B\n\n\n\\text{NOT } A = \\bar{A}\n\n\nThe operation AND yields true (binary value 1) if and only if both of its operands are true. The operation OR yields true if either or both of its operands are true. The unary operation NOT inverts the value of its operand. For example, consider the equation\n\n\nD = A + (\\bar{B} \\cdot C)\n\n\nD is equal to 1 if A is 1 or if both\n   \n    B = 0\n   \n   and\n   \n    C = 1\n   \n   . Otherwise D is equal to 0.\n\n\nSeveral points concerning the notation are needed. In the absence of parentheses, the AND operation takes precedence over the OR operation. Also, when no ambiguity will occur, the AND operation is represented by simple concatenation instead of the dot operator. Thus,\n\n\nA + B \\cdot C = A + (B \\cdot C) = A + BC\n\n\nall mean: Take the AND of B and C; then take the OR of the result and A.\n\n\nTable 11.1a defines the basic logical operations in a form known as a\n   *truth table*\n   , which lists the value of an operation for every possible combination of values of operands. The table also lists three other useful operators:\n   **XOR**\n   ,\n   **NAND**\n   , and\n   **NOR**\n   . The exclusive-or (XOR) of two logical operands is 1 if and only if exactly one of the operands has the value 1. The NAND function is the complement (NOT) of the AND function, and the NOR is the complement of OR:\n\n\nA \\text{ NAND } B = \\text{NOT } (A \\text{ AND } B) = \\overline{AB}\n\n\nA \\text{ NOR } B = \\text{NOT } (A \\text{ OR } B) = \\overline{A + B}\n\n\nAs we shall see, these three new operations can be useful in implementing certain digital circuits.\n\n\nThe logical operations, with the exception of NOT, can be generalized to more than two variables, as shown in Table 11.1b.\n\n\nTable 11.2 summarizes key identities of Boolean algebra. The equations have been arranged in two columns to show the complementary, or dual, nature of the AND and OR operations. There are two classes of identities: basic rules (or\n   *postulates*\n   ), which are stated without proof, and other identities that can be derived from the basic postulates. The postulates define the way in which Boolean expressions are interpreted. One of the two distributive laws is worth noting because it differs from what we would find in ordinary algebra:\n\n\nA + (B \\cdot C) = (A + B) \\cdot (A + C)\n\n\n2\n   \n   Logical NOT is often indicated by an apostrophe:\n   \n    \\text{NOT } A = A'\n   \n   .\n\n\n**Table 11.1**\n(a) Boolean Operators of Two Input Variables\n\n\n\nP | Q | NOT P\n       \n       (\n       \n        \\bar{P}\n       \n       ) | P AND Q\n       \n       (\n       \n        P \\cdot Q\n       \n       ) | P OR Q\n       \n       (\n       \n        P + Q\n       \n       ) | P NAND Q\n       \n       (\n       \n        \\bar{P \\cdot Q}\n       \n       ) | P NOR Q\n       \n       (\n       \n        \\bar{P + Q}\n       \n       ) | P XOR Q\n       \n       (\n       \n        P \\oplus Q\n       \n       )\n0 | 0 | 1 | 0 | 0 | 1 | 1 | 0\n0 | 1 | 1 | 0 | 1 | 1 | 0 | 1\n1 | 0 | 0 | 0 | 1 | 1 | 0 | 1\n1 | 1 | 0 | 1 | 1 | 0 | 0 | 0\n\n\n(b) Boolean Operators Extended to More than Two Inputs (A, B, ...)\n\n\n\nOperation | Expression | Output = 1 if\nAND | A \\cdot B \\cdot \\dots | All of the set {A, B, ...} are 1.\nOR | A + B + \\dots | Any of the set {A, B, ...} are 1.\nNAND | \\overline{A \\cdot B \\cdot \\dots} | Any of the set {A, B, ...} are 0.\nNOR | \\overline{A + B + \\dots} | All of the set {A, B, ...} are 0.\nXOR | A \\oplus B \\oplus \\dots | The set {A, B, ...} contains an odd number of ones.\n\n\nThe two bottommost expressions are referred to as DeMorgan's theorem. We can restate them as follows:\n\n\nA \\text{ NOR } B = \\bar{A} \\text{ AND } \\bar{B}\n   \n\n    A \\text{ NAND } B = \\bar{A} \\text{ OR } \\bar{B}\n\n\nThe reader is invited to verify the expressions in Table 11.2 by substituting actual values (1s and 0s) for the variables A, B, and C.\n\n\n**Table 11.2**\n\nBasic Postulates\nA \\cdot B = B \\cdot A | A + B = B + A | Commutative Laws\nA \\cdot (B + C) = (A \\cdot B) + (A \\cdot C) | A + (B \\cdot C) = (A + B) \\cdot (A + C) | Distributive Laws\n1 \\cdot A = A | 0 + A = A | Identity Elements\nA \\cdot \\bar{A} = 0 | A + \\bar{A} = 1 | Inverse Elements\nOther Identities\n0 \\cdot A = 0 | 1 + A = 1 | \nA \\cdot A = A | A + A = A | \nA \\cdot (B \\cdot C) = (A \\cdot B) \\cdot C | A + (B + C) = (A + B) + C | Associative Laws\n\\overline{A \\cdot B} = \\bar{A} + \\bar{B} | \\overline{A + B} = \\bar{A} \\cdot \\bar{B} | DeMorgan's Theorem"
        },
        {
          "name": "Gates",
          "content": "The fundamental building block of all digital logic circuits is the gate. Logical functions are implemented by the interconnection of gates.\n\n\nA gate is an electronic circuit that produces an output signal that is a simple Boolean operation on its input signals. The basic gates used in digital logic are AND, OR, NOT, NAND, NOR, and XOR. Figure 11.1 depicts these six gates. Each gate is defined in three ways: graphic symbol, algebraic notation, and truth table. The symbology used in this chapter is from the IEEE standard, IEEE Std 91. Note that the inversion (NOT) operation is indicated by a circle.\n\n\nEach gate shown in Figure 11.1 has one or two inputs and one output. However, as indicated in Table 11.1b, all of the gates except NOT can have more than two inputs. Thus,\n   \n    (X + Y + Z)\n   \n   can be implemented with a single\n   **OR gate**\n   with three inputs. When one or more of the values at the input are changed, the correct output signal appears almost instantaneously, delayed only by the propagation time of signals through the gate (known as the\n   *gate delay*\n   ). The significance of this delay is discussed in Section 11.3. In some cases, a gate is implemented with two outputs, one output being the negation of the other output.\n\n\n\nName | Graphical Symbol | Algebraic Function | Truth Table\nAND | Image: AND gate symbol: a D-shaped gate with two inputs A and B and one output F. | F = A \\cdot B\n      \n\n      or\n      \n\n       F = AB | A\n         \n\n          B\n         \n\n          F\n         \n\n\n\n\n\n          0\n         \n\n          0\n         \n\n          0\n         \n\n\n\n          0\n         \n\n          1\n         \n\n          0\n         \n\n\n\n          1\n         \n\n          0\n         \n\n          0\n         \n\n\n\n          1\n         \n\n          1\n         \n\n          1 | A | B | F | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 1\nA | B | F\n0 | 0 | 0\n0 | 1 | 0\n1 | 0 | 0\n1 | 1 | 1\nOR | Image: OR gate symbol: a D-shaped gate with two inputs A and B and one output F. | F = A + B | A\n         \n\n          B\n         \n\n          F\n         \n\n\n\n\n\n          0\n         \n\n          0\n         \n\n          0\n         \n\n\n\n          0\n         \n\n          1\n         \n\n          1\n         \n\n\n\n          1\n         \n\n          0\n         \n\n          1\n         \n\n\n\n          1\n         \n\n          1\n         \n\n          1 | A | B | F | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 1\nA | B | F\n0 | 0 | 0\n0 | 1 | 1\n1 | 0 | 1\n1 | 1 | 1\nNOT | Image: NOT gate symbol: a triangle with a small circle at the output end, with input A and output F. | F = \\bar{A}\n      \n\n      or\n      \n\n       F = A' | A\n         \n\n          F\n         \n\n\n\n\n\n          0\n         \n\n          1\n         \n\n\n\n          1\n         \n\n          0 | A | F | 0 | 1 | 1 | 0\nA | F\n0 | 1\n1 | 0\nNAND | Image: NAND gate symbol: an AND gate symbol with a small circle at the output end, with inputs A and B and output F. | F = \\overline{AB} | A\n         \n\n          B\n         \n\n          F\n         \n\n\n\n\n\n          0\n         \n\n          0\n         \n\n          1\n         \n\n\n\n          0\n         \n\n          1\n         \n\n          1\n         \n\n\n\n          1\n         \n\n          0\n         \n\n          1\n         \n\n\n\n          1\n         \n\n          1\n         \n\n          0 | A | B | F | 0 | 0 | 1 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0\nA | B | F\n0 | 0 | 1\n0 | 1 | 1\n1 | 0 | 1\n1 | 1 | 0\nNOR | Image: NOR gate symbol: an OR gate symbol with a small circle at the output end, with inputs A and B and output F. | F = \\overline{A + B} | A\n         \n\n          B\n         \n\n          F\n         \n\n\n\n\n\n          0\n         \n\n          0\n         \n\n          1\n         \n\n\n\n          0\n         \n\n          1\n         \n\n          0\n         \n\n\n\n          1\n         \n\n          0\n         \n\n          0\n         \n\n\n\n          1\n         \n\n          1\n         \n\n          0 | A | B | F | 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 0\nA | B | F\n0 | 0 | 1\n0 | 1 | 0\n1 | 0 | 0\n1 | 1 | 0\nXOR | Image: XOR gate symbol: a D-shaped gate with a curved bottom, with inputs A and B and output F. | F = A \\oplus B | A\n         \n\n          B\n         \n\n          F\n         \n\n\n\n\n\n          0\n         \n\n          0\n         \n\n          0\n         \n\n\n\n          0\n         \n\n          1\n         \n\n          1\n         \n\n\n\n          1\n         \n\n          0\n         \n\n          1\n         \n\n\n\n          1\n         \n\n          1\n         \n\n          0 | A | B | F | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 1 | 0\nA | B | F\n0 | 0 | 0\n0 | 1 | 1\n1 | 0 | 1\n1 | 1 | 0\n\n\nFigure 11.1 Basic Logic Gates\n\n\nHere we introduce a common term: we say that to\n   **assert**\n   a signal is to cause a signal line to make a transition from its logically false (0) state to its logically true (1) state. The true (1) state is either a high or low voltage state, depending on the type of electronic circuitry.\n\n\nTypically, not all gate types are used in implementation. Design and fabrication are simpler if only one or two types of gates are used. Thus, it is important to identify\n   *functionally complete*\n   sets of gates. This means that any Boolean function can be implemented using only the gates in the set. The following are functionally complete sets:\n\n\n  * ■ AND, OR, NOT\n  * ■ AND, NOT\n  * ■ OR, NOT\n  * ■ NAND\n  * ■ NOR\n\n\nIt should be clear that AND, OR, and NOT gates constitute a functionally complete set, because they represent the three operations of Boolean algebra. For the AND and NOT gates to form a functionally complete set, there must be a way to synthesize the OR operation from the AND and NOT operations. This can be done by applying DeMorgan's theorem:\n\n\nA + B = \\overline{\\overline{A} \\cdot \\overline{B}}\n\n\nA \\text{ OR } B = \\text{NOT}((\\text{NOT } A) \\text{ AND } (\\text{NOT } B))\n\n\nSimilarly, the OR and NOT operations are functionally complete because they can be used to synthesize the AND operation.\n\n\nFigure 11.2 shows how the AND, OR, and NOT functions can be implemented solely with NAND gates, and Figure 11.3 shows the same thing for NOR gates. For this reason, digital circuits can be, and frequently are, implemented solely with NAND gates or solely with NOR gates.\n\n\n\n\n![Figure 11.2: Some Uses of NAND Gates. The diagram shows three logic circuits using only NAND gates. 1. Top circuit: A single-input NAND gate with input A and output A-bar. 2. Middle circuit: Two-input NAND gate with inputs A and B, output A dot B bar, followed by a single-input NAND gate with input A dot B bar, resulting in output A dot B. 3. Bottom circuit: Two single-input NAND gates with inputs A and B, outputs A-bar and B-bar respectively, followed by a two-input NAND gate with inputs A-bar and B-bar, resulting in output A plus B.](images/image_0177.jpeg)\n\n\nFigure 11.2: Some Uses of NAND Gates. The diagram shows three logic circuits using only NAND gates. 1. Top circuit: A single-input NAND gate with input A and output A-bar. 2. Middle circuit: Two-input NAND gate with inputs A and B, output A dot B bar, followed by a single-input NAND gate with input A dot B bar, resulting in output A dot B. 3. Bottom circuit: Two single-input NAND gates with inputs A and B, outputs A-bar and B-bar respectively, followed by a two-input NAND gate with inputs A-bar and B-bar, resulting in output A plus B.\n\n\n**Figure 11.2**\n   Some Uses of NAND Gates\n\n\n\n\n![Figure 11.3: Some Uses of NOR Gates. The figure shows three logic circuit diagrams using NOR gates. 1. A single-input NOR gate with input A and output A-bar. 2. A two-input NOR gate with inputs A and B, output (A+B)-bar, followed by a one-input NOR gate with input (A+B)-bar and output A+B. 3. Two one-input NOR gates with inputs A and B, outputs A-bar and B-bar, followed by a two-input NOR gate with inputs A-bar and B-bar and output A dot B.](images/image_0178.jpeg)\n\n\nFigure 11.3: Some Uses of NOR Gates. The figure shows three logic circuit diagrams using NOR gates. 1. A single-input NOR gate with input A and output A-bar. 2. A two-input NOR gate with inputs A and B, output (A+B)-bar, followed by a one-input NOR gate with input (A+B)-bar and output A+B. 3. Two one-input NOR gates with inputs A and B, outputs A-bar and B-bar, followed by a two-input NOR gate with inputs A-bar and B-bar and output A dot B.\n\n\nFigure 11.3 Some Uses of NOR Gates\n\n\nWith gates, we have reached the most primitive circuit level of computer hardware. An examination of the transistor combinations used to construct gates departs from that realm and enters the realm of electrical engineering. For our purposes, however, we are content to describe how gates can be used as building blocks to implement the essential logical circuits of a digital computer."
        },
        {
          "name": "Combinational Circuits",
          "content": "A\n   **combinational circuit**\n   is an interconnected set of gates whose output at any time is a function only of the input at that time. As with a single gate, the appearance of the input is followed almost immediately by the appearance of the output, with only gate delays.\n\n\nIn general terms, a combinational circuit consists of\n   \n    n\n   \n   binary inputs and\n   \n    m\n   \n   binary outputs. As with a gate, a combinational circuit can be defined in three ways:\n\n\n  * ■\n    **Truth table:**\n    For each of the\n    \n     2^n\n    \n    possible combinations of input signals, the binary value of each of the\n    \n     m\n    \n    output signals is listed.\n  * ■\n    **Graphical symbols:**\n    The interconnected layout of gates is depicted.\n  * ■\n    **Boolean equations:**\n    Each output signal is expressed as a Boolean function of its input signals.\n\n\n\n\n**Implementation of Boolean Functions**\n\n\nAny Boolean function can be implemented in electronic form as a network of gates. For any given function, there are a number of alternative realizations. Consider the Boolean function represented by the truth table in Table 11.3. We can express this function by simply itemizing the combinations of values of A, B, and C that cause F to be 1:\n\n\nF + \\bar{A}\\bar{B}\\bar{C} + \\bar{A}\\bar{B}C + AB\\bar{C} \\quad (11.1)\n\n\n**Table 11.3**\n\nA | B | C | F\n0 | 0 | 0 | 0\n0 | 0 | 1 | 0\n0 | 1 | 0 | 1\n0 | 1 | 1 | 1\n1 | 0 | 0 | 0\n1 | 0 | 1 | 0\n1 | 1 | 0 | 1\n1 | 1 | 1 | 0\n\n\nThere are three combinations of input values that cause\n   \n    F\n   \n   to be 1, and if any one of these combinations occurs, the result is 1. This form of expression, for self-evident reasons, is known as the\n   **sum of products (SOP)**\n   form. Figure 11.4 shows a straightforward implementation with AND, OR, and NOT gates.\n\n\nAnother form can also be derived from the truth table. The SOP form expresses that the output is 1 if any of the input combinations that produce 1 is true. We can also say that the output is 1 if none of the input combinations that produce 0 is true. Thus,\n\n\nF = \\overline{(\\overline{A} \\overline{B} \\overline{C})} \\cdot \\overline{(\\overline{A} \\overline{B} C)} \\cdot \\overline{(\\overline{A} B \\overline{C})} \\cdot \\overline{(\\overline{A} B C)} \\cdot \\overline{(A \\overline{B} \\overline{C})}\n\n\nThis can be rewritten using a generalization of DeMorgan's theorem:\n\n\n\\overline{(X \\cdot Y \\cdot Z)} = \\overline{X} + \\overline{Y} + \\overline{Z}\n\n\n\n\n![Figure 11.4: Sum-of-Products Implementation of Table 11.3. The diagram shows three inputs, A, B, and C, each passing through a NOT gate (inverter). The outputs of these inverters are connected to three AND gates. The first AND gate takes inputs from the inverted A, inverted B, and inverted C lines. The second AND gate takes inputs from the inverted A, inverted B, and the C line. The third AND gate takes inputs from the inverted A, the B line, and the inverted C line. The outputs of these three AND gates are connected to a single OR gate, which produces the final output F.](images/image_0179.jpeg)\n\n\nFigure 11.4: Sum-of-Products Implementation of Table 11.3. The diagram shows three inputs, A, B, and C, each passing through a NOT gate (inverter). The outputs of these inverters are connected to three AND gates. The first AND gate takes inputs from the inverted A, inverted B, and inverted C lines. The second AND gate takes inputs from the inverted A, inverted B, and the C line. The third AND gate takes inputs from the inverted A, the B line, and the inverted C line. The outputs of these three AND gates are connected to a single OR gate, which produces the final output F.\n\n\n**Figure 11.4**\n\n\n![Figure 11.5: Product-of-Sums implementation of Table 11.3. The diagram shows five 3-input OR gates. The top gate has inputs A, B, and C. The second gate has inputs A, B, and C-bar. The third gate has inputs A-bar, B, and C. The fourth gate has inputs A-bar, B, and C-bar. The fifth gate has inputs A-bar, B-bar, and C-bar. The outputs of these five OR gates are connected to a single 5-input AND gate, which produces the final output F.](images/image_0180.jpeg)\n\n\nFigure 11.5: Product-of-Sums implementation of Table 11.3. The diagram shows five 3-input OR gates. The top gate has inputs A, B, and C. The second gate has inputs A, B, and C-bar. The third gate has inputs A-bar, B, and C. The fourth gate has inputs A-bar, B, and C-bar. The fifth gate has inputs A-bar, B-bar, and C-bar. The outputs of these five OR gates are connected to a single 5-input AND gate, which produces the final output F.\n\n\n**Figure 11.5**\n   Product-of-Sums\n   \n\n   Implementation of Table 11.3\n\n\nThus,\n\n\nF = (\\bar{A} + \\bar{B} + \\bar{C}) \\cdot (\\bar{A} + \\bar{B} + \\bar{C}) \\cdot (\\bar{A} + \\bar{B} + \\bar{C}) \\cdot (\\bar{A} + \\bar{B} + \\bar{C}) \\cdot (\\bar{A} + \\bar{B} + \\bar{C}) \\quad (11.2)\n   \n\n    = (A + B + C) \\cdot (A + B + \\bar{C}) \\cdot (\\bar{A} + B + C) \\cdot (\\bar{A} + B + \\bar{C}) \\cdot (\\bar{A} + \\bar{B} + \\bar{C})\n\n\nThis is in the\n   **product of sums (POS)**\n   form, which is illustrated in Figure 11.5. For clarity, NOT gates are not shown. Rather, it is assumed that each input signal and its complement are available. This simplifies the logic diagram and makes the inputs to the gates more readily apparent.\n\n\nThus, a Boolean function can be realized in either SOP or POS form. At this point, it would seem that the choice would depend on whether the truth table contains more 1s or 0s for the output function: The SOP has one term for each 1, and the POS has one term for each 0. However, there are other considerations:\n\n\n  * ■ It is often possible to derive a simpler Boolean expression from the truth table than either SOP or POS.\n  * ■ It may be preferable to implement the function with a single gate type (NAND or NOR).\n\n\nThe significance of the first point is that, with a simpler Boolean expression, fewer gates will be needed to implement the function. Three methods that can be used to achieve simplification are\n\n\n  * ■ Algebraic simplification\n  * ■ Karnaugh maps\n  * ■ Quine–McCluskey tables\n\n\n**ALGEBRAIC SIMPLIFICATION**\n   Algebraic simplification involves the application of the identities of Table 11.2 to reduce the Boolean expression to one with fewer elements. For example, consider again Equation (11.1). Some thought should convince the reader that an equivalent expression is\n\n\nF = \\bar{A}B + B\\bar{C} \\quad (11.3)\n\n\nOr, even simpler,\n\n\nF = B(\\bar{A} + \\bar{C})\n\n\nThis expression can be implemented as shown in Figure 11.6. The simplification of Equation (11.1) was done essentially by observation. For more complex expressions, some more systematic approach is needed.\n\n\n**KARNAUGH MAPS**\n   For purposes of simplification, the\n   **Karnaugh map**\n   is a convenient way of representing a Boolean function of a small number (up to four) of variables. The map is an array of\n   \n    2^n\n   \n   squares, representing all possible combinations of values of\n   \n    n\n   \n   binary variables. Figure 11.7a shows the map of four squares for a function of two variables. It is essential for later purposes to list the combinations in the order 00, 01, 11, 10. Because the squares corresponding to the combinations are to be used for recording information, the combinations are customarily written above the squares. In the case of three variables, the representation is an arrangement of eight squares (Figure 11.7b), with the values for one of the variables to the left and for the other two variables above the squares. For four variables, 16 squares are needed, with the arrangement indicated in Figure 11.7c.\n\n\nThe map can be used to represent any Boolean function in the following way. Each square corresponds to a unique product in the sum-of-products form, with a 1 value corresponding to the variable and a 0 value corresponding to the NOT of that variable. Thus, the product\n   \n    A\\bar{B}\n   \n   corresponds to the fourth square in Figure 11.7a. For each such product in the function, 1 is placed in the corresponding square. Thus, for the two-variable example, the map corresponds to\n   \n    A\\bar{B} + \\bar{A}B\n   \n   . Given the truth table of a Boolean function, it is an easy matter to construct the map: for each combination of values of variables that produce a result of 1 in the truth table, fill in the corresponding square of the map with 1. Figure 11.7b shows the result for the truth table of Table 11.3. To convert from a Boolean expression to a map, it is first necessary to put the expression into what is referred to as\n   *canonical*\n   form: each term in the expression must contain each variable. So, for example, if we have Equation (11.3), we must first expand it into the full form of Equation (11.1) and then convert this to a map.\n\n\n\n\n![Figure 11.6: Simplified Implementation of Table A.3. The diagram shows a logic circuit. Two inputs, A-bar and C-bar, enter a 2-input OR gate. The output of this OR gate is connected to one input of a 2-input AND gate. The other input of the AND gate is input B. The final output of the AND gate is labeled F.](images/image_0181.jpeg)\n\n\nFigure 11.6: Simplified Implementation of Table A.3. The diagram shows a logic circuit. Two inputs, A-bar and C-bar, enter a 2-input OR gate. The output of this OR gate is connected to one input of a 2-input AND gate. The other input of the AND gate is input B. The final output of the AND gate is labeled F.\n\n\n**Figure 11.6**\n   Simplified Implementation of Table A.3\n\n\n\n\n![Figure 11.7: The Use of Karnaugh Maps to Represent Boolean Functions. (a) 2-variable map AB with 1s at (01,0) and (11,0). (b) 3-variable map BC with 1s at (00,0), (01,1), (11,1), and (10,1). (c) 4-variable map CD with 1s at (01,00), (11,01), (11,11), and (10,10). (d) Simplified labeling of map showing variables A, B, C, and D.](images/image_0182.jpeg)\n\n\n(a)\n    \n     F = \\bar{A}\\bar{B} + \\bar{A}B\n\n\n(b)\n    \n     F = \\bar{A}\\bar{B}\\bar{C} + \\bar{A}BC + AB\\bar{C}\n\n\n(c)\n    \n     F = \\bar{A}\\bar{B}\\bar{C}\\bar{D} + \\bar{A}\\bar{B}C\\bar{D} + AB\\bar{C}\\bar{D}\n\n\n(d) Simplified labeling of map\n\n\nFigure 11.7: The Use of Karnaugh Maps to Represent Boolean Functions. (a) 2-variable map AB with 1s at (01,0) and (11,0). (b) 3-variable map BC with 1s at (00,0), (01,1), (11,1), and (10,1). (c) 4-variable map CD with 1s at (01,00), (11,01), (11,11), and (10,10). (d) Simplified labeling of map showing variables A, B, C, and D.\n\n\n**Figure 11.7**\n   The Use of Karnaugh Maps to Represent Boolean Functions\n\n\nThe labeling used in Figure 11.7d emphasizes the relationship between variables and the rows and columns of the map. Here the two rows embraced by the symbol A are those in which the variable A has the value 1; the rows not embraced by the symbol A are those in which A is 0; similarly for B, C, and D.\n\n\nOnce the map of a function is created, we can often write a simple algebraic expression for it by noting the arrangement of the 1s on the map. The principle is as follows. Any two squares that are adjacent differ in only one of the variables. If two adjacent squares both have an entry of one, then the corresponding product terms differ in only one variable. In such a case, the two terms can be merged by eliminating that variable. For example, in Figure 11.8a, the two adjacent squares correspond to the two terms\n   \n    \\bar{A}\\bar{B}\\bar{C}\\bar{D}\n   \n   and\n   \n    \\bar{A}\\bar{B}C\\bar{D}\n   \n   . Thus, the function expressed is\n\n\n\\bar{A}\\bar{B}\\bar{C}\\bar{D} + \\bar{A}\\bar{B}C\\bar{D} = \\bar{A}\\bar{B}\\bar{D}\n\n\nThis process can be extended in several ways. First, the concept of adjacency can be extended to include wrapping around the edge of the map. Thus, the top square of a column is adjacent to the bottom square, and the leftmost square of a row is adjacent to the rightmost square. These conditions are illustrated in Figures 11.8b and c. Second, we can group not just 2 squares but\n   \n    2^n\n   \n   adjacent squares (i.e., 2, 4, 8, etc.). The next three examples in Figure 11.8 show groupings of 4 squares. Note that in this case, two of the variables can be eliminated. The last three examples show groupings of 8 squares, which allow three variables to be eliminated.\n\n\nWe can summarize the rules for simplification as follows:\n\n\n  * 1. Among the marked squares (squares with a 1), find those that belong to a unique largest block of 1, 2, 4, or 8 and circle those blocks.\n\n\n\n\n![Figure 11.8: Nine Karnaugh maps (a) through (i) showing the use of grouping 1s for simplification. Each map has CD as the top header (00, 01, 11, 10) and AB as the left header (00, 01, 11, 10).](images/image_0183.jpeg)\n\n\nFigure 11.8 displays nine Karnaugh maps, labeled (a) through (i), illustrating the use of grouping 1s for simplification. Each map is a 4x4 grid with CD as the top header (00, 01, 11, 10) and AB as the left header (00, 01, 11, 10). The maps show various groupings of 1s:\n\n\n  * (a)\n     \n      \\bar{A}\\bar{B}D\n     \n     : 1s at (01,01), (01,00), (11,01), (11,00). Grouped as a 2x2 block.\n  * (b)\n     \n      \\bar{B}\\bar{C}D\n     \n     : 1s at (01,01), (10,01). Grouped as a vertical pair.\n  * (c)\n     \n      \\bar{A}\\bar{B}\\bar{D}\n     \n     : 1s at (01,01), (01,10). Grouped as a horizontal pair.\n  * (d)\n     \n      \\bar{A}\\bar{B}\n     \n     : 1s at (00,00), (00,01), (00,11), (00,10). Grouped as a full row.\n  * (e)\n     \n      \\bar{B}\\bar{C}\n     \n     : 1s at (01,01), (11,01), (11,11). Grouped as a vertical column.\n  * (f)\n     \n      \\bar{B}\\bar{D}\n     \n     : 1s at (01,01), (11,01), (11,10). Grouped as a vertical column.\n  * (g)\n     \n      \\bar{A}\n     \n     : 1s at (00,00), (00,01), (00,11), (00,10), (01,00), (01,01), (01,11), (01,10). Grouped as a full row.\n  * (h)\n     \n      \\bar{D}\n     \n     : 1s at (00,00), (00,10), (01,01), (01,11), (10,00), (10,10), (11,01), (11,11). Grouped as two vertical columns.\n  * (i)\n     \n      C\n     \n     : 1s at (00,11), (00,10), (01,11), (01,10), (10,11), (10,10), (11,11), (11,10). Grouped as two vertical columns.\n\n\nFigure 11.8: Nine Karnaugh maps (a) through (i) showing the use of grouping 1s for simplification. Each map has CD as the top header (00, 01, 11, 10) and AB as the left header (00, 01, 11, 10).\n\n\n**Figure 11.8**\n   The Use of Karnaugh Maps\n\n\n  * 2. Select additional blocks of marked squares that are as large as possible and as few in number as possible, but include every marked square at least once. The results may not be unique in some cases. For example, if a marked square combines with exactly two other squares, and there is no fourth marked square to complete a larger group, then there is a choice to be made as to which of the two groupings to choose. When you are circling groups, you are allowed to use the same 1 value more than once.\n  * 3. Continue to draw loops around single marked squares, or pairs of adjacent marked squares, or groups of four, eight, and so on in such a way that every marked square belongs to at least one loop; then use as few of these blocks as possible to include all marked squares.\n\n\nFigure 11.9a, based on Table 11.3, illustrates the simplification process. If any isolated 1s remain after the groupings, then each of these is circled as a group of 1s.\n\n\n\n\n![Figure 11.9: Overlapping Groups. (a) Karnaugh map for F = ĀB + BC̄. (b) Karnaugh map for F = B̄CD̄ + ACD.](images/image_0184.jpeg)\n\n\nFigure 11.9 consists of two Karnaugh maps, (a) and (b), illustrating overlapping groups.\n\n\n(a) Karnaugh map for\n    \n     F = \\bar{A}B + B\\bar{C}\n    \n    . The map has columns labeled\n    \n     BC\n    \n    with values 00, 01, 11, 10. The rows are labeled\n    \n     A\n    \n    with values 0, 1. The cells contain the following values:\n\n\n\nA \\setminus BC | 00 | 01 | 11 | 10\n0 |  |  | 1 | 1\n1 |  |  |  | 1\n\n\nTwo groups of 1s are circled: a vertical group of two 1s in the 11 column (cells (0,11) and (1,11)), and a horizontal group of two 1s in the 10 column (cells (0,10) and (1,10)).\n\n\n(b) Karnaugh map for\n    \n     F = \\bar{B}\\bar{C}\\bar{D} + ACD\n    \n    . The map has columns labeled\n    \n     CD\n    \n    with values 00, 01, 11, 10. The rows are labeled\n    \n     AB\n    \n    with values 00, 01, 11, 10. The cells contain the following values:\n\n\n\nAB \\setminus CD | 00 | 01 | 11 | 10\n00 |  |  |  | \n01 |  | 1 |  | \n11 |  | 1 | 1 | \n10 |  |  | 1 | \n\n\nTwo groups of 1s are circled: a vertical group of two 1s in the 01 column (cells (0,01) and (1,01)), and a horizontal group of two 1s in the 11 column (cells (1,11) and (1,10)).\n\n\nFigure 11.9: Overlapping Groups. (a) Karnaugh map for F = ĀB + BC̄. (b) Karnaugh map for F = B̄CD̄ + ACD.\n\n\nFigure 11.9 Overlapping Groups\n\n\nFinally, before going from the map to a simplified Boolean expression, any group of 1s that is completely overlapped by other groups can be eliminated. This is shown in Figure 11.9b. In this case, the horizontal group is redundant and may be ignored in creating the Boolean expression.\n\n\nOne additional feature of Karnaugh maps needs to be mentioned. In some cases, certain combinations of values of variables never occur, and therefore the corresponding output never occurs. These are referred to as “don’t care” conditions. For each such condition, the letter “d” is entered into the corresponding square of the map. In doing the grouping and simplification, each “d” can be treated as a 1 or 0, whichever leads to the simplest expression.\n\n\nAn example, presented in [HAYE98], illustrates the points we have been discussing. We would like to develop the Boolean expressions for a circuit that adds 1 to a packed decimal digit. For packed decimal, each decimal digit is represented by a 4-bit code, in the obvious way. Thus,\n   \n    0 = 0000\n   \n   ,\n   \n    1 = 0001\n   \n   ,\n   \n    \\dots\n   \n   ,\n   \n    8 = 1000\n   \n   , and\n   \n    9 = 1001\n   \n   . The remaining 4-bit values, from 1010 to 1111, are not used. This code is also referred to as\n   **Binary Coded Decimal (BCD)**\n   .\n\n\nTable 11.4 shows the truth table for producing a 4-bit result that is one more than a 4-bit BCD input. The addition is modulo 10. Thus,\n   \n    9 + 1 = 0\n   \n   . Also, note that six of the input codes produce “don’t care” results, because those are not valid BCD inputs. Figure 11.10 shows the resulting Karnaugh maps for each of the output variables. The d squares are used to achieve the best possible groupings.\n\n\n**THE QUINE–MCCLUSKEY METHOD**\n   For more than four variables, the Karnaugh map method becomes increasingly cumbersome. With five variables, two\n   \n    16 \\times 16\n   \n   maps are needed, with one map considered to be on top of the other in three dimensions to achieve adjacency. Six variables require the use of four\n   \n    16 \\times 16\n\n\n**Table 11.4**\n\nNumber | Input | Number | Output\nA | B | C | D | W | X | Y | Z\n0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1\n1 | 0 | 0 | 0 | 1 | 2 | 0 | 0 | 1 | 0\n2 | 0 | 0 | 1 | 0 | 3 | 0 | 0 | 1 | 1\n3 | 0 | 0 | 1 | 1 | 4 | 0 | 1 | 0 | 0\n4 | 0 | 1 | 0 | 0 | 5 | 0 | 1 | 0 | 1\n5 | 0 | 1 | 0 | 1 | 6 | 0 | 1 | 1 | 0\n6 | 0 | 1 | 1 | 0 | 7 | 0 | 1 | 1 | 1\n7 | 0 | 1 | 1 | 1 | 8 | 1 | 0 | 0 | 0\n8 | 1 | 0 | 0 | 0 | 9 | 1 | 0 | 0 | 1\n9 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0\nDon't care condition | 1 | 0 | 1 | 0 |  | d | d | d | d\n1 | 0 | 1 | 1 |  | d | d | d | d\n1 | 1 | 0 | 0 |  | d | d | d | d\n1 | 1 | 0 | 1 |  | d | d | d | d\n1 | 1 | 1 | 0 |  | d | d | d | d\n1 | 1 | 1 | 1 |  | d | d | d | D\n\n\ntables in four dimensions! An alternative approach is a tabular technique, referred to as the Quine–McCluskey method. The method is suitable for programming on a computer to give an automatic tool for producing minimized Boolean expressions.\n\n\n\n\n![Four Karnaugh maps (a, b, c, d) for the One-Digit Packed Decimal Incrementer. Each map has CD as the top axis (00, 01, 11, 10) and AB as the left axis (00, 01, 11, 10). (a) W = ĀD̄ + ĀBCD̄, showing 1s at (00,00), (00,10), (01,11), and (10,00). (b) X = B̄D̄ + B̄C̄ + BCD, showing 1s at (00,00), (00,11), (01,00), (01,01), (01,11), (10,00), (10,11), and (11,00). (c) Y = ĀC̄D̄ + ĀC̄D̄, showing 1s at (00,00), (00,01), (00,11), and (00,10). (d) Z = D̄, showing 1s at (00,00), (00,01), (00,11), (00,10), (01,00), (01,01), (01,11), (01,10), (10,00), (10,01), (10,11), and (10,10).](images/image_0185.jpeg)\n\n\n(a)\n    \n     W = \\bar{A}\\bar{D} + \\bar{A}BC\\bar{D}\n\n\n(b)\n    \n     X = \\bar{B}\\bar{D} + \\bar{B}\\bar{C} + BCD\n\n\n(c)\n    \n     Y = \\bar{A}\\bar{C}\\bar{D} + \\bar{A}\\bar{C}D\n\n\n(d)\n    \n     Z = \\bar{D}\n\n\nFour Karnaugh maps (a, b, c, d) for the One-Digit Packed Decimal Incrementer. Each map has CD as the top axis (00, 01, 11, 10) and AB as the left axis (00, 01, 11, 10). (a) W = ĀD̄ + ĀBCD̄, showing 1s at (00,00), (00,10), (01,11), and (10,00). (b) X = B̄D̄ + B̄C̄ + BCD, showing 1s at (00,00), (00,11), (01,00), (01,01), (01,11), (10,00), (10,11), and (11,00). (c) Y = ĀC̄D̄ + ĀC̄D̄, showing 1s at (00,00), (00,01), (00,11), and (00,10). (d) Z = D̄, showing 1s at (00,00), (00,01), (00,11), (00,10), (01,00), (01,01), (01,11), (01,10), (10,00), (10,01), (10,11), and (10,10).\n\n\n**Figure 11.10**\nThe method is best explained by means of an example. Consider the following expression:\n\n\nABCD + AB\\bar{C}D + AB\\bar{C}\\bar{D} + A\\bar{B}CD + \\bar{A}BCD + \\bar{A}B\\bar{C}D + \\bar{A}B\\bar{C}\\bar{D} + \\bar{A}\\bar{B}\\bar{C}\\bar{D}\n\n\nLet us assume that this expression was derived from a truth table. We would like to produce a minimal expression suitable for implementation with gates.\n\n\nThe first step is to construct a table in which each row corresponds to one of the product terms of the expression. The terms are grouped according to the number of complemented variables. That is, we start with the term with no complements, if it exists, then all terms with one complement, and so on. Table 11.5 shows the list for our example expression, with horizontal lines used to indicate the grouping. For clarity, each term is represented by a 1 for each uncomplemented variable and a 0 for each complemented variable. Thus, we group terms according to the number of 1s they contain. The index column is simply the decimal equivalent and is useful in what follows.\n\n\nThe next step is to find all pairs of terms that differ in only one variable, that is, all pairs of terms that are the same except that one variable is 0 in one of the terms and 1 in the other. Because of the way in which we have grouped the terms, we can do this by starting with the first group and comparing each term of the first group with every term of the second group. Then compare each term of the second group with all of the terms of the third group, and so on. Whenever a match is found, place a check next to each term, combine the pair by eliminating the variable that differs in the two terms, and add that to a new list. Thus, for example, the terms\n   \n    \\bar{A}BC\\bar{D}\n   \n   and\n   \n    \\bar{A}BCD\n   \n   are combined to produce\n   \n    ABC\n   \n   . This process continues until the entire original table has been examined. The result is a new table with the following entries:\n\n\n\\begin{array}{ccc} \\bar{A}\\bar{C}D & AB\\bar{C} & ABD\\checkmark \\\\ \\hline B\\bar{C}D\\checkmark & ACD & \\\\ \\bar{A}BC & BCD\\checkmark & \\\\ \\bar{A}BD\\checkmark & & \\end{array}\n\n\n**Table 11.5**\n   First Stage of Quine–McCluskey Method\n\n\n(for\n   \n    F = ABCD + AB\\bar{C}D + AB\\bar{C}\\bar{D} + A\\bar{B}CD + \\bar{A}BCD + \\bar{A}B\\bar{C}D + \\bar{A}B\\bar{C}\\bar{D} + \\bar{A}\\bar{B}\\bar{C}\\bar{D}\n   \n   )\n\n\n\nProduct Term | Index | A | B | C | D | \n\\bar{A}BCD | 1 | 0 | 0 | 0 | 1 | ✓\n\\bar{A}B\\bar{C}D | 5 | 0 | 1 | 0 | 1 | ✓\n\\bar{A}BC\\bar{D} | 6 | 0 | 1 | 1 | 0 | ✓\nAB\\bar{C}\\bar{D} | 12 | 1 | 1 | 0 | 0 | ✓\n\\bar{A}\\bar{B}CD | 7 | 0 | 1 | 1 | 1 | ✓\nA\\bar{B}CD | 11 | 1 | 0 | 1 | 1 | ✓\nAB\\bar{C}D | 13 | 1 | 1 | 0 | 1 | ✓\nABCD | 15 | 1 | 1 | 1 | 1 | ✓\n\n\nThe new table is organized into groups, as indicated, in the same fashion as the first table. The second table is then processed in the same manner as the first. That is, terms that differ in only one variable are checked and a new term produced for a third table. In this example, the third table that is produced contains only one term:\n   \n    BD\n   \n   .\n\n\nIn general, the process would proceed through successive tables until a table with no matches was produced. In this case, this has involved three tables.\n\n\nOnce the process just described is completed, we have eliminated many of the possible terms of the expression. Those terms that have not been eliminated are used to construct a matrix, as illustrated in Table 11.6. Each row of the matrix corresponds to one of the terms that have not been eliminated (has no check) in any of the tables used so far. Each column corresponds to one of the terms in the original expression. An\n   \n    X\n   \n   is placed at each intersection of a row and a column such that the row element is “compatible” with the column element. That is, the variables present in the row element have the same value as the variables present in the column element. Next, circle each\n   \n    X\n   \n   that is alone in a column. Then place a square around each\n   \n    X\n   \n   in any row in which there is a circled\n   \n    X\n   \n   . If every column now has either a squared or a circled\n   \n    X\n   \n   , then we are done, and those row elements whose\n   \n    X\n   \n   s have been marked constitute the minimal expression. Thus, in our example, the final expression is\n\n\nAB\\bar{C} + ACD + \\bar{A}BC + \\bar{A}\\bar{C}D\n\n\nIn cases in which some columns have neither a circle nor a square, additional processing is required. Essentially, we keep adding row elements until all columns are covered.\n\n\nLet us summarize the Quine–McCluskey method to try to justify intuitively why it works. The first phase of the operation is reasonably straightforward. The process eliminates unneeded variables in product terms. Thus, the expression\n   \n    ABC + AB\\bar{C}\n   \n   is equivalent to\n   \n    AB\n   \n   , because\n\n\nABC + AB\\bar{C} = AB(C + \\bar{C}) = AB\n\n\nAfter the elimination of variables, we are left with an expression that is clearly equivalent to the original expression. However, there may be redundant terms in this expression, just as we found redundant groupings in Karnaugh maps. The matrix layout assures that each term in the original expression is covered and does so in a way that minimizes the number of terms in the final expression.\n\n\n**Table 11.6**\n   Last Stage of Quine–McCluskey Method\n\n\n(for\n   \n    F = ABCD + ABC\\bar{D} + AB\\bar{C}D + A\\bar{B}CD + \\bar{A}BCD + \\bar{A}BC\\bar{D} + \\bar{A}\\bar{B}CD + \\bar{A}\\bar{B}C\\bar{D}\n   \n   )\n\n\n\n | ABCD | AB\\bar{C}D | AB\\bar{C}\\bar{D} | A\\bar{B}CD | \\bar{A}BCD | \\bar{A}BC\\bar{D} | \\bar{A}\\bar{B}CD | \\bar{A}\\bar{B}C\\bar{D}\nBD | X | X |  |  | X |  | X | \n\\bar{A}\\bar{C}D |  |  |  |  |  |  | \\boxed{X} | \\otimes\n\\bar{A}BC |  |  |  |  | \\boxed{X} | \\otimes |  | \nAB\\bar{C} |  | \\boxed{X} | \\otimes |  |  |  |  | \nACD | \\boxed{X} |  |  | \\otimes |  |  |  | \n\n\n\n\n![Figure 11.11: NAND Implementation of Table 11.3. The diagram shows a logic circuit where two inputs, A-bar and B, are fed into a NAND gate. Simultaneously, inputs B and C-bar are fed into another NAND gate. The outputs of these two NAND gates are then fed into a third NAND gate, which produces the final output F.](images/image_0186.jpeg)\n\n\nFigure 11.11: NAND Implementation of Table 11.3. The diagram shows a logic circuit where two inputs, A-bar and B, are fed into a NAND gate. Simultaneously, inputs B and C-bar are fed into another NAND gate. The outputs of these two NAND gates are then fed into a third NAND gate, which produces the final output F.\n\n\n**Figure 11.11**\n   NAND Implementation of Table 11.3\n\n\n**NAND AND NOR IMPLEMENTATIONS**\n   Another consideration in the implementation of Boolean functions concerns the types of gates used. It is sometimes desirable to implement a Boolean function solely with NAND gates or solely with NOR gates. Although this may not be the minimum-gate implementation, it has the advantage of regularity, which can simplify the manufacturing process. Consider again Equation (11.3):\n\n\nF = B(\\bar{A} + \\bar{C})\n\n\nBecause the complement of the complement of a value is just the original value,\n\n\nF = B(\\bar{A} + \\bar{C}) = \\overline{\\overline{B(\\bar{A} + \\bar{C})}} = \\overline{(\\overline{B} \\cdot \\overline{(\\bar{A} + \\bar{C})})}\n\n\nApplying DeMorgan's theorem,\n\n\nF = \\overline{(\\overline{B} \\cdot \\overline{(\\bar{A} + \\bar{C})})}\n\n\nwhich has three NAND forms, as illustrated in Figure 11.11.\n\n\n\n\n**Multiplexers**\n\n\nThe\n   **multiplexer**\n   connects multiple inputs to a single output. At any time, one of the inputs is selected to be passed to the output. A general block diagram representation is shown in Figure 11.12. This represents a 4-to-1 multiplexer. There are four input lines, labeled D0, D1, D2, and D3. One of these lines is selected to provide the output\n\n\n\n\n![Figure 11.12: 4-to-1 Multiplexer Representation. The diagram shows a rectangular block labeled '4-to-1 MUX'. It has four input lines on the left labeled D0, D1, D2, and D3. It has two selection lines at the bottom labeled S2 and S1. The output line F is on the right side of the block.](images/image_0187.jpeg)\n\n\nFigure 11.12: 4-to-1 Multiplexer Representation. The diagram shows a rectangular block labeled '4-to-1 MUX'. It has four input lines on the left labeled D0, D1, D2, and D3. It has two selection lines at the bottom labeled S2 and S1. The output line F is on the right side of the block.\n\n\n**Figure 11.12**\n   4-to-1 Multiplexer Representation\n\n\n**Table 11.7**\n\nS2 | S1 | F\n0 | 0 | D0\n0 | 1 | D1\n1 | 0 | D2\n1 | 1 | D3\n\n\nsignal F. To select one of the four possible inputs, a 2-bit selection code is needed, and this is implemented as two select lines labeled S1 and S2.\n\n\nAn example 4-to-1 multiplexer is defined by the truth table in Table 11.7. This is a simplified form of a truth table. Instead of showing all possible combinations of input variables, it shows the output as data from line D0, D1, D2, or D3. Figure 11.13 shows an implementation using AND, OR, and NOT gates. S1 and S2 are connected to the AND gates in such a way that, for any combination of S1 and S2, three of the AND gates will output 0. The fourth\n   **AND gate**\n   will output the value of the selected line, which is either 0 or 1. Thus, three of the inputs to the OR gate are always 0, and the output of the OR gate will equal the value of the selected input gate. Using this regular organization, it is easy to construct multiplexers of size 8-to-1, 16-to-1, and so on.\n\n\nMultiplexers are used in digital circuits to control signal and data routing. An example is the loading of the program\n   **counter (PC)**\n   . The value to be loaded into the program counter may come from one of several different sources:\n\n\n\n\n![Figure 11.13: Multiplexer Implementation. A 4-to-1 multiplexer circuit diagram. It features two select lines, S1 and S2, each with an inverter (NOT gate) at its input. Four data input lines, D0, D1, D2, and D3, are connected to four 2-input AND gates. The connections are: D0 to the AND gate with inverted S1 and S2; D1 to the AND gate with inverted S1 and S2; D2 to the AND gate with S1 and inverted S2; D3 to the AND gate with S1 and S2. The outputs of these four AND gates are connected to a single 4-input OR gate, which produces the final output F.](images/image_0188.jpeg)\n\n\nFigure 11.13: Multiplexer Implementation. A 4-to-1 multiplexer circuit diagram. It features two select lines, S1 and S2, each with an inverter (NOT gate) at its input. Four data input lines, D0, D1, D2, and D3, are connected to four 2-input AND gates. The connections are: D0 to the AND gate with inverted S1 and S2; D1 to the AND gate with inverted S1 and S2; D2 to the AND gate with S1 and inverted S2; D3 to the AND gate with S1 and S2. The outputs of these four AND gates are connected to a single 4-input OR gate, which produces the final output F.\n\n\n**Figure 11.13**\n\n\n![Figure 11.14: Multiplexer Input to Program Counter. The diagram shows a series of 16 4-to-1 MUXes connected in a chain. Each MUX has two select lines, S1 and S2. The inputs to the MUXes are labeled C0, IR0, and ALU0 for the first one, and C1, IR1, and ALU1 for the second, with an ellipsis indicating the pattern continues up to C15, IR15, and ALU15 for the last one. The outputs of the MUXes are labeled PC0, PC1, ..., PC15, representing the 16 bits of the Program Counter.](images/image_0189.jpeg)\n\n\nFigure 11.14: Multiplexer Input to Program Counter. The diagram shows a series of 16 4-to-1 MUXes connected in a chain. Each MUX has two select lines, S1 and S2. The inputs to the MUXes are labeled C0, IR0, and ALU0 for the first one, and C1, IR1, and ALU1 for the second, with an ellipsis indicating the pattern continues up to C15, IR15, and ALU15 for the last one. The outputs of the MUXes are labeled PC0, PC1, ..., PC15, representing the 16 bits of the Program Counter.\n\n\n**Figure 11.14**\n   Multiplexer Input to Program Counter\n\n\n  * ■ A binary counter, if the PC is to be incremented for the next instruction.\n  * ■ The instruction\n    **register**\n    , if a branch instruction using a direct address has just been executed.\n  * ■ The output of the ALU, if the branch instruction specifies the address using a displacement mode.\n\n\nThese various inputs could be connected to the input lines of a multiplexer, with the PC connected to the output line. The select lines determine which value is loaded into the PC. Because the PC contains multiple bits, multiple multiplexers are used, one per bit. Figure 11.14 illustrates this for 16-bit addresses.\n\n\n\n\n**Decoders**\n\n\nA\n   **decoder**\n   is a combinational circuit with a number of output lines, only one of which is asserted at any time. Which output line is asserted depends on the pattern of input lines. In general, a decoder has\n   \n    n\n   \n   inputs and\n   \n    2^n\n   \n   outputs. Figure 11.15 shows a decoder with three inputs and eight outputs.\n\n\nDecoders find many uses in digital computers. One example is address decoding. Suppose we wish to construct a 1K-byte memory using four\n   \n    256 \\times 8\n   \n   -bit RAM chips. We want a single unified address space, which can be broken down as follows:\n\n\n\nAddress | Chip\n0000–00FF | 0\n0100–01FF | 1\n0200–02FF | 2\n0300–03FF | 3\n\n\nEach chip requires 8 address lines, and these are supplied by the lower-order 8 bits of the address. The higher-order 2 bits of the 10-bit address are used to select one of the four RAM chips. For this purpose, a 2-to-4 decoder is used whose output enables one of the four chips, as shown in Figure 11.16.\n\n\nWith an additional input line, a decoder can be used as a demultiplexer. The demultiplexer performs the inverse function of a multiplexer; it connects a single input to one of several outputs. This is shown in Figure 11.17. As before,\n   \n    n\n   \n   inputs are decoded to produce a single one of\n   \n    2^n\n   \n   outputs. All of the\n   \n    2^n\n   \n   output lines are ANDed\n\n\n\n\n![Figure 11.15: A 3-to-8 line decoder circuit. It has three inputs, A, B, and C. Input A is connected to an inverter. The outputs are labeled D0 through D7, each representing a unique 3-bit combination of the inputs. The outputs are: D0 = 000, D1 = 001, D2 = 010, D3 = 011, D4 = 100, D5 = 101, D6 = 110, and D7 = 111.](images/image_0190.jpeg)\n\n\nThe diagram shows a 3-to-8 line decoder. Inputs A, B, and C are connected to a grid of logic gates. Input A is inverted. The outputs are 8 AND gates labeled D\n    \n     0\n    \n    through D\n    \n     7\n    \n    , each corresponding to a unique combination of the 3 inputs. The binary values for each output are: D\n    \n     0\n    \n    = 000, D\n    \n     1\n    \n    = 001, D\n    \n     2\n    \n    = 010, D\n    \n     3\n    \n    = 011, D\n    \n     4\n    \n    = 100, D\n    \n     5\n    \n    = 101, D\n    \n     6\n    \n    = 110, and D\n    \n     7\n    \n    = 111.\n\n\nFigure 11.15: A 3-to-8 line decoder circuit. It has three inputs, A, B, and C. Input A is connected to an inverter. The outputs are labeled D0 through D7, each representing a unique 3-bit combination of the inputs. The outputs are: D0 = 000, D1 = 001, D2 = 010, D3 = 011, D4 = 100, D5 = 101, D6 = 110, and D7 = 111.\n\n\n**Figure 11.15**\n   Decoder with 3 Inputs and\n   \n    2^3 = 8\n   \n   Outputs\n\n\n\n\n![Figure 11.16: Address decoding circuit. It uses a 2-to-4 decoder to enable four 256 x 8 RAM chips. The 2-to-4 decoder has inputs A8 and A9. Its four outputs are connected to the 'Enable' inputs of the four RAM chips. The RAM chips have address inputs A0 through A7.](images/image_0191.jpeg)\n\n\nThe diagram illustrates address decoding. A 2-to-4 decoder takes inputs A\n    \n     8\n    \n    and A\n    \n     9\n    \n    . Its four outputs are connected to the 'Enable' inputs of four separate\n    \n     256 \\times 8\n    \n    RAM chips. The RAM chips have address inputs A\n    \n     0\n    \n    through A\n    \n     7\n    \n    .\n\n\nFigure 11.16: Address decoding circuit. It uses a 2-to-4 decoder to enable four 256 x 8 RAM chips. The 2-to-4 decoder has inputs A8 and A9. Its four outputs are connected to the 'Enable' inputs of the four RAM chips. The RAM chips have address inputs A0 through A7.\n\n\n**Figure 11.16**\n   Address Decoding\n\n\n\n\n![Figure 11.17: Implementation of a Demultiplexer Using a Decoder. The diagram shows a block labeled 'n-to-2^n decoder'. On the left, there are two input lines: 'n-bit destination address' (represented by three dots) and 'Data input'. On the right, there are '2^n outputs' (represented by three dots).](images/image_0192.jpeg)\n\n\nFigure 11.17: Implementation of a Demultiplexer Using a Decoder. The diagram shows a block labeled 'n-to-2^n decoder'. On the left, there are two input lines: 'n-bit destination address' (represented by three dots) and 'Data input'. On the right, there are '2^n outputs' (represented by three dots).\n\n\n**Figure 11.17**\n   Implementation of a Demultiplexer Using a Decoder\n\n\nwith a data input line. Thus, the\n   \n    n\n   \n   inputs act as an address to select a particular output line, and the value on the data input line (0 or 1) is routed to that output line.\n\n\nThe configuration in Figure 11.17 can be viewed in another way. Change the label on the new line from\n   *Data Input*\n   to\n   *Enable*\n   . This allows for the control of the timing of the decoder. The decoded output appears only when the encoded input is present\n   *and*\n   the enable line has a value of 1.\n\n\n\n\n**Read-Only Memory**\n\n\nCombinational circuits are often referred to as “memoryless” circuits, because their output depends only on their current input and no history of prior inputs is retained. However, there is one sort of memory that is implemented with combinational circuits, namely\n   **read-only memory (ROM)**\n   .\n\n\nRecall that a ROM is a memory unit that performs only the read operation. This implies that the binary information stored in a ROM is permanent and was created during the fabrication process. Thus, a given input to the ROM (address lines) always produces the same output (data lines). Because the outputs are a function only of the present inputs, the ROM is in fact a combinational circuit.\n\n\nA ROM can be implemented with a decoder and a set of OR gates. As an example, consider Table 11.8. This can be viewed as a truth table with four inputs and four outputs. For each of the 16 possible input values, the corresponding set of values of the outputs is shown. It can also be viewed as defining the contents of a 64-bit ROM consisting of 16 words of 4 bits each. The four inputs specify an address, and the four outputs specify the contents of the location specified by the address. Figure 11.18 shows how this memory could be implemented using a 4-to-16 decoder and four OR gates. As with the PLA, a regular organization is used, and the interconnections are made to reflect the desired result.\n\n\n\n\n**Adders**\n\n\nSo far, we have seen how interconnected gates can be used to implement such functions as the routing of signals, decoding, and ROM. One essential area not yet addressed is that of arithmetic. In this brief overview, we will content ourselves with looking at the addition function.\n\n\nBinary addition differs from Boolean algebra in that the result includes a carry term. Thus,\n\n\n**Table 11.8**\n   Truth Table for a ROM\n\n\n\nInput | Output\nX_1 | X_2 | X_3 | X_4 | Z_1 | Z_2 | Z_3 | Z_4\n0 | 0 | 0 | 0 | 0 | 0 | 0 | 0\n0 | 0 | 0 | 1 | 0 | 0 | 0 | 1\n0 | 0 | 1 | 0 | 0 | 0 | 1 | 1\n0 | 0 | 1 | 1 | 0 | 0 | 1 | 0\n0 | 1 | 0 | 0 | 0 | 1 | 1 | 0\n0 | 1 | 0 | 1 | 0 | 1 | 1 | 1\n0 | 1 | 1 | 0 | 0 | 1 | 0 | 1\n0 | 1 | 1 | 1 | 0 | 1 | 0 | 0\n1 | 0 | 0 | 0 | 1 | 1 | 0 | 0\n1 | 0 | 0 | 1 | 1 | 1 | 0 | 1\n1 | 0 | 1 | 0 | 1 | 1 | 1 | 1\n1 | 0 | 1 | 1 | 1 | 1 | 1 | 0\n1 | 1 | 0 | 0 | 1 | 0 | 1 | 0\n1 | 1 | 0 | 1 | 1 | 0 | 1 | 1\n1 | 1 | 1 | 0 | 1 | 0 | 0 | 1\n1 | 1 | 1 | 1 | 1 | 0 | 0 | 0\n\n\n\n\n![Diagram of a 64-bit ROM structure. A four-input sixteen-output decoder on the left takes inputs X1, X2, X3, and X4. Its sixteen outputs are horizontal lines representing binary addresses from 0000 to 1111. These lines intersect with sixteen vertical lines representing data bits. The intersections are marked with dots, indicating the stored data for each address. The four output lines at the bottom are labeled Z1, Z2, Z3, and Z4, each connected to a buffer.](images/image_0193.jpeg)\n\n\nDiagram of a 64-bit ROM structure. A four-input sixteen-output decoder on the left takes inputs X1, X2, X3, and X4. Its sixteen outputs are horizontal lines representing binary addresses from 0000 to 1111. These lines intersect with sixteen vertical lines representing data bits. The intersections are marked with dots, indicating the stored data for each address. The four output lines at the bottom are labeled Z1, Z2, Z3, and Z4, each connected to a buffer.\n\n\n**Figure 11.18**\n   A 64-Bit ROM\n\n\n**Table 11.9**\n\n(a) Single-Bit Addition\nA | B | Sum | Carry\n0 | 0 | 0 | 0\n0 | 1 | 1 | 0\n1 | 0 | 1 | 0\n1 | 1 | 0 | 1\n\n\n\n(b) Addition with Carry Input\nC\n       \n        in | A | B | Sum | C\n       \n        out\n0 | 0 | 0 | 0 | 0\n0 | 0 | 1 | 1 | 0\n0 | 1 | 0 | 1 | 0\n0 | 1 | 1 | 0 | 1\n1 | 0 | 0 | 1 | 0\n1 | 0 | 1 | 0 | 1\n1 | 1 | 0 | 0 | 1\n1 | 1 | 1 | 1\n\n\n\n0 | 0 | 1 | 1\n+0 | +1 | +0 | +1\n0 | 1 | 1 | 10\n\n\nHowever, addition can still be dealt with in Boolean terms. In Table 11.9a, we show the logic for adding two input bits to produce a 1-bit sum and a carry bit. This truth table could easily be implemented in digital logic. However, we are not interested in performing addition on just a single pair of bits. Rather, we wish to add two\n   \n    n\n   \n   -bit numbers. This can be done by putting together a set of adders so that the carry from one\n   **adder**\n   is provided as input to the next. A 4-bit adder is depicted in Figure 11.19.\n\n\nFor a multiple-bit adder to work, each of the single-bit adders must have three inputs, including the carry from the next-lower-order adder. The revised truth table appears in Table 11.9b. The two outputs can be expressed:\n\n\n\\text{Sum} = \\bar{A}\\bar{B}C + \\bar{A}B\\bar{C} + A\\bar{B}C + ABC\n\n\n\\text{Carry} = AB + AC + BC\n\n\nFigure 11.20 is an implementation using AND, OR, and NOT gates.\n\n\n\n\n![Diagram of a 4-bit adder showing four 1-bit adders (C3, C2, C1, C0) connected in series. Inputs A3, B3, A2, B2, A1, B1, A0, B0 are fed into the adders. Carry-in (Cin) for C0 is 0. Carry-out (Cout) from each adder becomes the carry-in for the next. The final carry-out (Cout from C3) is labeled 'Overflow signal'. Sums S3, S2, S1, S0 are outputs from each adder.](images/image_0194.jpeg)\n\n\nDiagram of a 4-bit adder showing four 1-bit adders (C3, C2, C1, C0) connected in series. Inputs A3, B3, A2, B2, A1, B1, A0, B0 are fed into the adders. Carry-in (Cin) for C0 is 0. Carry-out (Cout) from each adder becomes the carry-in for the next. The final carry-out (Cout from C3) is labeled 'Overflow signal'. Sums S3, S2, S1, S0 are outputs from each adder.\n\n\n**Figure 11.19**\n\n\n![Figure 11.20: Implementation of an Adder. The diagram shows the logic gates for a full adder. On the left, there are six input lines: A-bar, B-bar, C-bar (top row), A, B, C (middle row), and A, C, B (bottom row). The top three lines (A-bar, B-bar, C-bar) are connected to three 3-input AND gates. The outputs of these gates are connected to a 3-input OR gate, which produces the 'Sum' output. The middle three lines (A, B, C) are connected to three 2-input AND gates. The outputs of these gates are connected to a 3-input OR gate, which produces the 'Carry' output. The bottom three lines (A, C, B) are connected to three 2-input AND gates. The outputs of these gates are connected to a 3-input OR gate, which also produces the 'Carry' output.](images/image_0195.jpeg)\n\n\nFigure 11.20: Implementation of an Adder. The diagram shows the logic gates for a full adder. On the left, there are six input lines: A-bar, B-bar, C-bar (top row), A, B, C (middle row), and A, C, B (bottom row). The top three lines (A-bar, B-bar, C-bar) are connected to three 3-input AND gates. The outputs of these gates are connected to a 3-input OR gate, which produces the 'Sum' output. The middle three lines (A, B, C) are connected to three 2-input AND gates. The outputs of these gates are connected to a 3-input OR gate, which produces the 'Carry' output. The bottom three lines (A, C, B) are connected to three 2-input AND gates. The outputs of these gates are connected to a 3-input OR gate, which also produces the 'Carry' output.\n\n\n**Figure 11.20**\n   Implementation of an Adder\n\n\nThus we have the necessary logic to implement a multiple-bit adder such as shown in Figure 11.21. Note that because the output from each adder depends on the carry from the previous adder, there is an increasing delay from the least significant to the most significant bit. Each single-bit adder experiences a certain amount of gate delay, and this gate delay accumulates. For larger adders, the accumulated delay can become unacceptably high.\n\n\nIf the carry values could be determined without having to ripple through all the previous stages, then each single-bit adder could function independently, and delay would not accumulate. This can be achieved with an approach known as\n   *carry lookahead*\n   . Let us look again at the 4-bit adder to explain this approach.\n\n\nWe would like to come up with an expression that specifies the carry input to any stage of the adder without reference to previous carry values. We have\n\n\n\n\n![Figure 11.21: Construction of a 32-Bit Adder Using 8-Bit Adders. The diagram shows four 8-bit adder blocks connected in series. The first block takes inputs A31, B31, ..., A24, B24 and produces outputs S31, ..., S24 and carry C23. The second block takes inputs A23, B23, ..., A16, B16 and carry C23, producing outputs S23, ..., S16 and carry C15. The third block takes inputs A15, B15, ..., A8, B8 and carry C15, producing outputs S15, ..., S8 and carry C7. The fourth block takes inputs A7, B7, ..., A0, B0, carry C7, and an external carry input Cin, producing the final outputs S7, ..., S0 and the final carry output Cout.](images/image_0196.jpeg)\n\n\nFigure 11.21: Construction of a 32-Bit Adder Using 8-Bit Adders. The diagram shows four 8-bit adder blocks connected in series. The first block takes inputs A31, B31, ..., A24, B24 and produces outputs S31, ..., S24 and carry C23. The second block takes inputs A23, B23, ..., A16, B16 and carry C23, producing outputs S23, ..., S16 and carry C15. The third block takes inputs A15, B15, ..., A8, B8 and carry C15, producing outputs S15, ..., S8 and carry C7. The fourth block takes inputs A7, B7, ..., A0, B0, carry C7, and an external carry input Cin, producing the final outputs S7, ..., S0 and the final carry output Cout.\n\n\n**Figure 11.21**\n   Construction of a 32-Bit Adder Using 8-Bit Adders\n\n\nC_0 = A_0B_0 \\quad (11.4)\n\n\nC_1 = A_1B_1 + A_1A_0B_0 + B_1A_0B_0 \\quad (11.5)\n\n\nFollowing the same procedure, we get\n\n\nC_2 = A_2B_2 + A_2A_1B_1 + A_2A_1A_0B_0 + A_2B_1A_0B_0 + B_2A_1B_1 + B_2A_1A_0B_0 + B_2B_1A_0B_0\n\n\nThis process can be repeated for arbitrarily long adders. Each carry term can be expressed in SOP form as a function only of the original inputs, with no dependence on the carries. Thus, only two levels of gate delay occur regardless of the length of the adder.\n\n\nFor long numbers, this approach becomes excessively complicated. Evaluating the expression for the most significant bit of an\n   \n    n\n   \n   -bit adder requires an OR gate with\n   \n    2^n - 1\n   \n   inputs and\n   \n    2^n - 1\n   \n   AND gates with from 2 to\n   \n    n + 1\n   \n   inputs. Accordingly, full carry lookahead is typically done only 4 to 8 bits at a time. Figure 11.21 shows how a 32-bit adder can be constructed out of four 8-bit adders. In this case, the carry must ripple through the four 8-bit adders, but this will be substantially quicker than a ripple through thirty-two 1-bit adders."
        },
        {
          "name": "Sequential Circuits",
          "content": "Combinational circuits implement the essential functions of a digital computer. However, except for the special case of ROM, they provide no memory or state information, elements also essential to the operation of a digital computer. For the latter purposes, a more complex form of digital logic circuit is used: the\n   **sequential circuit**\n   . The current output of a sequential circuit depends not only on the current input, but also on the past history of inputs. Another and generally more useful way to view it is that the current output of a sequential circuit depends on the current input and the current state of that circuit.\n\n\nIn this section, we examine some simple but useful examples of sequential circuits. As will be seen, the sequential circuit makes use of combinational circuits.\n\n\n\n\n**Flip-Flops**\n\n\nThe simplest form of sequential circuit is the\n   **flip-flop**\n   . There are a variety of flip-flops, all of which share two properties:\n\n\n  * ■ The flip-flop is a bistable device. It exists in one of two states and, in the absence of input, remains in that state. Thus, the flip-flop can function as a 1-bit memory.\n  * ■ The flip-flop has two outputs, which are always the complements of each other. These are generally labeled\n    \n     Q\n    \n    and\n    \n     \\bar{Q}\n    \n    .\n\n\n**THE S–R LATCH**\n   Figure 11.22 shows a common configuration known as the S–R flip-flop or\n   **S–R latch**\n   . The circuit has two inputs,\n   \n    S\n   \n   (Set) and\n   \n    R\n   \n   (Reset), and two outputs,\n   \n    Q\n   \n   and\n   \n    \\bar{Q}\n   \n   , and consists of two NOR gates connected in a feedback arrangement.\n\n\n\n\n![Circuit diagram of an S-R Latch implemented with two cross-coupled NOR gates. The inputs are S (Set) and R (Reset). The outputs are Q and Q-bar. The upper NOR gate has inputs R and Q-bar, and its output is Q. The lower NOR gate has inputs S and Q, and its output is Q-bar. The outputs Q and Q-bar are cross-connected to the inputs of the opposite gate.](images/image_0197.jpeg)\n\n\nCircuit diagram of an S-R Latch implemented with two cross-coupled NOR gates. The inputs are S (Set) and R (Reset). The outputs are Q and Q-bar. The upper NOR gate has inputs R and Q-bar, and its output is Q. The lower NOR gate has inputs S and Q, and its output is Q-bar. The outputs Q and Q-bar are cross-connected to the inputs of the opposite gate.\n\n\n**Figure 11.22**\n   The S-R Latch Implemented with NOR Gates\n\n\nFirst, let us show that the circuit is bistable. Assume that both\n   \n    S\n   \n   and\n   \n    R\n   \n   are 0 and that\n   \n    Q\n   \n   is 0. The inputs to the lower NOR gate are\n   \n    Q = 0\n   \n   and\n   \n    S = 0\n   \n   . Thus, the output\n   \n    \\bar{Q} = 1\n   \n   means that the inputs to the upper NOR gate are\n   \n    \\bar{Q} = 1\n   \n   and\n   \n    R = 0\n   \n   , which has the output\n   \n    Q = 0\n   \n   . Thus, the state of the circuit is internally consistent and remains stable as long as\n   \n    S = R = 0\n   \n   . A similar line of reasoning shows that the state\n   \n    Q = 1, \\bar{Q} = 0\n   \n   is also stable for\n   \n    R = S = 0\n   \n   .\n\n\nThus, this circuit can function as a 1-bit memory. We can view the output\n   \n    Q\n   \n   as the “value” of the bit. The inputs\n   \n    S\n   \n   and\n   \n    R\n   \n   serve to write the values 1 and 0, respectively, into memory. To see this, consider the state\n   \n    Q = 0, \\bar{Q} = 1, S = 0, R = 0\n   \n   . Suppose that\n   \n    S\n   \n   changes to the value 1. Now the inputs to the lower NOR gate are\n   \n    S = 1, Q = 0\n   \n   . After some time delay\n   \n    \\Delta t\n   \n   , the output of the lower NOR gate will be\n   \n    \\bar{Q} = 0\n   \n   (see Figure 11.23). So, at this point in time, the inputs to the upper NOR gate become\n   \n    R = 0, \\bar{Q} = 0\n   \n   . After another gate delay of\n   \n    \\Delta t\n   \n   the output\n   \n    Q\n   \n   becomes 1. This is again a stable state. The inputs to the lower gate are now\n   \n    S = 1, Q = 1\n   \n   , which maintain the output\n   \n    \\bar{Q} = 0\n   \n   . As long as\n   \n    S = 1\n   \n   and\n   \n    R = 0\n   \n   , the outputs will remain\n   \n    Q = 1, \\bar{Q} = 0\n   \n   . Furthermore, if\n   \n    S\n   \n   returns to 0, the outputs will remain unchanged.\n\n\nThe\n   \n    R\n   \n   output performs the opposite function. When\n   \n    R\n   \n   goes to 1, it forces\n   \n    Q = 0, \\bar{Q} = 1\n   \n   regardless of the previous state of\n   \n    Q\n   \n   and\n   \n    \\bar{Q}\n   \n   . Again, a time delay of\n   \n    2\\Delta t\n   \n   occurs before the final state is established (Figure 11.23).\n\n\nThe S-R latch can be defined with a table similar to a truth table, called a\n   *characteristic table*\n   , which shows the next state or states of a sequential circuit as a function of current states and inputs. In the case of the S-R latch, the state can be defined by the value of\n   \n    Q\n   \n   . Table 11.10a shows the resulting characteristic table. Observe that the inputs\n   \n    S = 1, R = 1\n   \n   are not allowed, because these would produce an inconsistent output (both\n   \n    Q\n   \n   and\n   \n    \\bar{Q}\n   \n   equal 0). The table can be expressed more compactly, as in Table 11.10b. An illustration of the behavior of the S-R latch is shown in Table 11.10c.\n\n\n**CLOCKED S-R FLIP-FLOP**\n   The output of the S-R latch changes, after a brief time delay, in response to a change in the input. This is referred to as asynchronous operation. More typically, events in the digital computer are synchronized to a clock pulse, so that changes occur only when a clock pulse occurs. Figure 11.24 shows this\n\n\n\n\n![Timing diagram for a NOR S-R Latch showing inputs S and R, and outputs Q and Q-bar over time t. S is high, then low, then high. R is high, then low, then high. Q transitions from high to low when S goes low, and from low to high when R goes low. Q-bar transitions from low to high when S goes low, and from high to low when R goes low. Transition times are labeled as 2Δt and Δt.](images/image_0198.jpeg)\n\n\nTiming diagram for a NOR S-R Latch showing inputs S and R, and outputs Q and Q-bar over time t. S is high, then low, then high. R is high, then low, then high. Q transitions from high to low when S goes low, and from low to high when R goes low. Q-bar transitions from low to high when S goes low, and from high to low when R goes low. Transition times are labeled as 2Δt and Δt.\n\n\n**Figure 11.23**\n   NOR S-R Latch Timing Diagram\n\n\n**Table 11.10**\n   The S-R Latch\n\n\n\n(a) Characteristic Table\nCurrent Inputs | Current State | Next State\nSR | Q_n | Q_{n+1}\n00 | 0 | 0\n00 | 1 | 1\n01 | 0 | 0\n01 | 1 | 0\n10 | 0 | 1\n10 | 1 | 1\n11 | 0 | —\n11 | 1 | —\n\n\n\n(b) Simplified Characteristic Table\nS | R | Q_{n+1}\n0 | 0 | Q_n\n0 | 1 | 0\n1 | 0 | 1\n1 | 1 | —\n\n\n\n(c) Response to Series of Inputs\nt | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\nS | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0\nR | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0\nQ_{n+1} | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1\n\n\n\n\n![Figure 11.24: Clocked S-R Flip-Flop logic diagram. It consists of two NOR gates. The inputs R and S are connected to the nonclock inputs of both NOR gates. The outputs of the NOR gates are cross-connected to each other. The output of the top NOR gate is Q, and the output of the bottom NOR gate is Q-bar. The Clock signal is connected to the clock inputs of both NOR gates.](images/image_0199.jpeg)\n\n\nFigure 11.24: Clocked S-R Flip-Flop logic diagram. It consists of two NOR gates. The inputs R and S are connected to the nonclock inputs of both NOR gates. The outputs of the NOR gates are cross-connected to each other. The output of the top NOR gate is Q, and the output of the bottom NOR gate is Q-bar. The Clock signal is connected to the clock inputs of both NOR gates.\n\n\n**Figure 11.24**\n   Clocked S-R Flip-Flop\n\n\n\n\n![Figure 11.25: D Flip-Flop logic diagram. It consists of two NOR gates. The input D is connected to the nonclock input of the top NOR gate. The output of the top NOR gate is connected to the nonclock input of the bottom NOR gate. The output of the bottom NOR gate is Q-bar. The output of the top NOR gate is Q. The Clock signal is connected to the clock inputs of both NOR gates. An inverter is shown between the D input and the top NOR gate's nonclock input.](images/image_0200.jpeg)\n\n\nFigure 11.25: D Flip-Flop logic diagram. It consists of two NOR gates. The input D is connected to the nonclock input of the top NOR gate. The output of the top NOR gate is connected to the nonclock input of the bottom NOR gate. The output of the bottom NOR gate is Q-bar. The output of the top NOR gate is Q. The Clock signal is connected to the clock inputs of both NOR gates. An inverter is shown between the D input and the top NOR gate's nonclock input.\n\n\n**Figure 11.25**\n   D Flip-Flop\n\n\narrangement. This device is referred to as a\n   **clocked S-R flip-flop**\n   . Note that the R and S inputs are passed to the NOR gates only during the clock pulse.\n\n\n**D FLIP-FLOP**\n   One problem with S-R flip-flop is that the condition\n   \n    R = 1, S = 1\n   \n   must be avoided. One way to do this is to allow just a single input. The\n   **D flip-flop**\n   accomplishes this. Figure 11.25 shows a gate implementation of the D flip-flop. By using an inverter, the nonclock inputs to the two AND gates are guaranteed to be the opposite of each other.\n\n\nThe D flip-flop is sometimes referred to as the data flip-flop because it is, in effect, storage for one bit of data. The output of the D flip-flop is always equal to the most recent value applied to the input. Hence, it remembers and produces the last input. It is also referred to as the delay flip-flop, because it delays a 0 or 1 applied to its input for a single clock pulse. We can capture the logic of the D flip-flop in the following truth table:\n\n\n\nD | Q_{n+1}\n0 | 0\n1 | 1\n\n\n**J-K FLIP-FLOP**\n   Another useful flip-flop is the\n   **J-K flip-flop**\n   . Like the S-R flip-flop, it has two inputs. However, in this case all possible combinations of input values are valid. Figure 11.26 shows a gate implementation of the J-K flip-flop, and Figure 11.27 shows its characteristic table (along with those for the S-R and D flip-flops). Note that the first three combinations are the same as for the S-R flip-flop. With no input asserted, the output is stable. If only the J input is asserted, the result is a set\n\n\n\n\n![Circuit diagram of a J-K Flip-Flop. It consists of two AND gates, two NOR gates, and a clock input. The J and K inputs are connected to the first AND gate. The output of this AND gate and the clock signal are connected to the second AND gate. The output of the second AND gate is connected to the input of the top NOR gate. The J and K inputs are also connected to the third AND gate. The output of this AND gate and the clock signal are connected to the fourth AND gate. The output of the fourth AND gate is connected to the input of the bottom NOR gate. The outputs of the top and bottom NOR gates are Q and Q-bar respectively.](images/image_0201.jpeg)\n\n\nCircuit diagram of a J-K Flip-Flop. It consists of two AND gates, two NOR gates, and a clock input. The J and K inputs are connected to the first AND gate. The output of this AND gate and the clock signal are connected to the second AND gate. The output of the second AND gate is connected to the input of the top NOR gate. The J and K inputs are also connected to the third AND gate. The output of this AND gate and the clock signal are connected to the fourth AND gate. The output of the fourth AND gate is connected to the input of the bottom NOR gate. The outputs of the top and bottom NOR gates are Q and Q-bar respectively.\n\n\nFigure 11.26 J-K Flip-Flop\n\n\nfunction, causing the output to be 1; if only the K input is asserted, the result is a reset function, causing the output to be 0. When both J and K are 1, the function performed is referred to as the toggle function: the output is reversed. Thus, if Q is 1 and 1 is applied to J and K, then Q becomes 0. The reader should verify that the implementation of Figure 11.26 produces this characteristic function.\n\n\n\nName | Graphical Symbol | Truth Table\nS-R | Image: S-R flip-flop symbol: a rectangle with S and R inputs on the left, Q and Q-bar outputs on the right, and a clock input (Ck) with a triangle and a bubble on the bottom side. | \\begin{array}{c|c|c} S & R & Q_{n+1} \\\\ \\hline 0 & 0 & Q_n \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & - \\end{array}\nJ-K | Image: J-K flip-flop symbol: a rectangle with J and K inputs on the left, Q and Q-bar outputs on the right, and a clock input (Ck) with a triangle and a bubble on the bottom side. | \\begin{array}{c|c|c} J & K & Q_{n+1} \\\\ \\hline 0 & 0 & Q_n \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & \\overline{Q_n} \\end{array}\nD | Image: D flip-flop symbol: a rectangle with D input on the left, Q and Q-bar outputs on the right, and a clock input (Ck) with a triangle and a bubble on the bottom side. | \\begin{array}{c|c} D & Q_{n+1} \\\\ \\hline 0 & 0 \\\\ 1 & 1 \\end{array}\n\n\nFigure 11.27 Basic Flip-Flops\n\n\n\n\n![Figure 11.28: 8-Bit Parallel Register. The diagram shows eight D flip-flops connected in parallel. Data lines D18 through D11 are connected to the D inputs of the flip-flops. Output lines D01 through D08 are connected to the Q outputs. A common clock line is connected to the clock inputs (Clk) of all flip-flops through an AND gate labeled 'Clock Load'.](images/image_0202.jpeg)\n\n\nFigure 11.28: 8-Bit Parallel Register. The diagram shows eight D flip-flops connected in parallel. Data lines D18 through D11 are connected to the D inputs of the flip-flops. Output lines D01 through D08 are connected to the Q outputs. A common clock line is connected to the clock inputs (Clk) of all flip-flops through an AND gate labeled 'Clock Load'.\n\n\n**Figure 11.28**\n   8-Bit Parallel Register\n\n\n\n\n**Registers**\n\n\nAs an example of the use of flip-flops, let us first examine one of the essential elements of the CPU: the register. As we know, a register is a digital circuit used within the CPU to store one or more bits of data. Two basic types of registers are commonly used: parallel registers and shift registers.\n\n\n**PARALLEL REGISTERS**\n   A\n   **parallel register**\n   consists of a set of 1-bit memories that can be read or written simultaneously. It is used to store data. The registers that we have discussed throughout this book are parallel registers.\n\n\nThe 8-bit register of Figure 11.28 illustrates the operation of a parallel register using D flip-flops. A control signal, labeled\n   *load*\n   , controls writing into the register from signal lines, D11 through D18. These lines might be the output of multiplexers, so that data from a variety of sources can be loaded into the register.\n\n\n**SHIFT REGISTER**\n   A\n   **shift register**\n   accepts and/or transfers information serially. Consider, for example, Figure 11.29, which shows a 5-bit shift register constructed from clocked D flip-flops. Data are input only to the leftmost flip-flop. With each clock pulse, data are shifted to the right one position, and the rightmost bit is transferred out.\n\n\nShift registers can be used to interface to serial I/O devices. In addition, they can be used within the ALU to perform logical shift and rotate functions. In this\n\n\n\n\n![Figure 11.29: 5-Bit Shift Register. The diagram shows five D flip-flops connected in a serial chain. The first flip-flop has a 'Serial in' input to its D input. The Q output of each flip-flop is connected to the D input of the next flip-flop to the right. The Q output of the last (fifth) flip-flop is labeled 'Serial out'. A common clock line is connected to the clock inputs (Clk) of all flip-flops.](images/image_0203.jpeg)\n\n\nFigure 11.29: 5-Bit Shift Register. The diagram shows five D flip-flops connected in a serial chain. The first flip-flop has a 'Serial in' input to its D input. The Q output of each flip-flop is connected to the D input of the next flip-flop to the right. The Q output of the last (fifth) flip-flop is labeled 'Serial out'. A common clock line is connected to the clock inputs (Clk) of all flip-flops.\n\n\n**Figure 11.29**\n   5-Bit Shift Register\n\n\nlatter capacity, they need to be equipped with parallel read/write circuitry as well as serial.\n\n\n\n\n**Counters**\n\n\nAnother useful category of sequential circuit is the\n   **counter**\n   . A counter is a register whose value is easily incremented by 1 modulo the capacity of the register; that is, after the maximum value is achieved the next increment sets the counter value to 0. Thus, a register made up of\n   \n    n\n   \n   flip-flops can count up to\n   \n    2^n - 1\n   \n   . An example of a counter in the CPU is the program counter.\n\n\nCounters can be designated as asynchronous or synchronous, depending on the way in which they operate. Asynchronous counters are relatively slow because the output of one flip-flop triggers a change in the status of the next flip-flop. In a\n   **synchronous counter**\n   , all of the flip-flops change state at the same time. Because the latter type is much faster, it is the kind used in CPUs. However, it is useful to begin the discussion with a description of an asynchronous counter.\n\n\n**RIPPLE COUNTER**\n   An asynchronous counter is also referred to as a\n   **ripple counter**\n   , because the change that occurs to increment the counter starts at one end and “ripples” through to the other end. Figure 11.30 shows an implementation of a 4-bit counter using J–K flip-flops, together with a timing diagram that illustrates its behavior. The timing diagram is idealized in that it does not show the propagation delay that occurs as the signals move down the series of flip-flops. The output of the leftmost flip-flop (\n   \n    Q_0\n   \n   ) is the least significant bit. The design could clearly be extended to an arbitrary number of bits by cascading more flip-flops.\n\n\n\n\n![Figure 11.30: Ripple Counter. (a) Sequential circuit diagram showing four J-K flip-flops cascaded in an asynchronous configuration. The clock input (Ck) of each flip-flop is connected to the Q output of the previous flip-flop. The J and K inputs of all flip-flops are connected to a common 'High' signal. The outputs are labeled Q0, Q1, Q2, and Q3 from left to right. (b) Timing diagram showing the clock signal (Clock) and the outputs Q0, Q1, Q2, and Q3. The clock is a continuous square wave. Q0 toggles on every clock edge. Q1 toggles on every falling edge of Q0. Q2 toggles on every falling edge of Q1. Q3 toggles on every falling edge of Q2, resulting in a divide-by-16 counter.](images/image_0204.jpeg)\n\n\n(a) Sequential circuit\n\n\n(b) Timing diagram\n\n\nFigure 11.30: Ripple Counter. (a) Sequential circuit diagram showing four J-K flip-flops cascaded in an asynchronous configuration. The clock input (Ck) of each flip-flop is connected to the Q output of the previous flip-flop. The J and K inputs of all flip-flops are connected to a common 'High' signal. The outputs are labeled Q0, Q1, Q2, and Q3 from left to right. (b) Timing diagram showing the clock signal (Clock) and the outputs Q0, Q1, Q2, and Q3. The clock is a continuous square wave. Q0 toggles on every clock edge. Q1 toggles on every falling edge of Q0. Q2 toggles on every falling edge of Q1. Q3 toggles on every falling edge of Q2, resulting in a divide-by-16 counter.\n\n\n**Figure 11.30**\n   Ripple Counter\n\n\nIn the illustrated implementation, the counter is incremented with each clock pulse. The J and K inputs to each flip-flop are held at a constant 1. This means that, when there is a clock pulse, the output at Q will be inverted (1 to 0; 0 to 1). Note that the change in state is shown as occurring with the falling edge of the clock pulse; this is known as an edge-triggered flip-flop. Using flip-flops that respond to the transition in a clock pulse rather than the pulse itself provides better timing control in complex circuits. If one looks at patterns of output for this counter, it can be seen that it cycles through 0000, 0001, ..., 1110, 1111, 0000, and so on.\n\n\n**SYNCHRONOUS COUNTERS**\n   The ripple counter has the disadvantage of the delay involved in changing value, which is proportional to the length of the counter. To overcome this disadvantage, CPUs make use of synchronous counters, in which all of the flip-flops of the counter change at the same time. In this subsection, we present a design for a 3-bit synchronous counter. In doing so, we illustrate some basic concepts in the design of a synchronous circuit.\n\n\nFor a 3-bit counter, three flip-flops will be needed. Let us use J–K flip-flops. Label the uncomplemented output of the three flip-flops C, B, and A, respectively, with C representing the most significant bit. The first step is to construct a truth table that relates the J–K inputs and outputs, to allow us to design the overall circuit. Such a truth table is shown in Figure 11.31a. The first three columns show the possible combinations of outputs C, B, and A. They are listed in the order that they will appear as the counter is incremented. Each row lists the current value of C, B, and A and the inputs to the three flip-flops that will be required to reach the next value of C, B, and A.\n\n\nTo understand the way in which the truth table of Figure 11.31a is constructed, it may be helpful to recast the characteristic table for the J–K flip-flop. Recall that this table was presented as follows:\n\n\n\nJ | K | Q_{n+1}\n0 | 0 | Q_n\n0 | 1 | 0\n1 | 0 | 1\n1 | 1 | \\overline{Q_{n+1}}\n\n\nIn this form, the table shows the effect that the J and K inputs have on the output. Now consider the following organization of the same information:\n\n\n\nQ_n | J | K | Q_{n+1}\n0 | 0 | d | 0\n0 | 1 | d | 1\n1 | d | 1 | 0\n1 | d | 0 | 1\n\n\nIn this form, the table provides the value of the next output when the inputs and the present output are known. This is exactly the information needed to design the counter or, indeed, any sequential circuit. In this form, the table is referred to as an\n   **excitation table**\n   .\n\n\n\n\n![Logic diagram of a synchronous counter using three JK flip-flops labeled A, B, and C. The clock input (Ck) is connected to the negative-edge triggered clock inputs of all three flip-flops. The J and K inputs are as follows: J_A = 1, K_A = 1; J_B = A, K_B = A-bar; J_C = BA, K_C = BA-bar. The outputs are Q_A = A, Q_B = B, and Q_C = C, which form the binary output. A high signal is also connected to the J_A input.](images/image_0205.jpeg)\n\n\n(a) Truth table\n\n\n\nC | B | A | Jc | Kc | Jb | Kb | Ja | Ka\n0 | 0 | 0 | 0 | d | 0 | d | 1 | d\n0 | 0 | 1 | 0 | d | 1 | d | d | 1\n0 | 1 | 0 | 0 | d | d | 0 | 1 | d\n0 | 1 | 1 | 1 | d | d | 1 | d | 1\n1 | 0 | 0 | d | 0 | 0 | d | 1 | d\n1 | 0 | 1 | d | 0 | 1 | d | d | 1\n1 | 1 | 0 | d | 0 | d | 0 | 1 | d\n1 | 1 | 1 | d | 1 | d | 1 | d | 1\n\n\n(b) Karnaugh maps\n\n\nJc = BA\n\n\n\n | BA\n00 | 01 | 11 | 10\nC | 0 |  |  | 1 | \n1 | d | d | d | d\n\n\nKc = BA\n\n\n\n | BA\n00 | 01 | 11 | 10\nC | 0 | d | d | d | d\n1 |  |  | 1 | \n\n\nJb = A\n\n\n\n | BA\n00 | 01 | 11 | 10\nC | 0 |  | 1 | d | d\n1 |  | 1 | d | d\n\n\nKb = A\n\n\n\n | BA\n00 | 01 | 11 | 10\nC | 0 | d | d | 1 | \n1 | d | d | 1 | \n\n\nJa = 1\n\n\n\n | BA\n00 | 01 | 11 | 10\nC | 0 | 1 | d | d | 1\n1 | 1 | d | d | 1\n\n\nKa = 1\n\n\n\n | BA\n00 | 01 | 11 | 10\nC | 0 | d | 1 | 1 | d\n1 | d | 1 | 1 | d\n\n\n(c) Logic diagram\n\n\nLogic diagram of a synchronous counter using three JK flip-flops labeled A, B, and C. The clock input (Ck) is connected to the negative-edge triggered clock inputs of all three flip-flops. The J and K inputs are as follows: J_A = 1, K_A = 1; J_B = A, K_B = A-bar; J_C = BA, K_C = BA-bar. The outputs are Q_A = A, Q_B = B, and Q_C = C, which form the binary output. A high signal is also connected to the J_A input.\n\n\n**Figure 11.31**\n   Design of a Synchronous Counter\n\n\nLet us return to Figure 11.31a. Consider the first row. We want the value of C to remain 0, the value of B to remain 0, and the value of A to go from 0 to 1 with the next application of a clock pulse. The excitation table shows that to maintain an output of 0, we must have inputs of\n   \n    J = 0\n   \n   and don't care for\n   \n    K\n   \n   . To effect a transition from 0 to 1, the inputs must be\n   \n    J = 1\n   \n   and\n   \n    K = d\n   \n   . These values are shown in the first row of the table. By similar reasoning, the remainder of the table can be filled in.\n\n\nHaving constructed the truth table of Figure 11.31a, we see that the table shows the required values of all of the J and K inputs as functions of the current values of C, B, and A. With the aid of Karnaugh maps, we can develop Boolean expressions for these six functions. This is shown in part b of the figure. For example, the Karnaugh map for the variable\n   \n    J_a\n   \n   (the J input to the flip-flop that produces the A output) yields the expression\n   \n    J_a = BC\n   \n   . When all six expressions are derived, it is a straightforward matter to design the actual circuit, as shown in part c of the figure."
        },
        {
          "name": "Programmable Logic Devices",
          "content": "Thus far, we have treated individual gates as building blocks, from which arbitrary functions can be realized. The designer could pursue a strategy of minimizing the number of gates to be used by manipulating the corresponding Boolean expressions.\n\n\nAs the level of integration provided by integrated circuits increases, other considerations apply. Early integrated circuits, using small-scale integration (SSI), provided from one to ten gates on a chip. Each gate is treated independently, in the building-block approach described so far. To construct a logic function, a number of these chips are laid out on a printed circuit board and the appropriate pin interconnections are made.\n\n\nIncreasing levels of integration made it possible to put more gates on a chip and to make gate interconnections on the chip as well. This yields the advantages of decreased cost, decreased size, and increased speed (because on-chip delays are of shorter duration than off-chip delays). A design problem arises, however. For each particular logic function or set of functions, the layout of gates and interconnections on the chip must be designed. The cost and time involved in such custom chip design is high. Thus, it becomes attractive to develop a general-purpose chip that can be readily adapted to specific purposes. This is the intent of the\n   *programmable logic device*\n   (PLD).\n\n\nThere are a number of different types of PLDs in commercial use. Table 11.11 lists some of the key terms and defines some of the most important types. In this section, we first look at one of the simplest such devices, the\n   *programmable logic array*\n   (PLA) and then introduce perhaps the most important and widely used type of PLD, the field-programmable gate array (FPGA).\n\n\n\n\n**Programmable Logic Array**\n\n\nThe PLA is based on the fact that any Boolean function (truth table) can be expressed in a sum-of-products (SOP) form, as we have seen. The PLA consists of a regular arrangement of NOT, AND, and OR gates on a chip. Each chip input is passed through a NOT gate so that each input and its complement are available to each AND gate. The output of each AND gate is available to each OR gate, and the output of each OR gate is a chip output. By making the appropriate connections, arbitrary SOP expressions can be implemented.\n\n\nFigure 11.32a shows a PLA with three inputs, eight gates, and two outputs. On the left is a programmable AND array. The AND array is programmed by establishing a connection between any PLA input or its negation and any AND gate input by connecting the corresponding lines at their point of intersection. On the\n\n\n**Table 11.11**\n\n\n**Programmable Logic Device (PLD)**\n\n\nA general term that refers to any type of integrated circuit used for implementing digital hardware, where the chip can be configured by the end user to realize different designs. Programming of such a device often involves placing the chip into a special programming unit, but some chips can also be configured “in-system.” Also referred to as a field-programmable device (FPD).\n\n\n\n\n**Programmable Logic Array (PLA)**\n\n\nA relatively small PLD that contains two levels of logic, an AND-plane and an OR-plane, where both levels are programmable.\n\n\n\n\n**Programmable Array Logic (PAL)**\n\n\nA relatively small PLD that has a programmable AND-plane followed by a fixed OR-plane.\n\n\n\n\n**Simple PLD (SPLD)**\n\n\nA PLA or PAL.\n\n\n\n\n**Complex PLD (CPLD)**\n\n\nA more complex PLD that consists of an arrangement of multiple SPLD-like blocks on a single chip.\n\n\n\n\n**Field-Programmable Gate Array (FPGA)**\n\n\nA PLD featuring a general structure that allows very high logic capacity. Whereas CPLDs feature logic resources with a wide number of inputs (AND planes), FPGAs offer more narrow logic resources. FPGAs also offer a higher ratio of flip-flops to logic resources than do CPLDs.\n\n\n\n\n**Logic Block**\n\n\nA relatively small circuit block that is replicated in an array in an FPD. When a circuit is implemented in an FPD, it is first decomposed into smaller subcircuits that can each be mapped into a logic block. The term\n   *logic block*\n   is mostly used in the context of FPGAs, but it could also refer to a block of circuitry in a CPLD.\n\n\nright is a programmable OR array, which involves connecting AND gate outputs to OR gate inputs. Most larger PLAs contain several hundred gates, 15 to 25 inputs, and 5 to 15 outputs. The connections from the inputs to the AND gates, and from the AND gates to the OR gates, are not specified until programming time.\n\n\nPLAs are manufactured in two different ways to allow easy programming (making of connections). In the first, every possible connection is made through a fuse at every intersection point. The undesired connections can then be later removed by blowing the fuses. This type of PLA is referred to as a\n   *field-programmable logic array (FPLA)*\n   . Alternatively, the proper connections can be made during chip fabrication by using an appropriate mask supplied for a particular interconnection pattern. In either case, the PLA provides a flexible, inexpensive way of implementing digital logic functions.\n\n\nFigure 11.32b shows a programmed PLA that realizes two Boolean expressions.\n\n\n\n\n**Field-Programmable Gate Array**\n\n\nThe PLA is an example of a simple PLD (SPLD). The difficulty with increasing capacity of a strict SPLD architecture is that the structure of the programmable logic-planes grows too quickly in size as the number of inputs is increased. The only feasible way to provide large capacity devices based on SPLD architectures is to then integrate multiple SPLDs onto a single chip and provide interconnect to programmably connect the SPLD blocks together. Many commercial PLD products\n\n\n\n\n![Diagram (a) showing the layout of a 3-input 2-output PLA. It consists of an AND array with 3 inputs (I1, I2, I3) and 3 AND gates, followed by an OR array with 2 OR gates producing outputs O1 and O2.](images/image_0206.jpeg)\n\n\nDiagram (a) illustrates the layout of a 3-input 2-output PLA. The structure is divided into two main sections: the \"AND\" array and the \"OR\" array. The \"AND\" array has three inputs,\n    \n     I_1\n    \n    ,\n    \n     I_2\n    \n    , and\n    \n     I_3\n    \n    , each connected to a programmable switch (represented by a triangle with a circle). These switches connect to three horizontal lines representing the AND gates. The \"OR\" array consists of two OR gates, each receiving inputs from the three horizontal lines of the AND array. The final outputs are\n    \n     O_1\n    \n    and\n    \n     O_2\n    \n    .\n\n\nDiagram (a) showing the layout of a 3-input 2-output PLA. It consists of an AND array with 3 inputs (I1, I2, I3) and 3 AND gates, followed by an OR array with 2 OR gates producing outputs O1 and O2.\n\n\n(a) Layout for 3-input 2-output PLA\n\n\n\n\n![Diagram (b) showing a programmed PLA. Inputs A, B, and C are connected to switches that determine which of the three AND gates are active. The outputs of the AND gates are connected to switches that determine the final outputs AB̄C̄, ĀB̄, and ĀC̄.](images/image_0207.jpeg)\n\n\nDiagram (b) shows a programmed PLA with three inputs,\n    \n     A\n    \n    ,\n    \n     B\n    \n    , and\n    \n     C\n    \n    . Each input line has a programmable switch (triangle with a circle) that connects to one of three horizontal lines representing AND gates. The outputs of these AND gates are connected to two OR gates. The final outputs are\n    \n     AB\\bar{C}\n    \n    ,\n    \n     \\bar{A}\\bar{B}\n    \n    , and\n    \n     \\bar{A}\\bar{C}\n    \n    . The switches are programmed as follows: Input\n    \n     A\n    \n    connects to the first AND gate; Input\n    \n     B\n    \n    connects to the second AND gate; Input\n    \n     C\n    \n    connects to the third AND gate. The first OR gate receives inputs from the first and second AND gates, while the second OR gate receives inputs from the second and third AND gates.\n\n\nDiagram (b) showing a programmed PLA. Inputs A, B, and C are connected to switches that determine which of the three AND gates are active. The outputs of the AND gates are connected to switches that determine the final outputs AB̄C̄, ĀB̄, and ĀC̄.\n\n\n(b) Programmed PLA\n\n\n**Figure 11.32**\n   An Example of a Programmable Logic Array (PLA)\n\n\nexist on the market today with this basic structure, and are collectively referred to as Complex PLDs (CPLDs). The most important type of CPLD is the FPGA.\n\n\nAn FPGA consists of an array of uncommitted circuit elements, called\n   **logic blocks**\n   , and interconnect resources. An illustration of a typical FPGA architecture is shown in Figure 11.33. The key components of an FPGA are:\n\n\n  * ■\n    **Logic block:**\n    The configurable logic blocks are where the computation of the user's circuit takes place.\n  * ■\n    **I/O block:**\n    The I/O blocks connect I/O pins to the circuitry on the chip.\n  * ■\n    **Interconnect:**\n    These are signal paths available for establishing connections among I/O blocks and logic blocks.\n\n\nThe logic block can be either a combinational circuit or a sequential circuit. In essence, the programming of a logic block is done by downloading the contents of a truth table for a logic function. Figure 11.34 shows an example of a simple logic block consisting of a D flip-flop, a 2-to-1 multiplexer, and a 16-bit\n   **lookup table**\n   . The lookup table is a memory consisting of 16 1-bit elements, so that 4 input lines are required to select one of the 16 bits. Larger logic blocks have larger lookup tables and multiple interconnected lookup tables. The combinational logic realized by the lookup table can be output directly or stored in the D flip-flop and output synchronously. A separate one-bit memory controls the multiplexer to determine whether the output comes directly from the lookup table or from the flip-flop.\n\n\nBy interconnecting numerous logic blocks, very complex logic functions can be easily implemented.\n\n\n\n\n![Diagram illustrating the structure of an FPGA. It shows a grid of logic blocks (green squares) interconnected by a dense grid of lines (interconnects). On the left, an I/O block (a small square) is connected to the grid. On the right, a label 'Logic block' points to one of the green squares in the grid.](images/image_0208.jpeg)\n\n\nThe diagram illustrates the structure of an FPGA. It features a central grid of logic blocks, represented by green squares. These blocks are interconnected by a dense grid of horizontal and vertical lines, representing the interconnect resources. On the left side, an I/O block is shown, consisting of a small square connected to the main grid. On the right side, an arrow points from the text 'Logic block' to one of the green squares in the grid, highlighting the basic building block of the FPGA.\n\n\nDiagram illustrating the structure of an FPGA. It shows a grid of logic blocks (green squares) interconnected by a dense grid of lines (interconnects). On the left, an I/O block (a small square) is connected to the grid. On the right, a label 'Logic block' points to one of the green squares in the grid.\n\n\n**Figure 11.33**\n   Structure of an FPGA\n\n\n\n\n![Figure 11.34: A Simple FPGA Logic Block. The diagram shows a 16x1 lookup table (LUT) with four inputs (A0, A1, A2, A3) and one output. This output is connected to the D input of a D flip-flop. The clock input (Clock) is connected to the Ck input of the D flip-flop. The Q output of the D flip-flop is connected to the data input of a 2-to-1 multiplexer (MUX). The output of the MUX is the final output of the logic block. A small square symbol is also connected to the MUX's control input.](images/image_0209.jpeg)\n\n\nFigure 11.34: A Simple FPGA Logic Block. The diagram shows a 16x1 lookup table (LUT) with four inputs (A0, A1, A2, A3) and one output. This output is connected to the D input of a D flip-flop. The clock input (Clock) is connected to the Ck input of the D flip-flop. The Q output of the D flip-flop is connected to the data input of a 2-to-1 multiplexer (MUX). The output of the MUX is the final output of the logic block. A small square symbol is also connected to the MUX's control input.\n\n\nFigure 11.34 A Simple FPGA Logic Block"
        }
      ]
    },
    {
      "name": "Instruction Sets: Characteristics and Functions",
      "sections": [
        {
          "name": "Machine Instruction Characteristics",
          "content": "The operation of the processor is determined by the instructions it executes, referred to as\n   *machine instructions*\n   or\n   *computer instructions*\n   . The collection of different instructions that the processor can execute is referred to as the processor's\n   *instruction set*\n   .\n\n\n\n\n**Elements of a Machine Instruction**\n\n\nEach instruction must contain the information required by the processor for execution. Figure 12.1, which repeats Figure 3.6, shows the steps involved in instruction execution and, by implication, defines the elements of a machine instruction. These elements are as follows:\n\n\n  * ■\n    **Operation code:**\n    Specifies the operation to be performed (e.g., ADD, I/O). The operation is specified by a binary code, known as the operation code, or\n    **opcode**\n    .\n  * ■\n    **Source operand reference:**\n    The operation may involve one or more source operands, that is, operands that are inputs for the operation.\n\n\n\n\n![Figure 12.1 Instruction Cycle State Diagram. This state machine diagram shows the flow of an instruction cycle. It starts with 'Instruction address calculation' leading to 'Instruction fetch'. 'Instruction fetch' leads to 'Instruction operation decoding'. 'Instruction operation decoding' leads to 'Operand address calculation'. 'Operand address calculation' leads to 'Operand fetch'. 'Operand fetch' can lead to 'Data operation' (via 'Multiple operands') or 'Operand store' (via 'Multiple results'). 'Data operation' leads to 'Operand address calculation'. 'Operand address calculation' leads to 'Operand store'. 'Operand store' leads to 'Instruction complete, fetch next instruction' (via 'Return for string or vector data'). 'Instruction complete, fetch next instruction' leads back to 'Instruction address calculation'.](images/image_0210.jpeg)\n\n\ngraph TD\n    IAC((Instruction address calculation)) --> IF((Instruction fetch))\n    IF --> IOD((Instruction operation decoding))\n    IOD --> OAC((Operand address calculation))\n    OAC --> OF((Operand fetch))\n    OF -- \"Multiple operands\" --> DO((Data operation))\n    OF -- \"Multiple results\" --> OS((Operand store))\n    DO --> OAC2((Operand address calculation))\n    OAC2 --> OS\n    OS -- \"Return for string or vector data\" --> ICI((Instruction complete, fetch next instruction))\n    ICI --> IAC\n  \nFigure 12.1 Instruction Cycle State Diagram. This state machine diagram shows the flow of an instruction cycle. It starts with 'Instruction address calculation' leading to 'Instruction fetch'. 'Instruction fetch' leads to 'Instruction operation decoding'. 'Instruction operation decoding' leads to 'Operand address calculation'. 'Operand address calculation' leads to 'Operand fetch'. 'Operand fetch' can lead to 'Data operation' (via 'Multiple operands') or 'Operand store' (via 'Multiple results'). 'Data operation' leads to 'Operand address calculation'. 'Operand address calculation' leads to 'Operand store'. 'Operand store' leads to 'Instruction complete, fetch next instruction' (via 'Return for string or vector data'). 'Instruction complete, fetch next instruction' leads back to 'Instruction address calculation'.\n\n\n**Figure 12.1**\n   Instruction Cycle State Diagram\n\n\n  * ■\n    **Result operand reference:**\n    The operation may produce a result.\n  * ■\n    **Next instruction reference:**\n    This tells the processor where to fetch the next instruction after the execution of this instruction is complete.\n\n\nThe address of the next instruction to be fetched could be either a real address or a virtual address, depending on the architecture. Generally, the distinction is transparent to the instruction set architecture. In most cases, the next instruction to be fetched immediately follows the current instruction. In those cases, there is no explicit reference to the next instruction. When an explicit reference is needed, the main memory or virtual memory address must be supplied. The form in which that address is supplied is discussed in Chapter 13.\n\n\nSource and result operands can be in one of four areas:\n\n\n  * ■\n    **Main or virtual memory:**\n    As with next instruction references, the main or virtual memory address must be supplied.\n  * ■\n    **Processor register:**\n    With rare exceptions, a processor contains one or more registers that may be referenced by machine instructions. If only one register exists, reference to it may be implicit. If more than one register exists, then each register is assigned a unique name or number, and the instruction must contain the number of the desired register.\n  * ■\n    **Immediate:**\n    The value of the operand is contained in a field in the instruction being executed.\n  * ■\n    **I/O device:**\n    The instruction must specify the I/O module and device for the operation. If memory-mapped I/O is used, this is just another main or virtual memory address.\n\n\n\n\n**Instruction Representation**\n\n\nWithin the computer, each instruction is represented by a sequence of bits. The instruction is divided into fields, corresponding to the constituent elements of the\n\n\n\n\n![](images/image_0211.jpeg)\n\n\n4 Bits | 6 Bits | 6 Bits\nOpcode | Operand reference | Operand reference\n\n\n← 16 Bits →\n\n\n**Figure 12.2**\n   A Simple Instruction Format\n\n\ninstruction. A simple example of an instruction format is shown in Figure 12.2. As another example, the IAS instruction format is shown in Figure 2.2. With most instruction sets, more than one format is used. During instruction execution, an instruction is read into an instruction register (IR) in the processor. The processor must be able to extract the data from the various instruction fields to perform the required operation.\n\n\nIt is difficult for both the programmer and the reader of textbooks to deal with binary representations of machine instructions. Thus, it has become common practice to use a\n   *symbolic representation*\n   of machine instructions. An example of this was used for the IAS instruction set, in Table 1.1.\n\n\nOp codes are represented by abbreviations, called\n   *mnemonics*\n   , that indicate the operation. Common examples include\n\n\n\nADD | Add\nSUB | Subtract\nMUL | Multiply\nDIV | Divide\nLOAD | Load data from memory\nSTOR | Store data to memory\n\n\nOperands are also represented symbolically. For example, the instruction\n\n\nADD R, Y\n\n\nmay mean add the value contained in data location Y to the contents of register R. In this example, Y refers to the address of a location in memory, and R refers to a particular register. Note that the operation is performed on the contents of a location, not on its address.\n\n\nThus, it is possible to write a machine-language program in symbolic form. Each symbolic opcode has a fixed binary representation, and the programmer specifies the location of each symbolic operand. For example, the programmer might begin with a list of definitions:\n\n\nX = 513\n\n\nY = 514\n\n\nand so on. A simple program would accept this symbolic input, convert opcodes and operand references to binary form, and construct binary machine instructions.\n\n\nMachine-language programmers are rare to the point of nonexistence. Most programs today are written in a high-level language or, failing that, assembly language, which is discussed in Appendix B. However, symbolic machine language remains a useful tool for describing machine instructions, and we will use it for that purpose.\n\n\n\n\n**Instruction Types**\n\n\nConsider a high-level language instruction that could be expressed in a language such as BASIC or FORTRAN. For example,\n\n\nX = X + Y\n\n\nThis statement instructs the computer to add the value stored in\n   \n    Y\n   \n   to the value stored in\n   \n    X\n   \n   and put the result in\n   \n    X\n   \n   . How might this be accomplished with machine instructions? Let us assume that the variables\n   \n    X\n   \n   and\n   \n    Y\n   \n   correspond to locations 513 and 514. If we assume a simple set of machine instructions, this operation could be accomplished with three instructions:\n\n\n  * 1. Load a register with the contents of memory location 513.\n  * 2. Add the contents of memory location 514 to the register.\n  * 3. Store the contents of the register in memory location 513.\n\n\nAs can be seen, the single BASIC instruction may require three machine instructions. This is typical of the relationship between a high-level language and a machine language. A high-level language expresses operations in a concise algebraic form, using variables. A machine language expresses operations in a basic form involving the movement of data to or from registers.\n\n\nWith this simple example to guide us, let us consider the types of instructions that must be included in a practical computer. A computer should have a set of instructions that allows the user to formulate any data processing task. Another way to view it is to consider the capabilities of a high-level programming language. Any program written in a high-level language must be translated into machine language to be executed. Thus, the set of machine instructions must be sufficient to express any of the instructions from a high-level language. With this in mind we can categorize instruction types as follows:\n\n\n  * ■\n    **Data processing:**\n    Arithmetic and logic instructions.\n  * ■\n    **Data storage:**\n    Movement of data into or out of register and or memory locations.\n  * ■\n    **Data movement:**\n    I/O instructions.\n  * ■\n    **Control:**\n    Test and branch instructions.\n\n\n*Arithmetic*\n   instructions provide computational capabilities for processing numeric data.\n   *Logic*\n   (Boolean) instructions operate on the bits of a word as bits rather than as numbers; thus, they provide capabilities for processing any other type of data the user may wish to employ. These operations are performed primarily on data in processor registers. Therefore, there must be\n   *memory*\n   instructions for moving data between memory and the registers.\n   *I/O*\n   instructions are needed to transfer programs and data into memory and the results of computations back out to the user.\n   *Test*\n   instructions are used to test the value of a data word or the status of a computation.\n   *Branch*\n   instructions are then used to branch to a different set of instructions depending on the decision made.\n\n\nWe will examine the various types of instructions in greater detail later in this chapter.\n\n\n\n\n**Number of Addresses**\n\n\nOne of the traditional ways of describing processor architecture is in terms of the number of addresses contained in each instruction. This dimension has become less significant with the increasing complexity of processor design. Nevertheless, it is useful at this point to draw and analyze this distinction.\n\n\nWhat is the maximum number of addresses one might need in an instruction? Evidently, arithmetic and logic instructions will require the most operands. Virtually all arithmetic and logic operations are either unary (one source operand) or binary (two source operands). Thus, we would need a maximum of two addresses to reference source operands. The result of an operation must be stored, suggesting a third address, which defines a destination operand. Finally, after completion of an instruction, the next instruction must be fetched, and its address is needed.\n\n\nThis line of reasoning suggests that an instruction could plausibly be required to contain four address references: two source operands, one destination operand, and the address of the next instruction. In most architectures, many instructions have one, two, or three operand addresses, with the address of the next instruction being implicit (obtained from the program counter). Most architectures also have a few special-purpose instructions with more operands. For example, the load and store multiple instructions of the ARM architecture, described in Chapter 13, designate up to 17 register operands in a single instruction.\n\n\nFigure 12.3 compares typical one-, two-, and three-address instructions that could be used to compute\n   \n    Y = (A - B)/[C + (D \\times E)]\n   \n   . With three addresses, each instruction specifies two source operand locations and a destination operand location. Because we choose not to alter the value of any of the operand locations,\n\n\n\nInstruction | Comment\nSUB Y, A, B | Y \\leftarrow A - B\nMPY T, D, E | T \\leftarrow D \\times E\nADD T, T, C | T \\leftarrow T + C\nDIV Y, Y, T | Y \\leftarrow Y \\div T\n\n\n(a) Three-address instructions\n\n\n\nInstruction | Comment\nMOVE Y, A | Y \\leftarrow A\nSUB Y, B | Y \\leftarrow Y - B\nMOVE T, D | T \\leftarrow D\nMPY T, E | T \\leftarrow T \\times E\nADD T, C | T \\leftarrow T + C\nDIV Y, T | Y \\leftarrow Y \\div T\n\n\n(b) Two-address instructions\n\n\n\nInstruction | Comment\nLOAD D | AC \\leftarrow D\nMPY E | AC \\leftarrow AC \\times E\nADD C | AC \\leftarrow AC + C\nSTOR Y | Y \\leftarrow AC\nLOAD A | AC \\leftarrow A\nSUB B | AC \\leftarrow AC - B\nDIV Y | AC \\leftarrow AC \\div Y\nSTOR Y | Y \\leftarrow AC\n\n\n(c) One-address instructions\n\n\n**Figure 12.3**\n   Programs to Execute\n   \n    Y = \\frac{A - B}{C + (D \\times E)}\n\n\na temporary location,\n   \n    T\n   \n   , is used to store some intermediate results. Note that there are four instructions and that the original expression had five operands.\n\n\nThree-address instruction formats are not common because they require a relatively long instruction format to hold the three address references. With two-address instructions, and for binary operations, one address must do double duty as both an operand and a result. Thus, the instruction\n   \n    \\text{SUB } Y, B\n   \n   carries out the calculation\n   \n    Y - B\n   \n   and stores the result in\n   \n    Y\n   \n   . The two-address format reduces the space requirement but also introduces some awkwardness. To avoid altering the value of an operand, a\n   **MOVE**\n   instruction is used to move one of the values to a result or temporary location before performing the operation. Our sample program expands to six instructions.\n\n\nSimpler yet is the one-address instruction. For this to work, a second address must be implicit. This was common in earlier machines, with the implied address being a processor register known as the\n   **accumulator**\n   (AC). The accumulator contains one of the operands and is used to store the result. In our example, eight instructions are needed to accomplish the task.\n\n\nIt is, in fact, possible to make do with zero addresses for some instructions. Zero-address instructions are applicable to a special memory organization called a stack. A stack is a last-in-first-out set of locations. The stack is in a known location and, often, at least the top two elements are in processor registers. Thus, zero-address instructions would reference the top two stack elements. Stacks are described in Appendix I. Their use is explored further later in this chapter and in Chapter 13.\n\n\nTable 12.1 summarizes the interpretations to be placed on instructions with zero, one, two, or three addresses. In each case in the table, it is assumed that the address of the next instruction is implicit, and that one operation with two source operands and one result operand is to be performed.\n\n\nThe number of addresses per instruction is a basic design decision. Fewer addresses per instruction result in instructions that are more primitive, requiring a less complex processor. It also results in instructions of shorter length. On the other hand, programs contain more total instructions, which in general results in longer execution times and longer, more complex programs. Also, there is an important threshold between one-address and multiple-address instructions. With one-address instructions, the programmer generally has available only one general-purpose\n\n\n**Table 12.1**\n   Utilization of Instruction Addresses (Nonbranching Instructions)\n\n\n\nNumber of Addresses | Symbolic Representation | Interpretation\n3 | OP A, B, C | A \\leftarrow B \\text{ OP } C\n2 | OP A, B | A \\leftarrow A \\text{ OP } B\n1 | OP A | AC \\leftarrow AC \\text{ OP } A\n0 | OP | T \\leftarrow (T - 1) \\text{ OP } T\n\n\nAC = accumulator\n\n\nT = top of stack\n\n\n(T - 1)\n   \n   = second element of stack\n\n\nA, B, C = memory or register locations\n\n\nregister, the accumulator. With multiple-address instructions, it is common to have multiple general-purpose registers. This allows some operations to be performed solely on registers. Because register references are faster than memory references, this speeds up execution. For reasons of flexibility and ability to use multiple registers, most contemporary machines employ a mixture of two- and three-address instructions.\n\n\nThe design trade-offs involved in choosing the number of addresses per instruction are complicated by other factors. There is the issue of whether an address references a memory location or a register. Because there are fewer registers, fewer bits are needed for a register reference. Also, as we will see in Chapter 13, a machine may offer a variety of addressing modes, and the specification of mode takes one or more bits. The result is that most processor designs involve a variety of instruction formats.\n\n\n\n\n**Instruction Set Design**\n\n\nOne of the most interesting, and most analyzed, aspects of computer design is instruction set design. The design of an instruction set is very complex because it affects so many aspects of the computer system. The instruction set defines many of the functions performed by the processor and thus has a significant effect on the implementation of the processor. The instruction set is the programmer's means of controlling the processor. Thus, programmer requirements must be considered in designing the instruction set.\n\n\nIt may surprise you to know that some of the most fundamental issues relating to the design of instruction sets remain in dispute. Indeed, in recent years, the level of disagreement concerning these fundamentals has actually grown. The most important of these fundamental design issues include the following:\n\n\n  * ■\n    **Operation repertoire:**\n    How many and which operations to provide, and how complex operations should be.\n  * ■\n    **Data types:**\n    The various types of data upon which operations are performed.\n  * ■\n    **Instruction format:**\n    Instruction length (in bits), number of addresses, size of various fields, and so on.\n  * ■\n    **Registers:**\n    Number of processor registers that can be referenced by instructions, and their use.\n  * ■\n    **Addressing:**\n    The mode or modes by which the address of an operand is specified.\n\n\nThese issues are highly interrelated and must be considered together in designing an instruction set. This book, of course, must consider them in some sequence, but an attempt is made to show the interrelationships.\n\n\nBecause of the importance of this topic, much of Part Three is devoted to instruction set design. Following this overview section, this chapter examines data types and operation repertoire. Chapter 13 examines addressing modes (which includes a consideration of registers) and instruction formats. Chapter 15 examines the reduced instruction set computer (RISC). RISC architecture calls into question many of the instruction set design decisions traditionally made in commercial computers."
        },
        {
          "name": "Types of Operands",
          "content": "Machine instructions operate on data. The most important general categories of data are\n\n\n  * ■ Addresses\n  * ■ Numbers\n  * ■ Characters\n  * ■ Logical data\n\n\nWe shall see, in discussing addressing modes in Chapter 13, that addresses are, in fact, a form of data. In many cases, some calculation must be performed on the operand reference in an instruction to determine the main or virtual memory address. In this context, addresses can be considered to be unsigned integers.\n\n\nOther common data types are numbers, characters, and logical data, and each of these is briefly examined in this section. Beyond that, some machines define specialized data types or data structures. For example, there may be machine operations that operate directly on a list or a string of characters.\n\n\n\n\n**Numbers**\n\n\nAll machine languages include numeric data types. Even in nonnumeric data processing, there is a need for numbers to act as counters, field widths, and so forth. An important distinction between numbers used in ordinary mathematics and numbers stored in a computer is that the latter are limited. This is true in two senses. First, there is a limit to the magnitude of numbers representable on a machine and second, in the case of floating-point numbers, a limit to their precision. Thus, the programmer is faced with understanding the consequences of rounding, overflow, and underflow.\n\n\nThree types of numerical data are common in computers:\n\n\n  * ■ Binary integer or binary fixed point\n  * ■ Binary floating point\n  * ■ Decimal\n\n\nWe examined the first two in some detail in Chapter 10. It remains to say a few words about decimal numbers.\n\n\nAlthough all internal computer operations are binary in nature, the human users of the system deal with decimal numbers. Thus, there is a necessity to convert from decimal to binary on input and from binary to decimal on output. For applications in which there is a great deal of I/O and comparatively little, comparatively simple computation, it is preferable to store and operate on the numbers in decimal form. The most common representation for this purpose is\n   **packed decimal**\n   .\n   \n    1\n\n\n1\n   \n   Textbooks often refer to this as binary coded decimal (BCD). Strictly speaking, BCD refers to the encoding of each decimal digit by a unique 4-bit sequence. Packed decimal refers to the storage of BCD-encoded digits using one byte for each two digits.\n\n\nWith packed decimal, each decimal digit is represented by a 4-bit code, in the obvious way, with two digits stored per byte. Thus, 0 = 0000, 1 = 0001, ..., 8 = 1000, and 9 = 1001. Note that this is a rather inefficient code because only 10 of 16 possible 4-bit values are used. To form numbers, 4-bit codes are strung together, usually in multiples of 8 bits. Thus, the code for 246 is 0000 0010 0100 0110. This code is clearly less compact than a straight binary representation, but it avoids the conversion overhead. Negative numbers can be represented by including a 4-bit sign digit at either the left or right end of a string of packed decimal digits. Standard sign values are 1100 for positive (+) and 1101 for negative (−).\n\n\nMany machines provide arithmetic instructions for performing operations directly on packed decimal numbers. The algorithms are quite similar to those described in Section 9.3 but must take into account the decimal carry operation.\n\n\n\n\n**Characters**\n\n\nA common form of data is text or character strings. While textual data are most convenient for human beings, they cannot, in character form, be easily stored or transmitted by data processing and communications systems. Such systems are designed for binary data. Thus, a number of codes have been devised by which characters are represented by a sequence of bits. Perhaps the earliest common example of this is the Morse code. Today, the most commonly used character code in the International Reference Alphabet (IRA), referred to in the United States as the American Standard Code for Information Interchange (ASCII; see Appendix H). Each character in this code is represented by a unique 7-bit pattern; thus, 128 different characters can be represented. This is a larger number than is necessary to represent printable characters, and some of the patterns represent\n   *control*\n   characters. Some of these control characters have to do with controlling the printing of characters on a page. Others are concerned with communications procedures. IRA-encoded characters are almost always stored and transmitted using 8 bits per character. The eighth bit may be set to 0 or used as a parity bit for error detection. In the latter case, the bit is set such that the total number of binary 1s in each octet is always odd (odd parity) or always even (even parity).\n\n\nNote in Table H.1 (Appendix H) that for the IRA bit pattern 011XXXX, the digits 0 through 9 are represented by their binary equivalents, 0000 through 1001, in the rightmost 4 bits. This is the same code as packed decimal. This facilitates conversion between 7-bit IRA and 4-bit packed decimal representation.\n\n\nAnother code used to encode characters is the Extended Binary Coded Decimal Interchange Code (EBCDIC). EBCDIC is used on IBM mainframes. It is an 8-bit code. As with IRA, EBCDIC is compatible with packed decimal. In the case of EBCDIC, the codes 11110000 through 11111001 represent the digits 0 through 9.\n\n\n\n\n**Logical Data**\n\n\nNormally, each word or other addressable unit (byte, halfword, and so on) is treated as a single unit of data. It is sometimes useful, however, to consider an\n   \n    n\n   \n   -bit unit as consisting of\n   \n    n\n   \n   1-bit items of data, each item having the value 0 or 1. When data are viewed this way, they are considered to be\n   *logical*\n   data.\n\n\nThere are two advantages to the bit-oriented view. First, we may sometimes wish to store an array of Boolean or binary data items, in which each item can take on only the values 1 (true) and 0 (false). With logical data, memory can be used most efficiently for this storage. Second, there are occasions when we wish to manipulate the bits of a data item. For example, if floating-point operations are implemented in software, we need to be able to shift significant bits in some operations. Another example: To convert from IRA to packed decimal, we need to extract the rightmost 4 bits of each byte.\n\n\nNote that, in the preceding examples, the same data are treated sometimes as logical and other times as numerical or text. The “type” of a unit of data is determined by the operation being performed on it. While this is not normally the case in high-level languages, it is almost always the case with machine language."
        },
        {
          "name": "Intel x86 and ARM Data Types",
          "content": "**x86 Data Types**\n\n\nThe x86 can deal with data types of 8 (byte), 16 (word), 32 (doubleword), 64 (quadword), and 128 (double quadword) bits in length. To allow maximum flexibility in data structures and efficient memory utilization, words need not be aligned at even-numbered addresses; doublewords need not be aligned at addresses evenly divisible by 4; quadwords need not be aligned at addresses evenly divisible by 8; and so on. However, when data are accessed across a 32-bit bus, data transfers take place in units of doublewords, beginning at addresses divisible by 4. The processor converts the request for misaligned values into a sequence of requests for the bus transfer. As with all of the Intel 80x86 machines, the x86 uses the little-endian style; that is, the least significant byte is stored in the lowest address (see Appendix 12A for a discussion of endianness).\n\n\nThe byte, word, doubleword, quadword, and double quadword are referred to as general data types. In addition, the x86 supports an impressive array of specific data types that are recognized and operated on by particular instructions. Table 12.2 summarizes these types.\n\n\nFigure 12.4 illustrates the x86 numerical data types. The signed integers are in twos complement representation and may be 16, 32, or 64 bits long. The floating-point type actually refers to a set of types that are used by the floating-point unit and operated on by floating-point instructions. The floating-point representations conform to the IEEE 754 standard.\n\n\nThe packed SIMD (single-instruction-multiple-data) data types were introduced to the x86 architecture as part of the extensions of the instruction set to optimize performance of multimedia applications. These extensions include MMX (multimedia extensions) and SSE (streaming SIMD extensions). The basic concept is that multiple operands are packed into a single referenced memory item and that these multiple operands are operated on in parallel. The data types are as follows:\n\n\n  * ■\n    **Packed byte and packed byte integer:**\n    Bytes packed into a 64-bit quadword or 128-bit double quadword, interpreted as a bit field or as an integer.\n  * ■\n    **Packed word and packed word integer:**\n    16-bit words packed into a 64-bit quadword or 128-bit double quadword, interpreted as a bit field or as an integer.\n\n\n**Table 12.2**\n\nData Type | Description\nGeneral | Byte, word (16 bits), doubleword (32 bits), quadword (64 bits), and double quadword (128 bits) locations with arbitrary binary contents.\nInteger | A signed binary value contained in a byte, word, or doubleword, using twos complement representation.\nOrdinal | An unsigned integer contained in a byte, word, or doubleword.\nUnpacked binary coded decimal (BCD) | A representation of a BCD digit in the range 0 through 9, with one digit in each byte.\nPacked BCD | Packed byte representation of two BCD digits; value in the range 0 to 99.\nNear pointer | A 16-bit, 32-bit, or 64-bit effective address that represents the offset within a segment. Used for all pointers in a nonsegmented memory and for references within a segment in a segmented memory.\nFar pointer | A logical address consisting of a 16-bit segment selector and an offset of 16, 32, or 64 bits. Far pointers are used for memory references in a segmented memory model where the identity of a segment being accessed must be specified explicitly.\nBit field | A contiguous sequence of bits in which the position of each bit is considered as an independent unit. A bit string can begin at any bit position of any byte and can contain up to 32 bits.\nBit string | A contiguous sequence of bits, containing from zero to\n      \n       2^{23} - 1\n      \n      bits.\nByte string | A contiguous sequence of bytes, words, or doublewords, containing from zero to\n      \n       2^{23} - 1\n      \n      bytes.\nFloating point | See Figure 12.4.\nPacked SIMD (single instruction, multiple data) | Packed 64-bit and 128-bit data types.\n\n\n  * ■\n    **Packed doubleword and packed doubleword integer:**\n    32-bit doublewords packed into a 64-bit quadword or 128-bit double quadword, interpreted as a bit field or as an integer.\n  * ■\n    **Packed quadword and packed quadword integer:**\n    Two 64-bit quadwords packed into a 128-bit double quadword, interpreted as a bit field or as an integer.\n  * ■\n    **Packed single-precision floating-point and packed double-precision floating-point:**\n    Four 32-bit floating-point or two 64-bit floating-point values packed into a 128-bit double quadword.\n\n\n\n\n**ARM Data Types**\n\n\nARM processors support data types of 8 (byte), 16 (halfword), and 32 (word) bits in length. Normally, halfword access should be halfword aligned and word accesses should be word aligned. For nonaligned access attempts, the architecture supports three alternatives.\n\n\n  * ■ Default case:\n      * – The address is treated as truncated, with address bits[1:0] treated as zero for word accesses, and address bit[0] treated as zero for halfword accesses.\n\n\n\n\n![Diagram of x86 Numeric Data Formats showing bit layouts for integers and floating-point numbers.](images/image_0212.jpeg)\n\n\nThe diagram illustrates various x86 numeric data formats, showing their bit layouts and bit ranges:\n\n\n  * **Byte unsigned integer:**\n     8 bits, labeled 7 to 0.\n  * **Word unsigned integer:**\n     16 bits, labeled 15 to 0.\n  * **Doubleword unsigned integer:**\n     32 bits, labeled 31 to 0.\n  * **Quadword unsigned integer:**\n     64 bits, labeled 63 to 0.\n  * **Byte signed integer (twos complement):**\n     8 bits, labeled 7 to 0, with a 'sign bit' and 'twos comp' label.\n  * **Word signed integer (twos complement):**\n     16 bits, labeled 15 to 0, with a 'sign bit' label.\n  * **Doubleword signed integer (twos complement):**\n     32 bits, labeled 31 to 0, with a 'sign bit' label.\n  * **Quadword signed integer (twos complement):**\n     64 bits, labeled 63 to 0, with a 'sign bit' label.\n  * **Half precision floating point:**\n     16 bits, labeled 15 to 0, with fields for 'sign bit', 'exp' (9 bits), and 'signif.' (7 bits).\n  * **Single precision floating point:**\n     32 bits, labeled 31 to 0, with fields for 'sign bit', 'exp' (8 bits), and 'significand' (23 bits).\n  * **Double precision floating point:**\n     64 bits, labeled 63 to 0, with fields for 'sign bit', 'exp' (11 bits), and 'significand' (52 bits).\n  * **Double extended precision floating point:**\n     80 bits, labeled 79 to 0, with fields for 'sign bit', 'exponent' (15 bits), 'integer bit', and 'significand' (64 bits).\n\n\nDiagram of x86 Numeric Data Formats showing bit layouts for integers and floating-point numbers.\n\n\n**Figure 12.4**\n   x86 Numeric Data Formats\n\n\n  * – Load single word ARM instructions are architecturally defined to rotate right the word-aligned data transferred by a non word-aligned address one, two, or three bytes depending on the value of the two least significant address bits.\n  * ■\n    **Alignment checking:**\n    When the appropriate control bit is set, a data abort signal indicates an alignment fault for attempting unaligned access.\n  * ■\n    **Unaligned access:**\n    When this option is enabled, the processor uses one or more memory accesses to generate the required transfer of adjacent bytes transparently to the programmer.\n\n\nFor all three data types (byte, halfword, and word) an unsigned interpretation is supported, in which the value represents an unsigned, nonnegative integer. All three data types can also be used for twos complement signed integers.\n\n\nThe majority of ARM processor implementations do not provide floating-point hardware, which saves power and area. If floating-point arithmetic is required in such processors, it must be implemented in software. ARM does support an optional floating-point coprocessor that supports the single- and double-precision floating point data types defined in IEEE 754.\n\n\n\n\n![Diagram illustrating ARM Endian Support—Word Load/Store with E-Bit. It shows two scenarios: E-bit = 0 (Little Endian) and E-bit = 1 (Big Endian).](images/image_0213.jpeg)\n\n\nThe diagram illustrates the ARM Endian Support mechanism for Word Load/Store operations. It shows two scenarios based on the Program status register E-bit value.\n\n\n**Scenario 1: Program status register E-bit = 0 (Little Endian)**\n\n\n  * **Data bytes in memory (ascending address values from byte 0 to byte 3):**\n     Byte 0, Byte 1, Byte 2, Byte 3.\n  * **ARM register (31 to 0):**\n     Byte 3, Byte 2, Byte 1, Byte 0.\n  * **Mapping:**\n     Byte 0 from memory is loaded into the Byte 0 position of the register. Byte 1 is loaded into Byte 1, Byte 2 into Byte 2, and Byte 3 into Byte 3.\n\n\n**Scenario 2: Program status register E-bit = 1 (Big Endian)**\n\n\n  * **Data bytes in memory (ascending address values from byte 0 to byte 3):**\n     Byte 0, Byte 1, Byte 2, Byte 3.\n  * **ARM register (31 to 0):**\n     Byte 0, Byte 1, Byte 2, Byte 3.\n  * **Mapping:**\n     Byte 0 from memory is loaded into the Byte 3 position of the register. Byte 1 is loaded into Byte 2, Byte 2 into Byte 1, and Byte 3 into Byte 0.\n\n\nDiagram illustrating ARM Endian Support—Word Load/Store with E-Bit. It shows two scenarios: E-bit = 0 (Little Endian) and E-bit = 1 (Big Endian).\n\n\n**Figure 12.5**\n   ARM Endian Support—Word Load/Store with E-Bit\n\n\n**ENDIAN SUPPORT**\n   A state bit (E-bit) in the system control register is set and cleared under program control using the SETEND instruction. The E-bit defines which endian to load and store data. Figure 12.5 illustrates the functionality associated with the E-bit for a word load or store operation. This mechanism enables efficient dynamic data load/store for system designers who know they need to access data structures in the opposite endianness to their OS/environment. Note that the address of each data byte is fixed in memory. However, the byte lane in a register is different."
        },
        {
          "name": "Types of Operations",
          "content": "The number of different opcodes varies widely from machine to machine. However, the same general types of operations are found on all machines. A useful and typical categorization is the following:\n\n\n  * ■ Data transfer\n  * ■ Arithmetic\n  * ■ Logical\n  * ■ Conversion\n  * ■ I/O\n  * ■ System control\n  * ■ Transfer of control\n\n\nTable 12.3 (based on [HAYE98]) lists common instruction types in each category. This section provides a brief survey of these various types of operations, together with a brief discussion of the actions taken by the processor to execute a particular type of operation (summarized in Table 12.4). The latter topic is examined in more detail in Chapter 14.\n\n\n**Table 12.3**\n\nType | Operation Name | Description\nData transfer | Move (transfer) | Transfer word or block from source to destination\nStore | Transfer word from processor to memory\nLoad (fetch) | Transfer word from memory to processor\nExchange | Swap contents of source and destination\nClear (reset) | Transfer word of 0s to destination\nSet | Transfer word of 1s to destination\nPush | Transfer word from source to top of stack\n | Pop | Transfer word from top of stack to destination\nArithmetic | Add | Compute sum of two operands\nSubtract | Compute difference of two operands\nMultiply | Compute product of two operands\nDivide | Compute quotient of two operands\nAbsolute | Replace operand by its absolute value\nNegate | Change sign of operand\nIncrement | Add 1 to operand\nDecrement | Subtract 1 from operand\nLogical | AND | Perform logical AND\nOR | Perform logical OR\nNOT | (complement) Perform logical NOT\nExclusive-OR | Perform logical XOR\nTest | Test specified condition; set flag(s) based on outcome\nCompare | Make logical or arithmetic comparison of two or more operands; set flag(s) based on outcome\nSet Control Variables | Class of instructions to set controls for protection purposes, interrupt handling, timer control, etc.\nShift | Left (right) shift operand, introducing constants at end\nRotate | Left (right) shift operand, with wraparound end\nTransfer of control | Jump (branch) | Unconditional transfer; load PC with specified address\nJump Conditional | Test specified condition; either load PC with specified address or do nothing, based on condition\nJump to Subroutine | Place current program control information in known location; jump to specified address\nReturn | Replace contents of PC and other register from known location\nExecute | Fetch operand from specified location and execute as instruction; do not modify PC\nSkip | Increment PC to skip next instruction\nSkip Conditional | Test specified condition; either skip or do nothing based on condition\nHalt | Stop program execution\nWait (hold) | Stop program execution; test specified condition repeatedly; resume execution when condition is satisfied\nNo operation | No operation is performed, but program execution is continued\n\n\n\nType | Operation Name | Description\nInput/output | Input (read) | Transfer data from specified I/O port or device to destination (e.g., main memory or processor register)\nOutput (write) | Transfer data from specified source to I/O port or device\nStart I/O | Transfer instructions to I/O processor to initiate I/O operation\nTest I/O | Transfer status information from I/O system to specified destination\nConversion | Translate | Translate values in a section of memory based on a table of correspondences\nConvert | Convert the contents of a word from one form to another (e.g., packed decimal to binary)\n\n\n**Table 12.4**\n\nData transfer | Transfer data from one location to another\nIf memory is involved:\n      \n      Determine memory address\n      \n      Perform virtual-to-actual-memory address transformation\n      \n      Check cache\n      \n      Initiate memory read/write\nMay involve data transfer, before and/or after\nPerform function in ALU\nArithmetic | Set condition codes and flags\nLogical | Same as arithmetic\nConversion | Similar to arithmetic and logical. May involve special logic to perform conversion\nTransfer of control | Update program counter. For subroutine call/return, manage parameter passing and linkage\nI/O | Issue command to I/O module\nIf memory-mapped I/O, determine memory-mapped address\n\n\n\n\n**Data Transfer**\n\n\nThe most fundamental type of machine instruction is the data transfer instruction. The data transfer instruction must specify several things. First, the location of the source and destination operands must be specified. Each location could be memory, a register, or the top of the stack. Second, the length of data to be transferred must be indicated. Third, as with all instructions with operands, the mode of addressing for each operand must be specified. This latter point is discussed in Chapter 13.\n\n\nThe choice of data transfer instructions to include in an instruction set exemplifies the kinds of trade-offs the designer must make. For example, the general location (memory or register) of an operand can be indicated in either the specification of the opcode or the operand. Table 12.5 shows examples of the most common IBM EAS/390 data transfer instructions. Note that there are variants to indicate\n\n\n**Table 12.5**\n\nOperation Mnemonic | Name | Number of Bits Transferred | Description\nL | Load | 32 | Transfer from memory to register\nLH | Load Halfword | 16 | Transfer from memory to register\nLR | Load | 32 | Transfer from register to register\nLER | Load (short) | 32 | Transfer from floating-point register to floating-point register\nLE | Load (short) | 32 | Transfer from memory to floating-point register\nLDR | Load (long) | 64 | Transfer from floating-point register to floating-point register\nLD | Load (long) | 64 | Transfer from memory to floating-point register\nST | Store | 32 | Transfer from register to memory\nSTH | Store Halfword | 16 | Transfer from register to memory\nSTC | Store Character | 8 | Transfer from register to memory\nSTE | Store (short) | 32 | Transfer from floating-point register to memory\nSTD | Store (long) | 64 | Transfer from floating-point register to memory\n\n\nthe amount of data to be transferred (8, 16, 32, or 64 bits). Also, there are different instructions for register to register, register to memory, memory to register, and memory to memory transfers. In contrast, the VAX has a move (MOV) instruction with variants for different amounts of data to be moved, but it specifies whether an operand is register or memory as part of the operand. The VAX approach is somewhat easier for the programmer, who has fewer mnemonics to deal with. However, it is also somewhat less compact than the IBM EAS/390 approach because the location (register versus memory) of each operand must be specified separately in the instruction. We will return to this distinction when we discuss instruction formats in Chapter 13.\n\n\nIn terms of processor action, data transfer operations are perhaps the simplest type. If both source and destination are registers, then the processor simply causes data to be transferred from one register to another; this is an operation internal to the processor. If one or both operands are in memory, then the processor must perform some or all of the following actions:\n\n\n  * 1. Calculate the memory address, based on the address mode (discussed in Chapter 13).\n  * 2. If the address refers to virtual memory, translate from virtual to real memory address.\n  * 3. Determine whether the addressed item is in cache.\n  * 4. If not, issue a command to the memory module.\n\n\n\n\n**Arithmetic**\n\n\nMost machines provide the basic arithmetic operations of add, subtract, multiply, and divide. These are invariably provided for signed integer (fixed-point) numbers. Often they are also provided for floating-point and packed decimal numbers.\n\n\nOther possible operations include a variety of single-operand instructions; for example,\n\n\n  * ■\n    **Absolute:**\n    Take the absolute value of the operand.\n  * ■\n    **Negate:**\n    Negate the operand.\n  * ■\n    **Increment:**\n    Add 1 to the operand.\n  * ■\n    **Decrement:**\n    Subtract 1 from the operand.\n\n\nThe execution of an arithmetic instruction may involve data transfer operations to position operands for input to the ALU, and to deliver the output of the ALU. Figure 3.5 illustrates the movements involved in both data transfer and arithmetic operations. In addition, of course, the ALU portion of the processor performs the desired operation.\n\n\n\n\n**Logical**\n\n\nMost machines also provide a variety of operations for manipulating individual bits of a word or other addressable units, often referred to as “bit twiddling.” They are based upon Boolean operations (see Chapter 11).\n\n\nSome of the basic logical operations that can be performed on Boolean or binary data are shown in Table 12.6. The NOT operation inverts a bit. AND, OR, and Exclusive-OR (XOR) are the most common logical functions with two operands. EQUAL is a useful binary test.\n\n\nThese logical operations can be applied bitwise to\n   \n    n\n   \n   -bit logical data units. Thus, if two registers contain the data\n\n\n(R1) = 10100101\n\n\n(R2) = 00001111\n\n\nthen\n\n\n(R1) \\text{ AND } (R2) = 00000101\n\n\n**Table 12.6**\n   Basic Logical Operations\n\n\n\nP | Q | NOT P | P AND Q | P OR Q | P XOR Q | P = Q\n0 | 0 | 1 | 0 | 0 | 0 | 1\n0 | 1 | 1 | 0 | 1 | 1 | 0\n1 | 0 | 0 | 0 | 1 | 1 | 0\n1 | 1 | 0 | 1 | 1 | 0 | 1\n\n\nwhere the notation (X) means the contents of location X. Thus, the AND operation can be used as a\n   *mask*\n   that selects certain bits in a word and zeros out the remaining bits. As another example, if two registers contain\n\n\n(R1) = 10100101\n\n\n(R2) = 11111111\n\n\nthen\n\n\n(R1) \\text{ XOR } (R2) = 01011010\n\n\nWith one word set to all 1s, the XOR operation inverts all of the bits in the other word (ones complement).\n\n\nIn addition to bitwise logical operations, most machines provide a variety of shifting and rotating functions. The most basic operations are illustrated in Figure 12.6. With a\n   **logical shift**\n   , the bits of a word are shifted left or right. On one end, the bit shifted out is lost. On the other end, a 0 is shifted in. Logical shifts are useful primarily for isolating fields within a word. The 0s that are shifted into a word displace unwanted information that is shifted off the other end.\n\n\n\n\n![Figure 12.6: Shift and Rotate Operations. The diagram shows six types of bit manipulation on a 16-bit word represented by a row of boxes. (a) Logical right shift: bits shift right, 0 is shifted in from the left. (b) Logical left shift: bits shift left, 0 is shifted in from the right. (c) Arithmetic right shift: bits shift right, the sign bit (S) is shifted in from the left. (d) Arithmetic left shift: bits shift left, 0 is shifted in from the right. (e) Right rotate: bits shift right, the bit shifted out from the right is rotated back into the leftmost position. (f) Left rotate: bits shift left, the bit shifted out from the left is rotated back into the rightmost position.](images/image_0214.jpeg)\n\n\nThe diagram illustrates six types of bit manipulation operations on a 16-bit word represented by a row of boxes. Each operation is shown with curved arrows indicating the direction of bit movement.\n\n\n  * (a) Logical right shift: Bits shift right, and a 0 is shifted in from the left.\n  * (b) Logical left shift: Bits shift left, and a 0 is shifted in from the right.\n  * (c) Arithmetic right shift: Bits shift right, and the sign bit (S) is shifted in from the left.\n  * (d) Arithmetic left shift: Bits shift left, and a 0 is shifted in from the right.\n  * (e) Right rotate: Bits shift right, and the bit shifted out from the right is rotated back into the leftmost position.\n  * (f) Left rotate: Bits shift left, and the bit shifted out from the left is rotated back into the rightmost position.\n\n\nFigure 12.6: Shift and Rotate Operations. The diagram shows six types of bit manipulation on a 16-bit word represented by a row of boxes. (a) Logical right shift: bits shift right, 0 is shifted in from the left. (b) Logical left shift: bits shift left, 0 is shifted in from the right. (c) Arithmetic right shift: bits shift right, the sign bit (S) is shifted in from the left. (d) Arithmetic left shift: bits shift left, 0 is shifted in from the right. (e) Right rotate: bits shift right, the bit shifted out from the right is rotated back into the leftmost position. (f) Left rotate: bits shift left, the bit shifted out from the left is rotated back into the rightmost position.\n\n\n**Figure 12.6**\n   Shift and Rotate Operations\n\n\nAs an example, suppose we wish to transmit characters of data to an I/O device 1 character at a time. If each memory word is 16 bits in length and contains two characters, we must\n   *unpack*\n   the characters before they can be sent. To send the two characters in a word;\n\n\n  * 1. Load the word into a register.\n  * 2. Shift to the right eight times. This shifts the remaining character to the right half of the register.\n  * 3. Perform I/O. The I/O module reads the lower-order 8 bits from the data bus.\n\n\nThe preceding steps result in sending the left-hand character. To send the right-hand character;\n\n\n  * 1. Load the word again into the register.\n  * 2. AND with 0000000011111111. This masks out the character on the left.\n  * 3. Perform I/O.\n\n\nThe\n   **arithmetic shift**\n   operation treats the data as a signed integer and does not shift the sign bit. On a right arithmetic shift, the sign bit is replicated into the bit position to its right. On a left arithmetic shift, a logical left shift is performed on all bits but the sign bit, which is retained. These operations can speed up certain arithmetic operations. With numbers in twos complement notation, a right arithmetic shift corresponds to a division by 2, with truncation for odd numbers. Both an arithmetic left shift and a logical left shift correspond to a multiplication by 2 when there is no overflow. If overflow occurs, arithmetic and logical left shift operations produce different results, but the arithmetic left shift retains the sign of the number. Because of the potential for overflow, many processors do not include this instruction, including PowerPC and Itanium. Others, such as the IBM EAS/390, do offer the instruction. Curiously, the x86 architecture includes an arithmetic left shift but defines it to be identical to a logical left shift.\n\n\n**Rotate**\n   , or cyclic shift, operations preserve all of the bits being operated on. One use of a rotate is to bring each bit successively into the leftmost bit, where it can be identified by testing the sign of the data (treated as a number).\n\n\nAs with arithmetic operations, logical operations involve ALU activity and may involve data transfer operations. Table 12.7 gives examples of all of the shift and rotate operations discussed in this subsection.\n\n\n**Table 12.7**\n   Examples of Shift and Rotate Operations\n\n\n\nInput | Operation | Result\n10100110 | Logical right shift (3 bits) | 00010100\n10100110 | Logical left shift (3 bits) | 00110000\n10100110 | Arithmetic right shift (3 bits) | 11110100\n10100110 | Arithmetic left shift (3 bits) | 10110000\n10100110 | Right rotate (3 bits) | 11010100\n10100110 | Left rotate (3 bits) | 00110101\n\n\n\n\n**Conversion**\n\n\nConversion instructions are those that change the format or operate on the format of data. An example is converting from decimal to binary. An example of a more complex editing instruction is the EAS/390 Translate (TR) instruction. This instruction can be used to convert from one 8-bit code to another, and it takes three operands:\n\n\nTR R1 (L), R2\n\n\nThe operand R2 contains the address of the start of a table of 8-bit codes. The L bytes starting at the address specified in R1 are translated, each byte being replaced by the contents of a table entry indexed by that byte. For example, to translate from EBCDIC to IRA, we first create a 256-byte table in storage locations, say, 1000-10FF hexadecimal. The table contains the characters of the IRA code in the sequence of the binary representation of the EBCDIC code; that is, the IRA code is placed in the table at the relative location equal to the binary value of the EBCDIC code of the same character. Thus, locations 10F0 through 10F9 will contain the values 30 through 39, because F0 is the EBCDIC code for the digit 0, and 30 is the IRA code for the digit 0, and so on through digit 9. Now suppose we have the EBCDIC for the digits 1984 starting at location 2100 and we wish to translate to IRA. Assume the following:\n\n\n  * ■ Locations 2100–2103 contain F1 F9 F8 F4.\n  * ■ R1 contains 2100.\n  * ■ R2 contains 1000.\n\n\nThen, if we execute\n\n\nTR R1 (4), R2\n\n\nlocations 2100–2103 will contain 31 39 38 34.\n\n\n\n\n**Input/Output**\n\n\nInput/output instructions were discussed in some detail in Chapter 7. As we saw, there are a variety of approaches taken, including isolated programmed I/O, memory-mapped programmed I/O, DMA, and the use of an I/O processor. Many implementations provide only a few I/O instructions, with the specific actions specified by parameters, codes, or command words.\n\n\n\n\n**System Control**\n\n\nSystem control instructions are those that can be executed only while the processor is in a certain privileged state or is executing a program in a special privileged area of memory. Typically, these instructions are reserved for the use of the operating system.\n\n\nSome examples of system control operations are as follows. A system control instruction may read or alter a control register; we discuss control registers in Chapter 14. Another example is an instruction to read or modify a storage protection key, such as is used in the EAS/390 memory system. Yet another example is access to process control blocks in a multiprogramming system.\n\n\n\n\n**Transfer of Control**\n\n\nFor all of the operation types discussed so far, the next instruction to be performed is the one that immediately follows, in memory, the current instruction. However, a significant fraction of the instructions in any program have as their function changing the sequence of instruction execution. For these instructions, the operation performed by the processor is to update the program counter to contain the address of some instruction in memory.\n\n\nThere are a number of reasons why transfer-of-control operations are required. Among the most important are the following:\n\n\n  * 1. In the practical use of computers, it is essential to be able to execute each instruction more than once and perhaps many thousands of times. It may require thousands or perhaps millions of instructions to implement an application. This would be unthinkable if each instruction had to be written out separately. If a table or a list of items is to be processed, a program loop is needed. One sequence of instructions is executed repeatedly to process all the data.\n  * 2. Virtually all programs involve some decision making. We would like the computer to do one thing if one condition holds, and another thing if another condition holds. For example, a sequence of instructions computes the square root of a number. At the start of the sequence, the sign of the number is tested. If the number is negative, the computation is not performed, but an error condition is reported.\n  * 3. To compose correctly a large or even medium-size computer program is an exceedingly difficult task. It helps if there are mechanisms for breaking the task up into smaller pieces that can be worked on one at a time.\n\n\nWe now turn to a discussion of the most common transfer-of-control operations found in instruction sets:\n   **branch**\n   ,\n   **skip**\n   , and\n   **procedure call**\n   .\n\n\n**BRANCH INSTRUCTIONS**\n   A branch instruction, also called a jump instruction, has as one of its operands the address of the next instruction to be executed. Most often, the instruction is a\n   **conditional branch**\n   instruction. That is, the branch is made (update program counter to equal address specified in operand) only if a certain condition is met. Otherwise, the next instruction in sequence is executed (increment program counter as usual). A branch instruction in which the branch is always taken is an\n   **unconditional branch**\n   .\n\n\nThere are two common ways of generating the condition to be tested in a conditional branch instruction. First, most machines provide a 1-bit or multiple-bit condition code that is set as the result of some operations. This code can be thought of as a short user-visible register. As an example, an arithmetic operation (ADD, SUBTRACT, and so on) could set a 2-bit condition code with one of the following four values: 0, positive, negative, overflow. On such a machine, there could be four different conditional branch instructions:\n\n\nBRP X Branch to location X if result is positive.\n\n\nBRN X Branch to location X if result is negative.\n\n\nBRZ X Branch to location X if result is zero.\n\n\nBRO X Branch to location X if overflow occurs.\n\n\n\n\n![Figure 12.7: Branch Instructions. A diagram showing memory addresses (200 to 235) and corresponding instructions. An unconditional branch from address 202 to 210 is shown. Conditional branches from 211 to 202 and from 235 to 211 are also shown.](images/image_0215.jpeg)\n\n\nMemory address | Instruction\n200 | \n201 | \n202 | \n203 | \n⋮ | \n210 | \n211 | \n⋮ | \n225 | \n⋮ | \n235 | \n\n\nThe diagram illustrates branch instructions with arrows:\n\n\n  * An\n     **Unconditional branch**\n     arrow points from address 202 to address 210.\n  * A\n     **Conditional branch**\n     arrow points from address 211 to address 202.\n  * A\n     **Conditional branch**\n     arrow points from address 235 to address 211.\n\n\nFigure 12.7: Branch Instructions. A diagram showing memory addresses (200 to 235) and corresponding instructions. An unconditional branch from address 202 to 210 is shown. Conditional branches from 211 to 202 and from 235 to 211 are also shown.\n\n\n**Figure 12.7**\n   Branch Instructions\n\n\nIn all of these cases, the result referred to is the result of the most recent operation that set the condition code.\n\n\nAnother approach that can be used with a three-address instruction format is to perform a comparison and specify a branch in the same instruction. For example,\n\n\n**BRE R1, R2, X**\n   Branch to X if contents of R1 = contents of R2.\n\n\nFigure 12.7 shows examples of these operations. Note that a branch can be either\n   *forward*\n   (an instruction with a higher address) or\n   *backward*\n   (lower address). The example shows how an unconditional and a conditional branch can be used to create a repeating loop of instructions. The instructions in locations 202 through 210 will be executed repeatedly until the result of subtracting Y from X is 0.\n\n\n**SKIP INSTRUCTIONS**\n   Another form of transfer-of-control instruction is the skip instruction. The skip instruction includes an implied address. Typically, the skip implies that one instruction be skipped; thus, the implied address equals the address of the next instruction plus one instruction length. Because the skip instruction does not require a destination address field, it is free to do other things. A typical example is the increment-and-skip-if-zero (ISZ) instruction. Consider the following program fragment:\n\n\n301\n:\n309 ISZ R1\n310 BR 301\n311\n\nIn this fragment, the two transfer-of-control instructions are used to implement an iterative loop. R1 is set with the negative of the number of iterations to be performed. At the end of the loop, R1 is incremented. If it is not 0, the program branches back to the beginning of the loop. Otherwise, the branch is skipped, and the program continues with the next instruction after the end of the loop.\n\n\n**PROCEDURE CALL INSTRUCTIONS**\n   Perhaps the most important innovation in the development of programming languages is the\n   *procedure*\n   . A procedure is a self-contained computer program that is incorporated into a larger program. At any point in the program the procedure may be invoked, or\n   *called*\n   . The processor is instructed to go and execute the entire procedure and then return to the point from which the call took place.\n\n\nThe two principal reasons for the use of procedures are economy and modularity. A procedure allows the same piece of code to be used many times. This is important for economy in programming effort and for making the most efficient use of storage space in the system (the program must be stored). Procedures also allow large programming tasks to be subdivided into smaller units. This use of\n   *modularity*\n   greatly eases the programming task.\n\n\nThe procedure mechanism involves two basic instructions: a call instruction that branches from the present location to the procedure, and a return instruction that returns from the procedure to the place from which it was called. Both of these are forms of branching instructions.\n\n\nFigure 12.8a illustrates the use of procedures to construct a program. In this example, there is a main program starting at location 4000. This program includes a call to procedure PROC1, starting at location 4500. When this call instruction is encountered, the processor suspends execution of the main program and begins execution of PROC1 by fetching the next instruction from location 4500. Within PROC1, there are two calls to PROC2 at location 4800. In each case, the execution of PROC1\n\n\n\n\n![Figure 12.8: Nested Procedures. (a) Calls and returns: A table showing memory addresses and instructions. (b) Execution sequence: A flowchart showing the execution path between the main program, PROC1, and PROC2.](images/image_0216.jpeg)\n\n\n**(a) Calls and returns**\n\n\n\nAddresses | Main memory | \n4000 | ... | Main program\n4100\n       \n       4101 | CALL Proc1 | \n4500 | ... | Procedure Proc1\n4600\n       \n       4601 | CALL Proc2 | \n4650\n       \n       4651 | CALL Proc2 | \n | RETURN | \n4800 | ... | Procedure Proc2\n | RETURN | \n\n\n**(b) Execution sequence**\n\n\nThe execution sequence diagram shows the flow of control between three blocks: Main program (top), Procedure Proc1 (middle), and Procedure Proc2 (bottom). The flow starts in the Main program, goes down to Proc1, then down to Proc2. Within Proc2, there are two nested calls to Proc2, indicated by two separate downward arrows from the Proc1 block to the Proc2 block. After the second call to Proc2, the flow returns to Proc1, and then finally returns to the Main program.\n\n\nFigure 12.8: Nested Procedures. (a) Calls and returns: A table showing memory addresses and instructions. (b) Execution sequence: A flowchart showing the execution path between the main program, PROC1, and PROC2.\n\n\n**Figure 12.8**\n   Nested Procedures\n\n\nis suspended and PROC2 is executed. The RETURN statement causes the processor to go back to the calling program and continue execution at the instruction after the corresponding CALL instruction. This behavior is illustrated in Figure 12.8b.\n\n\nThree points are worth noting:\n\n\n  * 1. A procedure can be called from more than one location.\n  * 2. A procedure call can appear in a procedure. This allows the\n    *nesting*\n    of procedures to an arbitrary depth.\n  * 3. Each procedure call is matched by a return in the called program.\n\n\nBecause we would like to be able to call a procedure from a variety of points, the processor must somehow save the return address so that the return can take place appropriately. There are three common places for storing the return address:\n\n\n  * ■ Register\n  * ■ Start of called procedure\n  * ■ Top of stack\n\n\nConsider a machine-language instruction CALL X, which stands for\n   *call procedure at location X*\n   . If the register approach is used, CALL X causes the following actions:\n\n\n\\begin{aligned} RN &\\leftarrow PC + \\Delta \\\\ PC &\\leftarrow X \\end{aligned}\n\n\nwhere RN is a register that is always used for this purpose, PC is the program counter, and\n   \n    \\Delta\n   \n   is the instruction length. The called procedure can now save the contents of RN to be used for the later return.\n\n\nA second possibility is to store the return address at the start of the procedure. In this case, CALL X causes\n\n\n\\begin{aligned} X &\\leftarrow PC + \\Delta \\\\ PC &\\leftarrow X + 1 \\end{aligned}\n\n\nThis is quite handy. The return address has been stored safely away.\n\n\nBoth of the preceding approaches work and have been used. The only limitation of these approaches is that they complicate the use of\n   *reentrant*\n   procedures. A\n   **reentrant procedure**\n   is one in which it is possible to have several calls open to it at the same time. A recursive procedure (one that calls itself) is an example of the use of this feature (see Appendix M). If parameters are passed via registers or memory for a reentrant procedure, some code must be responsible for saving the parameters so that the registers or memory space are available for other procedure calls.\n\n\nA more general and powerful approach is to use a stack (see Appendix I for a discussion of stacks). When the processor executes a call, it places the return address on the stack. When it executes a return, it uses the address on the stack. Figure 12.9 illustrates the use of the stack.\n\n\nIn addition to providing a return address, it is also often necessary to pass parameters with a procedure call. These can be passed in registers. Another possibility is to store the parameters in memory just after the CALL instruction. In this case, the return must be to the location following the parameters. Again, both of\n\n\n\n\n![Figure 12.9: Use of Stack to Implement Nested Subroutines. The diagram shows seven vertical stack frames labeled (a) through (g). (a) Initial stack contents: a single cell with a dot. (b) After CALL Proc1: a cell with 4101, then a dot. (c) Initial CALL Proc2: a cell with 4601, then a cell with 4101, then a dot. (d) After RETURN: a cell with 4101, then a dot. (e) After CALL Proc2: a cell with 4651, then a cell with 4101, then a dot. (f) After RETURN: a cell with 4101, then a dot. (g) After RETURN: a single cell with a dot.](images/image_0217.jpeg)\n\n\nFigure 12.9: Use of Stack to Implement Nested Subroutines. The diagram shows seven vertical stack frames labeled (a) through (g). (a) Initial stack contents: a single cell with a dot. (b) After CALL Proc1: a cell with 4101, then a dot. (c) Initial CALL Proc2: a cell with 4601, then a cell with 4101, then a dot. (d) After RETURN: a cell with 4101, then a dot. (e) After CALL Proc2: a cell with 4651, then a cell with 4101, then a dot. (f) After RETURN: a cell with 4101, then a dot. (g) After RETURN: a single cell with a dot.\n\n\n**Figure 12.9**\n   Use of Stack to Implement Nested Subroutines of Figure 12.8\n\n\nthese approaches have drawbacks. If registers are used, the called program and the calling program must be written to assure that the registers are used properly. The storing of parameters in memory makes it difficult to exchange a variable number of parameters. Both approaches prevent the use of reentrant procedures.\n\n\nA more flexible approach to parameter passing is the stack. When the processor executes a call, it not only stacks the return address, it stacks parameters to be passed to the called procedure. The called procedure can access the parameters from the stack. Upon return, return parameters can also be placed on the stack. The entire set of parameters, including return address, that is stored for a procedure invocation is referred to as a\n   *stack frame*\n   .\n\n\nAn example is provided in Figure 12.10. The example refers to procedure P in which the local variables\n   \n    x_1\n   \n   and\n   \n    x_2\n   \n   are declared, and procedure Q, which P can call and in which the local variables\n   \n    y_1\n   \n   and\n   \n    y_2\n   \n   are declared. In this figure, the return\n\n\n\n\n![Figure 12.10: Stack Frame Growth Using Sample Procedures P and Q. The diagram shows two vertical stack frames. The left frame, labeled (a) P is active, contains cells for Return point, Old frame pointer, x1, x2, and a large teal section at the top. The right frame, labeled (b) P has called Q, contains cells for Return point, Old frame pointer, x1, x2, Return point, Old frame pointer, y1, y2, and a large teal section at the top. Arrows indicate the Stack pointer (pointing to the top of the stack) and the Frame pointer (pointing to the Old frame pointer cell).](images/image_0218.jpeg)\n\n\nFigure 12.10: Stack Frame Growth Using Sample Procedures P and Q. The diagram shows two vertical stack frames. The left frame, labeled (a) P is active, contains cells for Return point, Old frame pointer, x1, x2, and a large teal section at the top. The right frame, labeled (b) P has called Q, contains cells for Return point, Old frame pointer, x1, x2, Return point, Old frame pointer, y1, y2, and a large teal section at the top. Arrows indicate the Stack pointer (pointing to the top of the stack) and the Frame pointer (pointing to the Old frame pointer cell).\n\n\n**Figure 12.10**\n   Stack Frame Growth Using Sample Procedures P and Q\n\n\npoint for each procedure is the first item stored in the corresponding stack frame. Next is stored a pointer to the beginning of the previous frame. This is needed if the number or length of parameters to be stacked is variable."
        },
        {
          "name": "Intel x86 and ARM Operation Types",
          "content": "**x86 Operation Types**\n\n\nThe x86 provides a complex array of operation types, including a number of specialized instructions. The intent was to provide tools for the compiler writer to produce optimized machine language translation of high-level language programs. Most of these are the conventional instructions found in most machine instruction sets, but several types of instructions are tailored to the x86 architecture and are of particular interest. Appendix A of [CART06] lists the x86 instructions, together with the operands for each and the effect of the instruction on the condition codes. Appendix B of the NASM assembly language manual [NASM12] provides a more detailed description of each x86 instruction. Both documents are available at\n   box.com/COA10e\n   .\n\n\n***CALL/RETURN INSTRUCTIONS***\n   The x86 provides four instructions to support procedure call/return:\n   **CALL**\n   ,\n   **ENTER**\n   ,\n   **LEAVE**\n   ,\n   **RETURN**\n   . It will be instructive to look at the support provided by these instructions. Recall from Figure 12.10 that a common means of implementing the procedure call/return mechanism is via the use of stack frames. When a new procedure is called, the following must be performed upon entry to the new procedure:\n\n\n  * ■ Push the return point on the stack.\n  * ■ Push the current frame pointer on the stack.\n  * ■ Copy the stack pointer as the new value of the frame pointer.\n  * ■ Adjust the stack pointer to allocate a frame.\n\n\nThe\n   **CALL**\n   instruction pushes the current instruction pointer value onto the stack and causes a jump to the entry point of the procedure by placing the address of the entry point in the instruction pointer. In the 8088 and 8086 machines, the typical procedure began with the sequence\n\n\nPUSH    EBP\nMOV     EBP, ESP\nSUB     ESP, space_for_locals\nwhere EBP is the frame pointer and ESP is the stack pointer. In the 80286 and later machines, the\n   **ENTER**\n   instruction performs all the aforementioned operations in a single instruction.\n\n\nThe\n   **ENTER**\n   instruction was added to the instruction set to provide direct support for the compiler. The instruction also includes a feature for support of what are called nested procedures in languages such as Pascal, COBOL, and Ada (not found in C or FORTRAN). It turns out that there are better ways of handling nested procedure calls for these languages. Furthermore, although the\n   **ENTER**\n   instruction\n\n\nsaves a few bytes of memory compared with the PUSH, MOV, SUB sequence (4 bytes versus 6 bytes), it actually takes longer to execute (10 clock cycles versus 6 clock cycles). Thus, although it may have seemed a good idea to the instruction set designers to add this feature, it complicates the implementation of the processor while providing little or no benefit. We will see that, in contrast, a RISC approach to processor design would avoid complex instructions such as ENTER and might produce a more efficient implementation with a sequence of simpler instructions.\n\n\n**MEMORY MANAGEMENT**\n   Another set of specialized instructions deals with memory segmentation. These are privileged instructions that can only be executed from the operating system. They allow local and global segment tables (called descriptor tables) to be loaded and read, and for the privilege level of a segment to be checked and altered.\n\n\nThe special instructions for dealing with the on-chip cache were discussed in Chapter 4.\n\n\n**STATUS FLAGS AND CONDITION CODES**\n   Status flags are bits in special registers that may be set by certain operations and used in conditional branch instructions. The term\n   *condition code*\n   refers to the settings of one or more status flags. In the x86 and many other architectures, status flags are set by arithmetic and compare operations. The compare operation in most languages subtracts two operands, as does a subtract operation. The difference is that a compare operation only sets status flags, whereas a subtract operation also stores the result of the subtraction in the destination operand. Some architectures also set status flags for data transfer instructions.\n\n\nTable 12.8 lists the status flags used on the x86. Each flag, or combinations of these flags, can be tested for a conditional jump. Table 12.9 shows the condition codes (combinations of status flag values) for which conditional jump opcodes have been defined.\n\n\nSeveral interesting observations can be made about this list. First, we may wish to test two operands to determine if one number is bigger than another. But this will depend on whether the numbers are signed or unsigned. For example, the 8-bit number 11111111 is bigger than 00000000 if the two numbers are interpreted\n\n\n**Table 12.8**\n   x86 Status Flags\n\n\n\nStatus Bit | Name | Description\nC | Carry | Indicates carrying or borrowing out of the left-most bit position following an arithmetic operation. Also modified by some of the shift and rotate operations.\nP | Parity | Parity of the least-significant byte of the result of an arithmetic or logic operation. 1 indicates even parity; 0 indicates odd parity.\nA | Auxiliary Carry | Represents carrying or borrowing between half-bytes of an 8-bit arithmetic or logic operation. Used in binary-coded decimal arithmetic.\nZ | Zero | Indicates that the result of an arithmetic or logic operation is 0.\nS | Sign | Indicates the sign of the result of an arithmetic or logic operation.\nO | Overflow | Indicates an arithmetic overflow after an addition or subtraction for twos complement arithmetic.\n\n\n**Table 12.9**\n\nSymbol | Condition Tested | Comment\nA, NBE | C = 0 \\text{ AND } Z = 0 | Above; Not below or equal (greater than, unsigned)\nAE, NB, NC | C = 0 | Above or equal; Not below (greater than or equal, unsigned); Not carry\nB, NAE, C | C = 1 | Below; Not above or equal (less than, unsigned); Carry set\nBE, NA | C = 1 \\text{ OR } Z = 1 | Below or equal; Not above (less than or equal, unsigned)\nE, Z | Z = 1 | Equal; Zero (signed or unsigned)\nG, NLE | [(S = 1 \\text{ AND } O = 1) \\text{ OR } (S = 0 \\text{ AND } O = 0)] \\text{ AND } [Z = 0] | Greater than; Not less than or equal (signed)\nGE, NL | (S = 1 \\text{ AND } O = 1) \\text{ OR } (S = 0 \\text{ AND } O = 0) | Greater than or equal; Not less than (signed)\nL, NGE | (S = 1 \\text{ AND } O = 0) \\text{ OR } (S = 0 \\text{ AND } O = 0) | Less than; Not greater than or equal (signed)\nLE, NG | (S = 1 \\text{ AND } O = 0) \\text{ OR } (S = 0 \\text{ AND } O = 1) \\text{ OR } (Z = 1) | Less than or equal; Not greater than (signed)\nNE, NZ | Z = 0 | Not equal; Not zero (signed or unsigned)\nNO | O = 0 | No overflow\nNS | S = 0 | Not sign (not negative)\nNP, PO | P = 0 | Not parity; Parity odd\nO | O = 1 | Overflow\nP | P = 1 | Parity; Parity even\nS | S = 1 | Sign (negative)\n\n\nas unsigned integers (\n   \n    255 > 0\n   \n   ) but is less if they are considered as 8-bit twos complement numbers (\n   \n    -1 < 0\n   \n   ). Many assembly languages therefore introduce two sets of terms to distinguish the two cases: If we are comparing two numbers as signed integers, we use the terms\n   *less than*\n   and\n   *greater than*\n   ; if we are comparing them as unsigned integers, we use the terms\n   *below*\n   and\n   *above*\n   .\n\n\nA second observation concerns the complexity of comparing signed integers. A signed result is greater than or equal to zero if (1) the sign bit is zero and there is no overflow (\n   \n    S = 0 \\text{ AND } O = 0\n   \n   ), or (2) the sign bit is one and there is an overflow. A study of Figure 10.4 should convince you that the conditions tested for the various signed operations are appropriate.\n\n\n**x86 SIMD INSTRUCTIONS**\n   In 1996, Intel introduced MMX technology into its Pentium product line. MMX is set of highly optimized instructions for multimedia tasks. There are 57 new instructions that treat data in a SIMD (single-instruction, multiple-data) fashion, which makes it possible to perform the same operation, such as addition or multiplication, on multiple data elements at once. Each instruction typically takes a single clock cycle to execute. For the proper application, these fast parallel operations can yield a speedup of two to eight times over comparable algorithms that do not use the MMX instructions [ATKI96]. With the introduction of 64-bit x86 architecture, Intel has expanded this extension to include double\n\n\nquadword (128 bits) operands and floating-point operations. In this subsection, we describe the MMX features.\n\n\nThe focus of MMX is multimedia programming. Video and audio data are typically composed of large arrays of small data types, such as 8 or 16 bits, whereas conventional instructions are tailored to operate on 32- or 64-bit data. Here are some examples: In graphics and video, a single scene consists of an array of pixels,\n   \n    2\n   \n   and there are 8 bits for each pixel or 8 bits for each pixel color component (red, green, blue). Typical audio samples are quantized using 16 bits. For some 3D graphics algorithms, 32 bits are common for basic data types. To provide for parallel operation on these data lengths, three new data types are defined in MMX. Each data type is 64 bits in length and consists of multiple smaller data fields, each of which holds a fixed-point integer. The types are as follows:\n\n\n  * ■\n    **Packed byte:**\n    Eight bytes packed into one 64-bit quantity.\n  * ■\n    **Packed word:**\n    Four 16-bit words packed into 64 bits.\n  * ■\n    **Packed doubleword:**\n    Two 32-bit doublewords packed into 64 bits.\n\n\nTable 12.10 lists the MMX instruction set. Most of the instructions involve parallel operation on bytes, words, or doublewords. For example, the PSLLW instruction performs a left logical shift separately on each of the four words in the packed word operand; the PADDW instruction takes packed byte operands as input and performs parallel additions on each byte position independently to produce a packed byte output.\n\n\nOne unusual feature of the new instruction set is the introduction of\n   **saturation arithmetic**\n   for byte and 16-bit word operands. With ordinary unsigned arithmetic, when an operation overflows (i.e., a carry out of the most significant bit), the extra bit is truncated. This is referred to as wraparound, because the effect of the truncation can be, for example, to produce an addition result that is smaller than the two input operands. Consider the addition of the two words, in hexadecimal, F000h and 3000h. The sum would be expressed as\n\n\n\\begin{array}{r}\n    \\text{F000h} = 1111\\ 0000\\ 0000\\ 0000 \\\\\n    + \\text{3000h} = \\underline{0011\\ 0000\\ 0000\\ 0000} \\\\\n    10010\\ 0000\\ 0000\\ 0000 = 2000\\text{h}\n    \\end{array}\n\n\nIf the two numbers represented image intensity, then the result of the addition is to make the combination of two dark shades turn out to be lighter. This is typically not what is intended. With saturation arithmetic, if addition results in overflow or subtraction results in underflow, the result is set to the largest or smallest value representable. For the preceding example, with saturation arithmetic, we have\n\n\n\\begin{array}{r}\n    \\text{F000h} = 1111\\ 0000\\ 0000\\ 0000 \\\\\n    + \\text{3000h} = \\underline{0011\\ 0000\\ 0000\\ 0000} \\\\\n    10010\\ 0000\\ 0000\\ 0000 \\\\\n    1111\\ 1111\\ 1111\\ 1111 = \\text{FFFFh}\n    \\end{array}\n\n\n2\n   \n   A pixel, or picture element, is the smallest element of a digital image that can be assigned a gray level. Equivalently, a pixel is an individual dot in a dot-matrix representation of a picture.\n\n\n**Table 12.10**\n\nCategory | Instruction | Description\nArithmetic | PADD [B, W, D] | Parallel add of packed eight bytes, four 16-bit words, or two 32-bit doublewords, with wraparound.\nPADDS [B, W] | Add with saturation.\nPADDUS [B, W] | Add unsigned with saturation.\nPSUB [B, W, D] | Subtract with wraparound.\nPSUBS [B, W] | Subtract with saturation.\nPSUBUS [B, W] | Subtract unsigned with saturation.\nPMULHW | Parallel multiply of four signed 16-bit words, with high-order 16 bits of 32-bit result chosen.\nPMULLW | Parallel multiply of four signed 16-bit words, with low-order 16 bits of 32-bit result chosen.\n | PMADDWD | Parallel multiply of four signed 16-bit words; add together adjacent pairs of 32-bit results.\nComparison | PCMPEQ [B, W, D] | Parallel compare for equality; result is mask of 1s if true or 0s if false.\nPCMPTG [B, W, D] | Parallel compare for greater than; result is mask of 1s if true or 0s if false.\nConversion | PACKUSWB | Pack words into bytes with unsigned saturation.\nPACKSS [WB, DW] | Pack words into bytes, or doublewords into words, with signed saturation.\nPUNPCKH [BW, WD, DQ] | Parallel unpack (interleaved merge) high-order bytes, words, or doublewords from MMX register.\nPUNPCKL [BW, WD, DQ] | Parallel unpack (interleaved merge) low-order bytes, words, or doublewords from MMX register.\nLogical | PAND | 64-bit bitwise logical AND\nPNDN | 64-bit bitwise logical AND NOT\nPOR | 64-bit bitwise logical OR\nPXOR | 64-bit bitwise logical XOR\nShift | PSLL [W, D, Q] | Parallel logical left shift of packed words, doublewords, or quadword by amount specified in MMX register or immediate value.\nPSRL [W, D, Q] | Parallel logical right shift of packed words, doublewords, or quadword.\nPSRA [W, D] | Parallel arithmetic right shift of packed words, doublewords, or quadword.\nData transfer | MOV [D, Q] | Move doubleword or quadword to/from MMX register.\nStatemgt | EMMS | Empty MMX state (empty FP registers tag bits).\n\n\nNote: If an instruction supports multiple data types [byte (B), word (W), doubleword (D), quadword (Q)], the data types are indicated in brackets.\n\n\nTo provide a feel for the use of MMX instructions, we look at an example, taken from [PELE97]. A common video application is the fade-out, fade-in effect, in which one scene gradually dissolves into another. Two images are combined with a weighted average:\n\n\n\\text{Result\\_pixel} = \\text{A\\_pixel} \\times \\text{fade} + \\text{B\\_pixel} \\times (1 - \\text{fade})\n\n\nThis calculation is performed on each pixel position in A and B. If a series of video frames is produced while gradually changing the fade value from 1 to 0 (scaled appropriately for an 8-bit integer), the result is to fade from image A to image B.\n\n\nFigure 12.11 shows the sequence of steps required for one set of pixels. The 8-bit pixel components are converted to 16-bit elements to accommodate the MMX 16-bit multiply capability. If these images use\n   \n    640 \\times 480\n   \n   resolution, and the dissolve technique uses all 255 possible values of the fade value, then the total number of\n\n\n\n\n![Diagram illustrating the 5-step process for image compositing on color plane representation. Step 1: Unpack byte R pixel components from images A and B into 16-bit values Ar3, Ar2, Ar1, Ar0 and Br3, Br2, Br1, Br0. Step 2: Subtract image B from image A (r3 = Ar3 - Br3, etc.). Step 3: Multiply result by fade value (fade * r3, etc.). Step 4: Add image B pixels (newr3 = fade * r3 + Br3, etc.). Step 5: Pack new composite pixels back to bytes (r3, r2, r1, r0).](images/image_0219.jpeg)\n\n\nThe diagram illustrates the 5-step process for image compositing on color plane representation:\n\n\n  * Unpack byte R pixel components from images A and B into 16-bit values\n     \n      Ar3, Ar2, Ar1, Ar0\n     \n     and\n     \n      Br3, Br2, Br1, Br0\n     \n     .\n  * Subtract image B from image A to get intermediate values\n     \n      r3, r2, r1, r0\n     \n     .\n  * Multiply the result by the fade value to get\n     \n      \\text{fade} \\times r3, \\text{fade} \\times r2, \\text{fade} \\times r1, \\text{fade} \\times r0\n     \n     .\n  * Add image B pixels back to get the new composite values\n     \n      \\text{newr3}, \\text{newr2}, \\text{newr1}, \\text{newr0}\n     \n     .\n  * Pack the new composite pixels back to bytes\n     \n      r3, r2, r1, r0\n     \n     .\n\n\nDiagram illustrating the 5-step process for image compositing on color plane representation. Step 1: Unpack byte R pixel components from images A and B into 16-bit values Ar3, Ar2, Ar1, Ar0 and Br3, Br2, Br1, Br0. Step 2: Subtract image B from image A (r3 = Ar3 - Br3, etc.). Step 3: Multiply result by fade value (fade * r3, etc.). Step 4: Add image B pixels (newr3 = fade * r3 + Br3, etc.). Step 5: Pack new composite pixels back to bytes (r3, r2, r1, r0).\n\n\nMMX code sequence performing this operation:\n\n\npxor      mm7, mm7      ;zero out mm7\nmovq      mm3, fad_val  ;load fade value replicated 4 times\nmovd      mm0, imageA   ;load 4 red pixel components from image A\nmovd      mm1, imageB   ;load 4 red pixel components from image B\npunpckblw mm0, mm7      ;unpack 4 pixels to 16 bits\npunpckblw mm1, mm7      ;unpack 4 pixels to 16 bits\npsubw     mm0, mm1      ;subtract image B from image A\npmullw    mm0, mm3      ;multiply the subtract result by fade values\npaddwd    mm0, mm1      ;add result to image B\npackuswb  mm0, mm7      ;pack 16-bit results back to bytes\n\n**Figure 12.11**\n   Image Compositing on Color Plane Representation\n\n\ninstructions executed using MMX is 535 million. The same calculation, performed without the MMX instructions, requires 1.4 billion instruction executions [INTE98].\n\n\n\n\n**ARM Operation Types**\n\n\nThe ARM architecture provides a large collection of operation types. The following are the principal categories:\n\n\n  * ■\n    **Load and store instructions:**\n    In the ARM architecture, only load and store instructions access memory locations; arithmetic and logical instructions are performed only on registers and immediate values encoded in the instruction. This limitation is characteristic of RISC design and it is explored further in Chapter 15. The ARM architecture supports two broad types of instruction that load or store the value of a single register, or a pair of registers, from or to memory: (1) load or store a 32-bit word or an 8-bit unsigned byte, and (2) load or store a 16-bit unsigned halfword, and load and sign extend a 16-bit halfword or an 8-bit byte.\n  * ■\n    **Branch instructions:**\n    ARM supports a branch instruction that allows a conditional branch forwards or backwards up to 32 MB. A subroutine call can be performed by a variant of the standard branch instruction. As well as allowing a branch forward or backward up to 32 MB, the Branch with Link (BL) instruction preserves the address of the instruction after the branch (the return address) in the LR (R14). Branches are determined by a 4-bit condition field in the instruction.\n  * ■\n    **Data-processing instructions:**\n    This category includes logical instructions (AND, OR, XOR), add and subtract instructions, and test and compare instructions.\n  * ■\n    **Multiply instructions:**\n    The integer multiply instructions operate on word or halfword operands and can produce normal or long results. For example, there is a multiply instruction that takes two 32-bit operands and produces a 64-bit result.\n  * ■\n    **Parallel addition and subtraction instructions:**\n    In addition to the normal data processing and multiply instructions, there are a set of parallel addition and subtraction instructions, in which portions of two operands are operated on in parallel. For example, ADD16 adds the top halfwords of two registers to form the top halfword of the result and adds the bottom halfwords of the same two registers to form the bottom halfword of the result. These instructions are useful in image processing applications, similar to the x86 MMX instructions.\n  * ■\n    **Extend instructions:**\n    There are several instructions for unpacking data by sign or zero extending bytes to halfwords or words, and halfwords to words.\n  * ■\n    **Status register access instructions:**\n    ARM provides the ability to read and also to write portions of the status register.\n\n\n**CONDITION CODES**\n   The ARM architecture defines four condition flags that are stored in the program status register: N, Z, C, and V (Negative, Zero, Carry and Overflow), with meanings essentially the same as the S, Z, C, and V flags in the\n\n\n**Table 12.11**\n\nCode | Symbol | Condition Tested | Comment\n0000 | EQ | Z = 1 | Equal\n0001 | NE | Z = 0 | Not equal\n0010 | CS/HS | C = 1 | Carry set/unsigned higher or same\n0011 | CC/LO | C = 0 | Carry clear/unsigned lower\n0100 | MI | N = 1 | Minus/negative\n0101 | PL | N = 0 | Plus/positive or zero\n0110 | VS | V = 1 | Overflow\n0111 | VC | V = 0 | No overflow\n1000 | HI | C = 1 AND Z = 0 | Unsigned higher\n1001 | LS | C = 0 OR Z = 1 | Unsigned lower or same\n1010 | GE | N = V\n      \n      [(N = 1 AND V = 1)\n      \n      OR (N = 0 AND V = 0)] | Signed greater than or equal\n1011 | LT | N\n      \n       \\neq\n      \n      V\n      \n      [(N = 1 AND V = 0)\n      \n      OR (N = 0 AND V = 1)] | Signed less than\n1100 | GT | (Z = 0) AND (N = V) | Signed greater than\n1101 | LE | (Z = 1) OR (N\n      \n       \\neq\n      \n      V) | Signed less than or equal\n1110 | AL | — | Always (unconditional)\n1111 | — | — | This instruction can only be executed unconditionally\n\n\nx86 architecture. These four flags constitute a condition code in ARM. Table 12.11 shows the combination of conditions for which conditional execution is defined.\n\n\nThere are two unusual aspects to the use of condition codes in ARM:\n\n\n  * 1. All instructions, not just branch instructions, include a condition code field, which means that virtually all instructions may be conditionally executed. Any combination of flag settings except 1110 or 1111 in an instruction's condition code field signifies that the instruction will be executed only if the condition is met.\n  * 2. All data processing instructions (arithmetic, logical) include an S bit that signifies whether the instruction updates the condition flags.\n\n\nThe use of conditional execution and conditional setting of the condition flags helps in the design of shorter programs that use less memory. On the other hand, all instructions include 4 bits for the condition code, so there is a trade-off in that fewer bits in the 32-bit instruction are available for opcode and operands. Because the ARM is a RISC design that relies heavily on register addressing, this seems to be a reasonable trade-off."
        }
      ]
    },
    {
      "name": "Instruction Sets: Addressing Modes and Formats",
      "sections": [
        {
          "name": "Addressing Modes",
          "content": "The address field or fields in a typical instruction format are relatively small. We would like to be able to reference a large range of locations in main memory or, for some systems, virtual memory. To achieve this objective, a variety of addressing techniques has been employed. They all involve some trade-off between address range and/or addressing flexibility, on the one hand, and the number of memory references in the instruction and/or the complexity of address calculation, on the other. In this section, we examine the most common addressing techniques, or modes:\n\n\n  * ■ Immediate\n  * ■ Direct\n  * ■ Indirect\n  * ■ Register\n  * ■ Register indirect\n  * ■ Displacement\n  * ■ Stack\n\n\nThese modes are illustrated in Figure 13.1. In this section, we use the following notation:\n\n\nA = contents of an address field in the instruction\n\n\nR = contents of an address field in the instruction that refers to a register\n\n\nEA = actual (effective) address of the location containing the referenced operand\n\n\n(X) = contents of memory location X or register X\n\n\n\n\n![Figure 13.1: Addressing Modes. A 3x3 grid of diagrams showing different ways to calculate the effective address of an operand.](images/image_0220.jpeg)\n\n\nFigure 13.1 illustrates seven addressing modes, each showing the relationship between the instruction, operands, and memory or registers:\n\n\n  * **(a) Immediate:**\n     The operand is part of the instruction itself.\n  * **(b) Direct:**\n     The address field (A) in the instruction points directly to the operand in memory.\n  * **(c) Indirect:**\n     The address field (A) in the instruction points to a memory location that contains the actual operand address.\n  * **(d) Register:**\n     The operand is located in a register, addressed by the register field (R) in the instruction.\n  * **(e) Register indirect:**\n     The address field (A) in the instruction points to a register that contains the operand's address.\n  * **(f) Displacement:**\n     The operand's address is calculated by adding a displacement value to the address field (A) in the instruction, which points to a register.\n  * **(g) Stack:**\n     The operand is located at the top of the stack, addressed by an implicit register.\n\n\nFigure 13.1: Addressing Modes. A 3x3 grid of diagrams showing different ways to calculate the effective address of an operand.\n\n\n**Figure 13.1**\n   Addressing Modes\n\n\nTable 13.1 indicates the address calculation performed for each addressing mode.\n\n\nBefore beginning this discussion, two comments need to be made. First, virtually all computer architectures provide more than one of these addressing modes. The question arises as to how the processor can determine which address mode is being used in a particular instruction. Several approaches are taken. Often, different opcodes will use different addressing modes. Also, one or more bits in the instruction format can be used as a\n   *mode field*\n   . The value of the mode field determines which addressing mode is to be used.\n\n\nThe second comment concerns the interpretation of the effective address (EA). In a system without virtual memory, the\n   **effective address**\n   will be either a main memory address or a register. In a virtual memory system, the effective address is a virtual address or a register. The actual mapping to a physical address is a function of the memory management unit (MMU) and is invisible to the programmer.\n\n\n**Table 13.1**\n\nMode | Algorithm | Principal Advantage | Principal Disadvantage\nImmediate | Operand = A | No memory reference | Limited operand magnitude\nDirect | EA = A | Simple | Limited address space\nIndirect | EA = (A) | Large address space | Multiple memory references\nRegister | EA = R | No memory reference | Limited address space\nRegister indirect | EA = (R) | Large address space | Extra memory reference\nDisplacement | EA = A + (R) | Flexibility | Complexity\nStack | EA = top of stack | No memory reference | Limited applicability\n\n\n\n\n**Immediate Addressing**\n\n\nThe simplest form of addressing is\n   **immediate addressing**\n   , in which the operand value is present in the instruction\n\n\n\\text{Operand} = A\n\n\nThis mode can be used to define and use constants or set initial values of variables. Typically, the number will be stored in twos complement form; the leftmost bit of the operand field is used as a sign bit. When the operand is loaded into a data register, the sign bit is extended to the left to the full data\n   **word**\n   size. In some cases, the immediate binary value is interpreted as an unsigned nonnegative integer.\n\n\nThe advantage of immediate addressing is that no memory reference other than the instruction fetch is required to obtain the operand, thus saving one memory or cache cycle in the instruction cycle. The disadvantage is that the size of the number is restricted to the size of the address field, which, in most instruction sets, is small compared with the word length.\n\n\n\n\n**Direct Addressing**\n\n\nA very simple form of addressing is direct addressing, in which the address field contains the effective address of the operand:\n\n\n\\text{EA} = A\n\n\nThe technique was common in earlier generations of computers but is not common on contemporary architectures. It requires only one memory reference and no special calculation. The obvious limitation is that it provides only a limited address space.\n\n\n\n\n**Indirect Addressing**\n\n\nWith direct addressing, the length of the address field is usually less than the word length, thus limiting the address range. One solution is to have the address field refer to the address of a word in memory, which in turn contains a full-length address of the operand. This is known as\n   **indirect addressing**\n   :\n\n\n\\text{EA} = (A)\n\n\nAs defined earlier, the parentheses are to be interpreted as meaning\n   *contents of*\n   . The obvious advantage of this approach is that for a word length of\n   \n    N\n   \n   , an address space of\n   \n    2^N\n   \n   is now available. The disadvantage is that instruction execution requires two memory references to fetch the operand: one to get its address and a second to get its value.\n\n\nAlthough the number of words that can be addressed is now equal to\n   \n    2^N\n   \n   , the number of different effective addresses that may be referenced at any one time is limited to\n   \n    2^K\n   \n   , where\n   \n    K\n   \n   is the length of the address field. Typically, this is not a burdensome restriction, and it can be an asset. In a virtual memory environment, all the effective address locations can be confined to page 0 of any process. Because the address field of an instruction is small, it will naturally produce low-numbered direct addresses, which would appear in page 0. (The only restriction is that the page size must be greater than or equal to\n   \n    2^K\n   \n   .) When a process is active, there will be repeated references to page 0, causing it to remain in real memory. Thus, an indirect memory reference will involve, at most, one page fault rather than two.\n\n\nA rarely used variant of indirect addressing is multilevel or cascaded indirect addressing:\n\n\nEA = (\\dots (A) \\dots)\n\n\nIn this case, one bit of a full-word address is an indirect flag (\n   \n    I\n   \n   ). If the\n   \n    I\n   \n   bit is 0, then the word contains the EA. If the\n   \n    I\n   \n   bit is 1, then another level of indirection is invoked. There does not appear to be any particular advantage to this approach, and its disadvantage is that three or more memory references could be required to fetch an operand.\n\n\n\n\n**Register Addressing**\n\n\n**Register addressing**\n   is similar to direct addressing. The only difference is that the address field refers to a register rather than a main memory address:\n\n\nEA = R\n\n\nTo clarify, if the contents of a register address field in an instruction is 5, then register\n   \n    R5\n   \n   is the intended address, and the operand value is contained in\n   \n    R5\n   \n   . Typically, an address field that references registers will have from 3 to 5 bits, so that a total of from 8 to 32 general-purpose registers can be referenced.\n\n\nThe advantages of register addressing are that (1) only a small address field is needed in the instruction, and (2) no time-consuming memory references are required. As was discussed in Chapter 4, the memory access time for a register internal to the processor is much less than that for a main memory address. The disadvantage of register addressing is that the address space is very limited.\n\n\nIf register addressing is heavily used in an instruction set, this implies that the processor registers will be heavily used. Because of the severely limited number of registers (compared with main memory locations), their use in this fashion makes sense only if they are employed efficiently. If every operand is brought into a register from main memory, operated on once, and then returned to main memory, then a wasteful intermediate step has been added. If, instead, the operand in a register remains in use for multiple operations, then a real savings is achieved. An example is the intermediate result in a calculation. In particular, suppose that the algorithm\n\n\nfor twos complement multiplication were to be implemented in software. The location labeled A in the flowchart (Figure 10.12) is referenced many times and should be implemented in a register rather than a main memory location.\n\n\nIt is up to the programmer or compiler to decide which values should remain in registers and which should be stored in main memory. Most modern processors employ multiple general-purpose registers, placing a burden for efficient execution on the assembly-language programmer (e.g., compiler writer).\n\n\n\n\n**Register Indirect Addressing**\n\n\nJust as register addressing is analogous to direct addressing,\n   **register indirect addressing**\n   is analogous to indirect addressing. In both cases, the only difference is whether the address field refers to a memory location or a register. Thus, for register indirect address,\n\n\nEA = (R)\n\n\nThe advantages and limitations of register indirect addressing are basically the same as for indirect addressing. In both cases, the address space limitation (limited range of addresses) of the address field is overcome by having that field refer to a word-length location containing an address. In addition, register indirect addressing uses one less memory reference than indirect addressing.\n\n\n\n\n**Displacement Addressing**\n\n\nA very powerful mode of addressing combines the capabilities of direct addressing and register indirect addressing. It is known by a variety of names depending on the context of its use, but the basic mechanism is the same. We will refer to this as\n   **displacement addressing**\n   :\n\n\nEA = A + (R)\n\n\nDisplacement addressing requires that the instruction have two address fields, at least one of which is explicit. The value contained in one address field (value = A) is used directly. The other address field, or an implicit reference based on opcode, refers to a register whose contents are added to A to produce the effective address.\n\n\nWe will describe three of the most common uses of displacement addressing:\n\n\n  * ■ Relative addressing\n  * ■ Base-register addressing\n  * ■ Indexing\n\n\n**RELATIVE ADDRESSING**\n   For relative addressing, also called PC-relative addressing, the implicitly referenced register is the program counter (PC). That is, the next instruction address is added to the address field to produce the EA. Typically, the address field is treated as a twos complement number for this operation. Thus, the effective address is a displacement relative to the address of the instruction.\n\n\nRelative addressing exploits the concept of locality that was discussed in Chapters 4 and 8. If most memory references are relatively near to the instruction being executed, then the use of relative addressing saves address bits in the instruction.\n\n\n**BASE-REGISTER ADDRESSING**\n   For\n   **base-register addressing**\n   , the interpretation is the following: The referenced register contains a main memory address, and the address field contains a displacement (usually an unsigned integer representation) from that address. The register reference may be explicit or implicit.\n\n\nBase-register addressing also exploits the locality of memory references. It is a convenient means of implementing segmentation, which was discussed in Chapter 8. In some implementations, a single segment-base register is employed and is used implicitly. In others, the programmer may choose a register to hold the base address of a segment, and the instruction must reference it explicitly. In this latter case, if the length of the address field is\n   \n    K\n   \n   and the number of possible registers is\n   \n    N\n   \n   , then one instruction can reference any one of\n   \n    N\n   \n   areas of\n   \n    2^K\n   \n   words.\n\n\n**INDEXING**\n   For indexing, the interpretation is typically the following: The address field references a main memory address, and the referenced register contains a positive displacement from that address. Note that this usage is just the opposite of the interpretation for base-register addressing. Of course, it is more than just a matter of user interpretation. Because the address field is considered to be a memory address in indexing, it generally contains more bits than an address field in a comparable base-register instruction. Also, we will see that there are some refinements to indexing that would not be as useful in the base-register context. Nevertheless, the method of calculating the EA is the same for both base-register addressing and indexing, and in both cases the register reference is sometimes explicit and sometimes implicit (for different processor types).\n\n\nAn important use of indexing is to provide an efficient mechanism for performing iterative operations. Consider, for example, a list of numbers stored starting at location\n   \n    A\n   \n   . Suppose that we would like to add 1 to each element on the list. We need to fetch each value, add 1 to it, and store it back. The sequence of effective addresses that we need is\n   \n    A\n   \n   ,\n   \n    A + 1\n   \n   ,\n   \n    A + 2\n   \n   , ..., up to the last location on the list. With indexing, this is easily done. The value\n   \n    A\n   \n   is stored in the instruction's address field, and the chosen register, called an\n   *index register*\n   , is initialized to 0. After each operation, the index register is incremented by 1.\n\n\nBecause index registers are commonly used for such iterative tasks, it is typical that there is a need to increment or decrement the index register after each reference to it. Because this is such a common operation, some systems will automatically do this as part of the same instruction cycle. This is known as\n   **autoindexing**\n   . If certain registers are devoted exclusively to indexing, then autoindexing can be invoked implicitly and automatically. If general-purpose registers are used, the autoindex operation may need to be signaled by a bit in the instruction. Autoindexing using increment can be depicted as follows.\n\n\n\\begin{aligned} \\text{EA} &= A + (R) \\\\ (R) &\\leftarrow (R) + 1 \\end{aligned}\n\n\nIn some machines, both indirect addressing and indexing are provided, and it is possible to employ both in the same instruction. There are two possibilities: the indexing is performed either before or after the indirection.\n\n\nIf indexing is performed after the indirection, it is termed\n   **postindexing**\n   :\n\n\n\\text{EA} = (A) + (R)\n\n\nFirst, the contents of the address field are used to access a memory location containing a direct address. This address is then indexed by the register value. This technique is useful for accessing one of a number of blocks of data of a fixed format. For example, it was described in Chapter 8 that the operating system needs to employ a process control block for each process. The operations performed are the same regardless of which block is being manipulated. Thus, the addresses in the instructions that reference the block could point to a location (value = A) containing a variable pointer to the start of a process control block. The index register contains the displacement within the block.\n\n\nWith\n   **preindexing**\n   , the indexing is performed before the indirection:\n\n\nEA = (A + (R))\n\n\nAn address is calculated as with simple indexing. In this case, however, the calculated address contains not the operand, but the address of the operand. An example of the use of this technique is to construct a multiway branch table. At a particular point in a program, there may be a branch to one of a number of locations depending on conditions. A table of addresses can be set up starting at location A. By indexing into this table, the required location can be found.\n\n\nTypically, an instruction set will not include both preindexing and postindexing.\n\n\n\n\n**Stack Addressing**\n\n\nThe final addressing mode that we consider is stack addressing. As defined in Appendix I, a stack is a linear array of locations. It is sometimes referred to as a\n   *pushdown list*\n   or\n   *last-in-first-out queue*\n   . The stack is a reserved block of locations. Items are appended to the top of the stack so that, at any given time, the block is partially filled. Associated with the stack is a pointer whose value is the address of the top of the stack. Alternatively, the top two elements of the stack may be in processor registers, in which case the stack pointer references the third element of the stack. The stack pointer is maintained in a register. Thus, references to stack locations in memory are in fact register indirect addresses.\n\n\nThe stack mode of addressing is a form of implied addressing. The machine instructions need not include a memory reference but implicitly operate on the top of the stack."
        },
        {
          "name": "x86 and ARM Addressing Modes",
          "content": "**x86 Addressing Modes**\n\n\nRecall from Figure 8.21 that the x86 address translation mechanism produces an address, called a virtual or effective address, that is an offset into a segment. The sum of the starting address of the segment and the effective address produces a linear address. If paging is being used, this linear address must pass through a page-translation mechanism to produce a physical address. In what follows, we ignore this last step because it is transparent to the instruction set and to the programmer.\n\n\nThe x86 is equipped with a variety of addressing modes intended to allow the efficient execution of high-level languages. Figure 13.2 indicates the logic\n\n\n\n\n![Diagram illustrating the x86 Addressing Mode Calculation process. It shows the flow from Segment registers (SS, GS, FS, ES, DS, CS) through Selectors to Descriptor registers. The Descriptor registers contain Access rights and Base Address/Limit for each segment. The Base Address is used to calculate the Segment base address. The Segment base address is then combined with the Base register, Index register (scaled by 1, 2, 4, or 8), and Displacement (0, 8, or 32 bits) to produce the Effective address. This Effective address is then added to the Segment base address to produce the Linear address, which is used to access the Segment memory.](images/image_0221.jpeg)\n\n\nThe diagram illustrates the x86 addressing mode calculation process. It shows the following components and their interactions:\n\n\n  * **Segment registers:**\n     SS, GS, FS, ES, DS, and CS. Each register is connected to a\n     **Selector**\n     block.\n  * **Descriptor registers:**\n     These are accessed by the selectors. Each descriptor register contains\n     **Access rights**\n     and a\n     **Base Address**\n     . The CS selector also accesses a\n     **Limit**\n     register.\n  * **Base register:**\n     A register that holds the base address for the segment.\n  * **Index register:**\n     A register that holds the index value for the segment.\n  * **Scale:**\n     A value of 1, 2, 4, or 8 used to scale the index register value.\n  * **Displacement:**\n     A value of 0, 8, or 32 bits provided in the instruction.\n  * **Segment base address:**\n     The base address of the segment, derived from the Base register and the Base Address from the descriptor register.\n  * **Effective address:**\n     Calculated by adding the scaled index register value and the displacement:\n     \n      (\\text{Index register} \\times \\text{Scale}) + \\text{Displacement}\n     \n     .\n  * **Linear address:**\n     Calculated by adding the segment base address and the effective address:\n     \n      \\text{Segment base address} + \\text{Effective address}\n     \n     .\n  * **Segment:**\n     A memory segment represented as a vertical bar with a\n     **Segment base address**\n     and a\n     **Limit**\n     indicating its size.\n\n\nDiagram illustrating the x86 Addressing Mode Calculation process. It shows the flow from Segment registers (SS, GS, FS, ES, DS, CS) through Selectors to Descriptor registers. The Descriptor registers contain Access rights and Base Address/Limit for each segment. The Base Address is used to calculate the Segment base address. The Segment base address is then combined with the Base register, Index register (scaled by 1, 2, 4, or 8), and Displacement (0, 8, or 32 bits) to produce the Effective address. This Effective address is then added to the Segment base address to produce the Linear address, which is used to access the Segment memory.\n\n\n**Figure 13.2**\n   x86 Addressing Mode Calculation\n\n\ninvolved. The segment register determines the segment that is the subject of the reference. There are six segment registers; the one being used for a particular reference depends on the context of execution and the instruction. Each segment register holds an index into the segment descriptor table (Figure 8.20), which holds the starting address of the corresponding segments. Associated with each user-visible segment register is a segment descriptor register (not programmer visible), which records the access rights for the segment as well as the starting address and limit (length) of the segment. In addition, there are two registers that may be used in constructing an address: the base register and the index register.\n\n\nTable 13.2 lists the x86 addressing modes. Let us consider each of these in turn.\n\n\nFor the\n   **immediate mode**\n   , the operand is included in the instruction. The operand can be a byte, word, or doubleword of data.\n\n\nFor\n   **register operand mode**\n   , the operand is located in a register. For general instructions, such as data transfer, arithmetic, and logical instructions, the operand can be one of the 32-bit general registers (EAX, EBX, ECX, EDX, ESI, EDI, ESP, EBP), one of the 16-bit general registers (AX, BX, CX, DX, SI, DI, SP, BP), or one of the 8-bit general registers (AH, BH, CH, DH, AL, BL, CL, DL). There are also some instructions that reference the segment selector registers (CS, DS, ES, SS, FS, GS).\n\n\n**Table 13.2**\n\nMode | Algorithm\nImmediate | Operand = A\nRegister Operand | LA = R\nDisplacement | LA = (SR) + A\nBase | LA = (SR) + (B)\nBase with Displacement | LA = (SR) + (B) + A\nScaled Index with Displacement | LA = (SR) + (I) × S + A\nBase with Index and Displacement | LA = (SR) + (B) + (I) + A\nBase with Scaled Index and Displacement | LA = (SR) + (I) × S + (B) + A\nRelative | LA = (PC) + A\n\n\nLA = linear address\n\n\n(X) = contents of X\n\n\nSR = segment register\n\n\nPC = program counter\n\n\nA = contents of an address field in the instruction\n\n\nR = register\n\n\nB = base register\n\n\nI = index register\n\n\nS = scaling factor\n\n\nThe remaining addressing modes reference locations in memory. The memory location must be specified in terms of the segment containing the location and the offset from the beginning of the segment. In some cases, a segment is specified explicitly; in others, the segment is specified by simple rules that assign a segment by default.\n\n\nIn the\n   **displacement mode**\n   , the operand's offset (the effective address of Figure 13.2) is contained as part of the instruction as an 8-, 16-, or 32-bit displacement. With segmentation, all addresses in instructions refer merely to an offset in a segment. The displacement addressing mode is found on few machines because, as mentioned earlier, it leads to long instructions. In the case of the x86, the displacement value can be as long as 32 bits, making for a 6-byte instruction. Displacement addressing can be useful for referencing global variables.\n\n\nThe remaining addressing modes are indirect, in the sense that the address portion of the instruction tells the processor where to look to find the address. The\n   **base mode**\n   specifies that one of the 8-, 16-, or 32-bit registers contains the effective address. This is equivalent to what we have referred to as register indirect addressing.\n\n\nIn the\n   **base with displacement mode**\n   , the instruction includes a displacement to be added to a base register, which may be any of the general-purpose registers. Examples of uses of this mode are as follows:\n\n\n  * ■ Used by a compiler to point to the start of a local variable area. For example, the base register could point to the beginning of a stack frame, which contains the local variables for the corresponding procedure.\n  * ■ Used to index into an array when the element size is not 1, 2, 4, or 8 bytes and which therefore cannot be indexed using an index register. In this case, the displacement points to the beginning of the array, and the base register holds the results of a calculation to determine the offset to a specific element within the array.\n\n\n  * ■ Used to access a field of a record. The base register points to the beginning of the record, while the displacement is an offset to the field.\n\n\nIn the\n   **scaled index with displacement mode**\n   , the instruction includes a displacement to be added to a register, in this case called an index register. The index register may be any of the general-purpose registers except the one called ESP, which is generally used for stack processing. In calculating the effective address, the contents of the index register are multiplied by a scaling factor of 1, 2, 4, or 8, and then added to a displacement. This mode is very convenient for indexing arrays. A scaling factor of 2 can be used for an array of 16-bit integers. A scaling factor of 4 can be used for 32-bit integers or floating-point numbers. Finally, a scaling factor of 8 can be used for an array of double-precision floating-point numbers.\n\n\nThe\n   **base with index and displacement mode**\n   sums the contents of the base register, the index register, and a displacement to form the effective address. Again, the base register can be any general-purpose register and the index register can be any general-purpose register except ESP. As an example, this addressing mode could be used for accessing a local array on a stack frame. This mode can also be used to support a two-dimensional array; in this case, the displacement points to the beginning of the array, and each register handles one dimension of the array.\n\n\nThe\n   **based scaled index with displacement mode**\n   sums the contents of the index register multiplied by a scaling factor, the contents of the base register, and the displacement. This is useful if an array is stored in a stack frame; in this case, the array elements would be 2, 4, or 8 bytes each in length. This mode also provides efficient indexing of a two-dimensional array when the array elements are 2, 4, or 8 bytes in length.\n\n\nFinally,\n   **relative addressing**\n   can be used in transfer-of-control instructions. A displacement is added to the value of the program counter, which points to the next instruction. In this case, the displacement is treated as a signed byte, word, or doubleword value, and that value either increases or decreases the address in the program counter.\n\n\n\n\n**ARM Addressing Modes**\n\n\nTypically, a RISC machine, unlike a CISC machine, uses a simple and relatively straightforward set of addressing modes. The ARM architecture departs somewhat from this tradition by providing a relatively rich set of addressing modes. These modes are most conveniently classified with respect to the type of instruction.\n   \n    1\n\n\n**LOAD/STORE ADDRESSING**\n   Load and store instructions are the only instructions that reference memory. This is always done indirectly through a base register plus offset. There are three alternatives with respect to indexing (Figure 13.3):\n\n\n  * ■\n    **Offset:**\n    For this addressing method,\n    **indexing**\n    is not used. An offset value is added to or subtracted from the value in the base register to form the memory address. As an example Figure 13.3a illustrates this method with the assembly language instruction\n    \n     STRB r0, [r1, #12]\n    \n    . This is the store byte instruction.\n\n\n1\n   \n   As with our discussion of x86 addressing, we ignore the translation from virtual to physical address in the following discussion.\n\n\nSTRB\n   \n    r0\n   \n   , [\n   \n    r1\n   \n   , #12]\n\n\n\n\n![Diagram (a) Offset: Illustrates the Offset addressing mode. The original base register r1 contains 0x200. An offset of 0xC is added to this base address to form the memory address 0x20C. The destination register r0 contains the value 0x5, which is to be stored at the memory location 0x20C. The memory structure shows a stack of cells with addresses 0x200, 0x20C, and 0x205, with vertical dots indicating other cells.](images/image_0222.jpeg)\n\n\n(a) Offset\n\n\nDiagram (a) Offset: Illustrates the Offset addressing mode. The original base register r1 contains 0x200. An offset of 0xC is added to this base address to form the memory address 0x20C. The destination register r0 contains the value 0x5, which is to be stored at the memory location 0x20C. The memory structure shows a stack of cells with addresses 0x200, 0x20C, and 0x205, with vertical dots indicating other cells.\n\n\nSTRB\n   \n    r0\n   \n   , [\n   \n    r1\n   \n   , #12]!\n\n\n\n\n![Diagram (b) Preindex: Illustrates the Preindex addressing mode. The original base register r1 contains 0x200. The offset 0xC is added to the base address to form the memory address 0x20C. The destination register r0 contains the value 0x5, which is stored at the memory location 0x20C. After the store operation, the base register r1 is updated to contain the new address 0x20C. The memory structure is the same as in diagram (a).](images/image_0223.jpeg)\n\n\n(b) Preindex\n\n\nDiagram (b) Preindex: Illustrates the Preindex addressing mode. The original base register r1 contains 0x200. The offset 0xC is added to the base address to form the memory address 0x20C. The destination register r0 contains the value 0x5, which is stored at the memory location 0x20C. After the store operation, the base register r1 is updated to contain the new address 0x20C. The memory structure is the same as in diagram (a).\n\n\nSTRB\n   \n    r0\n   \n   , [\n   \n    r1\n   \n   ], #12\n\n\n\n\n![Diagram (c) Postindex: Illustrates the Postindex addressing mode. The original base register r1 contains 0x200. The destination register r0 contains the value 0x5, which is stored at the memory location 0x200. After the store operation, the base register r1 is updated to contain the new address 0x20C. The memory structure shows the value 0x5 at address 0x200 and vertical dots above and below it.](images/image_0224.jpeg)\n\n\n(c) Postindex\n\n\nDiagram (c) Postindex: Illustrates the Postindex addressing mode. The original base register r1 contains 0x200. The destination register r0 contains the value 0x5, which is stored at the memory location 0x200. After the store operation, the base register r1 is updated to contain the new address 0x20C. The memory structure shows the value 0x5 at address 0x200 and vertical dots above and below it.\n\n\n**Figure 13.3**\n   ARM Indexing Methods\n\n\nIn this case the base address is in register\n   \n    r1\n   \n   and the displacement is an immediate value of decimal 12. The resulting address (base plus offset) is the location where the least significant byte from\n   \n    r0\n   \n   is to be stored.\n\n\n  * ■\n    **Preindex:**\n    The memory address is formed in the same way as for offset addressing. The memory address is also written back to the base register. In other words, the base register value is incremented or decremented by the offset value. Figure 13.3b illustrates this method with the assembly language instruction STRB\n    \n     r0\n    \n    , [\n    \n     r1\n    \n    , #12]!. The exclamation point signifies preindexing.\n\n\n  * ■\n    **Postindex:**\n    The memory address is the base register value. An offset is added to or subtracted from the base register value and the result is written back to the base register. Figure 13.3c illustrates this method with the assembly language instruction\n    \n     STRB r0, [r1], #12\n    \n    .\n\n\nNote that what ARM refers to as a base register acts as an index register for preindex and postindex addressing. The offset value can either be an immediate value stored in the instruction or it can be in another register. If the offset value is in a register, another useful feature is available: scaled register addressing. The value in the offset register is scaled by one of the shift operators: Logical Shift Left, Logical Shift Right, Arithmetic Shift Right, Rotate Right, or Rotate Right Extended (which includes the carry bit in the rotation). The amount of the shift is specified as an immediate value in the instruction.\n\n\n**DATA PROCESSING INSTRUCTION ADDRESSING**\n   Data processing instructions use either register addressing or a mixture of register and immediate addressing. For register addressing, the value in one of the register operands may be scaled using one of the five shift operators defined in the preceding paragraph.\n\n\n**BRANCH INSTRUCTIONS**\n   The only form of addressing for branch instructions is immediate addressing. The branch instruction contains a 24-bit value. For address calculation, this value is shifted left 2 bits, so that the address is on a word boundary. Thus the effective address range is\n   \n    \\pm 32\n   \n   MB from the program counter.\n\n\n**LOAD/STORE MULTIPLE ADDRESSING**\n   Load Multiple instructions load a subset (possibly all) of the general-purpose registers from memory. Store Multiple instructions store a subset (possibly all) of the general-purpose registers to memory. The list of registers for the load or store is specified in a 16-bit field in the instruction with each bit corresponding to one of the 16 registers. Load and Store Multiple addressing modes produce a sequential range of memory addresses. The lowest-numbered register is stored at the lowest memory address and the highest-numbered register at the highest memory address. Four addressing modes are used (Figure 13.4): increment after, increment before, decrement after, and decrement before. A base\n\n\nLDMMxx r10, {r0, r1, r4}\nSTMMxx r10, {r0, r1, r4}\n\n\n![Diagram illustrating ARM Load/Store Multiple Addressing modes. A base register r10 with value 0x20C points to a memory stack. The stack contains registers r4, r1, and r0. Four addressing modes are shown: Increment after (IA), Increment before (IB), Decrement after (DA), and Decrement before (DB). Each mode shows the order of registers accessed and the final value of the base register r10.](images/image_0225.jpeg)\n\n\nThe diagram illustrates the four addressing modes for Load/Store Multiple instructions. A base register\n    **r10**\n    with the value\n    **0x20C**\n    is shown. An arrow points from the base register to a memory stack of registers\n    **r4**\n    ,\n    **r1**\n    , and\n    **r0**\n    . The stack is represented as a series of horizontal bars, with the bottom bar being the lowest memory address and the top bar being the highest. The four addressing modes are:\n\n\n\nAddressing Mode | Registers Accessed (from bottom to top) | Final Value of r10\nIncrement after (IA) | r0, r1, r4 | 0x20C + 12 = 0x20D\nIncrement before (IB) | r4, r1, r0 | 0x20C + 12 = 0x20D\nDecrement after (DA) | r4, r1, r0 | 0x20C - 12 = 0x208\nDecrement before (DB) | r0, r1, r4 | 0x20C - 12 = 0x208\n\n\nDiagram illustrating ARM Load/Store Multiple Addressing modes. A base register r10 with value 0x20C points to a memory stack. The stack contains registers r4, r1, and r0. Four addressing modes are shown: Increment after (IA), Increment before (IB), Decrement after (DA), and Decrement before (DB). Each mode shows the order of registers accessed and the final value of the base register r10.\n\n\n**Figure 13.4**\n   ARM Load/Store Multiple Addressing\n\n\nregister specifies a main memory address where register values are stored in or loaded from in ascending (increment) or descending (decrement) word locations. Incrementing or decrementing starts either before or after the first memory access.\n\n\nThese instructions are useful for block loads or stores, stack operations, and procedure exit sequences."
        },
        {
          "name": "Instruction Formats 469",
          "content": "An instruction format defines the layout of the bits of an instruction, in terms of its constituent fields. An instruction format must include an opcode and, implicitly or explicitly, zero or more operands. Each explicit operand is referenced using one of the addressing modes described in Section 13.1. The format must, implicitly or explicitly, indicate the addressing mode for each operand. For most instruction sets, more than one instruction format is used.\n\n\nThe design of an instruction format is a complex art, and an amazing variety of designs have been implemented. We examine the key design issues, looking briefly at some designs to illustrate points, and then we examine the x86 and ARM solutions in detail.\n\n\n\n\n**Instruction Length**\n\n\nThe most basic design issue to be faced is the instruction format length. This decision affects, and is affected by, memory size, memory organization, bus structure, processor complexity, and processor speed. This decision determines the richness and flexibility of the machine as seen by the assembly-language programmer.\n\n\nThe most obvious trade-off here is between the desire for a powerful instruction repertoire and a need to save space. Programmers want more opcodes, more operands, more addressing modes, and greater address range. More opcodes and more operands make life easier for the programmer, because shorter programs can be written to accomplish given tasks. Similarly, more addressing modes give the programmer greater flexibility in implementing certain functions, such as table manipulations and multiple-way branching. And, of course, with the increase in main memory size and the increasing use of virtual memory, programmers want to be able to address larger memory ranges. All of these things (opcodes, operands, addressing modes, address range) require bits and push in the direction of longer instruction lengths. But longer instruction length may be wasteful. A 64-bit instruction occupies twice the space of a 32-bit instruction but is probably less than twice as useful.\n\n\nBeyond this basic trade-off, there are other considerations. Either the instruction length should be equal to the memory-transfer length (in a bus system, data-bus length) or one should be a multiple of the other. Otherwise, we will not get an integral number of instructions during a fetch cycle. A related consideration is the memory transfer rate. This rate has not kept up with increases in processor speed. Accordingly, memory can become a bottleneck if the processor can execute instructions faster than it can fetch them. One solution to this problem is to use cache memory (see Section 4.3); another is to use shorter instructions. Thus, 16-bit instructions can be fetched at twice the rate of 32-bit instructions but probably can be executed less than twice as rapidly.\n\n\nA seemingly mundane but nevertheless important feature is that the instruction length should be a multiple of the character length, which is usually 8 bits, and of the length of fixed-point numbers. To see this, we need to make use of that unfortunately ill-defined word,\n   *word*\n   [FRAI83]. The word length of memory is, in some sense, the “natural” unit of organization. The size of a word usually determines the size of fixed-point numbers (usually the two are equal). Word size is also typically equal to, or at least integrally related to, the memory transfer size. Because a common form of data is character data, we would like a word to store an integral number of characters. Otherwise, there are wasted bits in each word when storing multiple characters, or a character will have to straddle a word boundary. The importance of this point is such that IBM, when it introduced the System/360 and wanted to employ 8-bit characters, made the wrenching decision to move from the 36-bit architecture of the scientific members of the 700/7000 series to a 32-bit architecture.\n\n\n\n\n**Allocation of Bits**\n\n\nWe’ve looked at some of the factors that go into deciding the length of the instruction format. An equally difficult issue is how to allocate the bits in that format. The trade-offs here are complex.\n\n\nFor a given instruction length, there is clearly a trade-off between the number of opcodes and the power of the addressing capability. More opcodes obviously mean more bits in the opcode field. For an instruction format of a given length, this reduces the number of bits available for addressing. There is one interesting refinement to this trade-off, and that is the use of variable-length opcodes. In this approach, there is a minimum opcode length but, for some opcodes, additional operations may be specified by using additional bits in the instruction. For a fixed-length instruction, this leaves fewer bits for addressing. Thus, this feature is used for those instructions that require fewer operands and/or less powerful addressing.\n\n\nThe following interrelated factors go into determining the use of the addressing bits.\n\n\n  * ■\n    **Number of addressing modes:**\n    Sometimes an addressing mode can be indicated implicitly. For example, certain opcodes might always call for indexing. In other cases, the addressing modes must be explicit, and one or more mode bits will be needed.\n  * ■\n    **Number of operands:**\n    We have seen that fewer addresses can make for longer, more awkward programs (e.g., Figure 12.3). Typical instruction formats on today’s machines include two operands. Each operand address in the instruction might require its own mode indicator, or the use of a mode indicator could be limited to just one of the address fields.\n  * ■\n    **Register versus memory:**\n    A machine must have registers so that data can be brought into the processor for processing. With a single user-visible register (usually called the accumulator), one operand address is implicit and consumes no instruction bits. However, single-register programming is awkward and requires many instructions. Even with multiple registers, only a few bits are needed to specify the register. The more that registers can be used for\n\n\n  * operand references, the fewer bits are needed. A number of studies indicate that a total of 8 to 32 user-visible registers is desirable [LUND77, HUCK83]. Most contemporary architectures have at least 32 registers.\n  * * ■\n      **Number of register sets:**\n      Most contemporary machines have one set of general-purpose registers, with typically 32 or more registers in the set. These registers can be used to store data and can be used to store addresses for displacement addressing. Some architectures, including that of the x86, have a collection of two or more specialized sets (such as data and displacement). One advantage of this latter approach is that, for a fixed number of registers, a functional split requires fewer bits to be used in the instruction. For example, with two sets of eight registers, only 3 bits are required to identify a register; the opcode or mode register will determine which set of registers is being referenced.\n  * ■\n      **Address range:**\n      For addresses that reference memory, the range of addresses that can be referenced is related to the number of address bits. Because this imposes a severe limitation, direct addressing is rarely used. With displacement addressing, the range is opened up to the length of the address register. Even so, it is still convenient to allow rather large displacements from the register address, which requires a relatively large number of address bits in the instruction.\n  * ■\n      **Address granularity:**\n      For addresses that reference memory rather than registers, another factor is the granularity of addressing. In a system with 16- or 32-bit words, an address can reference a word or a byte at the designer's choice. Byte addressing is convenient for character manipulation but requires, for a fixed-size memory, more address bits.\n\n\nThus, the designer is faced with a host of factors to consider and balance. How critical the various choices are is not clear. As an example, we cite one study [CRAG79] that compared various instruction format approaches, including the use of a stack, general-purpose registers, an accumulator, and only memory-to-register approaches. Using a consistent set of assumptions, no significant difference in code space or execution time was observed.\n\n\nLet us briefly look at how two historical machine designs balance these various factors.\n\n\n**PDP-8**\n   One of the simplest instruction designs for a general-purpose computer was for the PDP-8 [BELL78b]. The PDP-8 uses 12-bit instructions and operates on 12-bit words. There is a single general-purpose register, the accumulator.\n\n\nDespite the limitations of this design, the addressing is quite flexible. Each memory reference consists of 7 bits plus two 1-bit modifiers. The memory is divided into fixed-length pages of\n   \n    2^7 = 128\n   \n   words each. Address calculation is based on references to page 0 or the current page (page containing this instruction) as determined by the page bit. The second modifier bit indicates whether direct or indirect addressing is to be used. These two modes can be used in combination, so that an indirect address is a 12-bit address contained in a word of page 0 or the current page. In addition, 8 dedicated words on page 0 are autoindex “registers.” When an indirect reference is made to one of these locations, preindexing occurs.\n\n\nFigure 13.5 shows the PDP-8 instruction format. There are a 3-bit opcode and three types of instructions. For opcodes 0 through 5, the format is a single-address\n\n\n\nMemory reference instructions\nOpcode | D/I | Z/C | Displacement\n0 |  | 2 | 3 | 4 | 5 |  |  |  |  |  | 11\nInput/output instructions\n1 | 1 | 0 | Device | Opcode\n0 |  | 2 | 3 |  |  |  | 8 | 9 |  |  | 11\nRegister reference instructions\nGroup 1 microinstructions | CLA | CLL | CMA | CML | RAR | RAL | BSW | IAC\n1 | 1 | 1 | 0 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11\nGroup 2 microinstructions | CLA | SMA | SZA | SNL | RSS | OSR | HLT | 0\n1 | 1 | 1 | 0 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11\nGroup 3 microinstructions | CLA | MQA | 0 | MQL | 0 | 0 | 0 | 1\n1 | 1 | 1 | 0 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11\n\n\nD/I = Direct/Indirect address\n   \n\n   Z/C = Page 0 or Current page\n   \n\n   CLA = Clear Accumulator\n   \n\n   CLL = Clear Link\n   \n\n   CMA = CoMplement Accumulator\n   \n\n   CML = CoMplement Link\n   \n\n   RAR = Rotate Accumulator Right\n   \n\n   RAL = Rotate Accumulator Left\n   \n\n   BSW = Byte SWap\n   \n\n   IAC = Increment ACcumulator\n   \n\n   SMA = Skip on Minus Accumulator\n   \n\n   SZA = Skip on Zero Accumulator\n   \n\n   SNL = Skip on Nonzero Link\n   \n\n   RSS = Reverse Skip Sense\n   \n\n   OSR = Or with Switch Register\n   \n\n   HLT = HaLT\n   \n\n   MQA = Multiplier Quotient into Accumulator\n   \n\n   MQL = Multiplier Quotient Load\n\n\n**Figure 13.5**\nmemory reference instruction including a page bit and an indirect bit. Thus, there are only six basic operations. To enlarge the group of operations, opcode 7 defines a register reference or\n   *microinstruction*\n   . In this format, the remaining bits are used to encode additional operations. In general, each bit defines a specific operation (e.g., clear accumulator), and these bits can be combined in a single instruction. The microinstruction strategy was used as far back as the PDP-1 by DEC and is, in a sense, a forerunner of today's microprogrammed machines, to be discussed in Part Four. Opcode 6 is the I/O operation; 6 bits are used to select one of 64 devices, and 3 bits specify a particular I/O command.\n\n\nThe PDP-8 instruction format is remarkably efficient. It supports indirect addressing, displacement addressing, and indexing. With the use of the opcode extension, it supports a total of approximately 35 instructions. Given the constraints of a 12-bit instruction length, the designers could hardly have done better.\n\n\n**PDP-10**\n   A sharp contrast to the instruction set of the PDP-8 is that of the PDP-10. The PDP-10 was designed to be a large-scale time-shared system, with an emphasis on making the system easy to program, even if additional hardware expense was involved.\n\n\nAmong the design principles employed in designing the instruction set were the following [BELL78c]:\n\n\n  * ■\n    **Orthogonality:**\n    Orthogonality is a principle by which two variables are independent of each other. In the context of an instruction set, the term indicates\n\n\nthat other elements of an instruction are independent of (not determined by) the opcode. The PDP-10 designers use the term to describe the fact that an address is always computed in the same way, independent of the opcode. This is in contrast to many machines, where the address mode sometimes depends implicitly on the operator being used.\n\n\n  * ■\n    **Completeness:**\n    Each arithmetic data type (integer, fixed-point, floating-point) should have a complete and identical set of operations.\n  * ■\n    **Direct addressing:**\n    Base plus displacement addressing, which places a memory organization burden on the programmer, was avoided in favor of direct addressing.\n\n\nEach of these principles advances the main goal of ease of programming.\n\n\nThe PDP-10 has a 36-bit word length and a 36-bit instruction length. The fixed instruction format is shown in Figure 13.6. The opcode occupies 9 bits, allowing up to 512 operations. In fact, a total of 365 different instructions are defined. Most instructions have two addresses, one of which is one of 16 general-purpose registers. Thus, this operand reference occupies 4 bits. The other operand reference starts with an 18-bit memory address field. This can be used as an immediate operand or a memory address. In the latter usage, both indexing and indirect addressing are allowed. The same general-purpose registers are also used as index registers.\n\n\nA 36-bit instruction length is true luxury. There is no need to do clever things to get more opcodes; a 9-bit opcode field is more than adequate. Addressing is also straightforward. An 18-bit address field makes direct addressing desirable. For memory sizes greater than\n   \n    2^{18}\n   \n   , indirection is provided. For the ease of the programmer, indexing is provided for table manipulation and iterative programs. Also, with an 18-bit operand field, immediate addressing becomes attractive.\n\n\nThe PDP-10 instruction set design does accomplish the objectives listed earlier [LUND77]. It eases the task of the programmer or compiler at the expense of an inefficient utilization of space. This was a conscious choice made by the designers and therefore cannot be faulted as poor design.\n\n\n\n\n**Variable-Length Instructions**\n\n\nThe examples we have looked at so far have used a single fixed instruction length, and we have implicitly discussed trade-offs in that context. But the designer may choose instead to provide a variety of instruction formats of different lengths. This tactic makes it easy to provide a large repertoire of opcodes, with different opcode lengths. Addressing can be more flexible, with various combinations of register and memory references plus addressing modes. With variable-length instructions, these many variations can be provided efficiently and compactly.\n\n\n\nOpcode | Register | I | Index register | Memory address\n0 | 8 9 | 12 | 14 17 | 18 35\n\n\nI = indirect bit\n\n\n**Figure 13.6**\n   PDP-10 Instruction Format\n\n\nThe principal price to pay for variable-length instructions is an increase in the complexity of the processor. Falling hardware prices, the use of microprogramming (discussed in Part Four), and a general increase in understanding the principles of processor design have all contributed to making this a small price to pay. However, we will see that RISC and superscalar machines can exploit the use of fixed-length instructions to provide improved performance.\n\n\nThe use of variable-length instructions does not remove the desirability of making all of the instruction lengths integrally related to the word length. Because the processor does not know the length of the next instruction to be fetched, a typical strategy is to fetch a number of bytes or words equal to at least the longest possible instruction. This means that sometimes multiple instructions are fetched. However, as we shall see in Chapter 14, this is a good strategy to follow in any case.\n\n\n**PDP-11**\n   The PDP-11 was designed to provide a powerful and flexible instruction set within the constraints of a 16-bit minicomputer [BELL70].\n\n\nThe PDP-11 employs a set of eight 16-bit general-purpose registers. Two of these registers have additional significance: one is used as a stack pointer for special-purpose stack operations, and one is used as the program counter, which contains the address of the next instruction.\n\n\nFigure 13.7 shows the PDP-11 instruction formats. Thirteen different formats are used, encompassing zero-, one-, and two-address instruction types. The opcode can vary from 4 to 16 bits in length. Register references are 6 bits in length. Three bits identify the register, and the remaining 3 bits identify the addressing mode. The PDP-11 is endowed with a rich set of addressing modes. One advantage of linking the addressing mode to the operand rather than the opcode, as is sometimes done, is that any addressing mode can be used with any opcode. As was mentioned, this independence is referred to as\n   *orthogonality*\n   .\n\n\nPDP-11 instructions are usually one word (16 bits) long. For some instructions, one or two memory addresses are appended, so that 32-bit and 48-bit instructions are part of the repertoire. This provides for further flexibility in addressing.\n\n\nThe PDP-11 instruction set and addressing capability are complex. This increases both hardware cost and programming complexity. The advantage is that more efficient or compact programs can be developed.\n\n\n**VAX**\n   Most architectures provide a relatively small number of fixed instruction formats. This can cause two problems for the programmer. First, addressing mode and opcode are not orthogonal. For example, for a given operation, one operand must come from a register and another from memory, or both from registers, and so on. Second, only a limited number of operands can be accommodated: typically up to two or three. Because some operations inherently require more operands, various strategies must be used to achieve the desired result using two or more instructions.\n\n\nTo avoid these problems, two criteria were used in designing the VAX instruction format [STRE78]:\n\n\n  * 1. All instructions should have the “natural” number of operands.\n  * 2. All operands should have the same generality in specification.\n\n\n\n1 | Opcode | Source | Destination | 2 | Opcode | R | Source | 3 | Opcode | Offset\n | 4 | 6 | 6 |  | 7 | 3 | 6 |  | 8 | 8\n4 | Opcode | FP | Destination | 5 | Opcode | Destination | 6 | Opcode | CC\n | 8 | 2 | 6 |  | 10 | 6 |  | 12 | 4\n7 | Opcode | R | 8 | Opcode\n | 13 | 3 |  | 16\n9 | Opcode | Source | Destination | Memory Address\n | 4 | 6 | 6 | 16\n10 | Opcode | R | Source | Memory Address\n | 7 | 3 | 6 | 16\n11 | Opcode | FP | Source | Memory Address\n | 8 | 2 | 6 | 16\n12 | Opcode | Destination | Memory Address\n | 10 | 6 | 16\n13 | Opcode | Source | Destination | Memory Address 1 | Memory Address 2\n | 4 | 6 | 6 | 16 | 16\n\n\nNumbers below fields indicate bit length.\n\n\nSource and destination each contain a 3-bit addressing mode field and a 3-bit register number.\n\n\nFP indicates one of four floating-point registers.\n\n\nR indicates one of the general-purpose registers.\n\n\nCC is the condition code field.\n\n\n**Figure 13.7**\n   Instruction Formats for the PDP-11\n\n\nThe result is a highly variable instruction format. An instruction consists of a 1- or 2-byte opcode followed by from zero to six operand specifiers, depending on the opcode. The minimal instruction length is 1 byte, and instructions up to 37 bytes can be constructed. Figure 13.8 gives a few examples.\n\n\nThe VAX instruction begins with a 1-byte opcode. This suffices to handle most VAX instructions. However, as there are over 300 different instructions, 8 bits are not enough. The hexadecimal codes FD and FF indicate an extended opcode, with the actual opcode being specified in the second byte.\n\n\nThe remainder of the instruction consists of up to six operand specifiers. An operand specifier is, at minimum, a 1-byte format in which the leftmost 4 bits are the address mode specifier. The only exception to this rule is the literal mode,\n\n\n\nHexadecimal Format | Explanation | Assembler Notation and Description\n\\xrightarrow{\\text{8 bits}}\n      \n\n\n\n\n         0\n        \n\n         5 | 0 | 5 | Opcode for RSB | RSB\n      \n      Return from subroutine\n0 | 5\nD\n        \n\n         4\n        \n\n\n\n         5\n        \n\n         9 | D | 4 | 5 | 9 | Opcode for CLRL\n      \n      Register R9 | CLRL R9\n      \n      Clear register R9\nD | 4\n5 | 9\nB\n        \n\n         0\n        \n\n\n\n         C\n        \n\n         4\n        \n\n\n\n         6\n        \n\n         4\n        \n\n\n\n         0\n        \n\n         1\n        \n\n\n\n         A\n        \n\n         B\n        \n\n\n\n         1\n        \n\n         9 | B | 0 | C | 4 | 6 | 4 | 0 | 1 | A | B | 1 | 9 | Opcode for MOVW\n      \n      Word displacement mode,\n      \n      Register R4\n      \n      356 in hexadecimal\n      \n      Byte displacement mode,\n      \n      Register R11\n      \n      25 in hexadecimal | MOVW 356(R4), 25(R11)\n      \n      Move a word from address\n      \n      that is 356 plus contents\n      \n      of R4 to address that is\n      \n      25 plus contents of R11\nB | 0\nC | 4\n6 | 4\n0 | 1\nA | B\n1 | 9\nC\n        \n\n         1\n        \n\n\n\n         0\n        \n\n         5\n        \n\n\n\n         5\n        \n\n         0\n        \n\n\n\n         4\n        \n\n         2\n        \n\n\n\n         D\n        \n\n         F | C | 1 | 0 | 5 | 5 | 0 | 4 | 2 | D | F |  | Opcode for ADDL3\n      \n      Short literal 5\n      \n      Register mode R0\n      \n      Index prefix R2\n      \n      Indirect word relative\n      \n      (displacement from PC)\n      \n      Amount of displacement from\n      \n      PC relative to location A | ADDL3 #5, R0, @A[R2]\n      \n      Add 5 to a 32-bit integer in\n      \n      R0 and store the result in\n      \n      location whose address is\n      \n      sum of A and 4 times the\n      \n      contents of R2\nC | 1\n0 | 5\n5 | 0\n4 | 2\nD | F\n\n\n**Figure 13.8**\n   Example of VAX Instructions\n\n\nwhich is signaled by the pattern 00 in the leftmost 2 bits, leaving space for a 6-bit literal. Because of this exception, a total of 12 different addressing modes can be specified.\n\n\nAn operand specifier often consists of just one byte, with the rightmost 4 bits specifying one of 16 general-purpose registers. The length of the operand specifier can be extended in one of two ways. First, a constant value of one or more bytes may immediately follow the first byte of the operand specifier. An example of this is the displacement mode, in which an 8-, 16-, or 32-bit displacement is used. Second, an index mode of addressing may be used. In this case, the first byte of the operand specifier consists of the 4-bit addressing mode code of 0100 and a 4-bit index register identifier. The remainder of the operand specifier consists of the base address specifier, which may itself be one or more bytes in length.\n\n\nThe reader may be wondering, as the author did, what kind of instruction requires six operands. Surprisingly, the VAX has a number of such instructions. Consider\n\n\nADDP6 OP1, OP2, OP3, OP4, OP5, OP6\n\n\nThis instruction adds two packed decimal numbers. OP1 and OP2 specify the length and starting address of one decimal string; OP3 and OP4 specify a second string. These two strings are added and the result is stored in the decimal string whose length and starting location are specified by OP5 and OP6.\n\n\nThe VAX instruction set provides for a wide variety of operations and addressing modes. This gives a programmer, such as a compiler writer, a very powerful and flexible tool for developing programs. In theory, this should lead to efficient machine-language compilations of high-level language programs and, in general, to effective and efficient use of processor resources. The penalty to be paid for these benefits is the increased complexity of the processor compared with a processor with a simpler instruction set and format.\n\n\nWe return to these matters in Chapter 15, where we examine the case for very simple instruction sets."
        },
        {
          "name": "x86 and ARM Instruction Formats",
          "content": "**x86 Instruction Formats**\n\n\nThe x86 is equipped with a variety of instruction formats. Of the elements described in this subsection, only the opcode field is always present. Figure 13.9 illustrates the general instruction format. Instructions are made up of from zero to four optional instruction prefixes, a 1- or 2-byte opcode, an optional address specifier (which consists of the ModR/M byte and the Scale Index Base byte) an optional displacement, and an optional immediate field.\n\n\n\n\n![Diagram of the x86 Instruction Format showing fields and their sizes.](images/image_0226.jpeg)\n\n\nThe diagram illustrates the x86 instruction format, showing the following fields and their sizes:\n\n\n  * **Instruction prefix:**\n     0 or 1 byte (top row, leftmost field).\n  * **Segment override:**\n     0 or 1 byte (top row, second field).\n  * **Operand size override:**\n     0 or 1 byte (top row, third field).\n  * **Address size override:**\n     0 or 1 byte (top row, rightmost field).\n  * **Instruction prefixes:**\n     0, 1, 2, 3, or 4 bytes (bottom row, leftmost field).\n  * **Opcode:**\n     1, 2, or 3 bytes (bottom row, second field).\n  * **ModR/M:**\n     0 or 1 byte (bottom row, third field).\n  * **SIB:**\n     0 or 1 byte (bottom row, fourth field).\n  * **Displacement:**\n     0, 1, 2, or 4 bytes (bottom row, fifth field).\n  * **Immediate:**\n     0, 1, 2, or 4 bytes (bottom row, rightmost field).\n\n\nDashed lines indicate the breakdown of the ModR/M and SIB bytes into their constituent fields:\n\n\n  * **ModR/M breakdown (7 bits):**\n     Mod (7), Reg/Opcode (6), R/M (5).\n  * **SIB breakdown (7 bits):**\n     Scale (7), Index (6), Base (5).\n\n\nDiagram of the x86 Instruction Format showing fields and their sizes.\n\n\n**Figure 13.9**\n   x86 Instruction Format\n\n\nLet us first consider the prefix bytes:\n\n\n  * ■\n    **Instruction prefixes:**\n    The instruction prefix, if present, consists of the LOCK prefix or one of the repeat prefixes. The LOCK prefix is used to ensure exclusive use of shared memory in multiprocessor environments. The repeat prefixes specify repeated operation of a string, which enables the x86 to process strings much faster than with a regular software loop. There are five different repeat prefixes: REP, REPE, REPZ, REPNE, and REPNZ. When the absolute REP prefix is present, the operation specified in the instruction is executed repeatedly on successive elements of the string; the number of repetitions is specified in register CX. The conditional REP prefix causes the instruction to repeat until the count in CX goes to zero or until the condition is met.\n  * ■\n    **Segment override:**\n    Explicitly specifies which segment register an instruction should use, overriding the default segment-register selection generated by the x86 for that instruction.\n  * ■\n    **Operand size:**\n    An instruction has a default operand size of 16 or 32 bits, and the operand prefix switches between 32-bit and 16-bit operands.\n  * ■\n    **Address size:**\n    The processor can address memory using either 16- or 32-bit addresses. The address size determines the displacement size in instructions and the size of address offsets generated during effective address calculation. One of these sizes is designated as default, and the address size prefix switches between 32-bit and 16-bit address generation.\n\n\nThe instruction itself includes the following fields:\n\n\n  * ■\n    **Opcode:**\n    The opcode field is 1, 2, or 3 bytes in length. The opcode may also include bits that specify if data is byte- or full-size (16 or 32 bits depending on context), direction of data operation (to or from memory), and whether an immediate data field must be sign extended.\n  * ■\n    **ModR/M:**\n    This byte, and the next, provide addressing information. The ModR/M byte specifies whether an operand is in a register or in memory; if it is in memory, then fields within the byte specify the addressing mode to be used. The ModR/M byte consists of three fields: The Mod field (2 bits) combines with the R/M field to form 32 possible values: 8 registers and 24 indexing modes; the Reg/Opcode field (3 bits) specifies either a register number or three more bits of opcode information; the R/M field (3 bits) can specify a register as the location of an operand, or it can form part of the addressing-mode encoding in combination with the Mod field.\n  * ■\n    **SIB:**\n    Certain encoding of the ModR/M byte specifies the inclusion of the SIB byte to specify fully the addressing mode. The SIB byte consists of three fields: The Scale field (2 bits) specifies the scale factor for scaled indexing; the Index field (3 bits) specifies the index register; the Base field (3 bits) specifies the base register.\n  * ■\n    **Displacement:**\n    When the addressing-mode specifier indicates that a displacement is used, an 8-, 16-, or 32-bit signed integer displacement field is added.\n  * ■\n    **Immediate:**\n    Provides the value of an 8-, 16-, or 32-bit operand.\n\n\nSeveral comparisons may be useful here. In the x86 format, the addressing mode is provided as part of the opcode sequence rather than with each operand.\n\n\nBecause only one operand can have address-mode information, only one memory operand can be referenced in an instruction. In contrast, the VAX carries the address-mode information with each operand, allowing memory-to-memory operations. The x86 instructions are therefore more compact. However, if a memory-to-memory operation is required, the VAX can accomplish this in a single instruction.\n\n\nThe x86 format allows the use of not only 1-byte, but also 2-byte and 4-byte offsets for indexing. Although the use of the larger index offsets results in longer instructions, this feature provides needed flexibility. For example, it is useful in addressing large arrays or large stack frames. In contrast, the IBM S/370 instruction format allows offsets no greater than 4 Kbytes (12 bits of offset information), and the offset must be positive. When a location is not in reach of this offset, the compiler must generate extra code to generate the needed address. This problem is especially apparent in dealing with stack frames that have local variables occupying in excess of 4 Kbytes. As [DEWA90] puts it, “generating code for the 370 is so painful as a result of that restriction that there have even been compilers for the 370 that simply chose to limit the size of the stack frame to 4 Kbytes.”\n\n\nAs can be seen, the encoding of the x86 instruction set is very complex. This has to do partly with the need to be backward compatible with the 8086 machine and partly with a desire on the part of the designers to provide every possible assistance to the compiler writer in producing efficient code. It is a matter of some debate whether an instruction set as complex as this is preferable to the opposite extreme of the RISC instruction sets.\n\n\n\n\n**ARM Instruction Formats**\n\n\nAll instructions in the ARM architecture are 32 bits long and follow a regular format (Figure 13.10). The first four bits of an instruction are the condition code. As discussed in Chapter 12, virtually all ARM instructions can be conditionally executed. The next three bits specify the general type of instruction. For most instructions other than branch instructions, the next five bits constitute an opcode and/or modifier bits for the operation. The remaining 20 bits are for operand addressing. The regular structure of the instruction formats eases the job of the instruction decode units.\n\n\n**IMMEDIATE CONSTANTS**\n   To achieve a greater range of immediate values, the data processing immediate format specifies both an immediate value and a rotate value. The 8-bit immediate value is expanded to 32 bits and then rotated right by a number of bits equal to twice the 4-bit rotate value. Several examples are shown in Figure 13.11.\n\n\n**THUMB INSTRUCTION SET**\n   The Thumb instruction set is a re-encoded subset of the ARM instruction set. Thumb is designed to increase the performance of ARM implementations that use a 16-bit or narrower memory data bus and to allow better code density than provided by the ARM instruction for both 16-bit and 32-bit processors. The Thumb instruction set was created by analyzing the 32-bit ARM instruction set and deriving the best fit 16-bit instruction set, thus reducing code size. The savings is achieved in the following way:\n\n\n  * 1. Thumb instructions are unconditional, so the condition code field is not used. Also, all Thumb arithmetic and logic instructions update the condition flags, so that the update-flag bit is not needed. Savings: 5 bits.\n\n\n\n | 31 | 30 | 29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nData processing immediate shift | cond | 0 | 0 | 0 | 0 | opcode | S | Rn | Rd | shift amount | shift | 0 | Rm\nData processing register shift | cond | 0 | 0 | 0 | 0 | opcode | S | Rn | Rd | Rs | 0 | shift | 1 | Rm\nData processing immediate | cond | 0 | 0 | 0 | 1 | opcode | S | Rn | Rd | rotate | immediate\nLoad/store immediate offset | cond | 0 | 1 | 0 | P | U | B | W | L | Rn | Rd | immediate\nLoad/store register offset | cond | 0 | 1 | 1 | P | U | B | W | L | Rn | Rd | shift amount | shift | 0 | Rm\nLoad/store multiple | cond | 1 | 0 | 0 | P | U | S | W | L | Rn | register list\nBranch/branch with link | cond | 1 | 0 | 1 | L | 24-bit offset\n\n\nS = For data processing instructions, signifies that the instruction updates the condition codes\n\n\nB = Distinguishes between an unsigned byte (B=1) and a word (B=0) access\n\n\nS = For load/store multiple instructions, signifies whether instruction execution is restricted to supervisor mode\n\n\nL = For load/store instructions, distinguishes between a Load (L=1) and a Store (L=0)\n\n\nP, U, W = bits that distinguish among different types of addressing mode\n\n\nL = For branch instructions, determines whether a return address is stored in the link register\n\n\n**Figure 13.10**\n   ARM Instruction Formats\n\n\n  * 2. Thumb has only a subset of the operations in the full instruction set and uses only a 2-bit opcode field, plus a 3-bit type field. Savings: 2 bits.\n  * 3. The remaining savings of 9 bits comes from reductions in the operand specifications. For example, Thumb instructions reference only registers r0 through r7, so only 3 bits are required for register references, rather than 4 bits. Immediate values do not include a 4-bit rotate field.\n\n\n\n31 | 30 | 29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\n0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0\n\n\nror #0—range 0 through 0x000000FF—step 0x00000001\n\n\n\n31 | 30 | 29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\n0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0\n\n\nror #8—range 0 through 0xFF000000—step 0x01000000\n\n\n\n31 | 30 | 29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\n0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0\n\n\nror #30—range 0 through 0x000003FC—step 0x00000004\n\n\n**Figure 13.11**\n   Examples of Use of ARM Immediate Constants\n\n\nThe ARM processor can execute a program consisting of a mixture of Thumb instructions and 32-bit ARM instructions. A bit in the processor control register determines which type of instruction is currently being executed. Figure 13.12 shows an example. The figure shows both the general format and a specific instance of an instruction in both 16-bit and 32-bit formats.\n\n\n**THUMB-2 INSTRUCTION SET**\n   With the introduction of the Thumb instruction set, the user was required to blend instruction sets by compiling performance critical code to ARM and the rest to Thumb. This manual code blending requires additional effort and it is difficult to achieve optimal results. To overcome these problems, ARM developed the Thumb-2 instruction set, which is the only instruction set available on the Cortex-M microcontroller products.\n\n\nThumb-2 is a major enhancement to the Thumb instruction set architecture (ISA). It introduces 32-bit instructions that can be intermixed freely with the older 16-bit Thumb instructions. These new 32-bit instructions cover almost all the functionality of the ARM instruction set. The most important difference between the Thumb ISA and the ARM ISA is that most 32-bit Thumb instructions are unconditional, whereas almost all ARM instructions can be conditional. However, Thumb-2 introduces a new If-Then (IT) instruction that delivers much of the functionality of the condition field in ARM instructions. Thumb-2 delivers overall code density comparable with Thumb, together with the performance levels associated with the ARM ISA. Before Thumb-2, developers had to choose between Thumb for size and ARM for performance.\n\n\n[ROBI07] reports on an analysis of the Thumb-2 instruction set compared with the ARM and original Thumb instruction sets. The analysis involved compiling and executing the Embedded Microprocessor Benchmark Consortium (EEMBC) benchmark suite using the three instruction sets, with the following results:\n\n\n  * ■ With compilers optimized for performance, Thumb-2 size was 26% smaller than ARM, and slightly larger than original Thumb.\n  * ■ With compilers optimized for space, Thumb-2 size was 32% smaller than ARM, and slightly smaller than original Thumb.\n\n\n\n\n![](images/image_0227.jpeg)\n\n\nFigure 13.12 illustrates the expansion of a Thumb ADD instruction into its ARM equivalent. The diagram shows the bit-level breakdown of the Thumb instruction and its corresponding ARM instruction.\n\n\n**Thumb Instruction (16-bit):**\n\n\n**Add/subtract/compare/move immediate format**\n\n\n\n15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\n0 | 0 | 1 | OP code | Rd/Rn | immediate\n\n\nADD r3, #19: 001 10 011 000010011\n\n\n**ARM Instruction (32-bit):**\n\n\n**Data processing immediate format**\n\n\n\n31 | 30 | 29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nAlways condition code | cond | 0 | 0 | 1 | opcode | S | Rn | Rd | rotate | immediate\n\n\nADDS r3, r3, #19: 1110 001 00101 00011 00001 00000 000010011\n\n\n**Decomposition of Thumb ADD r3, #19:**\n\n\n  * Major opcode denoting format 3 move/compare/add/sub with immediate value\n  * Minor opcode denoting ADD instruction\n  * Destination and source register\n  * Immediate value\n  * Zero rotation\n  * Update condition flags\n\n\n**Figure 13.12**\n   Expanding a Thumb ADD Instruction into its ARM Equivalent\n\n\n\n\n![Diagram illustrating Thumb-2 Encoding. It shows a sequence of memory locations: i (thm), i+2 (hw1), i+4 (hw2), i+6 (thm), i+8 (hw1), i+10 (hw2), and i+12 (thm). A large arrow labeled 'Instruction flow' points from the first 'thm' box to the last 'thm' box. Below this is a table defining the encoding rules.](images/image_0228.jpeg)\n\n\nHalfword1 [15:13] | Halfword1 [12:11] | Length | Functionality\nNot 111 | xx | 16 bits (1 halfword) | 16-bit Thumb instruction\n111 | 00 | 16 bits (1 halfword) | 16-bit Thumb unconditional branch instruction\n111 | Not 00 | 32 bits (2 halfwords) | 32-bit Thumb-2 instruction\n\n\nDiagram illustrating Thumb-2 Encoding. It shows a sequence of memory locations: i (thm), i+2 (hw1), i+4 (hw2), i+6 (thm), i+8 (hw1), i+10 (hw2), and i+12 (thm). A large arrow labeled 'Instruction flow' points from the first 'thm' box to the last 'thm' box. Below this is a table defining the encoding rules.\n\n\n**Figure 13.13**\n   Thumb-2 Encoding\n\n\n  * ■ With compilers optimized for performance, Thumb-2 performance on the benchmark suite was 98% of ARM performance and 125% of original Thumb performance.\n\n\nThese results confirm that Thumb-2 meets its design objectives.\n\n\nFigure 13.13 shows how the new 32-bit Thumb instructions are encoded. The encoding is compatible with the existing Thumb unconditional branch instructions, which has the bit pattern 11100 in the five leftmost bits of the instruction. No other 16-bit instruction begins with the pattern 111 in the three leftmost bits, so the bit patterns 11101, 11110, and 11111 indicate that this is a 32-bit Thumb instruction."
        },
        {
          "name": "Assembly Language",
          "content": "A processor can understand and execute machine instructions. Such instructions are simply binary numbers stored in the computer. If a programmer wished to program directly in machine language, then it would be necessary to enter the program as binary data.\n\n\nConsider the simple BASIC statement\n\n\nN = I + J + K\n\n\nSuppose we wished to program this statement in machine language and to initialize I, J, and K to 2, 3, and 4, respectively. This is shown in Figure 13.14a. The program starts in location 101 (hexadecimal). Memory is reserved for the four variables starting at location 201. The program consists of four instructions:\n\n\n  * 1. Load the contents of location 201 into the AC.\n  * 2. Add the contents of location 202 to the AC.\n  * 3. Add the contents of location 203 to the AC.\n  * 4. Store the contents of the AC in location 204.\n\n\nThis is clearly a tedious and very error-prone process.\n\n\nA slight improvement is to write the program in hexadecimal rather than binary notation (Figure 13.14b). We could write the program as a series of lines. Each\n\n\n\nAddress | Contents\n101 | 0010 | 0010 | 101 | 2201\n102 | 0001 | 0010 | 102 | 1202\n103 | 0001 | 0010 | 103 | 1203\n104 | 0011 | 0010 | 104 | 3204\n201 | 0000 | 0000 | 201 | 0002\n202 | 0000 | 0000 | 202 | 0003\n203 | 0000 | 0000 | 203 | 0004\n204 | 0000 | 0000 | 204 | 0000\n\n\n(a) Binary program\n\n\n\nAddress | Contents\n101 | 2201\n102 | 1202\n103 | 1203\n104 | 3204\n201 | 0002\n202 | 0003\n203 | 0004\n204 | 0000\n\n\n(b) Hexadecimal program\n\n\n\nAddress | Instruction\n101 | LDA 201\n102 | ADD 202\n103 | ADD 203\n104 | STA 204\n201 | DAT 2\n202 | DAT 3\n203 | DAT 4\n204 | DAT 0\n\n\n(c) Symbolic program\n\n\n\nLabel | Operation | Operand\nFORMUL | LDA | I\n | ADD | J\n | ADD | K\n | STA | N\nI | DATA | 2\nJ | DATA | 3\nK | DATA | 4\nN | DATA | 0\n\n\n(d) Assembly program\n\n\n**Figure 13.14**\n\n   N = I + J + K\n  \nline contains the address of a memory location and the hexadecimal code of the binary value to be stored in that location. Then we need a program that will accept this input, translate each line into a binary number, and store it in the specified location.\n\n\nFor more improvement, we can make use of the symbolic name or mnemonic of each instruction. This results in the\n   *symbolic program*\n   shown in Figure 13.14c. Each line of input still represents one memory location. Each line consists of three fields, separated by spaces. The first field contains the address of a location. For an instruction, the second field contains the three-letter symbol for the opcode. If it is a memory-referencing instruction, then a third field contains the address. To store arbitrary data in a location, we invent a\n   *pseudoinstruction*\n   with the symbol DAT. This is merely an indication that the third field on the line contains a hexadecimal number to be stored in the location specified in the first field.\n\n\nFor this type of input we need a slightly more complex program. The program accepts each line of input, generates a binary number based on the second and third (if present) fields, and stores it in the location specified by the first field.\n\n\nThe use of a symbolic program makes life much easier but is still awkward. In particular, we must give an absolute address for each word. This means that the program and data can be loaded into only one place in memory, and we must know that place ahead of time. Worse, suppose we wish to change the program some day by adding or deleting a line. This will change the addresses of all subsequent words.\n\n\nA much better system, and one commonly used, is to use symbolic addresses. This is illustrated in Figure 13.14d. Each line still consists of three fields. The first field is still for the address, but a symbol is used instead of an absolute numerical address. Some lines have no address, implying that the address of that line is one\n\n\nmore than the address of the previous line. For memory-reference instructions, the third field also contains a symbolic address.\n\n\nWith this last refinement, we have an\n   *assembly language*\n   . Programs written in assembly language (assembly programs) are translated into machine language by an\n   *assembler*\n   . This program must not only do the symbolic translation discussed earlier but also assign some form of memory addresses to symbolic addresses.\n\n\nThe development of assembly language was a major milestone in the evolution of computer technology. It was the first step to the high-level languages in use today. Although few programmers use assembly language, virtually all machines provide one. They are used, if at all, for systems programs such as compilers and I/O routines.\n\n\nAppendix B provides a more detailed examination of assembly language."
        }
      ]
    },
    {
      "name": "Processor Structure and Function",
      "sections": [
        {
          "name": "Processor Organization",
          "content": "To understand the organization of the processor, let us consider the requirements placed on the processor, the things that it must do:\n\n\n  * ■\n    **Fetch instruction:**\n    The processor reads an instruction from memory (register, cache, main memory).\n  * ■\n    **Interpret instruction:**\n    The instruction is decoded to determine what action is required.\n  * ■\n    **Fetch data:**\n    The execution of an instruction may require reading data from memory or an I/O module.\n  * ■\n    **Process data:**\n    The execution of an instruction may require performing some arithmetic or logical operation on data.\n  * ■\n    **Write data:**\n    The results of an execution may require writing data to memory or an I/O module.\n\n\nTo do these things, it should be clear that the processor needs to store some data temporarily. It must remember the location of the last instruction so that it can know where to get the next instruction. It needs to store instructions and data temporarily while an instruction is being executed. In other words, the processor needs a small internal memory.\n\n\nFigure 14.1 is a simplified view of a processor, indicating its connection to the rest of the system via the system bus. A similar interface would be needed for any\n\n\n\n\n![Figure 14.1: The CPU with the System Bus. The diagram shows a CPU block containing an ALU, Registers, and a Control unit. The CPU is connected to a System bus consisting of three parallel lines labeled Control bus, Data bus, and Address bus.](images/image_0229.jpeg)\n\n\nThe diagram illustrates the CPU's connection to the System Bus. The CPU is represented as a large rectangular block containing three sub-blocks: ALU, Registers, and Control unit. To the right of the CPU, three vertical lines represent the System Bus, labeled Control bus, Data bus, and Address bus. A bracket below these lines is labeled System bus.\n\n\nFigure 14.1: The CPU with the System Bus. The diagram shows a CPU block containing an ALU, Registers, and a Control unit. The CPU is connected to a System bus consisting of three parallel lines labeled Control bus, Data bus, and Address bus.\n\n\n**Figure 14.1**\n   The CPU with the System Bus\n\n\nof the interconnection structures described in Chapter 3. The reader will recall that the major components of the processor are an\n   *arithmetic and logic unit*\n   (ALU) and a\n   *control unit*\n   (CU). The ALU does the actual computation or processing of data. The control unit controls the movement of data and instructions into and out of the processor and controls the operation of the ALU. In addition, the figure shows a minimal internal memory, consisting of a set of storage locations, called\n   *registers*\n   .\n\n\nFigure 14.2 is a slightly more detailed view of the processor. The data transfer and logic control paths are indicated, including an element labeled\n   *internal*\n\n\n\n\n![Figure 14.2: Internal Structure of the CPU. The diagram shows the internal components of the CPU, including the Arithmetic and logic unit (containing Status flags, Shifter, Complementer, and Arithmetic and Boolean logic), the Internal CPU bus, Registers, and the Control unit. Data transfer paths are shown with double-headed arrows, and control paths are shown with thick arrows.](images/image_0230.jpeg)\n\n\nThe diagram provides a detailed view of the CPU's internal structure. On the left, a large block labeled 'Arithmetic and logic unit' contains four sub-blocks: 'Status flags', 'Shifter', 'Complementer', and 'Arithmetic and Boolean logic'. In the center is a vertical bar labeled 'Internal CPU bus'. To the right of the bus is a tall vertical block labeled 'Registers' with an ellipsis indicating multiple registers. Below the registers is a block labeled 'Control unit'. Double-headed arrows indicate data transfer paths between the ALU components, the Internal CPU bus, and the Registers. Thick arrows indicate control paths: a thick upward arrow from the Control unit to the Internal CPU bus, a thick downward arrow from the Control unit to the Registers, and a thick horizontal arrow from the Control unit to the ALU components.\n\n\nFigure 14.2: Internal Structure of the CPU. The diagram shows the internal components of the CPU, including the Arithmetic and logic unit (containing Status flags, Shifter, Complementer, and Arithmetic and Boolean logic), the Internal CPU bus, Registers, and the Control unit. Data transfer paths are shown with double-headed arrows, and control paths are shown with thick arrows.\n\n\n**Figure 14.2**\n   Internal Structure of the CPU\n\n\n*processor bus*\n   . This element is needed to transfer data between the various registers and the ALU because the ALU in fact operates only on data in the internal processor memory. The figure also shows typical basic elements of the ALU. Note the similarity between the internal structure of the computer as a whole and the internal structure of the processor. In both cases, there is a small collection of major elements (computer: processor, I/O, memory; processor: control unit, ALU, registers) connected by data paths."
        },
        {
          "name": "Register Organization",
          "content": "As we discussed in Chapter 4, a computer system employs a memory hierarchy. At higher levels of the hierarchy, memory is faster, smaller, and more expensive (per bit). Within the processor, there is a set of registers that function as a level of memory above main memory and cache in the hierarchy. The registers in the processor perform two roles:\n\n\n  * ■\n    **User-visible registers:**\n    Enable the machine- or assembly language programmer to minimize main memory references by optimizing use of registers.\n  * ■\n    **Control and status registers:**\n    Used by the control unit to control the operation of the processor and by privileged, operating system programs to control the execution of programs.\n\n\nThere is not a clean separation of registers into these two categories. For example, on some machines the program counter is user visible (e.g., x86), but on many it is not. For purposes of the following discussion, however, we will use these categories.\n\n\n\n\n**User-Visible Registers**\n\n\nA user-visible register is one that may be referenced by means of the machine language that the processor executes. We can characterize these in the following categories:\n\n\n  * ■ General purpose\n  * ■ Data\n  * ■ Address\n  * ■ Condition codes\n\n\n**General-purpose registers**\n   can be assigned to a variety of functions by the programmer. Sometimes their use within the instruction set is orthogonal to the operation. That is, any general-purpose register can contain the operand for any opcode. This provides true general-purpose register use. Often, however, there are restrictions. For example, there may be dedicated registers for floating-point and stack operations.\n\n\nIn some cases, general-purpose registers can be used for addressing functions (e.g., register indirect, displacement). In other cases, there is a partial or clean separation between data registers and address registers.\n   **Data registers**\n   may be used only to hold data and cannot be employed in the calculation of an operand address.\n\n\n**Address registers**\n   may themselves be somewhat general purpose, or they may be devoted to a particular addressing mode. Examples include the following:\n\n\n  * ■\n    **Segment pointers:**\n    In a machine with segmented addressing (see Section 8.3), a segment register holds the address of the base of the segment. There may be multiple registers: for example, one for the operating system and one for the current process.\n  * ■\n    **Index registers:**\n    These are used for indexed addressing and may be autoindexed.\n  * ■\n    **Stack pointer:**\n    If there is user-visible stack addressing, then typically there is a dedicated register that points to the top of the stack. This allows implicit addressing; that is, push, pop, and other stack instructions need not contain an explicit stack operand.\n\n\nThere are several design issues to be addressed here. An important issue is whether to use completely general-purpose registers or to specialize their use. We have already touched on this issue in the preceding chapter because it affects instruction set design. With the use of specialized registers, it can generally be implicit in the opcode which type of register a certain operand specifier refers to. The operand specifier must only identify one of a set of specialized registers rather than one out of all the registers, thus saving bits. On the other hand, this specialization limits the programmer's flexibility.\n\n\nAnother design issue is the number of registers, either general purpose or data plus address, to be provided. Again, this affects instruction set design because more registers require more operand specifier bits. As we previously discussed, somewhere between 8 and 32 registers appears optimum [LUND77]. Fewer registers result in more memory references; more registers do not noticeably reduce memory references (e.g., see [WILL90]). However, a new approach, which finds advantage in the use of hundreds of registers, is exhibited in some RISC systems and is discussed in Chapter 15.\n\n\nFinally, there is the issue of register length. Registers that must hold addresses obviously must be at least long enough to hold the largest address. Data registers should be able to hold values of most data types. Some machines allow two contiguous registers to be used as one for holding double-length values.\n\n\nA final category of registers, which is at least partially visible to the user, holds\n   **condition codes**\n   (also referred to as\n   *flags*\n   ). Condition codes are bits set by the processor hardware as the result of operations. For example, an arithmetic operation may produce a positive, negative, zero, or overflow result. In addition to the result itself being stored in a register or memory, a condition code is also set. The code may subsequently be tested as part of a conditional branch operation.\n\n\nCondition code bits are collected into one or more registers. Usually, they form part of a control register. Generally, machine instructions allow these bits to be read by implicit reference, but the programmer cannot alter them.\n\n\nMany processors, including those based on the IA-64 architecture and the MIPS processors, do not use condition codes at all. Rather, conditional branch instructions specify a comparison to be made and act on the result of the comparison, without storing a condition code. Table 14.1, based on [DERO87], lists key advantages and disadvantages of condition codes.\n\n\n**Table 14.1**\n\nAdvantages | Disadvantages\n1. Because condition codes are set by normal arithmetic and data movement instructions, they should reduce the number of COMPARE and TEST instructions needed.\n       \n\n        2. Conditional instructions, such as BRANCH are simplified relative to composite instructions, such as TEST and BRANCH.\n       \n\n        3. Condition codes facilitate multiway branches. For example, a TEST instruction can be followed by two branches, one on less than or equal to zero and one on greater than zero.\n       \n\n        4. Condition codes can be saved on the stack during subroutine calls along with other register information. | 1. Condition codes add complexity, both to the hardware and software. Condition code bits are often modified in different ways by different instructions, making life more difficult for both the microprogrammer and compiler writer.\n       \n\n        2. Condition codes are irregular; they are typically not part of the main data path, so they require extra hardware connections.\n       \n\n        3. Often condition code machines must add special non-condition-code instructions for special situations anyway, such as bit checking, loop control, and atomic semaphore operations.\n       \n\n        4. In a pipelined implementation, condition codes require special synchronization to avoid conflicts.\n\n\nIn some machines, a subroutine call will result in the automatic saving of all user-visible registers, to be restored on return. The processor performs the saving and restoring as part of the execution of call and return instructions. This allows each subroutine to use the user-visible registers independently. On other machines, it is the responsibility of the programmer to save the contents of the relevant user-visible registers prior to a subroutine call, by including instructions for this purpose in the program.\n\n\n\n\n**Control and Status Registers**\n\n\nThere are a variety of processor registers that are employed to control the operation of the processor. Most of these, on most machines, are not visible to the user. Some of them may be visible to machine instructions executed in a control or operating system mode.\n\n\nOf course, different machines will have different register organizations and use different terminology. We list here a reasonably complete list of register types, with a brief description.\n\n\nFour registers are essential to instruction execution:\n\n\n  * ■\n    **Program counter (PC):**\n    Contains the address of an instruction to be fetched.\n  * ■\n    **Instruction register (IR):**\n    Contains the instruction most recently fetched.\n  * ■\n    **Memory address register (MAR):**\n    Contains the address of a location in memory.\n  * ■\n    **Memory buffer register (MBR):**\n    Contains a word of data to be written to memory or the word most recently read.\n\n\nNot all processors have internal registers designated as MAR and MBR, but some equivalent buffering mechanism is needed whereby the bits to be transferred\n\n\nto the system bus are staged and the bits to be read from the data bus are temporarily stored.\n\n\nTypically, the processor updates the PC after each instruction fetch so that the PC always points to the next instruction to be executed. A branch or skip instruction will also modify the contents of the PC. The fetched instruction is loaded into an IR, where the opcode and operand specifiers are analyzed. Data are exchanged with memory using the MAR and MBR. In a bus-organized system, the MAR connects directly to the address bus, and the MBR connects directly to the data bus. User-visible registers, in turn, exchange data with the MBR.\n\n\nThe four registers just mentioned are used for the movement of data between the processor and memory. Within the processor, data must be presented to the ALU for processing. The ALU may have direct access to the MBR and user-visible registers. Alternatively, there may be additional buffering registers at the boundary to the ALU; these registers serve as input and output registers for the ALU and exchange data with the MBR and user-visible registers.\n\n\nMany processor designs include a register or set of registers, often known as the\n   *program status word*\n   (PSW), that contain status information. The PSW typically contains condition codes plus other status information. Common fields or flags include the following:\n\n\n  * ■\n    **Sign:**\n    Contains the sign bit of the result of the last arithmetic operation.\n  * ■\n    **Zero:**\n    Set when the result is 0.\n  * ■\n    **Carry:**\n    Set if an operation resulted in a carry (addition) into or borrow (subtraction) out of a high-order bit. Used for multiword arithmetic operations.\n  * ■\n    **Equal:**\n    Set if a logical compare result is equality.\n  * ■\n    **Overflow:**\n    Used to indicate arithmetic overflow.\n  * ■\n    **Interrupt Enable/Disable:**\n    Used to enable or disable interrupts.\n  * ■\n    **Supervisor:**\n    Indicates whether the processor is executing in supervisor or user mode. Certain privileged instructions can be executed only in supervisor mode, and certain areas of memory can be accessed only in supervisor mode.\n\n\nA number of other registers related to status and control might be found in a particular processor design. There may be a pointer to a block of memory containing additional status information (e.g., process control blocks). In machines using vectored interrupts, an interrupt vector register may be provided. If a stack is used to implement certain functions (e.g., subroutine call), then a system stack pointer is needed. A page table pointer is used with a virtual memory system. Finally, registers may be used in the control of I/O operations.\n\n\nA number of factors go into the design of the control and status register organization. One key issue is operating system support. Certain types of control information are of specific utility to the operating system. If the processor designer has a functional understanding of the operating system to be used, then the register organization can to some extent be tailored to the operating system.\n\n\nAnother key design decision is the allocation of control information between registers and memory. It is common to dedicate the first (lowest) few hundred or\n\n\n\n\n![Figure 14.3: Example Microprocessor Register Organizations. (a) MC68000: Data registers (D0-D7), Address registers (A0-A7), Program status (Program counter, Status register). (b) 8086: General registers (AX, BX, CX, DX), Pointers and index (SP, BP, SI, DI), Segment (CS, DS, SS, ES), Program status (Flags, Instr ptr). (c) 80386—Pentium 4: General registers (EAX, EBX, ECX, EDX), Pointers and index (ESP, EBP, ESI, EDI), Program status (FLAGS register, Instruction pointer).](images/image_0231.jpeg)\n\n\n**(a) MC68000**\n\n\n\nData registers\nD0 | \nD1 | \nD2 | \nD3 | \nD4 | \nD5 | \nD6 | \nD7 | \n\n\n\n\n\nAddress registers\nA0 | \nA1 | \nA2 | \nA3 | \nA4 | \nA5 | \nA6 | \nA7 | \n\n\n\n\n\nProgram status\nProgram counter | \nStatus register | \n\n\n\n\n**(b) 8086**\n\n\n\nGeneral registers\nAX | Accumulator\nBX | Base\nCX | Count\nDX | Data\n\n\n\n\n\nPointers and index\nSP | Stack ptr\nBP | Base ptr\nSI | Source index\nDI | Dest index\n\n\n\n\n\nSegment\nCS | Code\nDS | Data\nSS | Stack\nES | Extract\n\n\n\n\n\nProgram status\nFlags | \nInstr ptr | \n\n\n\n\n**(c) 80386—Pentium 4**\n\n\n\nGeneral registers\nEAX | AX\nEBX | BX\nECX | CX\nEDX | DX\n\n\n\n\n\nPointers and index\nESP | SP\nEBP | BP\nESI | SI\nEDI | DI\n\n\n\n\n\nProgram status\nFLAGS register | \nInstruction pointer | \n\n\nFigure 14.3: Example Microprocessor Register Organizations. (a) MC68000: Data registers (D0-D7), Address registers (A0-A7), Program status (Program counter, Status register). (b) 8086: General registers (AX, BX, CX, DX), Pointers and index (SP, BP, SI, DI), Segment (CS, DS, SS, ES), Program status (Flags, Instr ptr). (c) 80386—Pentium 4: General registers (EAX, EBX, ECX, EDX), Pointers and index (ESP, EBP, ESI, EDI), Program status (FLAGS register, Instruction pointer).\n\n\n**Figure 14.3**\n   Example Microprocessor Register Organizations\n\n\nthousand words of memory for control purposes. The designer must decide how much control information should be in registers and how much in memory. The usual trade-off of cost versus speed arises.\n\n\n\n\n**Example Microprocessor Register Organizations**\n\n\nIt is instructive to examine and compare the register organization of comparable systems. In this section, we look at two 16-bit microprocessors that were designed at about the same time: the Motorola MC68000 [STRI79] and the Intel 8086 [MORS78]. Figures 14.3a and b depict the register organization of each; purely internal registers, such as a memory address register, are not shown.\n\n\nThe MC68000 partitions its 32-bit registers into eight data registers and nine address registers. The eight data registers are used primarily for data manipulation and are also used in addressing as index registers. The width of the registers allows 8-, 16-, and 32-bit data operations, determined by opcode. The address registers contain 32-bit (no segmentation) addresses; two of these registers are also used as stack pointers, one for users and one for the operating system, depending on the current execution mode. Both registers are numbered 7, because only one can be used at a time. The MC68000 also includes a 32-bit program counter and a 16-bit status register.\n\n\nThe Motorola team wanted a very regular instruction set, with no special-purpose registers. A concern for code efficiency led them to divide the registers into\n\n\ntwo functional components, saving one bit on each register specifier. This seems a reasonable compromise between complete generality and code compaction.\n\n\nThe Intel 8086 takes a different approach to register organization. Every register is special purpose, although some registers are also usable as general purpose. The 8086 contains four 16-bit data registers that are addressable on a byte or 16-bit basis, and four 16-bit pointer and index registers. The data registers can be used as general purpose in some instructions. In others, the registers are used implicitly. For example, a multiply instruction always uses the accumulator. The four pointer registers are also used implicitly in a number of operations; each contains a segment offset. There are also four 16-bit segment registers. Three of the four segment registers are used in a dedicated, implicit fashion, to point to the segment of the current instruction (useful for branch instructions), a segment containing data, and a segment containing a stack, respectively. These dedicated and implicit uses provide for compact encoding at the cost of reduced flexibility. The 8086 also includes an instruction pointer and a set of 1-bit status and control flags.\n\n\nThe point of this comparison should be clear. There is no universally accepted philosophy concerning the best way to organize processor registers [TOON81]. As with overall instruction set design and so many other processor design issues, it is still a matter of judgment and taste.\n\n\nA second instructive point concerning register organization design is illustrated in Figure 14.3c. This figure shows the user-visible register organization for the Intel 80386 [ELAY85], which is a 32-bit microprocessor designed as an extension of the 8086.\n   \n    1\n   \n   The 80386 uses 32-bit registers. However, to provide upward compatibility for programs written on the earlier machine, the 80386 retains the original register organization embedded in the new organization. Given this design constraint, the architects of the 32-bit processors had limited flexibility in designing the register organization."
        },
        {
          "name": "Instruction Cycle",
          "content": "In Section 3.2, we described the processor's instruction cycle (Figure 3.9). To recall, an instruction cycle includes the following stages:\n\n\n  * ■\n    **Fetch:**\n    Read the next instruction from memory into the processor.\n  * ■\n    **Execute:**\n    Interpret the opcode and perform the indicated operation.\n  * ■\n    **Interrupt:**\n    If interrupts are enabled and an interrupt has occurred, save the current process state and service the interrupt.\n\n\nWe are now in a position to elaborate somewhat on the instruction cycle. First, we must introduce one additional stage, known as the indirect cycle.\n\n\n1\n   \n   Because the MC68000 already uses 32-bit registers, the MC68020 [MACD84], which is a full 32-bit architecture, uses the same register organization.\n\n\n\n\n**The Indirect Cycle**\n\n\nWe have seen, in Chapter 13, that the execution of an instruction may involve one or more operands in memory, each of which requires a memory access. Further, if indirect addressing is used, then additional memory accesses are required.\n\n\nWe can think of the fetching of indirect addresses as one more instruction stages. The result is shown in Figure 14.4. The main line of activity consists of alternating instruction fetch and instruction execution activities. After an instruction is fetched, it is examined to determine if any indirect addressing is involved. If so, the required operands are fetched using indirect addressing. Following execution, an interrupt may be processed before the next instruction fetch.\n\n\nAnother way to view this process is shown in Figure 14.5, which is a revised version of Figure 3.12. This illustrates more correctly the nature of the instruction cycle. Once an instruction is fetched, its operand specifiers must be identified. Each input operand in memory is then fetched, and this process may require indirect addressing. Register-based operands need not be fetched. Once the opcode is executed, a similar process may be needed to store the result in main memory.\n\n\n\n\n**Data Flow**\n\n\nThe exact sequence of events during an instruction cycle depends on the design of the processor. We can, however, indicate in general terms what must happen. Let us assume that a processor that employs a memory address register (MAR), a memory buffer register (MBR), a program counter (PC), and an instruction register (IR).\n\n\nDuring the\n   *fetch cycle*\n   , an instruction is read from memory. Figure 14.6 shows the flow of data during this cycle. The PC contains the address of the next instruction to be fetched. This address is moved to the MAR and placed on the address bus. The control unit requests a memory read, and the result is placed on the data bus and copied into the MBR and then moved to the IR. Meanwhile, the PC is incremented by 1, preparatory for the next fetch.\n\n\nOnce the fetch cycle is over, the control unit examines the contents of the IR to determine if it contains an operand specifier using indirect addressing. If so, an\n\n\n\n\n![A state transition diagram for the Instruction Cycle. It features four states: Fetch, Interrupt, Execute, and Indirect. The Fetch state is at the top, Execute is at the bottom, Interrupt is on the left, and Indirect is on the right. Transitions are as follows: Fetch to Interrupt, Fetch to Execute, Fetch to Indirect, Interrupt to Fetch, Interrupt to Execute, Execute to Fetch, Execute to Indirect, and Indirect to Fetch. The Fetch and Execute states have double-headed vertical arrows between them, indicating a bidirectional relationship.](images/image_0232.jpeg)\n\n\ngraph TD\n    Fetch[Fetch] <--> Execute[Execute]\n    Fetch --> Interrupt[Interrupt]\n    Fetch --> Execute\n    Fetch --> Indirect[Indirect]\n    Interrupt --> Fetch\n    Interrupt --> Execute\n    Execute --> Fetch\n    Execute --> Indirect\n    Indirect --> Fetch\n  \nA state transition diagram for the Instruction Cycle. It features four states: Fetch, Interrupt, Execute, and Indirect. The Fetch state is at the top, Execute is at the bottom, Interrupt is on the left, and Indirect is on the right. Transitions are as follows: Fetch to Interrupt, Fetch to Execute, Fetch to Indirect, Interrupt to Fetch, Interrupt to Execute, Execute to Fetch, Execute to Indirect, and Indirect to Fetch. The Fetch and Execute states have double-headed vertical arrows between them, indicating a bidirectional relationship.\n\n\n**Figure 14.4**\n   The Instruction Cycle\n\n\n\n\n![Instruction Cycle State Diagram](images/image_0233.jpeg)\n\n\nThe Instruction Cycle State Diagram illustrates the sequence of operations in a CPU. The states are represented by circles and connected by arrows indicating the flow of control. The cycle begins with 'Instruction address calculation', which leads to 'Instruction fetch'. 'Instruction fetch' then leads to 'Instruction operation decoding'. From 'Instruction operation decoding', the flow can go to 'Operand address calculation' (for operands) or 'Data Operation' (for data). 'Operand address calculation' can lead to 'Operand fetch' (with a self-loop for 'Indirection') or back to 'Instruction operation decoding' (labeled 'Multiple operands'). 'Data Operation' can lead to 'Operand address calculation' (for results) or back to 'Operand address calculation' (labeled 'Return for string or vector data'). 'Operand address calculation' (for results) can lead to 'Operand store' (with a self-loop for 'Indirection') or back to 'Operand address calculation' (labeled 'Multiple results'). 'Operand store' leads to 'Interrupt check'. 'Interrupt check' can lead to 'Interrupt' (labeled 'Interrupt') or back to 'Operand address calculation' (labeled 'No interrupt'). Finally, 'Interrupt' leads to 'Interrupt check'. A long arrow at the bottom labeled 'Instruction complete, fetch next instruction' points from 'Interrupt check' back to 'Instruction address calculation', completing the cycle.\n\n\nInstruction Cycle State Diagram\n\n\n**Figure 14.5**\n   Instruction Cycle State Diagram\n\n\n\n\n![Data Flow, Fetch Cycle diagram](images/image_0234.jpeg)\n\n\nThe Data Flow, Fetch Cycle diagram shows the internal components of a CPU and their interaction with external buses and memory. The CPU is represented by a large box containing the following components: Program Counter (PC), Memory Address Register (MAR), Control unit, Memory Buffer Register (MBR), and Instruction Register (IR). The flow of data is as follows: PC outputs to MAR; MAR outputs to the Address bus; the Control unit outputs to the Control bus; the Address bus outputs to Memory; Memory outputs to the Data bus; the Data bus outputs to MBR; MBR outputs to IR; and IR outputs back to the PC. The Control unit also has a feedback loop to the PC. The buses are labeled: Address bus, Data bus, and Control bus.\n\n\nData Flow, Fetch Cycle diagram\n\n\nMBR = Memory buffer register\n   \n\n   MAR = Memory address register\n   \n\n   IR = Instruction register\n   \n\n   PC = Program counter\n\n\n**Figure 14.6**\n   Data Flow, Fetch Cycle\n\n\n*indirect cycle*\n   is performed. As shown in Figure 14.7, this is a simple cycle. The right-most\n   \n    N\n   \n   bits of the MBR, which contain the address reference, are transferred to the MAR. Then the control unit requests a memory read, to get the desired address of the operand into the MBR.\n\n\nThe fetch and indirect cycles are simple and predictable. The\n   *execute cycle*\n   takes many forms; the form depends on which of the various machine instructions is in the IR. This cycle may involve transferring data among registers, read or write from memory or I/O, and/or the invocation of the ALU.\n\n\nLike the fetch and indirect cycles, the\n   *interrupt cycle*\n   is simple and predictable (Figure 14.8). The current contents of the PC must be saved so that the processor can resume normal activity after the interrupt. Thus, the contents of the PC are transferred to the MBR to be written into memory. The special memory location reserved for this purpose is loaded into the MAR from the control unit. It might, for example, be a stack pointer. The PC is loaded with the address of the interrupt routine. As a result, the next instruction cycle will begin by fetching the appropriate instruction.\n\n\n\n\n![Figure 14.7: Data Flow, Indirect Cycle. This diagram shows the internal components of a CPU (MAR, Control unit, MBR) and its interaction with three external buses (Address bus, Data bus, Control bus) and a Memory block. The flow is: MBR to MAR, MAR to Address bus, Control unit to Control bus, Control bus to Memory, Memory to Data bus, and Data bus to MBR.](images/image_0235.jpeg)\n\n\nThe diagram illustrates the data flow during an indirect cycle. Inside the CPU, the MBR (Memory Buffer Register) sends data to the MAR (Memory Address Register). The MAR then sends an address to the Address bus. The Control unit sends control signals to the Control bus. The Control bus interacts with the Memory block, which in turn sends data back to the Data bus. Finally, the Data bus sends data to the MBR, completing the cycle.\n\n\nFigure 14.7: Data Flow, Indirect Cycle. This diagram shows the internal components of a CPU (MAR, Control unit, MBR) and its interaction with three external buses (Address bus, Data bus, Control bus) and a Memory block. The flow is: MBR to MAR, MAR to Address bus, Control unit to Control bus, Control bus to Memory, Memory to Data bus, and Data bus to MBR.\n\n\n**Figure 14.7**\n   Data Flow, Indirect Cycle\n\n\n\n\n![Figure 14.8: Data Flow, Interrupt Cycle. This diagram shows the internal components of a CPU (PC, MAR, Control Unit, MBR) and its interaction with three external buses (Address bus, Data bus, Control bus) and a Memory block. The flow is: PC to MBR, MBR to MAR, MAR to Address bus, Control Unit to Control bus, Control bus to Memory, Memory to Data bus, and Data bus to PC.](images/image_0236.jpeg)\n\n\nThe diagram illustrates the data flow during an interrupt cycle. Inside the CPU, the PC (Program Counter) sends its current value to the MBR. The MBR then sends this value to the MAR. The MAR sends the address to the Address bus. The Control Unit sends control signals to the Control bus. The Control bus interacts with the Memory block, which sends data back to the Data bus. Finally, the Data bus sends data to the PC, updating it with the address of the interrupt routine.\n\n\nFigure 14.8: Data Flow, Interrupt Cycle. This diagram shows the internal components of a CPU (PC, MAR, Control Unit, MBR) and its interaction with three external buses (Address bus, Data bus, Control bus) and a Memory block. The flow is: PC to MBR, MBR to MAR, MAR to Address bus, Control Unit to Control bus, Control bus to Memory, Memory to Data bus, and Data bus to PC.\n\n\n**Figure 14.8**\n   Data Flow, Interrupt Cycle"
        },
        {
          "name": "Instruction Pipelining",
          "content": "As computer systems evolve, greater performance can be achieved by taking advantage of improvements in technology, such as faster circuitry. In addition, organizational enhancements to the processor can improve performance. We have already seen some examples of this, such as the use of multiple registers rather than a single accumulator, and the use of a cache memory. Another organizational approach, which is quite common, is instruction pipelining.\n\n\n\n\n**Pipelining Strategy**\n\n\nInstruction pipelining is similar to the use of an assembly line in a manufacturing plant. An assembly line takes advantage of the fact that a product goes through various stages of production. By laying the production process out in an assembly line, products at various stages can be worked on simultaneously. This process is also referred to as\n   *pipelining*\n   , because, as in a pipeline, new inputs are accepted at one end before previously accepted inputs appear as outputs at the other end.\n\n\nTo apply this concept to instruction execution, we must recognize that, in fact, an instruction has a number of stages. Figures 14.5, for example, breaks the instruction cycle up into 10 tasks, which occur in sequence. Clearly, there should be some opportunity for pipelining.\n\n\nAs a simple approach, consider subdividing instruction processing into two stages: fetch instruction and execute instruction. There are times during the execution of an instruction when main memory is not being accessed. This time could be used to fetch the next instruction in parallel with the execution of the current one. Figure 14.9a depicts this approach. The pipeline has two independent stages. The first stage fetches an instruction and buffers it. When the second stage is free, the first stage passes it the buffered instruction. While the second stage is executing the instruction, the first stage takes advantage of any unused memory cycles to fetch\n\n\n\n\n![Diagram (a) Simplified view of a two-stage instruction pipeline. An arrow labeled 'Instruction' enters a block labeled 'Fetch'. An arrow labeled 'Instruction' exits the 'Fetch' block and enters a block labeled 'Execute'. An arrow labeled 'Result' exits the 'Execute' block.](images/image_0237.jpeg)\n\n\nDiagram (a) Simplified view of a two-stage instruction pipeline. An arrow labeled 'Instruction' enters a block labeled 'Fetch'. An arrow labeled 'Instruction' exits the 'Fetch' block and enters a block labeled 'Execute'. An arrow labeled 'Result' exits the 'Execute' block.\n\n\n(a) Simplified view\n\n\n\n\n![Diagram (b) Expanded view of a two-stage instruction pipeline. An arrow labeled 'Instruction' enters the 'Fetch' block. A curved arrow labeled 'New address' points from the 'Execute' block back to the 'Fetch' block. A curved arrow labeled 'Wait' points from the 'Fetch' block back to itself. A curved arrow labeled 'Wait' points from the 'Execute' block back to itself. An arrow labeled 'Instruction' exits the 'Fetch' block and enters the 'Execute' block. An arrow labeled 'Result' exits the 'Execute' block. A downward arrow labeled 'Discard' points from the 'Fetch' block.](images/image_0238.jpeg)\n\n\nDiagram (b) Expanded view of a two-stage instruction pipeline. An arrow labeled 'Instruction' enters the 'Fetch' block. A curved arrow labeled 'New address' points from the 'Execute' block back to the 'Fetch' block. A curved arrow labeled 'Wait' points from the 'Fetch' block back to itself. A curved arrow labeled 'Wait' points from the 'Execute' block back to itself. An arrow labeled 'Instruction' exits the 'Fetch' block and enters the 'Execute' block. An arrow labeled 'Result' exits the 'Execute' block. A downward arrow labeled 'Discard' points from the 'Fetch' block.\n\n\n(b) Expanded view\n\n\n**Figure 14.9**\n   Two-Stage Instruction Pipeline\n\n\nand buffer the next instruction. This is called instruction prefetch or\n   *fetch overlap*\n   . Note that this approach, which involves instruction buffering, requires more registers. In general, pipelining requires registers to store data between stages.\n\n\nIt should be clear that this process will speed up instruction execution. If the fetch and execute stages were of equal duration, the instruction cycle time would be halved. However, if we look more closely at this pipeline (Figure 14.9b), we will see that this doubling of execution rate is unlikely for two reasons:\n\n\n  * 1. The execution time will generally be longer than the fetch time. Execution will involve reading and storing operands and the performance of some operation. Thus, the fetch stage may have to wait for some time before it can empty its buffer.\n  * 2. A conditional branch instruction makes the address of the next instruction to be fetched unknown. Thus, the fetch stage must wait until it receives the next instruction address from the execute stage. The execute stage may then have to wait while the next instruction is fetched.\n\n\nGuessing can reduce the time loss from the second reason. A simple rule is the following: When a conditional branch instruction is passed on from the fetch to the execute stage, the fetch stage fetches the next instruction in memory after the branch instruction. Then, if the branch is not taken, no time is lost. If the branch is taken, the fetched instruction must be discarded and a new instruction fetched.\n\n\nWhile these factors reduce the potential effectiveness of the two-stage pipeline, some speedup occurs. To gain further speedup, the pipeline must have more stages. Let us consider the following decomposition of the instruction processing.\n\n\n  * ■\n    **Fetch instruction (FI):**\n    Read the next expected instruction into a buffer.\n  * ■\n    **Decode instruction (DI):**\n    Determine the opcode and the operand specifiers.\n  * ■\n    **Calculate operands (CO):**\n    Calculate the effective address of each source operand. This may involve displacement, register indirect, indirect, or other forms of address calculation.\n  * ■\n    **Fetch operands (FO):**\n    Fetch each operand from memory. Operands in registers need not be fetched.\n  * ■\n    **Execute instruction (EI):**\n    Perform the indicated operation and store the result, if any, in the specified destination operand location.\n  * ■\n    **Write operand (WO):**\n    Store the result in memory.\n\n\nWith this decomposition, the various stages will be of more nearly equal duration. For the sake of illustration, let us assume equal duration. Using this assumption, Figure 14.10 shows that a six-stage pipeline can reduce the execution time for 9 instructions from 54 time units to 14 time units.\n\n\nSeveral comments are in order: The diagram assumes that each instruction goes through all six stages of the pipeline. This will not always be the case. For example, a load instruction does not need the WO stage. However, to simplify the pipeline hardware, the timing is set up assuming that each instruction requires all six stages. Also, the diagram assumes that all of the stages can be performed in parallel. In particular, it is assumed that there are no memory conflicts. For example, the FI, FO, and WO stages involve a memory access. The diagram implies that all these accesses can occur simultaneously. Most memory systems will not permit that.\n\n\n\n\n![Timing Diagram for Instruction Pipeline Operation showing 9 instructions over 14 time units.](images/image_0239.jpeg)\n\n\nThe diagram is a grid with 9 rows (instructions) and 14 columns (time units). A horizontal arrow at the top labeled 'Time' points to the right. The columns are numbered 1 through 14. The rows are labeled 'Instruction 1' through 'Instruction 9' on the left. The grid shows the progression of pipeline stages (FI, DI, CO, FO, EI, WO) for each instruction. Shaded cells indicate active stages.\n\n\n\n | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14\nInstruction 1 | FI | DI | CO | FO | EI | WO |  |  |  |  |  |  |  | \nInstruction 2 |  | FI | DI | CO | FO | EI | WO |  |  |  |  |  |  | \nInstruction 3 |  |  | FI | DI | CO | FO | EI | WO |  |  |  |  |  | \nInstruction 4 |  |  |  | FI | DI | CO | FO | EI | WO |  |  |  |  | \nInstruction 5 |  |  |  |  | FI | DI | CO | FO | EI | WO |  |  |  | \nInstruction 6 |  |  |  |  |  | FI | DI | CO | FO | EI | WO |  |  | \nInstruction 7 |  |  |  |  |  |  | FI | DI | CO | FO | EI | WO |  | \nInstruction 8 |  |  |  |  |  |  |  | FI | DI | CO | FO | EI | WO | \nInstruction 9 |  |  |  |  |  |  |  |  | FI | DI | CO | FO | EI | WO\n\n\nTiming Diagram for Instruction Pipeline Operation showing 9 instructions over 14 time units.\n\n\n**Figure 14.10**\n   Timing Diagram for Instruction Pipeline Operation\n\n\nHowever, the desired value may be in cache, or the FO or WO stage may be null. Thus, much of the time, memory conflicts will not slow down the pipeline.\n\n\nSeveral other factors serve to limit the performance enhancement. If the six stages are not of equal duration, there will be some waiting involved at various pipeline stages, as discussed before for the two-stage pipeline. Another difficulty is the conditional branch instruction, which can invalidate several instruction fetches. A similar unpredictable event is an interrupt. Figure 14.11 illustrates the effects of the conditional branch, using the same program as Figure 14.10. Assume that instruction 3 is a conditional branch to instruction 15. Until the instruction is executed, there is no way of knowing which instruction will come next. The pipeline, in this example, simply loads the next instruction in sequence (instruction 4) and proceeds. In Figure 14.10, the branch is not taken, and we get the full performance benefit of the enhancement. In Figure 14.11, the branch is taken. This is not determined until the end of time unit 7. At this point, the pipeline must be cleared of instructions that are not useful. During time unit 8, instruction 15 enters the pipeline. No instructions complete during time units 9 through 12; this is the performance penalty incurred because we could not anticipate the branch. Figure 14.12 indicates the logic needed for pipelining to account for branches and interrupts.\n\n\nOther problems arise that did not appear in our simple two-stage organization. The CO stage may depend on the contents of a register that could be altered by a previous instruction that is still in the pipeline. Other such register and memory conflicts could occur. The system must contain logic to account for this type of conflict.\n\n\nTo clarify pipeline operation, it might be useful to look at an alternative depiction. Figures 14.10 and 14.11 show the progression of time horizontally across the figures, with each row showing the progress of an individual instruction. Figure 14.13 shows same sequence of events, with time progressing vertically down\n\n\n\n | Time | Branch penalty\n | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14\nInstruction 1 | FI | DI | CO | FO | EI | WO |  |  |  |  |  |  |  | \nInstruction 2 |  | FI | DI | CO | FO | EI | WO |  |  |  |  |  |  | \nInstruction 3 |  |  | FI | DI | CO | FO | EI | WO |  |  |  |  |  | \nInstruction 4 |  |  |  | FI | DI | CO | FO |  |  |  |  |  |  | \nInstruction 5 |  |  |  |  | FI | DI | CO |  |  |  |  |  |  | \nInstruction 6 |  |  |  |  |  | FI | DI |  |  |  |  |  |  | \nInstruction 7 |  |  |  |  |  |  | FI |  |  |  |  |  |  | \nInstruction 15 |  |  |  |  |  |  |  | FI | DI | CO | FO | EI | WO | \nInstruction 16 |  |  |  |  |  |  |  |  | FI | DI | CO | FO | EI | WO\n\n\n**Figure 14.11**\n   The Effect of a Conditional Branch on Instruction Pipeline Operation\n\n\nthe figure, and each row showing the state of the pipeline at a given point in time. In Figure 14.13a (which corresponds to Figure 14.10), the pipeline is full at time 6, with 6 different instructions in various stages of execution, and remains full through time 9; we assume that instruction I9 is the last instruction to be executed. In Figure 14.13b, (which corresponds to Figure 14.11), the pipeline is full at times 6 and 7. At time 7, instruction 3 is in the execute stage and executes a branch to instruction 15. At this point, instructions I4 through I7 are flushed from the pipeline, so that at time 8, only two instructions are in the pipeline, I3 and I15.\n\n\nFrom the preceding discussion, it might appear that the greater the number of stages in the pipeline, the faster the execution rate. Some of the IBM S/360 designers pointed out two factors that frustrate this seemingly simple pattern for high-performance design [ANDE67a], and they remain elements that designer must still consider:\n\n\n  * 1. At each stage of the pipeline, there is some overhead involved in moving data from buffer to buffer and in performing various preparation and delivery functions. This overhead can appreciably lengthen the total execution time of a single instruction. This is significant when sequential instructions are logically dependent, either through heavy use of branching or through memory access dependencies.\n  * 2. The amount of control logic required to handle memory and register dependencies and to optimize the use of the pipeline increases enormously with the number of stages. This can lead to a situation where the logic controlling the gating between stages is more complex than the stages being controlled.\n\n\nAnother consideration is latching delay: It takes time for pipeline buffers to operate and this adds to instruction cycle time.\n\n\n\n\n![Flowchart of a Six-Stage CPU Instruction Pipeline. The stages are: FI (Fetch instruction), DI (Decode instruction), CO (Calculate operands), Unconditional branch? (Yes/No), FO (Fetch operands), EI (Execute instruction), WO (Write operands), Branch or interrupt? (Yes/No), Update PC, and Empty pipe. The flow starts at FI, goes through DI and CO, then to a decision diamond for unconditional branches. If 'Yes', it goes to 'Update PC' and then 'Empty pipe'. If 'No', it goes to FO, then EI, then WO, then to a second decision diamond for branches or interrupts. If 'Yes', it goes to 'Update PC' and then 'Empty pipe'. If 'No', it loops back to FI. The 'Empty pipe' stage has a feedback arrow pointing to the 'FI' stage.](images/image_0240.jpeg)\n\n\ngraph TD\n    FI[FI: Fetch instruction] --> DI[DI: Decode instruction]\n    DI --> CO[CO: Calculate operands]\n    CO --> Branch1{Unconditional branch?}\n    Branch1 -- Yes --> UpdatePC[Update PC]\n    Branch1 -- No --> FO[FO: Fetch operands]\n    FO --> EI[EI: Execute instruction]\n    EI --> WO[WO: Write operands]\n    WO --> Branch2{Branch or interrupt?}\n    Branch2 -- Yes --> UpdatePC\n    Branch2 -- No --> FI\n    UpdatePC --> EmptyPipe[Empty pipe]\n    EmptyPipe --> FI\n  \nFlowchart of a Six-Stage CPU Instruction Pipeline. The stages are: FI (Fetch instruction), DI (Decode instruction), CO (Calculate operands), Unconditional branch? (Yes/No), FO (Fetch operands), EI (Execute instruction), WO (Write operands), Branch or interrupt? (Yes/No), Update PC, and Empty pipe. The flow starts at FI, goes through DI and CO, then to a decision diamond for unconditional branches. If 'Yes', it goes to 'Update PC' and then 'Empty pipe'. If 'No', it goes to FO, then EI, then WO, then to a second decision diamond for branches or interrupts. If 'Yes', it goes to 'Update PC' and then 'Empty pipe'. If 'No', it loops back to FI. The 'Empty pipe' stage has a feedback arrow pointing to the 'FI' stage.\n\n\n**Figure 14.12**\n   Six-Stage CPU Instruction Pipeline\n\n\nInstruction pipelining is a powerful technique for enhancing performance but requires careful design to achieve optimum results with reasonable complexity.\n\n\n\n\n**Pipeline Performance**\n\n\nIn this subsection, we develop some simple measures of pipeline performance and relative speedup (based on a discussion in [HWAN93]). The cycle time\n   \n    \\tau\n   \n   of an\n   **instruction pipeline**\n   is the time needed to advance a set of instructions one stage through the pipeline; each column in Figures 14.10 and 14.11 represents one cycle time. The cycle time can be determined as\n\n\n\\tau = \\max_i[\\tau_i] + d = \\tau_m + d \\quad 1 \\le i \\le k\n\n\n\n\n![Figure 14.13: An Alternative Pipeline Depiction. (a) No branches: A 14x6 grid showing instructions I1 through I19 moving through stages FI, DI, CO, FO, EI, and WO over 14 time cycles. (b) With conditional branch: A 14x6 grid showing instructions I1 through I16 moving through the same stages, with a branch occurring at time cycle 9.](images/image_0241.jpeg)\n\n\n(a) No branches\n\n\n(b) With conditional branch\n\n\nFigure 14.13: An Alternative Pipeline Depiction. (a) No branches: A 14x6 grid showing instructions I1 through I19 moving through stages FI, DI, CO, FO, EI, and WO over 14 time cycles. (b) With conditional branch: A 14x6 grid showing instructions I1 through I16 moving through the same stages, with a branch occurring at time cycle 9.\n\n\n**Figure 14.13**\n   An Alternative Pipeline Depiction\n\n\nwhere\n\n\n  * \\tau_i\n    \n    = time delay of the circuitry in the\n    \n     i\n    \n    th stage of the pipeline\n  * \\tau_m\n    \n    = maximum stage delay (delay through stage which experiences the largest delay)\n  * k\n    \n    = number of stages in the instruction pipeline\n  * d\n    \n    = time delay of a latch, needed to advance signals and data from one stage to the next\n\n\nIn general, the time delay\n   \n    d\n   \n   is equivalent to a clock pulse and\n   \n    \\tau_m \\gg d\n   \n   . Now suppose that\n   \n    n\n   \n   instructions are processed, with no branches. Let\n   \n    T_{k,n}\n   \n   be the total time required for a pipeline with\n   \n    k\n   \n   stages to execute\n   \n    n\n   \n   instructions. Then\n\n\nT_{k,n} = [k + (n - 1)]\\tau \\quad (14.1)\n\n\nA total of\n   \n    k\n   \n   cycles are required to complete the execution of the first instruction, and the remaining\n   \n    n - 1\n   \n   instructions require\n   \n    n - 1\n   \n   cycles.\n   \n    2\n   \n   This equation is easily verified from Figure 14.10. The ninth instruction completes at time cycle 14:\n\n\n14 = [6 + (9 - 1)]\n\n\n2\n   \n   We are being a bit sloppy here. The cycle time will only equal the maximum value of\n   \n    \\tau\n   \n   when all the stages are full. At the beginning, the cycle time may be less for the first one or few cycles.\n\n\nNow consider a processor with equivalent functions but no pipeline, and assume that the instruction cycle time is\n   \n    k\\tau\n   \n   . The speedup factor for the instruction pipeline compared to execution without the pipeline is defined as\n\n\nS_k = \\frac{T_{1,n}}{T_{k,n}} = \\frac{n k \\tau}{[k + (n - 1)] \\tau} = \\frac{nk}{k + (n - 1)} \\quad (14.2)\n\n\nFigure 14.14a plots the speedup factor as a function of the number of instructions that are executed without a branch. As might be expected, at the limit (\n   \n    n \\to \\infty\n   \n   ), we have a\n   \n    k\n   \n   -fold speedup. Figure 14.14b shows the speedup factor as a function of the number of stages in the instruction pipeline.\n   \n    3\n   \n   In this case, the speedup factor approaches the number of instructions that can be fed into the pipeline without branches. Thus, the larger the number of pipeline stages, the greater the potential for speedup. However, as a practical matter, the potential gains of additional\n\n\n\n\n![Figure 14.14(a): Speedup factor vs. Number of instructions (log scale).](images/image_0242.jpeg)\n\n\nFigure 14.14(a) is a line graph showing the speedup factor as a function of the number of instructions (log scale). The x-axis represents the number of instructions, ranging from 1 to 128 on a logarithmic scale. The y-axis represents the speedup factor, ranging from 0 to 12. Three curves are plotted for different numbers of pipeline stages:\n    \n     k = 12\n    \n    stages,\n    \n     k = 9\n    \n    stages, and\n    \n     k = 6\n    \n    stages. All curves start at a speedup factor of 1 for 1 instruction and increase as the number of instructions increases, approaching a horizontal asymptote at the value of\n    \n     k\n    \n    .\n\n\n\nNumber of instructions | k = 12\n       \n       stages | k = 9\n       \n       stages | k = 6\n       \n       stages\n1 | 1.0 | 1.0 | 1.0\n2 | 1.5 | 1.33 | 1.2\n4 | 2.5 | 2.0 | 1.7\n8 | 4.0 | 3.3 | 2.7\n16 | 6.0 | 5.0 | 4.0\n32 | 8.5 | 7.0 | 5.7\n64 | 10.5 | 8.5 | 7.0\n128 | 11.5 | 9.0 | 7.5\n\n\nFigure 14.14(a): Speedup factor vs. Number of instructions (log scale).\n\n\n\n\n![Figure 14.14(b): Speedup factor vs. Number of stages.](images/image_0243.jpeg)\n\n\nFigure 14.14(b) is a line graph showing the speedup factor as a function of the number of stages in the instruction pipeline. The x-axis represents the number of stages, ranging from 0 to 20. The y-axis represents the speedup factor, ranging from 0 to 14. Three curves are plotted for different numbers of instructions:\n    \n     n = 30\n    \n    instructions,\n    \n     n = 20\n    \n    instructions, and\n    \n     n = 10\n    \n    instructions. All curves start at a speedup factor of 1 for 0 stages and increase as the number of stages increases, approaching a horizontal asymptote at the value of\n    \n     n\n    \n    .\n\n\n\nNumber of stages | n = 30\n       \n       instructions | n = 20\n       \n       instructions | n = 10\n       \n       instructions\n0 | 1.0 | 1.0 | 1.0\n5 | 3.5 | 2.5 | 1.8\n10 | 7.0 | 5.5 | 4.0\n15 | 10.0 | 8.0 | 6.0\n20 | 12.0 | 10.0 | 7.0\n\n\nFigure 14.14(b): Speedup factor vs. Number of stages.\n\n\n**Figure 14.14**\n   Speedup Factors with Instruction Pipelining\n\n\n3\n   \n   Note that the\n   \n    x\n   \n   -axis is logarithmic in Figure 14.14a and linear in Figure 14.14b.\n\n\npipeline stages are countered by increases in cost, delays between stages, and the fact that branches will be encountered requiring the flushing of the pipeline.\n\n\n\n\n**Pipeline Hazards**\n\n\nIn the previous subsection, we mentioned some of the situations that can result in less than optimal pipeline performance. In this subsection, we examine this issue in a more systematic way. Chapter 16 revisits this issue, in more detail, after we have introduced the complexities found in superscalar pipeline organizations.\n\n\nA\n   **pipeline hazard**\n   occurs when the pipeline, or some portion of the pipeline, must stall because conditions do not permit continued execution. Such a pipeline stall is also referred to as a\n   *pipeline bubble*\n   . There are three types of hazards: resource, data, and control.\n\n\n**RESOURCE HAZARDS**\n   A resource hazard occurs when two (or more) instructions that are already in the pipeline need the same resource. The result is that the instructions must be executed in serial rather than parallel for a portion of the pipeline. A resource hazard is sometime referred to as a\n   *structural hazard*\n   .\n\n\nLet us consider a simple example of a resource hazard. Assume a simplified five-stage pipeline, in which each stage takes one clock cycle. Figure 14.15a shows the ideal case, in which a new instruction enters the pipeline each clock cycle. Now assume that main memory has a single port and that all instruction fetches and data reads and writes must be performed one at a time. Further, ignore the cache. In this case, an operand read to or write from memory cannot be performed in parallel\n\n\n\n | Clock cycle\n | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\nInstruction | 11 | FI | DI | FO | EI | WO |  |  |  | \n12 |  | FI | DI | FO | EI | WO |  |  | \n13 |  |  | FI | DI | FO | EI | WO |  | \n14 |  |  |  | FI | DI | FO | EI | WO | \n\n\n(a) Five-stage pipeline, ideal case\n\n\n\n | Clock cycle\n | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\nInstruction | 11 | FI | DI | FO | EI | WO |  |  |  | \n12 |  | FI | DI | FO | EI | WO |  |  | \n13 |  |  | Idle | FI | DI | FO | EI | WO | \n14 |  |  |  |  | FI | DI | FO | EI | WO\n\n\n(b) I1 source operand in memory\n\n\n**Figure 14.15**\n   Example of Resource Hazard\n\n\nwith an instruction fetch. This is illustrated in Figure 14.15b, which assumes that the source operand for instruction I1 is in memory, rather than a register. Therefore, the fetch instruction stage of the pipeline must idle for one cycle before beginning the instruction fetch for instruction I3. The figure assumes that all other operands are in registers.\n\n\nAnother example of a resource conflict is a situation in which multiple instructions are ready to enter the execute instruction phase and there is a single ALU. One solutions to such resource hazards is to increase available resources, such as having multiple ports into main memory and multiple ALU units.\n\n\n\n\n![Online Interactive Simulation logo featuring a globe and the text 'www'.](images/image_0244.jpeg)\n\n\nOnline Interactive Simulation logo featuring a globe and the text 'www'.\n\n\n\n\n**Reservation Table Analyzer**\n\n\nOne approach to analyzing resource conflicts and aiding in the design of pipelines is the reservation table. We examine reservation tables in Appendix N.\n\n\n**DATA HAZARDS**\n   A data hazard occurs when there is a conflict in the access of an operand location. In general terms, we can state the hazard in this form: Two instructions in a program are to be executed in sequence and both access a particular memory or register operand. If the two instructions are executed in strict sequence, no problem occurs. However, if the instructions are executed in a pipeline, then it is possible for the operand value to be updated in such a way as to produce a different result than would occur with strict sequential execution. In other words, the program produces an incorrect result because of the use of pipelining.\n\n\nAs an example, consider the following x86 machine instruction sequence:\n\n\nADD EAX, EBX /* EAX = EAX + EBX\nSUB ECX, EAX /* ECX = ECX - EAX\nThe first instruction adds the contents of the 32-bit registers EAX and EBX and stores the result in EAX. The second instruction subtracts the contents of EAX from ECX and stores the result in ECX. Figure 14.16 shows the pipeline behavior.\n\n\n\n | Clock cycle\n | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10\nADD EAX, EBX | FI | DI | FO | EI | WO |  |  |  |  | \nSUB ECX, EAX |  | FI | DI | Idle | FO | EI | WO |  |  | \nI3 |  |  | FI |  | DI | FO | EI | WO |  | \nI4 |  |  |  |  | FI | DI | FO | EI | WO | \n\n\n**Figure 14.16**\n   Example of Data Hazard\n\n\nThe ADD instruction does not update register EAX until the end of stage 5, which occurs at clock cycle 5. But the SUB instruction needs that value at the beginning of its stage 2, which occurs at clock cycle 4. To maintain correct operation, the pipeline must stall for two clocks cycles. Thus, in the absence of special hardware and specific avoidance algorithms, such a data hazard results in inefficient pipeline usage.\n\n\nThere are three types of data hazards:\n\n\n  * ■\n    **Read after write (RAW), or true dependency:**\n    An instruction modifies a register or memory location and a succeeding instruction reads the data in that memory or register location. A hazard occurs if the read takes place before the write operation is complete.\n  * ■\n    **Write after read (WAR), or antidependency:**\n    An instruction reads a register or memory location and a succeeding instruction writes to the location. A hazard occurs if the write operation completes before the read operation takes place.\n  * ■\n    **Write after write (WAW), or output dependency:**\n    Two instructions both write to the same location. A hazard occurs if the write operations take place in the reverse order of the intended sequence.\n\n\nThe example of Figure 14.16 is a RAW hazard. The other two hazards are best discussed in the context of superscalar organization, discussed in Chapter 16.\n\n\n**CONTROL HAZARDS**\n   A control hazard, also known as a\n   *branch hazard*\n   , occurs when the pipeline makes the wrong decision on a branch prediction and therefore brings instructions into the pipeline that must subsequently be discarded. We discuss approaches to dealing with control hazards next.\n\n\n\n\n**Dealing with Branches**\n\n\nOne of the major problems in designing an instruction pipeline is assuring a steady flow of instructions to the initial stages of the pipeline. The primary impediment, as we have seen, is the conditional branch instruction. Until the instruction is actually executed, it is impossible to determine whether the branch will be taken or not.\n\n\nA variety of approaches have been taken for dealing with conditional branches:\n\n\n  * ■ Multiple streams\n  * ■ Prefetch branch target\n  * ■ Loop buffer\n  * ■ Branch prediction\n  * ■ Delayed branch\n\n\n**MULTIPLE STREAMS**\n   A simple pipeline suffers a penalty for a branch instruction because it must choose one of two instructions to fetch next and may make the wrong choice. A brute-force approach is to replicate the initial portions of the pipeline and allow the pipeline to fetch both instructions, making use of two streams. There are two problems with this approach:\n\n\n  * ■ With multiple pipelines there are contention delays for access to the registers and to memory.\n\n\n  * ■ Additional branch instructions may enter the pipeline (either stream) before the original branch decision is resolved. Each such instruction needs an additional stream.\n\n\nDespite these drawbacks, this strategy can improve performance. Examples of machines with two or more pipeline streams are the IBM 370/168 and the IBM 3033.\n\n\n**PREFETCH BRANCH TARGET**\n   When a conditional branch is recognized, the target of the branch is prefetched, in addition to the instruction following the branch. This target is then saved until the branch instruction is executed. If the branch is taken, the target has already been prefetched.\n\n\nThe IBM 360/91 uses this approach.\n\n\n**LOOP BUFFER**\n   A loop buffer is a small, very-high-speed memory maintained by the instruction fetch stage of the pipeline and containing the\n   \n    n\n   \n   most recently fetched instructions, in sequence. If a branch is to be taken, the hardware first checks whether the branch target is within the buffer. If so, the next instruction is fetched from the buffer. The loop buffer has three benefits:\n\n\n  * 1. With the use of prefetching, the loop buffer will contain some instruction sequentially ahead of the current instruction fetch address. Thus, instructions fetched in sequence will be available without the usual memory access time.\n  * 2. If a branch occurs to a target just a few locations ahead of the address of the branch instruction, the target will already be in the buffer. This is useful for the rather common occurrence of IF-THEN and IF-THEN-ELSE sequences.\n  * 3. This strategy is particularly well suited to dealing with loops, or iterations; hence the name\n    *loop buffer*\n    . If the loop buffer is large enough to contain all the instructions in a loop, then those instructions need to be fetched from memory only once, for the first iteration. For subsequent iterations, all the needed instructions are already in the buffer.\n\n\nThe loop buffer is similar in principle to a cache dedicated to instructions. The differences are that the loop buffer only retains instructions in sequence and is much smaller in size and hence lower in cost.\n\n\nFigure 14.17 gives an example of a loop buffer. If the buffer contains 256 bytes, and byte addressing is used, then the least significant 8 bits are used to index the\n\n\n\n\n![Diagram of a Loop Buffer showing address bits and buffer contents.](images/image_0245.jpeg)\n\n\nThe diagram illustrates a Loop Buffer (256 bytes). A 'Branch address' line enters from the top left. A vertical line from this address line splits into two paths. The upper path, labeled '8', points to the 'Loop buffer' block. The lower path points to the text 'Most significant address bits compared to determine a hit'. An arrow from the 'Loop buffer' block points to the right, labeled 'Instruction to be decoded in case of hit'.\n\n\nDiagram of a Loop Buffer showing address bits and buffer contents.\n\n\n**Figure 14.17**\n   Loop Buffer\n\n\nbuffer. The remaining most significant bits are checked to determine if the branch target lies within the environment captured by the buffer.\n\n\nAmong the machines using a loop buffer are some of the CDC machines (Star-100, 6600, 7600) and the CRAY-1. A specialized form of loop buffer is available on the Motorola 68010, for executing a three-instruction loop involving the DBcc (decrement and branch on condition) instruction (see Problem 14.14). A three-word buffer is maintained, and the processor executes these instructions repeatedly until the loop condition is satisfied.\n\n\n\n\n![Logo for Online Interactive Simulation (OIS) featuring a globe and the text 'www'.](images/image_0246.jpeg)\n\n\nLogo for Online Interactive Simulation (OIS) featuring a globe and the text 'www'.\n\n\n\n\n**Branch Prediction Simulator\n   \n   Branch Target Buffer**\n\n\n**BRANCH PREDICTION**\n   Various techniques can be used to predict whether a branch will be taken. Among the more common are the following:\n\n\n  * ■ Predict never taken\n  * ■ Predict always taken\n  * ■ Predict by opcode\n  * ■ Taken/not taken switch\n  * ■ Branch history table\n\n\nThe first three approaches are static: they do not depend on the execution history up to the time of the conditional branch instruction. The latter two approaches are dynamic: They depend on the execution history.\n\n\nThe first two approaches are the simplest. These either always assume that the branch will not be taken and continue to fetch instructions in sequence, or they always assume that the branch will be taken and always fetch from the branch target. The predict-never-taken approach is the most popular of all the branch prediction methods.\n\n\nStudies analyzing program behavior have shown that conditional branches are taken more than 50% of the time [LILJ88], and so if the cost of prefetching from either path is the same, then always prefetching from the branch target address should give better performance than always prefetching from the sequential path. However, in a paged machine, prefetching the branch target is more likely to cause a page fault than prefetching the next instruction in sequence, and so this performance penalty should be taken into account. An avoidance mechanism may be employed to reduce this penalty.\n\n\nThe final static approach makes the decision based on the opcode of the branch instruction. The processor assumes that the branch will be taken for certain branch opcodes and not for others. [LILJ88] reports success rates of greater than 75% with this strategy.\n\n\nDynamic branch strategies attempt to improve the accuracy of prediction by recording the history of conditional branch instructions in a program. For example, one or more bits can be associated with each conditional branch instruction that\n\n\nreflect the recent history of the instruction. These bits are referred to as a taken/not taken switch that directs the processor to make a particular decision the next time the instruction is encountered. Typically, these history bits are not associated with the instruction in main memory. Rather, they are kept in temporary high-speed storage. One possibility is to associate these bits with any conditional branch instruction that is in a cache. When the instruction is replaced in the cache, its history is lost. Another possibility is to maintain a small table for recently executed branch instructions with one or more history bits in each entry. The processor could access the table associatively, like a cache, or by using the low-order bits of the branch instruction's address.\n\n\nWith a single bit, all that can be recorded is whether the last execution of this instruction resulted in a branch or not. A shortcoming of using a single bit appears in the case of a conditional branch instruction that is almost always taken, such as a loop instruction. With only one bit of history, an error in prediction will occur twice for each use of the loop: once on entering the loop, and once on exiting.\n\n\nIf two bits are used, they can be used to record the result of the last two instances of the execution of the associated instruction, or to record a state in some other fashion. Figure 14.18 shows a typical approach (see Problem 14.13 for other\n\n\n\n\n![](images/image_0247.jpeg)\n\n\n**Figure 14.18**\n    Branch Prediction Flowchart\n\n\npossibilities). Assume that the algorithm starts at the upper-left-hand corner of the flowchart. As long as each succeeding conditional branch instruction that is encountered is taken, the decision process predicts that the next branch will be taken. If a single prediction is wrong, the algorithm continues to predict that the next branch is taken. Only if two successive branches are not taken does the algorithm shift to the right-hand side of the flowchart. Subsequently, the algorithm will predict that branches are not taken until two branches in a row are taken. Thus, the algorithm requires two consecutive wrong predictions to change the prediction decision.\n\n\nThe decision process can be represented more compactly by a finite-state machine, shown in Figure 14.19. The finite-state machine representation is commonly used in the literature.\n\n\nThe use of history bits, as just described, has one drawback: If the decision is made to take the branch, the target instruction cannot be fetched until the target address, which is an operand in the conditional branch instruction, is decoded. Greater efficiency could be achieved if the instruction fetch could be initiated as soon as the branch decision is made. For this purpose, more information must be saved, in what is known as a branch target buffer, or a branch history table.\n\n\nThe branch history table is a small cache memory associated with the instruction fetch stage of the pipeline. Each entry in the table consists of three elements: the address of a branch instruction, some number of history bits that record the state of use of that instruction, and information about the target instruction. In most proposals and implementations, this third field contains the address of the target instruction. Another possibility is for the third field to actually contain the target instruction. The trade-off is clear: Storing the target address yields a smaller table but a greater instruction fetch time compared with storing the target instruction [RECH98].\n\n\nFigure 14.20 contrasts this scheme with a predict-never-taken strategy. With the former strategy, the instruction fetch stage always fetches the next sequential\n\n\n\n\n![Branch Prediction State Diagram showing four states: Predict taken, Predict not taken, and two intermediate states. Transitions are labeled with 'Taken' and 'Not taken'.](images/image_0248.jpeg)\n\n\ngraph TD\n    P1((Predict taken)) -- Taken --> P1\n    P1 -- Not taken --> P2((Predict taken))\n    P2 -- Taken --> P1\n    P2 -- Not taken --> P3((Predict not taken))\n    P3 -- Taken --> P1\n    P3 -- Not taken --> P4((Predict not taken))\n    P4 -- Taken --> P3\n    P4 -- Not taken --> P4\n  \nThe diagram illustrates a branch prediction state machine with four states arranged in a 2x2 grid. The top row contains 'Predict taken' states, and the bottom row contains 'Predict not taken' states. Transitions between states are labeled with 'Taken' or 'Not taken'.\n\n\n  * **Top-Left State (Predict taken):**\n     A self-loop labeled 'Taken' and a transition to the top-right state labeled 'Not taken'.\n  * **Top-Right State (Predict taken):**\n     A transition to the top-left state labeled 'Taken' and a transition to the bottom-right state labeled 'Not taken'.\n  * **Bottom-Left State (Predict not taken):**\n     A transition to the top-left state labeled 'Taken' and a transition to the bottom-right state labeled 'Not taken'.\n  * **Bottom-Right State (Predict not taken):**\n     A self-loop labeled 'Not taken' and a transition to the bottom-left state labeled 'Taken'.\n\n\nBranch Prediction State Diagram showing four states: Predict taken, Predict not taken, and two intermediate states. Transitions are labeled with 'Taken' and 'Not taken'.\n\n\n**Figure 14.19**\n   Branch Prediction State Diagram\n\n\n\n\n![Figure 14.20: Dealing with Branches. (a) Predict never taken strategy. (b) Branch history table strategy.](images/image_0249.jpeg)\n\n\n**(a) Predict never taken strategy**\n\n\nDiagram (a) shows a simple flow: an 'E' block (Execute stage) feeds into a 'Branch miss handling' block. The output of 'Branch miss handling' goes to a 'Select' block. A 'Next sequential address' line also feeds into the 'Select' block. The 'Select' block then outputs to 'Memory'.\n\n\n**(b) Branch history table strategy**\n\n\nDiagram (b) is more complex. An 'E' block feeds into a 'Branch miss handling' block. The output of 'Branch miss handling' goes to a 'Redirect' block. The 'Redirect' block feeds into an 'IPFAR' (Instruction Prefix Address Register) block. The 'IPFAR' block feeds into a 'Lookup' input of a 'Branch history table' (a table with columns: Branch instruction address, Target address, State). The 'Branch history table' has an 'Add new entry' input and an 'Update state' input. The 'IPFAR' block also feeds into a 'Next sequential address' line. The 'Next sequential address' line and the 'Target address' column of the 'Branch history table' both feed into a 'Select' block. The 'Select' block outputs to 'Memory'. A legend states: 'IPFAR = instruction prefix address register'.\n\n\nFigure 14.20: Dealing with Branches. (a) Predict never taken strategy. (b) Branch history table strategy.\n\n\n**Figure 14.20**\naddress. If a branch is taken, some logic in the processor detects this and instructs that the next instruction be fetched from the target address (in addition to flushing the pipeline). The branch history table is treated as a cache. Each prefetch triggers a lookup in the branch history table. If no match is found, the next sequential address is used for the fetch. If a match is found, a prediction is made based on the state of the instruction: Either the next sequential address or the branch target address is fed to the select logic.\n\n\nWhen the branch instruction is executed, the execute stage signals the branch history table logic with the result. The state of the instruction is updated to reflect a correct or incorrect prediction. If the prediction is incorrect, the select logic is\n\n\nredirected to the correct address for the next fetch. When a conditional branch instruction is encountered that is not in the table, it is added to the table and one of the existing entries is discarded, using one of the cache replacement algorithms discussed in Chapter 4.\n\n\nA refinement of the branch history approach is referred to as two-level or correlation-based branch history [YEH91]. This approach is based on the assumption that whereas in loop-closing branches, the past history of a particular branch instruction is a good predictor of future behavior, with more complex control-flow structures, the direction of a branch is frequently correlated with the direction of related branches. An example is an if-then-else or case structure. There are a number of strategies possible. Typically, recent global branch history (i.e., the history of the most recent branches not just of this branch instruction) is used in addition to the history of the current branch instruction. The general structure is defined as an\n   \n    (m, n)\n   \n   correlator, which uses the behavior of the last\n   \n    m\n   \n   branches to choose from\n   \n    2^m\n   \n\n    n\n   \n   -bit branch predictors for the current branch instruction. In other words, an\n   \n    n\n   \n   -bit history is kept for a give branch for each possible combination of branches taken by the most recent\n   \n    m\n   \n   branches.\n\n\n**DELAYED BRANCH**\n   It is possible to improve pipeline performance by automatically rearranging instructions within a program, so that branch instructions occur later than actually desired. This intriguing approach is examined in Chapter 15.\n\n\n\n\n**Intel 80486 Pipelining**\n\n\nAn instructive example of an instruction pipeline is that of the Intel 80486. The 80486 implements a five-stage pipeline:\n\n\n  * ■\n    **Fetch:**\n    Instructions are fetched from the cache or from external memory and placed into one of the two 16-byte prefetch buffers. The objective of the fetch stage is to fill the prefetch buffers with new data as soon as the old data have been consumed by the instruction decoder. Because instructions are of variable length (from 1 to 11 bytes not counting prefixes), the status of the prefetcher relative to the other pipeline stages varies from instruction to instruction. On average, about five instructions are fetched with each 16-byte load [CRAW90]. The fetch stage operates independently of the other stages to keep the prefetch buffers full.\n  * ■\n    **Decode stage 1:**\n    All opcode and addressing-mode information is decoded in the D1 stage. The required information, as well as instruction-length information, is included in at most the first 3 bytes of the instruction. Hence, 3 bytes are passed to the D1 stage from the prefetch buffers. The D1 decoder can then direct the D2 stage to capture the rest of the instruction (displacement and immediate data), which is not involved in the D1 decoding.\n  * ■\n    **Decode stage 2:**\n    The D2 stage expands each opcode into control signals for the ALU. It also controls the computation of the more complex addressing modes.\n  * ■\n    **Execute:**\n    This stage includes ALU operations, cache access, and register update.\n\n\n  * ■\n    **Write back:**\n    This stage, if needed, updates registers and status flags modified during the preceding execute stage. If the current instruction updates memory, the computed value is sent to the cache and to the bus-interface write buffers at the same time.\n\n\nWith the use of two decode stages, the pipeline can sustain a throughput of close to one instruction per clock cycle. Complex instructions and conditional branches can slow down this rate.\n\n\nFigure 14.21 shows examples of the operation of the pipeline. Figure 14.21a shows that there is no delay introduced into the pipeline when a memory access is required. However, as Figure 14.21b shows, there can be a delay for values used to compute memory addresses. That is, if a value is loaded from memory into a register and that register is then used as a base register in the next instruction, the processor will stall for one cycle. In this example, the processor accesses the cache in the EX stage of the first instruction and stores the value retrieved in the register during the WB stage. However, the next instruction needs this register in its D2 stage. When the D2 stage lines up with the WB stage of the previous instruction, bypass signal paths allow the D2 stage to have access to the same data being used by the WB stage for writing, saving one pipeline stage.\n\n\nFigure 14.21c illustrates the timing of a branch instruction, assuming that the branch is taken. The compare instruction updates condition codes in the WB stage, and bypass paths make this available to the EX stage of the jump instruction at the same time. In parallel, the processor runs a speculative fetch cycle to the target of the jump during the EX stage of the jump instruction. If the processor determines a false branch condition, it discards this prefetch and continues execution with the next sequential instruction (already fetched and decoded).\n\n\n\nFetch | D1 | D2 | EX | WB |  | MOV Reg1, Mem1\nFetch | D1 | D2 | EX | WB |  | MOV Reg1, Reg2\nFetch | D1 | D2 | EX | WB |  | MOV Mem2, Reg1\n\n\n(a) No data load delay in the pipeline\n\n\n\nFetch | D1 | D2 | EX | WB |  | MOV Reg1, Mem1\nFetch | D1 |  |  | D2 | EX | MOV Reg2, (Reg1)\n\n\n(b) Pointer load delay\n\n\n\nFetch | D1 | D2 | EX | WB |  | CMP Reg1, Imm\nFetch | D1 | D2 | EX |  |  | Jcc Target\nFetch | D1 | D2 | EX |  |  | Target\n\n\n(c) Branch instruction timing\n\n\n**Figure 14.21**\n   80486 Instruction Pipeline Examples"
        },
        {
          "name": "The x86 Processor Family",
          "content": "The x86 organization has evolved dramatically over the years. In this section we examine some of the details of the most recent processor organizations, concentrating on common elements in single processors. Chapter 16 looks at superscalar aspects of the x86, and Chapter 18 examines the multicore organization. An overview of the Pentium 4 processor organization is depicted in Figure 4.18.\n\n\n\n\n**Register Organization**\n\n\nThe register organization includes the following types of registers (Table 14.2):\n\n\n  * ■\n    **General:**\n    There are eight 32-bit general-purpose registers (see Figure 14.3c). These may be used for all types of x86 instructions; they can also hold operands for address calculations. In addition, some of these registers also serve special purposes. For example, string instructions use the contents of the ECX, ESI, and EDI registers as operands without having to reference these registers explicitly in the instruction. As a result, a number of instructions can be\n\n\n**Table 14.2**\n   x86 Processor Registers\n\n\n(a) Integer Unit in 32-bit Mode\n\n\n\nType | Number | Length (bits) | Purpose\nGeneral | 8 | 32 | General-purpose user registers\nSegment | 6 | 16 | Contain segment selectors\nEFLAGS | 1 | 32 | Status and control bits\nInstruction Pointer | 1 | 32 | Instruction pointer\n\n\n(b) Integer Unit in 64-bit Mode\n\n\n\nType | Number | Length (bits) | Purpose\nGeneral | 16 | 32 | General-purpose user registers\nSegment | 6 | 16 | Contain segment selectors\nRFLAGS | 1 | 64 | Status and control bits\nInstruction Pointer | 1 | 64 | Instruction pointer\n\n\n(c) Floating-Point Unit\n\n\n\nType | Number | Length (bits) | Purpose\nNumeric | 8 | 80 | Hold floating-point numbers\nControl | 1 | 16 | Control bits\nStatus | 1 | 16 | Status bits\nTag Word | 1 | 16 | Specifies contents of numeric registers\nInstruction Pointer | 1 | 48 | Points to instruction interrupted by exception\nData Pointer | 1 | 48 | Points to operand interrupted by exception\n\n\n  * ■\n    **encoded**\n    more compactly. In 64-bit mode, there are sixteen 64-bit general-purpose registers.\n  * ■\n    **Segment:**\n    The six 16-bit segment registers contain segment selectors, which index into segment tables, as discussed in Chapter 8. The code segment (CS) register references the segment containing the instruction being executed. The stack segment (SS) register references the segment containing a user-visible stack. The remaining segment registers (DS, ES, FS, GS) enable the user to reference up to four separate data segments at a time.\n  * ■\n    **Flags:**\n    The 32-bit EFLAGS register contains condition codes and various mode bits. In 64-bit mode, this register is extended to 64 bits and referred to as RFLAGS. In the current architecture definition, the upper 32 bits of RFLAGS are unused.\n  * ■\n    **Instruction pointer:**\n    Contains the address of the current instruction.\n\n\nThere are also registers specifically devoted to the floating-point unit:\n\n\n  * ■\n    **Numeric:**\n    Each register holds an extended-precision 80-bit floating-point number. There are eight registers that function as a stack, with push and pop operations available in the instruction set.\n  * ■\n    **Control:**\n    The 16-bit control register contains bits that control the operation of the floating-point unit, including the type of rounding control; single, double, or extended precision; and bits to enable or disable various exception conditions.\n  * ■\n    **Status:**\n    The 16-bit status register contains bits that reflect the current state of the floating-point unit, including a 3-bit pointer to the top of the stack; condition codes reporting the outcome of the last operation; and exception flags.\n  * ■\n    **Tag word:**\n    This 16-bit register contains a 2-bit tag for each floating-point numeric register, which indicates the nature of the contents of the corresponding register. The four possible values are valid, zero, special (NaN, infinity, denormalized), and empty. These tags enable programs to check the contents of a numeric register without performing complex decoding of the actual data in the register. For example, when a context switch is made, the processor need not save any floating-point registers that are empty.\n\n\nThe use of most of the aforementioned registers is easily understood. Let us elaborate briefly on several of the registers.\n\n\n**EFLAGS REGISTER**\n   The EFLAGS register (Figure 14.22) indicates the condition of the processor and helps to control its operation. It includes the six condition codes defined in Table 12.9 (carry, parity, auxiliary, zero, sign, overflow), which report the results of an integer operation. In addition, there are bits in the register that may be referred to as control bits:\n\n\n  * ■\n    **Trap flag (TF):**\n    When set, causes an interrupt after the execution of each instruction. This is used for debugging.\n  * ■\n    **Interrupt enable flag (IF):**\n    When set, the processor will recognize external interrupts.\n\n\n\n31 | 30 | 29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\n0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | I | V | V | A | V | R | 0 | N | I | O | 0 | D | I | T | S | Z | 0 | A | 0 | P | 1 | C\n |  |  |  |  |  |  |  |  |  | D | I | I | C | M | F |  | T | P | P | F | F | F | F | F | F | F | F | F | F | F\n\n\nX ID = Identification flag\n\n\nX VIP = Virtual interrupt pending\n\n\nX VIF = Virtual interrupt flag\n\n\nX AC = Alignment check\n\n\nX VM = Virtual 8086 mode\n\n\nX RF = Resume flag\n\n\nX NT = Nested task flag\n\n\nX IOPL = I/O privilege level\n\n\nS OF = Overflow flag\n\n\nC DF = Direction flag\n\n\nX IF = Interrupt enable flag\n\n\nX TF = Trap flag\n\n\nS SF = Sign flag\n\n\nS ZF = Zero flag\n\n\nS AF = Auxiliary carry flag\n\n\nS PF = Parity flag\n\n\nS CF = Carry flag\n\n\nS indicates a status flag.\n\n\nC indicates a control flag.\n\n\nX indicates a system flag.\n\n\nShaded bits are reserved.\n\n\n**Figure 14.22**\n   x86 EFLAGS Register\n\n\n  * ■\n    **Direction flag (DF):**\n    Determines whether string processing instructions increment or decrement the 16-bit half-registers SI and DI (for 16-bit operations) or the 32-bit registers ESI and EDI (for 32-bit operations).\n  * ■\n    **I/O privilege flag (IOPL):**\n    When set, causes the processor to generate an exception on all accesses to I/O devices during protected-mode operation.\n  * ■\n    **Resume flag (RF):**\n    Allows the programmer to disable debug exceptions so that the instruction can be restarted after a debug exception without immediately causing another debug exception.\n  * ■\n    **Alignment check (AC):**\n    Activates if a word or doubleword is addressed on a nonword or nondoubleword boundary.\n  * ■\n    **Identification flag (ID):**\n    If this bit can be set and cleared, then this processor supports the processorID instruction. This instruction provides information about the vendor, family, and model.\n\n\nIn addition, there are 4 bits that relate to operating mode. The Nested Task (NT) flag indicates that the current task is nested within another task in protected-mode operation. The Virtual Mode (VM) bit allows the programmer to enable or disable virtual 8086 mode, which determines whether the processor runs as an 8086 machine. The Virtual Interrupt Flag (VIF) and Virtual Interrupt Pending (VIP) flag are used in a multitasking environment.\n\n\n**CONTROL REGISTERS**\n   The x86 employs four control registers (register CR1 is unused) to control various aspects of processor operation (Figure 14.23). All of the registers except CR0 are either 32 bits or 64 bits long, depending on whether the implementation supports the x86 64-bit architecture. The CR0 register contains system control flags, which control modes or indicate states that apply generally\n\n\n\n\n![Diagram of x86 Control Registers CR0, CR2, CR3 (PDDBR), and CR4. Each register is shown as a 32-bit field with bit numbers 31 down to 0. Shaded areas indicate reserved bits. CR0: bits 31-22, 20-17, 16-13, 11-9, 7-6, 4-3, 2-1 are reserved. CR2: bits 18-17, 16-13, 11-9, 7-6, 4-3, 2-1 are reserved. CR3 (PDDBR): bits 18-17, 16-13, 11-9, 7-6, 4-3, 2-1 are reserved. CR4: bits 18-17, 16-13, 11-9, 7-6, 4-3, 2-1 are reserved. Specific bits are labeled: OSXSAVE (bit 31), PCIDE (bit 30), FSGSBASE (bit 29), SMXE (bit 28), VMXE (bit 27), OSXMMEXCPT (bit 26), OSFXSR (bit 25), PCE (bit 24), PGE (bit 23), MCE (bit 22), PAE (bit 21), PSE (bit 20), DE (bit 19), TSD (bit 18), PVI (bit 17), AM (bit 16), WP (bit 15), PWT (bit 14), PCD (bit 13), CD (bit 12), NW (bit 11), NE (bit 10), ET (bit 9), TS (bit 8), EM (bit 7), MP (bit 6), PE (bit 5), NT (bit 4), TM (bit 3), and PM (bit 2).](images/image_0250.jpeg)\n\n\nDiagram of x86 Control Registers CR0, CR2, CR3 (PDDBR), and CR4. Each register is shown as a 32-bit field with bit numbers 31 down to 0. Shaded areas indicate reserved bits. CR0: bits 31-22, 20-17, 16-13, 11-9, 7-6, 4-3, 2-1 are reserved. CR2: bits 18-17, 16-13, 11-9, 7-6, 4-3, 2-1 are reserved. CR3 (PDDBR): bits 18-17, 16-13, 11-9, 7-6, 4-3, 2-1 are reserved. CR4: bits 18-17, 16-13, 11-9, 7-6, 4-3, 2-1 are reserved. Specific bits are labeled: OSXSAVE (bit 31), PCIDE (bit 30), FSGSBASE (bit 29), SMXE (bit 28), VMXE (bit 27), OSXMMEXCPT (bit 26), OSFXSR (bit 25), PCE (bit 24), PGE (bit 23), MCE (bit 22), PAE (bit 21), PSE (bit 20), DE (bit 19), TSD (bit 18), PVI (bit 17), AM (bit 16), WP (bit 15), PWT (bit 14), PCD (bit 13), CD (bit 12), NW (bit 11), NE (bit 10), ET (bit 9), TS (bit 8), EM (bit 7), MP (bit 6), PE (bit 5), NT (bit 4), TM (bit 3), and PM (bit 2).\n\n\nShaded area indicates reserved bits.\n\n\n\nOSXSAVE | = | XSAVE enable bit | VME | = | Virtual 8086 mode extensions\nPCIDE | = | Enables process-context identifiers | PCD | = | Page-level cache disable\nFSGSBASE | = | Enables segment base instructions | PWT | = | Page-level writes transparent\nSMXE | = | Enable safer mode extensions | PG | = | Paging\nVMXE | = | Enable virtual machine extensions | CD | = | Cache disable\nOSXMMEXCPT | = | Support unmasked SIMD FP exceptions | NW | = | Not write through\nOSFXSR | = | Support FXSAVE, FXSTOR | AM | = | Alignment mask\nPCE | = | Performance counter enable | WP | = | Write protect\nPGE | = | Page global enable | NE | = | Numeric error\nMCE | = | Machine check enable | ET | = | Extension type\nPAE | = | Physical address extension | TS | = | Task switched\nPSE | = | Page size extensions | EM | = | Emulation\nDE | = | Debug extensions | MP | = | Monitor coprocessor\nTSD | = | Time stamp disable | PE | = | Protection enable\nPVI | = | Protected mode virtual interrupt |  |  | \n\n\n**Figure 14.23**\n   x86 Control Registers\n\n\nto the processor rather than to the execution of an individual task. The flags are as follows:\n\n\n  * ■\n    **Protection Enable (PE):**\n    Enable/disable protected mode of operation.\n  * ■\n    **Monitor Coprocessor (MP):**\n    Only of interest when running programs from earlier machines on the x86; it relates to the presence of an arithmetic coprocessor.\n  * ■\n    **Emulation (EM):**\n    Set when the processor does not have a floating-point unit, and causes an interrupt when an attempt is made to execute floating-point instructions.\n  * ■\n    **Task Switched (TS):**\n    Indicates that the processor has switched tasks.\n  * ■\n    **Extension Type (ET):**\n    Not used on the Pentium and later machines; used to indicate support of math coprocessor instructions on earlier machines.\n\n\n  * ■\n    **Numeric Error (NE):**\n    Enables the standard mechanism for reporting floating-point errors on external bus lines.\n  * ■\n    **Write Protect (WP):**\n    When this bit is clear, read-only user-level pages can be written by a supervisor process. This feature is useful for supporting process creation in some operating systems.\n  * ■\n    **Alignment Mask (AM):**\n    Enables/disables alignment checking.\n  * ■\n    **Not Write Through (NW):**\n    Selects mode of operation of the data cache. When this bit is set, the data cache is inhibited from cache write-through operations.\n  * ■\n    **Cache Disable (CD):**\n    Enables/disables the internal cache fill mechanism.\n  * ■\n    **Paging (PG):**\n    Enables/disables paging.\n\n\nWhen paging is enabled, the CR2 and CR3 registers are valid. The CR2 register holds the 32-bit linear address of the last page accessed before a page fault interrupt. The leftmost 20 bits of CR3 hold the 20 most significant bits of the base address of the page directory; the remainder of the address contains zeros. Two bits of CR3 are used to drive pins that control the operation of an external cache. The page-level cache disable (PCD) enables or disables the external cache, and the page-level writes transparent (PWT) bit controls write through in the external cache. CR4 contains additional control bits.\n\n\n**MMX REGISTERS**\n   Recall from Section 10.3 that the x86 MMX capability makes use of several 64-bit data types. The MMX instructions make use of 3-bit register address fields, so that eight MMX registers are supported. In fact, the processor does not include specific MMX registers. Rather, the processor uses an aliasing technique (Figure 14.24). The existing floating-point registers are used to store MMX values. Specifically, the low-order 64 bits (mantissa) of each floating-point register are used to form the eight MMX registers. Thus, the older 32-bit x86 architecture is easily extended to support the MMX capability. Some key characteristics of the MMX use of these registers are as follows:\n\n\n  * ■ Recall that the floating-point registers are treated as a stack for floating-point operations. For MMX operations, these same registers are accessed directly.\n  * ■ The first time that an MMX instruction is executed after any floating-point operations, the FP tag word is marked valid. This reflects the change from stack operation to direct register addressing.\n  * ■ The EMMS (Empty MMX State) instruction sets bits of the FP tag word to indicate that all registers are empty. It is important that the programmer insert this instruction at the end of an MMX code block so that subsequent floating-point operations function properly.\n  * ■ When a value is written to an MMX register, bits [79:64] of the corresponding FP register (sign and exponent bits) are set to all ones. This sets the value in the FP register to NaN (not a number) or infinity when viewed as a floating-point value. This ensures that an MMX data value will not look like a valid floating-point value.\n\n\n\n\n![Diagram illustrating the mapping of MMX registers to Floating-Point registers. The diagram shows three main components: a vertical stack of 8 'Floating-point tag' registers (all containing '00'), a 2x8 grid of 'Floating-point registers' (with the top row containing '79' and '63', and the bottom row containing '0'), and a vertical stack of 8 'MMX registers' (labeled MM7 through MM0). Dashed lines indicate the mapping: the top two MMX registers (MM7 and MM6) map to the top two floating-point registers (79 and 63), and the bottom six MMX registers (MM0 through MM5) map to the bottom six floating-point registers (0).](images/image_0251.jpeg)\n\n\nDiagram illustrating the mapping of MMX registers to Floating-Point registers. The diagram shows three main components: a vertical stack of 8 'Floating-point tag' registers (all containing '00'), a 2x8 grid of 'Floating-point registers' (with the top row containing '79' and '63', and the bottom row containing '0'), and a vertical stack of 8 'MMX registers' (labeled MM7 through MM0). Dashed lines indicate the mapping: the top two MMX registers (MM7 and MM6) map to the top two floating-point registers (79 and 63), and the bottom six MMX registers (MM0 through MM5) map to the bottom six floating-point registers (0).\n\n\n**Figure 14.24**\n   Mapping of MMX Registers to Floating-Point Registers\n\n\n\n\n**Interrupt Processing**\n\n\nInterrupt processing within a processor is a facility provided to support the operating system. It allows an application program to be suspended, in order that a variety of interrupt conditions can be serviced and later resumed.\n\n\n**INTERRUPTS AND EXCEPTIONS**\n   Two classes of events cause the x86 to suspend execution of the current instruction stream and respond to the event: interrupts and exceptions. In both cases, the processor saves the context of the current process and transfers to a predefined routine to service the condition. An\n   *interrupt*\n   is generated by a signal from hardware, and it may occur at random times during the execution of a program. An\n   *exception*\n   is generated from software, and it is provoked by the execution of an instruction. There are two sources of interrupts and two sources of exceptions:\n\n\n\n\n**1. Interrupts**\n\n\n  * ■\n    **Maskable interrupts:**\n    Received on the processor's INTR pin. The processor does not recognize a maskable interrupt unless the interrupt enable flag (IF) is set.\n  * ■\n    **Nonmaskable interrupts:**\n    Received on the processor's NMI pin. Recognition of such interrupts cannot be prevented.\n\n\n\n\n**2. Exceptions**\n\n\n  * ■\n    **Processor-detected exceptions:**\n    Results when the processor encounters an error while attempting to execute an instruction.\n\n\n  * ■\n    **Programmed exceptions:**\n    These are instructions that generate an exception (e.g., INTO, INT3, INT, and BOUND).\n\n\n**INTERRUPT VECTOR TABLE**\n   Interrupt processing on the x86 uses the interrupt vector table. Every type of interrupt is assigned a number, and this number is used to index into the interrupt vector table. This table contains 256 32-bit interrupt vectors, which is the address (segment and offset) of the interrupt service routine for that interrupt number.\n\n\nTable 14.3 shows the assignment of numbers in the interrupt vector table; shaded entries represent interrupts, while nonshaded entries are exceptions. The NMI hardware interrupt is type 2. INTR hardware interrupts are assigned numbers in the range of 32 to 255; when an INTR interrupt is generated, it must be accompanied on the bus with the interrupt vector number for this interrupt. The remaining vector numbers are used for exceptions.\n\n\nIf more than one exception or interrupt is pending, the processor services them in a predictable order. The location of vector numbers within the table does not reflect priority. Instead, priority among exceptions and interrupts is organized into five classes. In descending order of priority, these are\n\n\n  * ■\n    **Class 1:**\n    Traps on the previous instruction (vector number 1)\n  * ■\n    **Class 2:**\n    External interrupts (2, 32–255)\n  * ■\n    **Class 3:**\n    Faults from fetching next instruction (3, 14)\n  * ■\n    **Class 4:**\n    Faults from decoding the next instruction (6, 7)\n  * ■\n    **Class 5:**\n    Faults on executing an instruction (0, 4, 5, 8, 10–14, 16, 17)\n\n\n**INTERRUPT HANDLING**\n   Just as with a transfer of execution using a CALL instruction, a transfer to an interrupt-handling routine uses the system stack to store the processor state. When an interrupt occurs and is recognized by the processor, a sequence of events takes place:\n\n\n  * 1. If the transfer involves a change of privilege level, then the current stack segment register and the current extended stack pointer (ESP) register are pushed onto the stack.\n  * 2. The current value of the EFLAGS register is pushed onto the stack.\n  * 3. Both the interrupt (IF) and trap (TF) flags are cleared. This disables INTR interrupts and the trap or single-step feature.\n  * 4. The current code segment (CS) pointer and the current instruction pointer (IP or EIP) are pushed onto the stack.\n  * 5. If the interrupt is accompanied by an error code, then the error code is pushed onto the stack.\n  * 6. The interrupt vector contents are fetched and loaded into the CS and IP or EIP registers. Execution continues from the interrupt service routine.\n\n\nTo return from an interrupt, the interrupt service routine executes an IRET instruction. This causes all of the values saved on the stack to be restored; execution resumes from the point of the interrupt.\n\n\n**Table 14.3**\n\nVector Number | Description\n0 | Divide error; division overflow or division by zero\n1 | Debug exception; includes various faults and traps related to debugging\n2 | NMI pin interrupt; signal on NMI pin\n3 | Breakpoint; caused by INT 3 instruction, which is a 1-byte instruction useful for debugging\n4 | INTO-detected overflow; occurs when the processor executes INTO with the OF flag set\n5 | BOUND range exceeded; the BOUND instruction compares a register with boundaries stored in memory and generates an interrupt if the contents of the register is out of bounds\n6 | Undefined opcode\n7 | Device not available; attempt to use ESC or WAIT instruction fails due to lack of external device\n8 | Double fault; two interrupts occur during the same instruction and cannot be handled serially\n9 | Reserved\n10 | Invalid task state segment; segment describing a requested task is not initialized or not valid\n11 | Segment not present; required segment not present\n12 | Stack fault; limit of stack segment exceeded or stack segment not present\n13 | General protection; protection violation that does not cause another exception (e.g., writing to a read-only segment)\n14 | Page fault\n15 | Reserved\n16 | Floating-point error; generated by a floating-point arithmetic instruction\n17 | Alignment check; access to a word stored at an odd byte address or a doubleword stored at an address not a multiple of 4\n18 | Machine check; model specific\n19–31 | Reserved\n32–255 | User interrupt vectors; provided when INTR signal is activated\n\n\nUnshaded: exceptions\n\n\nShaded: interrupts"
        },
        {
          "name": "The ARM Processor",
          "content": "In this section, we look at some of the key elements of the ARM architecture and organization. We defer a discussion of more complex aspects of organization and pipelining until Chapter 16. For the discussion in this section and in Chapter 16, it is useful to keep in mind key characteristics of the ARM architecture. ARM is primarily a RISC system with the following notable attributes:\n\n\n  * ■ A moderate array of uniform registers, more than are found on some CISC systems but fewer than are found on many RISC systems.\n  * ■ A load/store model of data processing, in which operations only perform on operands in registers and not directly in memory. All data must be loaded into registers before an operation can be performed; the result can then be used for further processing or stored into memory.\n  * ■ A uniform fixed-length instruction of 32 bits for the standard set and 16 bits for the Thumb instruction set.\n  * ■ To make each data processing instruction more flexible, either a shift or rotation can preprocess one of the source registers. To efficiently support this feature, there are separate arithmetic logic unit (ALU) and shifter units.\n  * ■ A small number of addressing modes with all load/store addressees determined from registers and instruction fields. Indirect or indexed addressing involving values in memory are not used.\n  * ■ Auto-increment and auto-decrement addressing modes are used to improve the operation of program loops.\n  * ■ Conditional execution of instructions minimizes the need for conditional branch instructions, thereby improving pipeline efficiency, because pipeline flushing is reduced.\n\n\n\n\n**Processor Organization**\n\n\nThe ARM processor organization varies substantially from one implementation to the next, particularly when based on different versions of the ARM architecture. However, it is useful for the discussion in this section to present a simplified, generic ARM organization, which is illustrated in Figure 14.25. In this figure, the arrows indicate the flow of data. Each box represents a functional hardware unit or a storage unit.\n\n\nData are exchanged with the processor from external memory through a data bus. The value transferred is either a data item, as a result of a load or store instruction, or an instruction fetch. Fetched instructions pass through an instruction decoder before execution, under control of a control unit. The latter includes pipeline logic and provides control signals (not shown) to all the hardware elements of the processor. Data items are placed in the register file, consisting of a set of 32-bit registers. Byte or halfword items treated as twos-complement numbers are sign-extended to 32 bits.\n\n\nARM data processing instructions typically have two source registers,\n   *Rn*\n   and\n   *Rm*\n   , and a single result or destination register,\n   *Rd*\n   . The source register values feed into the ALU or a separate multiply unit that makes use of an additional register to accumulate partial results. The ARM processor also includes a hardware unit that can shift or rotate the\n   *Rm*\n   value before it enters the ALU. This shift or rotate occurs within the cycle time of the instruction and increases the power and flexibility of many data processing operations.\n\n\nThe results of an operation are fed back to the destination register. Load/store instructions may also use the output of the arithmetic units to generate the memory address for a load or store.\n\n\n\n\n![Simplified ARM Organization block diagram](images/image_0252.jpeg)\n\n\nThe diagram illustrates the internal structure of an ARM processor. At the top, 'External memory (cache, main memory)' is shown. Below it, a 'Memory address register' and a 'Memory buffer register' are connected to the external memory. The 'Memory address register' has a bidirectional connection with the 'User Register File (R0-R15)' and a unidirectional connection to an 'Incrementer' block. The 'Memory buffer register' is connected to a 'Sign extend' block. The 'User Register File' contains registers R0 through R15. Register R15 is labeled 'R15 (PC)'. The 'User Register File' has a bidirectional connection with the 'Memory address register' and a unidirectional connection to the 'Memory buffer register'. It also has a bidirectional connection with the 'Instruction register'. The 'User Register File' provides inputs\n    *Rd*\n    ,\n    *Rn*\n    , and\n    *Rm*\n    to the 'ALU' and 'Multiply/accumulate' blocks. The 'ALU' block has a bidirectional connection with the 'User Register File' and a bidirectional connection with the 'Multiply/accumulate' block. The 'Multiply/accumulate' block has a bidirectional connection with the 'User Register File'. The 'Instruction register' is connected to an 'Instruction decoder', which is connected to a 'Control unit'. The 'Control unit' contains a 'CPSR' (Current Program Status Register) and has a bidirectional connection with the 'User Register File' and the 'Multiply/accumulate' block. A 'Barrel shifter' block is connected to the 'ALU' and receives inputs\n    *Rn*\n    and\n    *Rm*\n    from the 'User Register File'.\n\n\nSimplified ARM Organization block diagram\n\n\n**Figure 14.25**\n   Simplified ARM Organization\n\n\n\n\n**Processor Modes**\n\n\nIt is quite common for a processor to support only a small number of processor modes. For example, many operating systems make use of just two modes: a user mode and a kernel mode, with the latter mode used to execute privileged system software. In contrast, the ARM architecture provides a flexible foundation for operating systems to enforce a variety of protection policies.\n\n\nThe ARM architecture supports seven execution modes. Most application programs execute in\n   **user mode**\n   . While the processor is in user mode, the program being executed is unable to access protected system resources or to change mode, other than by causing an exception to occur.\n\n\nThe remaining six execution modes are referred to as privileged modes. These modes are used to run system software. There are two principal advantages to defining so many different privileged modes: (1) The OS can tailor the use of system software to a variety of circumstances, and (2) certain registers are dedicated for use for each of the privileged modes, allowing swifter changes in context.\n\n\nThe exception modes have full access to system resources and can change modes freely. Five of these modes are known as exception modes. These are entered when specific exceptions occur. Each of these modes has some dedicated registers that substitute for some of the user mode registers, and which are used to avoid corrupting User mode state information when the exception occurs. The exception modes are as follows:\n\n\n  * ■\n    **Supervisor mode:**\n    Usually what the OS runs in. It is entered when the processor encounters a software interrupt instruction. Software interrupts are a standard way to invoke operating system services on ARM.\n  * ■\n    **Abort mode:**\n    Entered in response to memory faults.\n  * ■\n    **Undefined mode:**\n    Entered when the processor attempts to execute an instruction that is supported neither by the main integer core nor by one of the coprocessors.\n  * ■\n    **Fast interrupt mode:**\n    Entered whenever the processor receives an interrupt signal from the designated fast interrupt source. A fast interrupt cannot be interrupted, but a fast interrupt may interrupt a normal interrupt.\n  * ■\n    **Interrupt mode:**\n    Entered whenever the processor receives an interrupt signal from any other interrupt source (other than fast interrupt). An interrupt may only be interrupted by a fast interrupt.\n\n\nThe remaining privileged mode is the\n   **System mode**\n   . This mode is not entered by any exception and uses the same registers available in User mode. The System mode is used for running certain privileged operating system tasks. System mode tasks may be interrupted by any of the five exception categories.\n\n\n\n\n**Register Organization**\n\n\nFigure 14.26 depicts the user-visible registers for the ARM. The ARM processor has a total of 37 32-bit registers, classified as follows:\n\n\n  * ■ Thirty-one registers referred to in the ARM manual as general-purpose registers. In fact, some of these, such as the program counters, have special purposes.\n  * ■ Six program status registers.\n\n\nRegisters are arranged in partially overlapping banks, with the current processor mode determining which bank is available. At any time, sixteen numbered registers and one or two program status registers are visible, for a total of 17 or 18 software-visible registers. Figure 14.26 is interpreted as follows:\n\n\n  * ■ Registers R0 through R7, register R15 (the program counter) and the current program status register (CPSR) are visible in and shared by all modes.\n  * ■ Registers R8 through R12 are shared by all modes except fast interrupt, which has its own dedicated registers R8_fiq through R12_fiq.\n  * ■ All the exception modes have their own versions of registers R13 and R14.\n  * ■ All the exception modes have a dedicated saved program status register (SPSR).\n\n\n\nModes\n | Privileged modes\n | Exception modes\nUser | System | Supervisor | Abort | Undefined | Interrupt | Fast interrupt\nR0 | R0 | R0 | R0 | R0 | R0 | R0\nR1 | R1 | R1 | R1 | R1 | R1 | R1\nR2 | R2 | R2 | R2 | R2 | R2 | R2\nR3 | R3 | R3 | R3 | R3 | R3 | R3\nR4 | R4 | R4 | R4 | R4 | R4 | R4\nR5 | R5 | R5 | R5 | R5 | R5 | R5\nR6 | R6 | R6 | R6 | R6 | R6 | R6\nR7 | R7 | R7 | R7 | R7 | R7 | R7\nR8 | R8 | R8 | R8 | R8 | R8 | R8_fiq\nR9 | R9 | R9 | R9 | R9 | R9 | R9_fiq\nR10 | R10 | R10 | R10 | R10 | R10 | R10_fiq\nR11 | R11 | R11 | R11 | R11 | R11 | R11_fiq\nR12 | R12 | R12 | R12 | R12 | R12 | R12_fiq\nR13(SP) | R13(SP) | R13_svc | R13_abt | R13_und | R13_irq | R13_fiq\nR14(LR) | R14(LR) | R14_svc | R14_abt | R14_und | R14_irq | R14_fiq\nR15(PC) | R15(PC) | R15(PC) | R15(PC) | R15(PC) | R15(PC) | R15(PC)\n\n\n\nCPSR | CPSR | CPSR | CPSR | CPSR | CPSR | CPSR\n |  | SPSR_svc | SPSR_abt | SPSR_und | SPSR_irq | SPSR_fiq\n\n\nShading indicates that the normal register used by User or System mode has been replaced by an alternative register specific to the exception mode.\n\n\nSP = stack pointer\n\n\nCPSR = current program status register\n\n\nLR = link register\n\n\nSPSR = saved program status register\n\n\nPC = program counter\n\n\n**Figure 14.26**\n   ARM Register Organization\n\n\n**GENERAL-PURPOSE REGISTERS**\n   Register R13 is normally used as a stack pointer and is also known as the SP. Because each exception mode has a separate R13, each exception mode can have its own dedicated program stack. R14 is known as the link register (LR) and is used to hold subroutine return addresses and exception mode returns. Register R15 is the program counter (PC).\n\n\n**PROGRAM STATUS REGISTERS**\n   The CPSR is accessible in all processor modes. Each exception mode also has a dedicated SPSR that is used to preserve the value of the CPSR when the associated exception occurs.\n\n\nThe 16 most significant bits of the CPSR contain user flags visible in User mode, and which can be used to affect the operation of a program (Figure 14.27). These are as follows:\n\n\n  * ■\n    **Condition code flags:**\n    The N, Z, C, and V flags, which are discussed in Chapter 12.\n  * ■\n    **Q flag:**\n    used to indicate whether overflow and/or saturation has occurred in some SIMD-oriented instructions.\n  * ■\n    **J bit:**\n    indicates the use of special 8-bit instructions, known as Jazelle instructions, which are beyond the scope of our discussion.\n  * ■\n    **GE[3:0] bits:**\n    SIMD instructions use bits [19:16] as Greater than or Equal (GE) flags for individual bytes or halfwords of the result.\n\n\nThe 16 least significant bits of the CPSR contain system control flags that can only be altered when the processor is in a privileged mode. The fields are as follows:\n\n\n  * ■\n    **E bit:**\n    Controls load and store endianness for data; ignored for instruction fetches.\n  * ■\n    **Interrupt disable bits:**\n    The A bit disables imprecise data aborts when set; the I bit disables IRQ interrupts when set; and the F bit disables FIQ interrupts when set.\n  * ■\n    **T bit:**\n    Indicates whether instructions should be interpreted as normal ARM instructions or Thumb instructions.\n  * ■\n    **Mode bits:**\n    Indicates the processor mode.\n\n\n\n\n**Interrupt Processing**\n\n\nAs with any processor, the ARM includes a facility that enables the processor to interrupt the currently executing program to deal with exception conditions. Exceptions are generated by internal and external sources to cause the processor to handle an event. The processor state just before handling the exception is normally preserved so that the original program can be resumed when the exception routine has completed. More than one exception can arise at the same time. The ARM architecture supports seven types of exceptions. Table 14.4 lists the types of exception and the processor mode that is used to process each type. When an exception occurs, execution is forced from a fixed memory address corresponding to the type of exception. These fixed addresses are called the exception vectors.\n\n\nIf more than one interrupt is outstanding, they are handled in priority order. Table 14.4 lists the exceptions in priority order, highest to lowest.\n\n\nWhen an exception occurs, the processor halts execution after the current instruction. The state of the processor is preserved in the SPSR that corresponds to\n\n\n\n\n![Diagram showing the 32-bit ARM CPSR register format. The top row shows bit positions from 31 to 0. The second row shows the fields: N, Z, C, V, Q (bits 31-28), Res, J (bits 27-26), Reserved (bits 25-24), GE[3:0] (bits 23-20), Reserved (bits 19-16), E, A, I, F, T (bits 15-12), and M[4:0] (bits 11-0). Brackets below the table group the first 16 bits as 'User flags' and the last 16 bits as 'System control flags'.](images/image_0253.jpeg)\n\n\n31 | 30 | 29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nN | Z | C | V | Q | Res | J | Reserved | GE[3:0] | Reserved | E | A | I | F | T | M[4:0]\n\n\nUser flags\n    \n\n     System control flags\n\n\nDiagram showing the 32-bit ARM CPSR register format. The top row shows bit positions from 31 to 0. The second row shows the fields: N, Z, C, V, Q (bits 31-28), Res, J (bits 27-26), Reserved (bits 25-24), GE[3:0] (bits 23-20), Reserved (bits 19-16), E, A, I, F, T (bits 15-12), and M[4:0] (bits 11-0). Brackets below the table group the first 16 bits as 'User flags' and the last 16 bits as 'System control flags'.\n\n\nFigure 14.27 Format of ARM CPSR and SPSR\n\n\n**Table 14.4**\n\nException type | Mode | Normal entry address | Description\nReset | Supervisor | 0x00000000 | Occurs when the system is initialized.\nData abort | Abort | 0x00000010 | Occurs when an invalid memory address has been accessed, such as if there is no physical memory for an address or the correct access permission is lacking.\nFIQ (fast interrupt) | FIQ | 0x0000001C | Occurs when an external device asserts the FIQ pin on the processor. An interrupt cannot be interrupted except by an FIQ. FIQ is designed to support a data transfer or channel process, and has sufficient private registers to remove the need for register saving in such applications, therefore minimizing the overhead of context switching. A fast interrupt cannot be interrupted.\nIRQ (interrupt) | IRQ | 0x00000018 | Occurs when an external device asserts the IRQ pin on the processor. An interrupt cannot be interrupted except by an FIQ.\nPrefetch abort | Abort | 0x0000000C | Occurs when an attempt to fetch an instruction results in a memory fault. The exception is raised when the instruction enters the execute stage of the pipeline.\nUndefined instructions | Undefined | 0x00000004 | Occurs when an instruction not in the instruction set reaches the execute stage of the pipeline.\nSoftware interrupt | Supervisor | 0x00000008 | Generally used to allow user mode programs to call the OS. The user program executes a SWI instruction with an argument that identifies the function the user wishes to perform.\n\n\nthe type of exception, so that the original program can be resumed when the exception routine has completed. The address of the instruction the processor was just about to execute is placed in the link register of the appropriate processor mode. To return after handling the exception, the SPSR is moved into the CPSR and R14 is moved into the PC."
        }
      ]
    },
    {
      "name": "Reduced Instruction Set Computers",
      "sections": [
        {
          "name": "Instruction Execution Characteristics",
          "content": "One of the most visible forms of evolution associated with computers is that of programming languages. As the cost of hardware has dropped, the relative cost of software has risen. Along with that, a chronic shortage of programmers has driven up software costs in absolute terms. Thus, the major cost in the life cycle of a system is software, not hardware. Adding to the cost, and to the inconvenience, is the element of unreliability: it is common for programs, both system and application, to continue to exhibit new bugs after years of operation.\n\n\nThe response from researchers and industry has been to develop ever more powerful and complex high-level programming languages. These\n   **high-level languages (HLLs)**\n   : (1) allow the programmer to express algorithms more concisely; (2) allow the compiler to take care of details that are not important in the programmer's expression of algorithms; and (3) often support naturally the use of structured programming and/or object-oriented design.\n\n\nAlas, this solution gave rise to a perceived problem, known as the\n   *semantic gap*\n   , the difference between the operations provided in HLLs and those provided in computer architecture. Symptoms of this gap are alleged to include execution inefficiency, excessive machine program size, and compiler complexity. Designers responded with architectures intended to close this gap. Key features include large instruction sets, dozens of addressing modes, and various HLL statements implemented in hardware. An example of the latter is the CASE machine instruction on the VAX. Such complex instruction sets are intended to:\n\n\n  * ■ Ease the task of the compiler writer.\n  * ■ Improve execution efficiency, because complex sequences of operations can be implemented in microcode.\n  * ■ Provide support for even more complex and sophisticated HLLs.\n\n\nMeanwhile, a number of studies have been done over the years to determine the characteristics and patterns of execution of machine instructions generated from HLL programs. The results of these studies inspired some researchers to look\n\n\n**Table 15.1**\n\nCharacteristic | Complex Instruction Set (CISC) Computer | Reduced Instruction Set (RISC) Computer\nIBM 370/168 | VAX 11/780 | Intel 80486 | SPARC | MIPS R4000\nYear developed | 1973 | 1978 | 1989 | 1987 | 1991\nNumber of instructions | 208 | 303 | 235 | 69 | 94\nInstruction size (bytes) | 2–6 | 2–57 | 1–11 | 4 | 4\nAddressing modes | 4 | 22 | 11 | 1 | 1\nNumber of general-purpose registers | 16 | 16 | 8 | 40–520 | 32\nControl memory size (kbits) | 420 | 480 | 246 | — | —\nCache size (kB) | 64 | 64 | 8 | 32 | 128\n\n\n\nCharacteristic | Superscalar\nPowerPC | Ultra SPARC | MIPS R10000\nYear developed | 1993 | 1996 | 1996\nNumber of instructions | 225 |  | \nInstruction size (bytes) | 4 | 4 | 4\nAddressing modes | 2 | 1 | 1\nNumber of general-purpose registers | 32 | 40–520 | 32\nControl memory size (kbits) | — | — | —\nCache size (kB) | 16–32 | 32 | 64\n\n\nfor a different approach: namely, to make the architecture that supports the HLL simpler, rather than more complex.\n\n\nTo understand the line of reasoning of the RISC advocates, we begin with a brief review of instruction execution characteristics. The aspects of computation of interest are as follows:\n\n\n  * ■\n    **Operations performed:**\n    These determine the functions to be performed by the processor and its interaction with memory.\n  * ■\n    **Operands used:**\n    The types of operands and the frequency of their use determine the memory organization for storing them and the addressing modes for accessing them.\n  * ■\n    **Execution sequencing:**\n    This determines the control and pipeline organization.\n\n\nIn the remainder of this section, we summarize the results of a number of studies of high-level-language programs. All of the results are based on dynamic measurements. That is, measurements are collected by executing the program and counting the number of times some feature has appeared or a particular property has held true. In contrast, static measurements merely perform these counts on the source text of a program. They give no useful information on performance, because they are not weighted relative to the number of times each statement is executed.\n\n\n\n\n**Operations**\n\n\nA variety of studies have been made to analyze the behavior of HLL programs. Table 4.7, discussed in Chapter 4, includes key results from a number of studies. There is quite good agreement in the results of this mixture of languages and applications. Assignment statements predominate, suggesting that the simple movement of data is of high importance. There is also a preponderance of conditional statements (IF, LOOP). These statements are implemented in machine language with some sort of compare and branch instruction. This suggests that the sequence control mechanism of the instruction set is important.\n\n\nThese results are instructive to the machine instruction set designer, indicating which types of statements occur most often and therefore should be supported in an “optimal” fashion. However, these results do not reveal which statements use the most time in the execution of a typical program. That is, we want to answer the question: Given a compiled machine-language program, which statements in the source language cause the execution of the most machine-language instructions and what is the execution time of these instructions?\n\n\nTo get at this underlying phenomenon, the Patterson programs [PATT82a], described in Appendix 4A, were compiled on the VAX, PDP-11, and Motorola 68000 to determine the average number of machine instructions and memory references per statement type. The second and third columns in Table 15.2 show the relative frequency of occurrence of various HLL statements in a variety of programs; the data were obtained by observing the occurrences in running programs rather than just the number of times that statements occur in the source code. Hence these metrics capture dynamic behavior. To obtain the data in columns four and five (machine-instruction weighted), each value in the second and third columns is multiplied by the number of machine instructions produced by the compiler. These results are then normalized so that columns four and five show the relative frequency of occurrence, weighted by the number of machine instructions per HLL statement. Similarly, the sixth and seventh columns are obtained by multiplying the frequency of occurrence of each statement type by the relative number of memory references caused by each statement. The data in columns four through seven provide surrogate measures of the actual time spent executing the various statement types. The results suggest that the procedure call/return is the most time-consuming operation in typical HLL programs.\n\n\nThe reader should be clear on the significance of Table 15.2. This table indicates the relative performance impact of various statement types in an HLL, when\n\n\n**Table 15.2**\n\n | Dynamic Occurrence | Machine-Instruction Weighted | Memory-Reference Weighted\nPascal | C | Pascal | C | Pascal | C\nASSIGN | 45% | 38% | 13% | 13% | 14% | 15%\nLOOP | 5% | 3% | 42% | 32% | 33% | 26%\nCALL | 15% | 12% | 31% | 33% | 44% | 45%\nIF | 29% | 43% | 11% | 21% | 7% | 13%\nGOTO | — | 3% | — | — | — | —\nOTHER | 6% | 1% | 3% | 1% | 2% | 1%\n\n\nthat HLL is compiled for a typical contemporary instruction set architecture. Some other architecture could conceivably produce different results. However, this study produces results that are representative for contemporary\n   **complex instruction set computer (CISC)**\n   architectures. Thus, they can provide guidance to those looking for more efficient ways to support HLLs.\n\n\n\n\n**Operands**\n\n\nMuch less work has been done on the occurrence of types of operands, despite the importance of this topic. There are several aspects that are significant.\n\n\nThe Patterson study already referenced [PATT82a] also looked at the dynamic frequency of occurrence of classes of variables (Table 15.3). The results, consistent between Pascal and C programs, show that most references are to simple scalar variables. Further, more than 80% of the scalars were local (to the procedure) variables. In addition, each reference to an array or a structure requires a reference to an index or pointer, which again is usually a local scalar. Thus, there is a preponderance of references to scalars, and these are highly localized.\n\n\nThe Patterson study examined the dynamic behavior of HLL programs, independent of the underlying architecture. As discussed before, it is necessary to deal with actual architectures to examine program behavior more deeply. One study, [LUND77], examined DEC-10 instructions dynamically and found that each instruction on the average references 0.5 operand in memory and 1.4 registers. Similar results are reported in [HUCK83] for C, Pascal, and FORTRAN programs on S/370, PDP-11, and VAX. Of course, these figures depend highly on both the architecture and the compiler, but they do illustrate the frequency of operand accessing.\n\n\n**Table 15.3**\n\n | Pascal | C | Average\nInteger constant | 16% | 23% | 20%\nScalar variable | 58% | 53% | 55%\nArray/Structure | 26% | 24% | 25%\n\n\nThese latter studies suggest the importance of an architecture that lends itself to fast operand accessing, because this operation is performed so frequently. The Patterson study suggests that a prime candidate for optimization is the mechanism for storing and accessing local scalar variables.\n\n\n\n\n**Procedure Calls**\n\n\nWe have seen that procedure calls and returns are an important aspect of HLL programs. The evidence (Table 15.2) suggests that these are the most time-consuming operations in compiled HLL programs. Thus, it will be profitable to consider ways of implementing these operations efficiently. Two aspects are significant: the number of parameters and variables that a procedure deals with, and the depth of nesting.\n\n\nTanenbaum's study [TANE78] found that 98% of dynamically called procedures were passed fewer than six arguments and that 92% of them used fewer than six local scalar variables. Similar results were reported by the Berkeley RISC team [KATE83], as shown in Table 15.4. These results show that the number of words required per procedure activation is not large. The studies reported earlier indicated that a high proportion of operand references is to local scalar variables. These studies show that those references are in fact confined to relatively few variables.\n\n\nThe same Berkeley group also looked at the pattern of procedure calls and returns in HLL programs. They found that it is rare to have a long uninterrupted sequence of procedure calls followed by the corresponding sequence of returns. Rather, they found that a program remains confined to a rather narrow window of procedure-invocation depth. This is illustrated in Figure 4.21, which was discussed in Chapter 4. These results reinforce the conclusion that operand references are highly localized.\n\n\n\n\n**Implications**\n\n\nA number of groups have looked at results such as those just reported and have concluded that the attempt to make the instruction set architecture close to HLLs is not the most effective design strategy. Rather, the HLLs can best be supported by optimizing performance of the most time-consuming features of typical HLL programs.\n\n\n**Table 15.4**\n   Procedure Arguments and Local Scalar Variables\n\n\n\nPercentage of Executed Procedure Calls With | Compiler, Interpreter, and Typesetter | Small Nonnumeric Programs\n> 3 arguments | 0–7% | 0–5%\n> 5 arguments | 0–3% | 0%\n> 8 words of arguments and local scalars | 1–20% | 0–6%\n> 12 words of arguments and local scalars | 1–6% | 0–3%\n\n\nGeneralizing from the work of a number of researchers, three elements emerge that, by and large, characterize RISC architectures. First, use a large number of registers or use a compiler to optimize register usage. This is intended to optimize operand referencing. The studies just discussed show that there are several references per HLL statement and that there is a high proportion of move (assignment) statements. This, coupled with the locality and predominance of scalar references, suggests that performance can be improved by reducing memory references at the expense of more register references. Because of the locality of these references, an expanded register set seems practical.\n\n\nSecond, careful attention needs to be paid to the design of instruction pipelines. Because of the high proportion of conditional branch and procedure call instructions, a straightforward instruction pipeline will be inefficient. This manifests itself as a high proportion of instructions that are prefetched but never executed.\n\n\nFinally, an instruction set consisting of high-performance primitives is indicated. Instructions should have predictable costs (measured in execution time, code size, and increasingly, in energy dissipation) and be consistent with a high-performance implementation (which harmonizes with predictable execution-time cost)."
        },
        {
          "name": "The Use of a Large Register File",
          "content": "The results summarized in Section 15.1 point out the desirability of quick access to operands. We have seen that there is a large proportion of assignment statements in HLL programs, and many of these are of the simple form\n   \n    A \\leftarrow B\n   \n   . Also, there is a significant number of operand accesses per HLL statement. If we couple these results with the fact that most accesses are to local scalars, heavy reliance on register storage is suggested.\n\n\nThe reason that register storage is indicated is that it is the fastest available storage device, faster than both main memory and cache. The register file is physically small, on the same chip as the ALU and control unit, and employs much shorter addresses than addresses for cache and memory. Thus, a strategy is needed that will allow the most frequently accessed operands to be kept in registers and to minimize register-memory operations.\n\n\nTwo basic approaches are possible, one based on software and the other on hardware. The software approach is to rely on the compiler to maximize register usage. The compiler will attempt to assign registers to those variables that will be used the most in a given time period. This approach requires the use of sophisticated program-analysis algorithms. The hardware approach is simply to use more registers so that more variables can be held in registers for longer periods of time.\n\n\nIn this section, we will discuss the hardware approach. This approach has been pioneered by the Berkeley RISC group [PATT82a]; was used in the first commercial RISC product, the Pyramid [RAGA83]; and is currently used in the popular\n   **SPARC**\n   architecture.\n\n\n\n\n**Register Windows**\n\n\nOn the face of it, the use of a large set of registers should decrease the need to access memory. The design task is to organize the registers in such a fashion that this goal is realized.\n\n\nBecause most operand references are to local scalars, the obvious approach is to store these in registers, with perhaps a few registers reserved for global variables. The problem is that the definition of\n   *local*\n   changes with each procedure call and return, operations that occur frequently. On every call, local variables must be saved from the registers into memory, so that the registers can be reused by the called procedure. Furthermore, parameters must be passed. On return, the variables of the calling procedure must be restored (loaded back into registers) and results must be passed back to the calling procedure.\n\n\nThe solution is based on two other results reported in Section 15.1. First, a typical procedure employs only a few passed parameters and local variables (Table 15.4). Second, the depth of procedure activation fluctuates within a relatively narrow range (Figure 4.21). To exploit these properties, multiple small sets of registers are used, each assigned to a different procedure. A procedure call automatically switches the processor to use a different fixed-size window of registers, rather than saving registers in memory. Windows for adjacent procedures are overlapped to allow parameter passing.\n\n\nThe concept is illustrated in Figure 15.1. At any time, only one window of registers is visible and is addressable as if it were the only set of registers (e.g., addresses 0 through\n   \n    N - 1\n   \n   ). The window is divided into three fixed-size areas. Parameter registers hold parameters passed down from the procedure that called the current procedure and hold results to be passed back up. Local registers are used for local variables, as assigned by the compiler. Temporary registers are used to exchange parameters and results with the next lower level (procedure called by current procedure). The temporary registers at one level are physically the same as the parameter registers at the next lower level. This overlap permits parameters to be passed without the actual movement of data. Keep in mind that, except for the overlap, the registers at two different levels are physically distinct. That is, the parameter and local registers at level\n   \n    J\n   \n   are disjoint from the local and temporary registers at level\n   \n    J + 1\n   \n   .\n\n\nTo handle any possible pattern of calls and returns, the number of\n   **register windows**\n   would have to be unbounded. Instead, the register windows can be used to hold the few most recent procedure activations. Older activations must be saved\n\n\n\n\n![Diagram illustrating Overlapping Register Windows. It shows two levels of register windows. Level J consists of three boxes: 'Parameter registers', 'Local registers', and 'Temporary registers'. Level J+1 consists of three boxes: 'Parameter registers', 'Local registers', and 'Temporary registers'. A bracket labeled 'Call/return' indicates the overlap between the 'Temporary registers' of Level J and the 'Parameter registers' of Level J+1.](images/image_0254.jpeg)\n\n\nThe diagram illustrates the overlapping nature of register windows across two levels,\n    \n     J\n    \n    and\n    \n     J+1\n    \n    . Each level is represented by a row of three boxes: 'Parameter registers', 'Local registers', and 'Temporary registers'. A bracket labeled 'Call/return' spans the 'Temporary registers' of Level\n    \n     J\n    \n    and the 'Parameter registers' of Level\n    \n     J+1\n    \n    , indicating that these two sets of registers share the same physical hardware. This overlap allows parameters to be passed between levels without moving data to memory.\n\n\nDiagram illustrating Overlapping Register Windows. It shows two levels of register windows. Level J consists of three boxes: 'Parameter registers', 'Local registers', and 'Temporary registers'. Level J+1 consists of three boxes: 'Parameter registers', 'Local registers', and 'Temporary registers'. A bracket labeled 'Call/return' indicates the overlap between the 'Temporary registers' of Level J and the 'Parameter registers' of Level J+1.\n\n\n**Figure 15.1**\n   Overlapping Register Windows\n\n\nin memory and later restored when the nesting depth decreases. Thus, the actual organization of the register file is as a circular buffer of overlapping windows. Two notable examples of this approach are Sun's SPARC architecture, described in Section 15.7, and the IA-64 architecture used in Intel's Itanium processor.\n\n\nThe circular organization is shown in Figure 15.2, which depicts a circular buffer of six windows. The buffer is filled to a depth of 4 (A called B; B called C; C called D) with procedure D active. The current-window pointer (CWP) points to the window of the currently active procedure. Register references by a machine instruction are offset by this pointer to determine the actual physical register. The saved-window pointer (SWP) identifies the window most recently saved in memory. If procedure D now calls procedure E, arguments for E are placed in D's temporary registers (the overlap between\n   \n    w_3\n   \n   and\n   \n    w_4\n   \n   ) and the CWP is advanced by one window.\n\n\nIf procedure E then makes a call to procedure F, the call cannot be made with the current status of the buffer. This is because F's window overlaps A's window. If F begins to load its temporary registers, preparatory to a call, it will overwrite the parameter registers of A (\n   \n    A.in\n   \n   ). Thus, when CWP is incremented (modulo 6) so that it becomes equal to SWP, an interrupt occurs, and A's window is saved. Only\n\n\n\n\n![Figure 15.2: Circular-Buffer Organization of Overlapped Windows. The diagram shows a circular buffer with six windows labeled w0 through w5. The outer ring is divided into six segments representing procedures A, B, C, D, E, and F. The segments are: A.temp = B.param, B.loc, B.temp = C.param, C.loc, C.temp = D.param, and D.loc. The inner ring contains the window labels w0, w1, w2, w3, w4, and w5. The current window pointer (CWP) is at w5, and the saved window pointer (SWP) is at w0. Arrows indicate a 'Call' from D to E and a 'Return' from E to D. A 'Save' arrow points from the CWP to the SWP, and a 'Restore' arrow points from the SWP back to the CWP.](images/image_0255.jpeg)\n\n\nFigure 15.2: Circular-Buffer Organization of Overlapped Windows. The diagram shows a circular buffer with six windows labeled w0 through w5. The outer ring is divided into six segments representing procedures A, B, C, D, E, and F. The segments are: A.temp = B.param, B.loc, B.temp = C.param, C.loc, C.temp = D.param, and D.loc. The inner ring contains the window labels w0, w1, w2, w3, w4, and w5. The current window pointer (CWP) is at w5, and the saved window pointer (SWP) is at w0. Arrows indicate a 'Call' from D to E and a 'Return' from E to D. A 'Save' arrow points from the CWP to the SWP, and a 'Restore' arrow points from the SWP back to the CWP.\n\n\n**Figure 15.2**\n   Circular-Buffer Organization of Overlapped Windows\n\n\nthe first two portions (A.in and A.loc) need be saved. Then, the SWP is incremented and the call to F proceeds. A similar interrupt can occur on returns. For example, subsequent to the activation of F, when B returns to A, CWP is decremented and becomes equal to SWP. This causes an interrupt that results in the restoration of A's window.\n\n\nFrom the preceding, it can be seen that an\n   \n    N\n   \n   -window register file can hold only\n   \n    N - 1\n   \n   procedure activations. The value of\n   \n    N\n   \n   need not be large. As was mentioned in Appendix 4A, one study [TAMI83] found that, with 8 windows, a save or restore is needed on only 1% of the calls or returns. The Berkeley RISC computers use 8 windows of 16 registers each. The Pyramid computer employs 16 windows of 32 registers each.\n\n\n\n\n**Global Variables**\n\n\nThe window scheme just described provides an efficient organization for storing local scalar variables in registers. However, this scheme does not address the need to store global variables, those accessed by more than one procedure. Two options suggest themselves. First, variables declared as global in an HLL can be assigned memory locations by the compiler, and all machine instructions that reference these variables will use memory-reference operands. This is straightforward, from both the hardware and software (compiler) points of view. However, for frequently accessed global variables, this scheme is inefficient.\n\n\nAn alternative is to incorporate a set of global registers in the processor. These registers would be fixed in number and available to all procedures. A unified numbering scheme can be used to simplify the instruction format. For example, references to registers 0 through 7 could refer to unique global registers, and references to registers 8 through 31 could be offset to refer to physical registers in the current window. There is an increased hardware burden to accommodate the split in register addressing. In addition, the linker must decide which global variables should be assigned to registers.\n\n\n\n\n**Large Register File versus Cache**\n\n\nThe register file, organized into windows, acts as a small, fast buffer for holding a subset of all variables that are likely to be used the most heavily. From this point of view, the register file acts much like a cache memory, although a much faster memory. The question therefore arises as to whether it would be simpler and better to use a cache and a small traditional register file.\n\n\nTable 15.5 compares characteristics of the two approaches. The window-based register file holds all the local scalar variables (except in the rare case of window overflow) of the most recent\n   \n    N - 1\n   \n   procedure activations. The cache holds a selection of recently used scalar variables. The register file should save time, because all local scalar variables are retained. On the other hand, the cache may make more efficient use of space, because it is reacting to the situation dynamically. Furthermore, caches generally treat all memory references alike, including instructions and other types of data. Thus, savings in these other areas are possible with a cache and not a register file.\n\n\n**Table 15.5**\n\nLarge Register File | Cache\nAll local scalars | Recently-used local scalars\nIndividual variables | Blocks of memory\nCompiler-assigned global variables | Recently-used global variables\nSave/Restore based on procedure nesting depth | Save/Restore based on cache replacement algorithm\nRegister addressing | Memory addressing\nMultiple operands addressed and accessed in one cycle | One operand addressed and accessed per cycle\n\n\nA register file may make inefficient use of space, because not all procedures will need the full window space allotted to them. On the other hand, the cache suffers from another sort of inefficiency: Data are read into the cache in blocks. Whereas the register file contains only those variables in use, the cache reads in a block of data, some or much of which will not be used.\n\n\nThe cache is capable of handling global as well as local variables. There are usually many global scalars, but only a few of them are heavily used [KATE83]. A cache will dynamically discover these variables and hold them. If the window-based register file is supplemented with global registers, it too can hold some global scalars. However, when program modules are separately compiled, it is impossible for the compiler to assign global values to registers; the linker must perform this task.\n\n\nWith the register file, the movement of data between registers and memory is determined by the procedure nesting depth. Because this depth usually fluctuates within a narrow range, the use of memory is relatively infrequent. Most cache memories are set associative with a small set size. Thus, there is the danger that other data or instructions will compete for cache residency.\n\n\nBased on the discussion so far, the choice between a large window-based register file and a cache is not clear-cut. There is one characteristic, however, in which the register approach is clearly superior and which suggests that a cache-based system will be noticeably slower. This distinction shows up in the amount of addressing overhead experienced by the two approaches.\n\n\nFigure 15.3 illustrates the difference. To reference a local scalar in a window-based register file, a “virtual” register number and a window number are used. These can pass through a relatively simple decoder to select one of the physical registers. To reference a memory location in cache, a full-width memory address must be generated. The complexity of this operation depends on the addressing mode. In a set associative cache, a portion of the address is used to read a number of words and tags equal to the set size. Another portion of the address is compared with the tags, and one of the words that were read is selected. It should be clear that even if the cache is as fast as the register file, the access time will be considerably longer. Thus, from the point of view of performance, the window-based register file is superior for local scalars. Further performance improvement could be achieved by the addition of a cache for instructions only.\n\n\n\n\n![Figure 15.3: Referencing a Scalar. (a) Window-based register file: An instruction with fields 'R' and 'W#' is processed by a decoder, which interacts with a register file to produce data. (b) Cache: An instruction with field 'A' is used to access a cache divided into 'Tags' and 'Data' sections. The tags are compared with the instruction's address, and the data is selected and output.](images/image_0256.jpeg)\n\n\n(a) Window-based register file\n\n\n(b) Cache\n\n\nFigure 15.3: Referencing a Scalar. (a) Window-based register file: An instruction with fields 'R' and 'W#' is processed by a decoder, which interacts with a register file to produce data. (b) Cache: An instruction with field 'A' is used to access a cache divided into 'Tags' and 'Data' sections. The tags are compared with the instruction's address, and the data is selected and output.\n\n\n**Figure 15.3**\n   Referencing a Scalar"
        },
        {
          "name": "Compiler-Based Register Optimization",
          "content": "Let us assume now that only a small number (e.g., 16–32) of registers is available on the target RISC machine. In this case, optimized register usage is the responsibility of the compiler. A program written in a high-level language has, of course, no explicit references to registers (the C-language keyword\n   \n    register\n   \n   notwithstanding). Rather, program quantities are referred to symbolically. The objective of the compiler is to keep the operands for as many computations as possible in registers rather than main memory, and to minimize load-and-store operations.\n\n\nIn general, the approach taken is as follows. Each program quantity that is a candidate for residing in a register is assigned to a symbolic or virtual register. The compiler then maps the unlimited number of symbolic registers into a fixed number of real registers. Symbolic registers whose usage does not overlap can share the same real register. If, in a particular portion of the program, there are more quantities to deal with than real registers, then some of the quantities are assigned to memory locations. Load-and-store instructions are used to position quantities in registers temporarily for computational operations.\n\n\nThe essence of the optimization task is to decide which quantities are to be assigned to registers at any given point in the program. The technique most commonly used in RISC compilers is known as graph coloring, which is a technique borrowed from the discipline of topology [CHAI82, CHOW86, COUT86, CHOW90].\n\n\nThe graph coloring problem is this. Given a graph consisting of nodes and edges, assign colors to nodes such that adjacent nodes have different colors, and do this in such a way as to minimize the number of different colors. This problem is adapted to the compiler problem in the following way. First, the program is analyzed to build a register interference graph. The nodes of the graph are the symbolic registers. If two symbolic registers are “live” during the same program fragment, then they are joined by an edge to depict interference. An attempt is then made to color the graph with\n   \n    n\n   \n   colors, where\n   \n    n\n   \n   is the number of registers. Nodes that share the same color can be assigned to the same register. If this process does not fully succeed, then those nodes that cannot be colored must be placed in memory, and loads and stores must be used to make space for the affected quantities when they are needed.\n\n\nFigure 15.4 is a simple example of the process. Assume a program with six symbolic registers to be compiled into three actual registers. Figure 15.4a shows the time sequence of active use of each symbolic register. The dashed horizontal lines indicate successive instruction executions. Figure 15.4b shows the register interference graph (shading and stripes are used instead of colors). A possible coloring with three colors is indicated. Because symbolic registers A and D do not interfere, the compile can assign both of these to physical register R1. Similarly, symbolic registers C and E can be assigned to register R3. One symbolic register, F, is left uncolored and must be dealt with using loads and stores.\n\n\nIn general, there is a trade-off between the use of a large set of registers and compiler-based register optimization. For example, [BRAD91a] reports on a study\n\n\n\n\n![Figure 15.4: Graph Coloring Approach. (a) Time sequence of active use of registers: A grid showing symbolic registers A-F over time. A is in R1, B is in R2, C is in R3, D is in R1, E is in R3, and F is uncolored. (b) Register interference graph: A graph with nodes A, B, C, D, E, F. Edges connect A-B, A-C, A-D, A-E, B-C, B-D, B-E, B-F, C-D, C-E, C-F, D-E, D-F, and E-F. Nodes A and D are shaded green, B and E are striped, and C and F are unshaded.](images/image_0257.jpeg)\n\n\nFigure 15.4 consists of two parts. Part (a), titled \"Time sequence of active use of registers\", is a grid with 6 columns labeled A through F (Symbolic registers) and 3 rows labeled R1, R2, and R3 (Actual registers). A vertical arrow on the left indicates \"Time\" increasing downwards. The grid shows the following assignments: A is in R1, B is in R2, C is in R3, D is in R1, E is in R3, and F is uncolored. Dashed horizontal lines represent instruction executions. Part (b), titled \"Register interference graph\", is a graph with nodes A, B, C, D, E, and F. Edges connect A-B, A-C, A-D, A-E, B-C, B-D, B-E, B-F, C-D, C-E, C-F, D-E, D-F, and E-F. Nodes A and D are shaded green, B and E are striped, and C and F are unshaded.\n\n\nFigure 15.4: Graph Coloring Approach. (a) Time sequence of active use of registers: A grid showing symbolic registers A-F over time. A is in R1, B is in R2, C is in R3, D is in R1, E is in R3, and F is uncolored. (b) Register interference graph: A graph with nodes A, B, C, D, E, F. Edges connect A-B, A-C, A-D, A-E, B-C, B-D, B-E, B-F, C-D, C-E, C-F, D-E, D-F, and E-F. Nodes A and D are shaded green, B and E are striped, and C and F are unshaded.\n\n\n**Figure 15.4**\n   Graph Coloring Approach\n\n\nthat modeled a RISC architecture with features similar to the Motorola 88000 and the MIPS R2000. The researchers varied the number of registers from 16 to 128, and they considered both the use of all general-purpose registers and registers split between integer and floating-point use. Their study showed that with even simple register optimization, there is little benefit to the use of more than 64 registers. With reasonably sophisticated register optimization techniques, there is only marginal performance improvement with more than 32 registers. Finally, they noted that with a small number of registers (e.g., 16), a machine with a shared register organization executes faster than one with a split organization. Similar conclusions can be drawn from [HUGU91], which reports on a study that is primarily concerned with optimizing the use of a small number of registers rather than comparing the use of large register sets with optimization efforts."
        },
        {
          "name": "Reduced Instruction Set Architecture",
          "content": "In this section, we look at some of the general characteristics of and the motivation for a reduced instruction set architecture. Specific examples will be seen later in this chapter. We begin with a discussion of motivations for contemporary complex instruction set architectures.\n\n\n\n\n**Why CISC**\n\n\nWe have noted the trend to richer instruction sets, which include a larger number of instructions and more complex instructions. Two principal reasons have motivated this trend: a desire to simplify compilers and a desire to improve performance. Underlying both of these reasons was the shift to HLLs on the part of programmers; architects attempted to design machines that provided better support for HLLs.\n\n\nIt is not the intent of this chapter to say that the CISC designers took the wrong direction. Indeed, because technology continues to evolve and because architectures exist along a spectrum rather than in two neat categories, a black-and-white assessment is unlikely ever to emerge. Thus, the comments that follow are simply meant to point out some of the potential pitfalls in the CISC approach and to provide some understanding of the motivation of the RISC adherents.\n\n\nThe first of the reasons cited, compiler simplification, seems obvious, but it is not. The task of the compiler writer is to build a compiler that generates good (fast, small, fast and small) sequences of machine instructions for HLL programs (i.e., the compiler views individual HLL statements in the context of surrounding HLL statements). If there are machine instructions that resemble HLL statements, this task is simplified. This reasoning has been disputed by the RISC researchers ([HENN82], [RADI83], [PATT82b]). They have found that complex machine instructions are often hard to exploit because the compiler must find those cases that exactly fit the construct. The task of optimizing the generated code to minimize code size, reduce instruction execution count, and enhance pipelining is much more difficult with a complex instruction set. As evidence of this, studies cited earlier in this chapter indicate that most of the instructions in a compiled program are the relatively simple ones.\n\n\nThe other major reason cited is the expectation that a CISC will yield smaller, faster programs. Let us examine both aspects of this assertion: that programs will be smaller and that they will execute faster.\n\n\nThere are two advantages to smaller programs. Because the program takes up less memory, there is a savings in that resource. With memory today being so inexpensive, this potential advantage is no longer compelling. More important, smaller programs should improve performance, and this will happen in three ways. First, fewer instructions means fewer instruction bytes to be fetched. Second, in a paging environment, smaller programs occupy fewer pages, reducing page faults. Third, more instructions fit in cache(s).\n\n\nThe problem with this line of reasoning is that it is far from certain that a CISC program will be smaller than a corresponding RISC program. In many cases, the CISC program, expressed in symbolic machine language, may be\n   *shorter*\n   (i.e., fewer instructions), but the number of bits of memory occupied may not be noticeably\n   *smaller*\n   . Table 15.6 shows results from three studies that compared the size of compiled C programs on a variety of machines, including RISC I, which has a reduced instruction set architecture. Note that there is little or no savings using a CISC over a RISC. It is also interesting to note that the VAX, which has a much more complex instruction set than the PDP-11, achieves very little savings over the latter. These results were confirmed by IBM researchers [RADI83], who found that the IBM 801 (a RISC) produced code that was 0.9 times the size of code on an IBM S/370. The study used a set of PL/I programs.\n\n\nThere are several reasons for these rather surprising results. We have already noted that compilers on CISCs tend to favor simpler instructions, so that the conciseness of the complex instructions seldom comes into play. Also, because there are more instructions on a CISC, longer opcodes are required, producing longer instructions. Finally, RISCs tend to emphasize register rather than memory references, and the former require fewer bits. An example of this last effect is discussed presently.\n\n\nSo the expectation that a CISC will produce smaller programs, with the attendant advantages, may not be realized. The second motivating factor for increasingly complex instruction sets was that instruction execution would be faster. It seems to make sense that a complex HLL operation will execute more quickly as a single machine instruction rather than as a series of more primitive instructions. However, because of the bias toward the use of those simpler instructions, this may not be so.\n\n\n**Table 15.6**\n   Code Size Relative to RISC I\n\n\n\n | [PATT82a] 11 C\n      \n      Programs | [KATE83] 12 C\n      \n      Programs | [HEAT84] 5 C\n      \n      Programs\nRISC I | 1.0 | 1.0 | 1.0\nVAX-11/780 | 0.8 | 0.67 | \nM68000 | 0.9 |  | 0.9\nZ8002 | 1.2 |  | 1.12\nPDP-11/70 | 0.9 | 0.71 | \n\n\nThe entire control unit must be made more complex, and/or the microprogram control store must be made larger, to accommodate a richer instruction set. Either factor increases the execution time of the simple instructions.\n\n\nIn fact, some researchers have found that the speedup in the execution of complex functions is due not so much to the power of the complex machine instructions as to their residence in high-speed control store [RAD183]. In effect, the control store acts as an instruction cache. Thus, the hardware architect is in the position of trying to determine which subroutines or functions will be used most frequently and assigning those to the control store by implementing them in microcode. The results have been less than encouraging. On S/390 systems, instructions such as Translate and Extended-Precision-Floating-Point-Divide reside in high-speed storage, while the sequence involved in setting up procedure calls or initiating an interrupt handler are in slower main memory.\n\n\nThus, it is far from clear that a trend to increasingly complex instruction sets is appropriate. This has led a number of groups to pursue the opposite path.\n\n\n\n\n**Characteristics of Reduced Instruction Set Architectures**\n\n\nAlthough a variety of different approaches to reduced instruction set architecture have been taken, certain characteristics are common to all of them:\n\n\n  * ■ One instruction per cycle\n  * ■ Register-to-register operations\n  * ■ Simple addressing modes\n  * ■ Simple instruction formats\n\n\nHere, we provide a brief discussion of these characteristics. Specific examples are explored later in this chapter.\n\n\nThe first characteristic listed is that there is\n   **one machine instruction per machine cycle**\n   . A\n   *machine cycle*\n   is defined to be the time it takes to fetch two operands from registers, perform an ALU operation, and store the result in a register. Thus, RISC machine instructions should be no more complicated than, and execute about as fast as, microinstructions on CISC machines (discussed in Part Four). With simple, one-cycle instructions, there is little or no need for microcode; the machine instructions can be hardwired. Such instructions should execute faster than comparable machine instructions on other machines, because it is not necessary to access a microprogram control store during instruction execution.\n\n\nA second characteristic is that most operations should be\n   **register to register**\n   , with only simple LOAD and STORE operations accessing memory. This design feature simplifies the instruction set and therefore the control unit. For example, a RISC instruction set may include only one or two ADD instructions (e.g., integer add, add with carry); the VAX has 25 different ADD instructions. Another benefit is that such an architecture encourages the optimization of register use, so that frequently accessed operands remain in high-speed storage.\n\n\nThis emphasis on register-to-register operations is notable for RISC designs. Contemporary CISC machines provide such instructions but also include memory-to-memory and mixed register/memory operations. Attempts to compare these\n\n\n\n\n![](images/image_0258.jpeg)\n\n\n8 | 16 | 16 | 16\nAdd | B | C | A\n\n\nMemory to memory\n    \n\n    I = 56, D = 96, M = 152\n\n\n\n8 | 4 | 16\nLoad | RB | B\nLoad | RC | B\nAdd | R\n       \n       A | RB | RC\nStore | R\n       \n       A | A\n\n\nRegister to memory\n    \n\n    I = 104, D = 96, M = 200\n\n\n(a)\n    \n     A \\leftarrow B + C\n\n\n\n\n\n8 | 16 | 16 | 16\nAdd | B | C | A\nAdd | A | C | B\nSub | B | D | D\n\n\nMemory to memory\n    \n\n    I = 168, D = 288, M = 456\n\n\n\n8 | 4 | 4 | 4\nAdd | RA | RB | RC\nAdd | RB | RA | RC\nSub | RD | RD | RB\n\n\nRegister to memory\n    \n\n    I = 60, D = 0, M = 60\n\n\n(b)\n    \n     A \\leftarrow B + C\n    \n    ;\n    \n     B \\leftarrow A + C\n    \n    ;\n    \n     D \\leftarrow D - B\n\n\nI = number of bytes occupied by executed instructions\n\n\nD = number of bytes occupied by data\n\n\nM = total memory traffic = I + D\n\n\n**Figure 15.5**\n   Two Comparisons of Register-to-Register and Memory-to-Memory Approaches\n\n\napproaches were made in the 1970s, before the appearance of RISCs. Figure 15.5a illustrates the approach taken. Hypothetical architectures were evaluated on program size and the number of bits of memory traffic. Results such as this one led one researcher to suggest that future architectures should contain no registers at all [MYER78]. One wonders what he would have thought, at the time, of the RISC machine once produced by Pyramid, which contained no less than 528 registers!\n\n\nWhat was missing from those studies was a recognition of the frequent access to a small number of local scalars and that, with a large bank of registers or an optimizing compiler, most operands could be kept in registers for long periods of time. Thus, Figure 15.5b may be a fairer comparison.\n\n\nA third characteristic is the use of\n   **simple addressing modes**\n   . Almost all RISC instructions use simple register addressing. Several additional modes, such as displacement and PC-relative, may be included. Other, more complex modes can be synthesized in software from the simple ones. Again, this design feature simplifies the instruction set and the control unit.\n\n\nA final common characteristic is the use of\n   **simple instruction formats**\n   . Generally, only one or a few formats are used. Instruction length is fixed and aligned on word boundaries. Field locations, especially the opcode, are fixed. This design feature has a number of benefits. With fixed fields, opcode decoding and register operating can occur simultaneously. Simplified formats simplify the control unit. Instruction fetching is optimized because word-length units are fetched. Alignment on a word boundary also means that a single instruction does not cross page boundaries.\n\n\nTaken together, these characteristics can be assessed to determine the potential performance benefits of the RISC approach. A certain amount of “circumstantial\n\n\nevidence” can be presented. First, more effective optimizing compilers can be developed. With more-primitive instructions, there are more opportunities for moving functions out of loops, reorganizing code for efficiency, maximizing register utilization, and so forth. It is even possible to compute parts of complex instructions at compile time. For example, the S/390 Move Characters (MVC) instruction moves a string of characters from one location to another. Each time it is executed, the move will depend on the length of the string, whether and in which direction the locations overlap, and what the alignment characteristics are. In most cases, these will all be known at compile time. Thus, the compiler could produce an optimized sequence of primitive instructions for this function.\n\n\nA second point, already noted, is that most instructions generated by a compiler are relatively simple anyway. It would seem reasonable that a control unit built specifically for those instructions and using little or no microcode could execute them faster than a comparable CISC.\n\n\nA third point relates to the use of instruction pipelining. RISC researchers feel that the instruction pipelining technique can be applied much more effectively with a reduced instruction set. We examine this point in some detail presently.\n\n\nA final, and somewhat less significant, point is that RISC processors are more responsive to interrupts because interrupts are checked between rather elementary operations. Architectures with complex instructions either restrict interrupts to instruction boundaries or must define specific interruptible points and implement mechanisms for restarting an instruction.\n\n\nThe case for improved performance for a reduced instruction set architecture is strong, but one could perhaps still make an argument for CISC. A number of studies have been done, but not on machines of comparable technology and power. Further, most studies have not attempted to separate the effects of a reduced instruction set and the effects of a large register file. The “circumstantial evidence,” however, is suggestive.\n\n\n\n\n**CISC versus RISC Characteristics**\n\n\nAfter the initial enthusiasm for RISC machines, there has been a growing realization that (1) RISC designs may benefit from the inclusion of some CISC features and that (2) CISC designs may benefit from the inclusion of some RISC features. The result is that the more recent RISC designs, notably the PowerPC, are no longer “pure” RISC and the more recent CISC designs, notably the Pentium II and later Pentium models, do incorporate some RISC characteristics.\n\n\nAn interesting comparison in [MASH95] provides some insight into this issue. Table 15.7 lists a number of processors and compares them across a number of characteristics. For purposes of this comparison, the following are considered typical of a classic RISC:\n\n\n  * 1. A single instruction size.\n  * 2. That size is typically 4 bytes.\n  * 3. A small number of data addressing modes, typically less than five. This parameter is difficult to pin down. In the table, register and literal modes are not counted and different formats with different offset sizes are counted separately.\n\n\n**Table 15.7**\n\nProcessor | Number of instruction sizes | Max instruction size in bytes | Number of addressing modes | Indirect addressing | Load/store combined with arithmetic | Max number of memory operands | Unaligned addressing allowed | Max number of MMU uses | Number of bits for integer register specifier | Number of bits for FP register specifier\nAMD29000 | 1 | 4 | 1 | no | no | 1 | no | 1 | 8 | 3\n      \n       a\nMIPS R2000 | 1 | 4 | 1 | no | no | 1 | no | 1 | 5 | 4\nSPARC | 1 | 4 | 2 | no | no | 1 | no | 1 | 5 | 4\nMC88000 | 1 | 4 | 3 | no | no | 1 | no | 1 | 5 | 4\nHP PA | 1 | 4 | 10\n      \n       a | no | no | 1 | no | 1 | 5 | 4\nIBM RT/PC | 2\n      \n       a | 4 | 1 | no | no | 1 | no | 1 | 4\n      \n       a | 3\n      \n       a\nIBM RS/6000 | 1 | 4 | 4 | no | no | 1 | yes | 1 | 5 | 5\nIntel i860 | 1 | 4 | 4 | no | no | 1 | no | 1 | 5 | 4\nIBM 3090 | 4 | 8 | 2\n      \n       b | no\n      \n       b | yes | 2 | yes | 4 | 4 | 2\nIntel 80486 | 12 | 12 | 15 | no\n      \n       b | yes | 2 | yes | 4 | 3 | 3\nNSC 32016 | 21 | 21 | 23 | yes | yes | 2 | yes | 4 | 3 | 3\nMC68040 | 11 | 22 | 44 | yes | yes | 2 | yes | 8 | 4 | 3\nVAX | 56 | 56 | 22 | yes | yes | 6 | yes | 24 | 4 | 0\nClipper | 4\n      \n       a | 8\n      \n       a | 9\n      \n       a | no | no | 1 | 0 | 2 | 4\n      \n       a | 3\n      \n       a\nIntel 80960 | 2\n      \n       a | 8\n      \n       a | 9\n      \n       a | no | no | 1 | yes\n      \n       d | — | 5 | 3\n      \n       a\n\n\nNotes:\n   \n    a\n   \n   RISC that does not conform to this characteristic.\n\n\nb\n   \n   CISC that does not conform to this characteristic.\n\n\n  * 4. No indirect addressing that requires you to make one memory access to get the address of another operand in memory.\n  * 5. No operations that combine load/store with arithmetic (e.g., add from memory, add to memory).\n  * 6. No more than one memory-addressed operand per instruction.\n  * 7. Does not support arbitrary alignment of data for load/store operations.\n  * 8. Maximum number of uses of the memory management unit (MMU) for a data address in an instruction.\n  * 9. Number of bits for integer register specifier equal to five or more. This means that at least 32 integer registers can be explicitly referenced at a time.\n  * 10. Number of bits for floating-point register specifier equal to four or more. This means that at least 16 floating-point registers can be explicitly referenced at a time.\n\n\nItems 1 through 3 are an indication of instruction decode complexity. Items 4 through 8 suggest the ease or difficulty of pipelining, especially in the presence of virtual memory requirements. Items 9 and 10 are related to the ability to take good advantage of compilers.\n\n\nIn the table, the first eight processors are clearly RISC architectures, the next five are clearly CISC, and the last two are processors often thought of as RISC that in fact have many CISC characteristics."
        },
        {
          "name": "RISC Pipelining",
          "content": "**Pipelining with Regular Instructions**\n\n\nAs we discussed in Section 12.4, instruction pipelining is often used to enhance performance. Let us reconsider this in the context of a RISC architecture. Most instructions are register to register, and an instruction cycle has the following two stages:\n\n\n  * ■ I: Instruction fetch.\n  * ■ E: Execute. Performs an ALU operation with register input and output.\n\n\nFor load and store operations, three stages are required:\n\n\n  * ■ I: Instruction fetch.\n  * ■ E: Execute. Calculates memory address.\n  * ■ D: Memory. Register-to-memory or memory-to-register operation.\n\n\nFigure 15.6a depicts the timing of a sequence of instructions using no pipelining. Clearly, this is a wasteful process. Even very simple pipelining can substantially improve performance. Figure 15.6b shows a two-stage pipelining scheme, in which the I and E stages of two different instructions are performed simultaneously. The two stages of the pipeline are an instruction fetch stage, and an execute/memory stage that executes the instruction, including register-to-memory and memory-to-register operations. Thus we see that the instruction fetch stage of the\n\n\n\n\n![](images/image_0259.jpeg)\n\n\n**Figure 15.6**\n    The Effects of Pipelining\n\n\nsecond instruction can be performed in parallel with the first part of the execute/memory stage. However, the execute/memory stage of the second instruction must be delayed until the first instruction clears the second stage of the pipeline. This scheme can yield up to twice the execution rate of a serial scheme. Two problems prevent the maximum speedup from being achieved. First, we assume that a single-port memory is used and that only one memory access is possible per stage. This requires the insertion of a wait state in some instructions. Second, a branch instruction interrupts the sequential flow of execution. To accommodate this with minimum circuitry, a NOOP instruction can be inserted into the instruction stream by the compiler or assembler.\n\n\nPipelining can be improved further by permitting two memory accesses per stage. This yields the sequence shown in Figure 15.6c. Now, up to three instructions can be overlapped, and the improvement is as much as a factor of 3. Again, branch instructions cause the speedup to fall short of the maximum possible. Also, note that data dependencies have an effect. If an instruction needs an operand that is altered by the preceding instruction, a delay is required. Again, this can be accomplished by a NOOP.\n\n\nThe pipelining discussed so far works best if the three stages are of approximately equal duration. Because the E stage usually involves an ALU operation, it may be longer. In this case, we can divide into two substages:\n\n\n  * * ■ E\n       \n        1\n       \n       : Register file read\n  * ■ E\n       \n        2\n       \n       : ALU operation and register write\n\n\nBecause of the simplicity and regularity of a RISC instruction set, the design of the phasing into three or four stages is easily accomplished. Figure 15.6d shows the result with a four-stage pipeline. Up to four instructions at a time can be under way, and the maximum potential speedup is a factor of 4. Note again the use of NOOPs to account for data and branch delays.\n\n\n\n\n**Optimization of Pipelining**\n\n\nBecause of the simple and regular nature of RISC instructions, it is easier for a hardware designer to implement a simple, fast pipeline. There are few variations in instruction execution duration, and the pipeline can be tailored to reflect this. However, we have seen that data and branch dependencies reduce the overall execution rate.\n\n\n**DELAYED BRANCH**\n   To compensate for these dependencies, code reorganization techniques have been developed. First, let us consider branching instructions.\n   **Delayed branch**\n   , a way of increasing the efficiency of the pipeline, makes use of a branch that does not take effect until after execution of the following instruction (hence the term\n   *delayed*\n   ). The instruction location immediately following the branch is referred to as the\n   *delay slot*\n   . This strange procedure is illustrated in Table 15.8. In the column labeled “normal branch,” we see a normal symbolic instruction machine-language program. After 102 is executed, the next instruction to be executed is 105. To regularize the pipeline, a NOOP is inserted after this branch. However, increased performance is achieved if the instructions at 101 and 102 are interchanged.\n\n\nFigure 15.7 shows the result. Figure 15.7a shows the traditional approach to pipelining, of the type discussed in Chapter 14 (e.g., see Figures 14.11 and 14.12). The JUMP instruction is fetched at time 4. At time 5, the JUMP instruction is executed at the same time that instruction 103 (ADD instruction) is fetched. Because a JUMP occurs, which updates the program counter, the pipeline must be cleared of instruction 103; at time 6, instruction 105, which is the target of the JUMP, is loaded. Figure 15.7b shows the same pipeline handled by a typical RISC organization. The timing is the same. However, because of the insertion of the NOOP instruction, we do not need special circuitry to clear the pipeline; the NOOP simply executes with no effect. Figure 15.7c shows the use of the delayed branch. The JUMP instruction is fetched at time 2, before the ADD instruction, which is fetched at time 3. Note, however, that the ADD instruction is fetched before the execution of the JUMP instruction has a chance to alter the program counter. Therefore, during time 4, the ADD instruction is executed at the same time that instruction 105 is fetched. Thus, the original semantics of the program are retained but two fewer clock cycles are required for execution.\n\n\nThis interchange of instructions will work successfully for unconditional branches, calls, and returns. For conditional branches, this procedure cannot be blindly applied. If the condition that is tested for the branch can be altered by the\n\n\n**Table 15.8**\n   Normal and Delayed Branch\n\n\n\nAddress | Normal Branch | Delayed Branch | Optimized Delayed Branch\n100 | LOAD | X, rA | LOAD | X, rA | LOAD | X, rA\n101 | ADD | 1, rA | ADD | 1, rA | JUMP | 105\n102 | JUMP | 105 | JUMP | 106 | ADD | 1, rA\n103 | ADD | rA, rB | NOOP |  | ADD | rA, rB\n104 | SUB | rC, rB | ADD | rA, rB | SUB | rC, rB\n105 | STORE | rA, Z | SUB | rC, rB | STORE | rA, Z\n106 |  |  | STORE | rA, Z |  | \n\n\n\n | Time →\n | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8\n100 LOAD X, rA | I | E | D |  |  |  |  | \n101 ADD 1, rA |  | I |  | E |  |  |  | \n102 JUMP 105 |  |  |  | I | E |  |  | \n103 ADD rA, rB |  |  |  |  | I | E |  | \n105 STORE rA, Z |  |  |  |  |  | I | E | D\n\n\n(a) Traditional pipeline\n\n\n\n | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8\n100 LOAD X, rA | I | E | D |  |  |  |  | \n101 ADD 1, rA |  | I |  | E |  |  |  | \n102 JUMP 106 |  |  |  | I | E |  |  | \n103 NOOP |  |  |  |  | I | E |  | \n106 STORE rA, Z |  |  |  |  |  | I | E | D\n\n\n(b) RISC pipeline with inserted NOOP\n\n\n\n | 1 | 2 | 3 | 4 | 5 | 6\n100 LOAD X, Ar | I | E | D |  |  | \n101 JUMP 105 |  | I | E |  |  | \n102 ADD 1, rA |  |  | I | E |  | \n105 STORE rA, Z |  |  |  | I | E | D\n\n\n(c) Reversed instructions\n\n\n**Figure 15.7**\nimmediately preceding instruction, then the compiler must refrain from doing the interchange and instead insert a NOOP. Otherwise, the compiler can seek to insert a useful instruction after the branch. The experience with both the Berkeley RISC and IBM 801 systems is that the majority of conditional branch instructions can be optimized in this fashion ([PATT82a], [RADI83]).\n\n\n**DELAYED LOAD**\n   A similar sort of tactic, called the\n   **delayed load**\n   , can be used on LOAD instructions. On LOAD instructions, the register that is to be the target of the load is locked by the processor. The processor then continues execution of the instruction stream until it reaches an instruction requiring that register, at which point it idles until the load is complete. If the compiler can rearrange instructions so that useful work can be done while the load is in the pipeline, efficiency is increased.\n\n\n\n\n![Online Interactive Simulator logo featuring a globe and the text 'Online Interactive Simulator' and 'www'.](images/image_0260.jpeg)\n\n\nOnline Interactive Simulator logo featuring a globe and the text 'Online Interactive Simulator' and 'www'.\n\n\ndo i=2, n-1\n    a[i] = a[i] + a[i-1] * a[i+1]\nend do\n\n(a) Original loop\n\n\ndo i=2, n-2, 2\n    a[i] = a[i] + a[i-1] * a[i+1]\n    a[i+1] = a[i+1] + a[i] * a[i+2]\nend do\n\nif (mod(n-2, 2) = i) then\n    a[n-1] = a[n-1] + a[n-2] * a[n]\nend if\n\n(b) Loop unrolled twice\n\n\n**Figure 15.8**\n**LOOP UNROLLING**\n   Another compiler technique to improve instruction parallelism is loop unrolling [BACO94]. Unrolling replicates the body of a loop some number of times called the unrolling factor (\n   \n    u\n   \n   ) and iterates by step\n   \n    u\n   \n   instead of step 1.\n\n\nUnrolling can improve the performance by\n\n\n  * ■ reducing loop overhead\n  * ■ increasing instruction parallelism by improving pipeline performance\n  * ■ improving register, data cache, or TLB locality\n\n\nFigure 15.8 illustrates all three of these improvements in an example. Loop overhead is cut in half because two iterations are performed before the test and branch at the end of the loop. Instruction parallelism is increased because the second assignment can be performed while the results of the first are being stored and the loop variables are being updated. If array elements are assigned to registers, register locality will improve because\n   \n    a[i]\n   \n   and\n   \n    a[i + 1]\n   \n   are used twice in the loop body, reducing the number of loads per iteration from three to two.\n\n\nAs a final note, we should point out that the design of the instruction pipeline should not be carried out in isolation from other optimization techniques applied to the system. For example, [BRAD91b] shows that the scheduling of instructions for the pipeline and the dynamic allocation of registers should be considered together to achieve the greatest efficiency."
        },
        {
          "name": "MIPS R4000",
          "content": "One of the first commercially available RISC chip sets was developed by MIPS Technology Inc. The system was inspired by an experimental system, also using the name MIPS, developed at Stanford [HENN84]. In this section we look at the MIPS\n\n\nR4000. It has substantially the same architecture and instruction set of the earlier MIPS designs: the R2000 and R3000. The most significant difference is that the R4000 uses 64 rather than 32 bits for all internal and external data paths and for addresses, registers, and the ALU.\n\n\nThe use of 64 bits has a number of advantages over a 32-bit architecture. It allows a bigger address space—large enough for an operating system to map more than a terabyte of files directly into virtual memory for easy access. With 1-terabyte and larger disk drives now common, the 4-gigabyte address space of a 32-bit machine becomes limiting. Also, the 64-bit capacity allows the R4000 to process data such as IEEE double-precision floating-point numbers and character strings, up to eight characters in a single action.\n\n\nThe R4000 processor chip is partitioned into two sections, one containing the CPU and the other containing a coprocessor for memory management. The processor has a very simple architecture. The intent was to design a system in which the instruction execution logic was as simple as possible, leaving space available for logic to enhance performance (e.g., the entire memory-management unit).\n\n\nThe processor supports thirty-two 64-bit registers. It also provides for up to 128 Kbytes of high-speed cache, half each for instructions and data. The relatively large cache (the IBM 3090 provides 128 to 256 Kbytes of cache) enables the system to keep large sets of program code and data local to the processor, off-loading the main memory bus and avoiding the need for a large register file with the accompanying windowing logic.\n\n\n\n\n**Instruction Set**\n\n\nAll MIPS R series instructions are encoded in a single 32-bit word format. All data operations are register to register; the only memory references are pure load/store operations.\n\n\nThe R4000 makes no use of condition codes. If an instruction generates a condition, the corresponding flags are stored in a general-purpose register. This avoids the need for special logic to deal with condition codes, as they affect the pipelining mechanism and the reordering of instructions by the compiler. Instead, the mechanisms already implemented to deal with register-value dependencies are employed. Further, conditions mapped onto the register files are subject to the same compile-time optimizations in allocation and reuse as other values stored in registers.\n\n\nAs with most RISC-based machines, the MIPS uses a single 32-bit instruction length. This single instruction length simplifies instruction fetch and decode, and it also simplifies the interaction of instruction fetch with the virtual memory management unit (i.e., instructions do not cross word or page boundaries). The three instruction formats (Figure 15.9) share common formatting of opcodes and register references, simplifying instruction decode. The effect of more complex instructions can be synthesized at compile time.\n\n\nOnly the simplest and most frequently used memory-addressing mode is implemented in hardware. All memory references consist of a 16-bit offset from a 32-bit register. For example, the “load word” instruction is of the form\n\n\nlw r2, 128(r3) /* load word at address 128 offset from\n                register 3 into register 2\n\n\n![](images/image_0261.jpeg)\n\n\n| 6 | 5 | 5 | 16\nI-type (immediate) | Operation | rs | rt | Immediate\n | 6 | 26\nJ-type (jump) | Operation | Target\n | 6 | 5 | 5 | 5 | 5 | 6\nR-type (register) | Operation | rs | rt | rd | Shift | Function\n\n\n\n\n\nOperation | Operation code\nrs | Source register specifier\nrt | Source/destination register specifier\nImmediate | Immediate, branch, or address displacement\nTarget | Jump target address\nrd | Destination register specifier\nShift | Shift amount\nFunction | ALU/shift function specifier\n\n\n**Figure 15.9**\n   MIPS Instruction Formats\n\n\nEach of the 32 general-purpose registers can be used as the base register. One register,\n   \n    r0\n   \n   , always contains 0.\n\n\nThe compiler makes use of multiple machine instructions to synthesize typical addressing modes in conventional machines. Here is an example from [CHOW87], which uses the instruction\n   \n    lui\n   \n   (load upper immediate). This instruction loads the upper half of a register with a 16-bit immediate value, setting the lower half to zero. Consider an assembly-language instruction that uses a 32-bit immediate argument\n\n\nlw r2, #imm(r4) /* load word at address using a 32-bit\n                 immediate offset #imm\n                 /* offset from register 4 into register 2\nThis instruction can be compiled into the following MIPS instructions\n\n\nlui r1, #imm-hi    /* where #imm-hi is the high-order\n                    16 bits of #imm\naddu r1, r1, r4    /* add unsigned #imm-hi to r4 and\n                    put in r1\nlw r2, #imm-lo(r1) /* where #imm-lo is the low-order\n                    16 bits of #imm\n\n\n**Instruction Pipeline**\n\n\nWith its simplified instruction architecture, the MIPS can achieve very efficient pipelining. It is instructive to look at the evolution of the MIPS pipeline, as it illustrates the evolution of RISC pipelining in general.\n\n\nThe initial experimental RISC systems and the first generation of commercial RISC processors achieve execution speeds that approach one instruction per system clock cycle. To improve on this performance, two classes of processors have evolved\n\n\nto offer execution of multiple instructions per clock cycle: superscalar and super-pipelined architectures. In essence, a superscalar architecture replicates each of the pipeline stages so that two or more instructions at the same stage of the pipeline can be processed simultaneously. A superpipelined architecture is one that makes use of more, and more fine-grained, pipeline stages. With more stages, more instructions can be in the pipeline at the same time, increasing parallelism.\n\n\nBoth approaches have limitations. With superscalar pipelining, dependencies between instructions in different pipelines can slow down the system. Also, overhead logic is required to coordinate these dependencies. With superpipelining, there is overhead associated with transferring instructions from one stage to the next.\n\n\nChapter 16 is devoted to a study of superscalar architecture. The MIPS R4000 is a good example of a RISC-based superpipeline architecture.\n\n\n\n\n![Logo for Online Interactive Simulation, featuring a globe and the text 'Online Interactive Simulation' and 'www'.](images/image_0262.jpeg)\n\n\nLogo for Online Interactive Simulation, featuring a globe and the text 'Online Interactive Simulation' and 'www'.\n\n\n\n\n**MIPS R3000 Five-Stage Pipeline Simulator**\n\n\nFigure 15.10a shows the instruction pipeline of the R3000. In the R3000, the pipeline advances once per clock cycle. The MIPS compiler is able to reorder instructions to fill delay slots with code 70 to 90% of the time. All instructions follow the same sequence of five pipeline stages:\n\n\n  * ■ Instruction fetch;\n  * ■ Source operand fetch from register file;\n  * ■ ALU operation or data operand address generation;\n  * ■ Data memory reference;\n  * ■ Write back into register file.\n\n\nAs illustrated in Figure 15.10a, there is not only parallelism due to pipelining but also parallelism within the execution of a single instruction. The 60-ns clock cycle is divided into two 30-ns stages. The external instruction and data access operations to the cache each require 60 ns, as do the major internal operations (OP, DA, IA). Instruction decode is a simpler operation, requiring only a single 30-ns stage, overlapped with register fetch in the same instruction. Calculation of an address for a branch instruction also overlaps instruction decode and register fetch, so that a branch at instruction\n   \n    i\n   \n   can address the ICACHE access of instruction\n   \n    i + 2\n   \n   . Similarly, a load at instruction\n   \n    i\n   \n   fetches data that are immediately used by the OP of instruction\n   \n    i + 1\n   \n   , while an ALU/shift result gets passed directly into instruction\n   \n    i + 1\n   \n   with no delay. This tight coupling between instructions makes for a highly efficient pipeline.\n\n\nIn detail, then, each clock cycle is divided into separate stages, denoted as\n   \n    \\phi 1\n   \n   and\n   \n    \\phi 2\n   \n   . The functions performed in each stage are summarized in Table 15.9.\n\n\nThe R4000 incorporates a number of technical advances over the R3000. The use of more advanced technology allows the clock cycle time to be cut in half, to\n\n\n\n\n![Detailed R3000 pipeline diagram showing stages IF, RD, ALU, MEM, and WB across multiple clock cycles (phi1, phi2).](images/image_0263.jpeg)\n\n\nThe diagram illustrates the detailed R3000 pipeline. It shows five main stages: IF (Instruction Fetch), RD (Read), ALU (Arithmetic Logic Unit), MEM (Memory Access), and WB (Write Back). Each stage is divided into two half-cycles,\n    \n     \\phi_1\n    \n    and\n    \n     \\phi_2\n    \n    . The IF stage includes ITLB (Instruction Translation Lookaside Buffer) and I-Cache. The RD stage includes RF (Register File) with sub-stages IDEC (Instruction Decode) and IA (Instruction Address). The ALU stage includes ALU OP with sub-stages DA (Data Address) and DTLB (Data Translation Lookaside Buffer). The MEM stage includes D-Cache. The WB stage is the final stage.\n\n\n\nClock Cycle | \\phi_1 | \\phi_2 | \\phi_1 | \\phi_2 | \\phi_1 | \\phi_2 | \\phi_1 | \\phi_2\nIF |  |  |  |  |  |  |  |  | \nRD |  |  |  |  |  |  |  |  | \nALU |  |  |  |  |  |  |  |  | \nMEM |  |  |  |  |  |  |  |  | \nWB |  |  |  |  |  |  |  |  | \n\n\n\n | I-Cache | RF | ALU OP | D-Cache | WB\nITLB |  | IDEC | DA | DTLB | \n |  | IA |  |  | \n\n\nDetailed R3000 pipeline diagram showing stages IF, RD, ALU, MEM, and WB across multiple clock cycles (phi1, phi2).\n\n\n(a) Detailed R3000 pipeline\n\n\n\n\n![Modified R3000 pipeline diagram showing reduced latencies across stages ITLB, I-Cache, RF, ALU, DTLB, D-Cache, and WB.](images/image_0264.jpeg)\n\n\nThe modified R3000 pipeline shows reduced latencies. It consists of seven stages: ITLB, I-Cache, RF, ALU, DTLB, D-Cache, and WB. Each stage is represented by a 'Cycle' column.\n\n\n\nCycle | Cycle | Cycle | Cycle | Cycle | Cycle\nITLB | I-Cache | RF | ALU | DTLB | D-Cache\n |  |  |  |  | WB\n\n\nModified R3000 pipeline diagram showing reduced latencies across stages ITLB, I-Cache, RF, ALU, DTLB, D-Cache, and WB.\n\n\n(b) Modified R3000 pipeline with reduced latencies\n\n\n\n\n![Optimized R3000 pipeline diagram showing parallel TLB and cache accesses across stages ITLB, RF, ALU, D-Cache, TC, and WB.](images/image_0265.jpeg)\n\n\nThe optimized R3000 pipeline shows parallel TLB and cache accesses. It consists of six stages: ITLB, RF, ALU, D-Cache, TC (Data Cache Tag Check), and WB. Each stage is represented by a 'Cycle' column.\n\n\n\nCycle | Cycle | Cycle | Cycle | Cycle\nITLB | RF | ALU | D-Cache | TC\n |  |  |  | WB\n\n\nOptimized R3000 pipeline diagram showing parallel TLB and cache accesses across stages ITLB, RF, ALU, D-Cache, TC, and WB.\n\n\n(c) Optimized R3000 pipeline with parallel TLB and cache accesses\n\n\n\nIF | = | Instruction fetch\nRD | = | Read\nMEM | = | Memory access\nWB | = | Write back to register file\nI-Cache | = | Instruction cache access\nRF | = | Fetch operand from register\nD-Cache | = | Data cache access\nITLB | = | Instruction address translation\nIDEC | = | Instruction decode\nIA | = | Compute instruction address\nDA | = | Calculate data virtual address\nDTLB | = | Data address translation\nTC | = | Data cache tag check\n\n\n**Figure 15.10**\n   Enhancing the R3000 Pipeline\n\n\n**Table 15.9**\n   R3000 Pipeline Stages\n\n\n\nPipeline Stage | Phase | Function\nIF | \\phi 1 | Using the TLB, translate an instruction virtual address to a physical address (after a branching decision).\nIF | \\phi 2 | Send the physical address to the instruction address.\nRD | \\phi 1 | Return instruction from instruction cache.\n      \n      Compare tags and validity of fetched instruction.\nRD | \\phi 2 | Decode instruction.\n      \n      Read register file.\n      \n      If branch, calculate branch target address.\nALU | \\phi 1 + \\phi 2 | If register-to-register operation, the arithmetic or logical operation is performed.\nALU | \\phi 1 | If a branch, decide whether the branch is to be taken or not.\n      \n      If a memory reference (load or store), calculate data virtual address.\nALU | \\phi 2 | If a memory reference, translate data virtual address to physical using TLB.\nMEM | \\phi 1 | If a memory reference, send physical address to data cache.\nMEM | \\phi 2 | If a memory reference, return data from data cache, and check tags.\nWB | \\phi 1 | Write to register file.\n\n\n30 ns, and for the access time to the register file to be cut in half. In addition, there is greater density on the chip, which enables the instruction and data caches to be incorporated on the chip. Before looking at the final R4000 pipeline, let us consider how the R3000 pipeline can be modified to improve performance using R4000 technology.\n\n\nFigure 15.10b shows a first step. Remember that the cycles in this figure are half as long as those in Figure 15.10a. Because they are on the same chip, the instruction and data cache stages take only half as long; so they still occupy only one clock cycle. Again, because of the speedup of the register file access, register read and write still occupy only half of a clock cycle.\n\n\nBecause the R4000 caches are on-chip, the virtual-to-physical address translation can delay the cache access. This delay is reduced by implementing virtually indexed caches and going to a parallel cache access and address translation. Figure 15.10c shows the optimized R3000 pipeline with this improvement. Because of the compression of events, the data cache tag check is performed separately on the next cycle after cache access. This check determines whether the data item is in the cache.\n\n\nIn a superpipelined system, existing hardware is used several times per cycle by inserting pipeline registers to split up each pipe stage. Essentially, each superpipeline stage operates at a multiple of the base clock frequency, the multiple depending on the degree of superpipelining. The R4000 technology has the speed and density to permit superpipelining of degree 2. Figure 15.11a shows the optimized R3000 pipeline using this superpipelining. Note that this is essentially the same dynamic structure as Figure 15.10c.\n\n\nFurther improvements can be made. For the R4000, a much larger and specialized adder was designed. This makes it possible to execute ALU operations at\n\n\n\n\n![Figure 15.11(a): Superpipelined implementation of the optimized R3000 pipeline. The diagram shows a 2-stage superpipeline with a clock cycle of 2 clock ticks. The stages are: IC1, IC2, RF, ALU, ALU, DC1, DC2, TC1, TC2, WB. The first stage (IC1, IC2, RF, ALU) occurs in the first clock tick, and the second stage (ALU, DC1, DC2, TC1, TC2, WB) occurs in the second clock tick. A vertical line labeled phi_2 separates the two stages.](images/image_0266.jpeg)\n\n\n| Clock Cycle\nIC1 | IC2 | RF | ALU | ALU | DC1 | DC2 | TC1 | TC2 | WB |  |  |  | \nIC1 | IC2 | RF | ALU | ALU | DC1 | DC2 | TC1 | TC2 | WB |  |  |  | \n\n\nFigure 15.11(a): Superpipelined implementation of the optimized R3000 pipeline. The diagram shows a 2-stage superpipeline with a clock cycle of 2 clock ticks. The stages are: IC1, IC2, RF, ALU, ALU, DC1, DC2, TC1, TC2, WB. The first stage (IC1, IC2, RF, ALU) occurs in the first clock tick, and the second stage (ALU, DC1, DC2, TC1, TC2, WB) occurs in the second clock tick. A vertical line labeled phi_2 separates the two stages.\n\n\n(a) Superpipelined implementation of the optimized R3000 pipeline\n\n\n\n\n![Figure 15.11(b): R4000 pipeline. The diagram shows a 5-stage superpipeline with a clock cycle of 5 clock ticks. The stages are: IF, IS, RF, EX, DF, DS, TC, WB. The first stage (IF) occurs in the first clock tick, and subsequent stages (IS, RF, EX, DF, DS, TC, WB) occur in subsequent ticks. Vertical lines labeled phi_1 and phi_2 separate the stages.](images/image_0267.jpeg)\n\n\n| Clock Cycle\n\\phi_1 | \\phi_2 | \\phi_1 | \\phi_2 | \\phi_1 | \\phi_2 | \\phi_1 | \\phi_2 | \\phi_1 | \\phi_2 | \\phi_1 | \\phi_2 | \\phi_1 | \\phi_2\nIF | IS | RF | EX | DF | DS | TC | WB |  |  |  |  |  | \nIF | IS | RF | EX | DF | DS | TC | WB |  |  |  |  |  | \n\n\nFigure 15.11(b): R4000 pipeline. The diagram shows a 5-stage superpipeline with a clock cycle of 5 clock ticks. The stages are: IF, IS, RF, EX, DF, DS, TC, WB. The first stage (IF) occurs in the first clock tick, and subsequent stages (IS, RF, EX, DF, DS, TC, WB) occur in subsequent ticks. Vertical lines labeled phi_1 and phi_2 separate the stages.\n\n\n(b) R4000 pipeline\n\n\nIF = Instruction fetch first half\n   \n\n   IS = Instruction fetch second half\n   \n\n   RF = Fetch operands from register\n   \n\n   EX = Instruction execute\n   \n\n   IC = Instruction cache\n\n\nDC = Data cache\n   \n\n   DF = Data cache first half\n   \n\n   DS = Data cache second half\n   \n\n   TC = Tag check\n   \n\n   WB = Write back to register file\n\n\n**Figure 15.11**\n   Theoretical R3000 and Actual R4000 Superpipelines\n\n\ntwice the rate. Other improvements allow the execution of loads and stores at twice the rate. The resulting pipeline is shown in Figure 15.11b.\n\n\nThe R4000 has eight pipeline stages, meaning that as many as eight instructions can be in the pipeline at the same time. The pipeline advances at the rate of two stages per clock cycle. The eight pipeline stages are as follows:\n\n\n  * ■\n    **Instruction fetch first half:**\n    Virtual address is presented to the instruction cache and the translation lookaside buffer.\n  * ■\n    **Instruction fetch second half:**\n    Instruction cache outputs the instruction and the TLB generates the physical address.\n  * ■\n    **Register file:**\n    Three activities occur in parallel:\n      * — Instruction is decoded and check made for interlock conditions (i.e., this instruction depends on the result of a preceding instruction).\n  * — Instruction cache tag check is made.\n  * — Operands are fetched from the register file.\n  * ■\n    **Instruction execute:**\n    One of three activities can occur:\n      * — If the instruction is a register-to-register operation, the ALU performs the arithmetic or logical operation.\n  * — If the instruction is a load or store, the data virtual address is calculated.\n  * — If the instruction is a branch, the branch target virtual address is calculated and branch conditions are checked.\n  * ■\n    **Data cache first:**\n    Virtual address is presented to the data cache and TLB.\n  * ■\n    **Data cache second:**\n    The TLB generates the physical address, and the data cache outputs the data.\n  * ■\n    **Tag check:**\n    Cache tag checks are performed for loads and stores.\n  * ■\n    **Write back:**\n    Instruction result is written back to register file."
        },
        {
          "name": "SPARC",
          "content": "SPARC (Scalable Processor Architecture) refers to an architecture defined by Sun Microsystems. Sun developed its own SPARC implementation but also licenses the architecture to other vendors to produce SPARC-compatible machines. The SPARC architecture is inspired by the Berkeley RISC I machine, and its instruction set and register organization is based closely on the Berkeley RISC model.\n\n\n\n\n**SPARC Register Set**\n\n\nAs with the Berkeley RISC, the SPARC makes use of register windows. Each window gives addressability to 24 registers, and the total number of windows is implementation dependent and ranges from 2 to 32 windows. Figure 15.12 illustrates an implementation that supports 8 windows, using a total of 136 physical registers; as the discussion in Section 15.2 indicates, this seems a reasonable number of windows. Physical registers 0 through 7 are global registers shared by all procedures.\n\n\n\n\n![Figure 15.12: SPARC Register Window Layout with Three Procedures. The diagram shows four vertical columns representing Physical registers, Procedure A, Procedure B, and Procedure C. Each column contains a list of registers with their types (Ins, Locals, Outs/Ins, Outs, Globals) and their physical numbers. The Physical registers column lists 135-128 (Ins), 127-120 (Locals), 119-112 (Outs/Ins), 111-104 (Locals), 103-96 (Outs/Ins), 95-88 (Locals), 87-80 (Outs), and 7-0 (Globals). The Procedure A column lists R31_A-R8_A (Ins), R24_A-R16_A (Locals), R15_A-R8_A (Outs), and R7-R0 (Globals). The Procedure B column lists R31_B-R8_B (Ins), R24_B-R16_B (Locals), R15_B-R8_B (Outs), and R7-R0 (Globals). The Procedure C column lists R31_C-R8_C (Ins), R23_C-R16_C (Locals), R15_C-R8_C (Outs), and R7-R0 (Globals). Vertical dots between the columns indicate the continuation of the register window structure.](images/image_0268.jpeg)\n\n\nFigure 15.12: SPARC Register Window Layout with Three Procedures. The diagram shows four vertical columns representing Physical registers, Procedure A, Procedure B, and Procedure C. Each column contains a list of registers with their types (Ins, Locals, Outs/Ins, Outs, Globals) and their physical numbers. The Physical registers column lists 135-128 (Ins), 127-120 (Locals), 119-112 (Outs/Ins), 111-104 (Locals), 103-96 (Outs/Ins), 95-88 (Locals), 87-80 (Outs), and 7-0 (Globals). The Procedure A column lists R31_A-R8_A (Ins), R24_A-R16_A (Locals), R15_A-R8_A (Outs), and R7-R0 (Globals). The Procedure B column lists R31_B-R8_B (Ins), R24_B-R16_B (Locals), R15_B-R8_B (Outs), and R7-R0 (Globals). The Procedure C column lists R31_C-R8_C (Ins), R23_C-R16_C (Locals), R15_C-R8_C (Outs), and R7-R0 (Globals). Vertical dots between the columns indicate the continuation of the register window structure.\n\n\n**Figure 15.12**\n   SPARC Register Window Layout with Three Procedures\n\n\nEach procedure sees logical registers 0 through 31. Logical registers 24 through 31, referred to as\n   *ins*\n   , are shared with the calling (parent) procedure; and logical registers 8 through 15, referred to as\n   *outs*\n   , are shared with any called (child) procedure. These two portions overlap with other windows. Logical registers 16 through 23, referred to as\n   *locals*\n   , are not shared and do not overlap with other windows. Again, as the discussion of Section 12.1 indicates, the availability of 8 registers for parameter passing should be adequate in most cases (e.g., see Table 15.4).\n\n\nFigure 15.13 is another view of the register overlap. The calling procedure places any parameters to be passed in its\n   *outs*\n   registers; the called procedure treats these same physical registers as it\n   *ins*\n   registers. The processor maintains a current window pointer (CWP), located in the processor status register (PSR), that points to the window of the currently executing procedure. The window invalid mask (WIM), also in the PSR, indicates which windows are invalid.\n\n\n\n\n![Diagram illustrating the eight register windows forming a circular stack in SPARC. The diagram shows eight concentric rings of registers. The outermost ring contains 'ins' (inputs) and 'outs' (outputs) for each window. The next ring in contains 'locals' (local registers) for each window. The innermost ring contains 'w0' through 'w7' labels. Arrows labeled 'CWP' (Current Window Pointer) and 'WIM' (Window Index Mask) point to specific windows. The 'CWP' arrow points to the 'w0' window, and the 'WIM' arrow points to the 'w1' window.](images/image_0269.jpeg)\n\n\nDiagram illustrating the eight register windows forming a circular stack in SPARC. The diagram shows eight concentric rings of registers. The outermost ring contains 'ins' (inputs) and 'outs' (outputs) for each window. The next ring in contains 'locals' (local registers) for each window. The innermost ring contains 'w0' through 'w7' labels. Arrows labeled 'CWP' (Current Window Pointer) and 'WIM' (Window Index Mask) point to specific windows. The 'CWP' arrow points to the 'w0' window, and the 'WIM' arrow points to the 'w1' window.\n\n\n**Figure 15.13**\n   Eight Register Windows Forming a Circular Stack in SPARC\n\n\nWith the SPARC register architecture, it is usually not necessary to save and restore registers for a procedure call. The compiler is simplified because the compiler need be concerned only with allocating the local registers for a procedure in an efficient manner and need not be concerned with register allocation between procedures.\n\n\n\n\n**Instruction Set**\n\n\nMost of the SPARC instructions reference only register operands. Register-to-register instructions have three operands and can be expressed in the form\n\n\nR_d \\to R_{S1} \\text{ op } S2\n\n\nwhere\n   \n    R_d\n   \n   and\n   \n    R_{S1}\n   \n   are register references;\n   \n    S2\n   \n   can refer either to a register or to a 13-bit immediate operand. Register zero (\n   \n    R_0\n   \n   ) is hardwired with the value 0. This form is well suited to typical programs, which have a high proportion of local scalars and constants.\n\n\nThe available ALU operations can be grouped as follows:\n\n\n  * ■ Integer addition (with or without carry).\n  * ■ Integer subtraction (with or without carry).\n  * ■ Bitwise Boolean AND, OR, XOR and their negations.\n  * ■ Shift left logical, right logical, or right arithmetic.\n\n\nAll of these instructions, except the shifts, can optionally set the four condition codes (ZERO, NEGATIVE, OVERFLOW, CARRY). Signed integers are represented in 32-bit twos complement form.\n\n\nOnly simple load and store instructions reference memory. There are separate load and store instructions for word (32 bits), doubleword, halfword, and byte. For the latter two cases, there are instructions for loading these quantities as signed or unsigned numbers. Signed numbers are sign extended to fill out the 32-bit destination register. Unsigned numbers are padded with zeros.\n\n\nThe only available addressing mode, other than register, is a displacement mode. That is, the effective address (EA) of an operand consists of a displacement from an address contained in a register:\n\n\nEA = (R_{S1}) + S2 \\\\ \\text{or } EA = (R_{S1}) + (R_{S2})\n\n\ndepending on whether the second operand is immediate or a register reference. To perform a load or store, an extra stage is added to the instruction cycle. During the second stage, the memory address is calculated using the ALU; the load or store occurs in a third stage. This single addressing mode is quite versatile and can be used to synthesize other addressing modes, as indicated in Table 15.10.\n\n\nIt is instructive to compare the SPARC addressing capability with that of the MIPS. The MIPS makes use of a 16-bit offset, compared with a 13-bit offset on the SPARC. On the other hand, the MIPS does not permit an address to be constructed from the contents of two registers.\n\n\n\n\n**Instruction Format**\n\n\nAs with the MIPS R4000, SPARC uses a simple set of 32-bit instruction formats (Figure 15.14). All instructions begin with a 2-bit opcode. For most instructions, this is extended with additional opcode bits elsewhere in the format. For the Call instruction, a 30-bit immediate operand is extended with two zero bits to the right to form a 32-bit PC-relative address in twos complement form. Instructions are aligned on a 32-bit boundary so that this form of addressing suffices.\n\n\nThe Branch instruction includes a 4-bit condition field that corresponds to the four standard condition code bits, so that any combination of conditions can be tested. The 22-bit PC-relative address is extended with two zero bits on the right to\n\n\n**Table 15.10**\n   Synthesizing Other Addressing Modes with SPARC Addressing Modes\n\n\n\nInstruction Type | Addressing Mode | Algorithm | SPARC Equivalent\nRegister-to-register | Immediate | operand = A | S2\nLoad, store | Direct | EA = A | R_0 + S_2\nRegister-to-register | Register | EA = R | R_{S1}, S_2\nLoad, store | Register Indirect | EA = (R) | R_{S1} + 0\nLoad, store | Displacement | EA = (R) + A | R_{S1} + S_2\n\n\nNote: S2 = either a register operand or a 13-bit immediate operand.\n\n\n\n\n![Figure 15.14: SPARC Instruction Formats. The diagram shows five instruction formats: Call format, Branch format, SETHI format, Floating-point format, and General formats. Each format is represented as a horizontal bar divided into fields, with bit positions indicated above them.](images/image_0270.jpeg)\n\n\n**Call format**\n\n\n\n2 |  | 30\nOp | PC-relative displacement\n\n\n**Branch format**\n\n\n\n2 | 1 | 4 | 3 | 22\nOp | a | Cond | Op2 | PC-relative displacement\n\n\n**SETHI format**\n\n\n\n2 | 5 | 3 | 22\nOp | Dest | Op2 | Immediate constant\n\n\n**Floating-point format**\n\n\n\n2 | 5 | 6 | 5 | 9 | 5\nOp | Dest | Op3 | Src-1 | FP-op | Src-2\n\n\n**General formats**\n\n\n\n2 | 5 | 6 | 5 | 1 | 8 | 5\nOp | Dest | Op3 | Src-1 | 0 | Ignored | Src-2\nOp | Dest | Op3 | Src-1 | 1 | Immediate constant\n\n\nFigure 15.14: SPARC Instruction Formats. The diagram shows five instruction formats: Call format, Branch format, SETHI format, Floating-point format, and General formats. Each format is represented as a horizontal bar divided into fields, with bit positions indicated above them.\n\n\n**Figure 15.14**\n   SPARC Instruction Formats\n\n\nform a 24-bit twos complement relative address. An unusual feature of the Branch instruction is the annul bit. When the annul bit is not set, the instruction after the branch is always executed, regardless of whether the branch is taken. This is the typical delayed branch operation found on many RISC machines and described in Section 15.5 (see Figure 15.7). However, when the annul bit is set, the instruction following the branch is executed only if the branch is taken. The processor suppresses the effect of that instruction even though it is already in the pipeline. This annul bit is useful because it makes it easier for the compiler to fill the delay slot following a conditional branch. The instruction that is the target of the branch can always be put in the delay slot, because if the branch is not taken, the instruction can be annulled. The reason this technique is desirable is that conditional branches are generally taken more than half the time.\n\n\nThe SETHI instruction is a special instruction used to form a 32-bit constant. This feature is needed to form large data constants; for example, it can be used to form a large offset for a load or store instruction. The SETHI instruction sets the 22 high-order bits of a register with its 22-bit immediate operand, and zeros out the low-order 10 bits. An immediate constant of up to 13 bits can be specified in one of the general formats, and such an instruction could be used to fill in the remaining 10 bits of the register. A load or store instruction can also be used to achieve a direct\n\n\naddressing mode. To load a value from location K in memory, we could use the following SPARC instructions:\n\n\n\nsethi | %hi(K), %r8 | :load high-order 22 bits of address of location\n      \n      K into register r8\nLd | [%r8 + %lo(K)], %r8 | :load contents of location K into r8\n\n\nThe macros %hi and %lo are used to define immediate operands consisting of the appropriate address bits of a location. This use of SETHI is similar to the use of the lui instruction on the MIPS.\n\n\nThe floating-point format is used for floating-point operations. Two source and one destination registers are designated.\n\n\nFinally, all other operations, including loads, stores, arithmetic, and logical operations use one of the last two formats shown in Figure 15.14. One of the formats makes use of two source registers and a destination register, while the other uses one source register, one 13-bit immediate operand, and one destination register."
        },
        {
          "name": "RISC versus CISC Controversy",
          "content": "For many years, the general trend in computer architecture and organization has been toward increasing processor complexity: more instructions, more addressing modes, more specialized registers, and so on. The RISC movement represents a fundamental break with the philosophy behind that trend. Naturally, the appearance of RISC systems, and the publication of papers by its proponents extolling RISC virtues, led to a reaction from those involved in the design of CISC architectures.\n\n\nThe work that has been done on assessing merits of the RISC approach can be grouped into two categories:\n\n\n  * ■\n    **Quantitative:**\n    Attempts to compare program size and execution speed of programs on RISC and CISC machines that use comparable technology.\n  * ■\n    **Qualitative:**\n    Examines issues such as high-level language support and optimum use of VLSI real estate.\n\n\nMost of the work on quantitative assessment has been done by those working on RISC systems [PATT82b, HEAT84, PATT84], and it has been, by and large, favorable to the RISC approach. Others have examined the issue and come away unconvinced [COLW85a, FLYN87, DAVI87]. There are several problems with attempting such comparisons [SERL86]:\n\n\n  * ■ There is no pair of RISC and CISC machines that are comparable in life-cycle cost, level of technology, gate complexity, sophistication of compiler, operating system support, and so on.\n  * ■ No definitive test set of programs exists. Performance varies with the program.\n  * ■ It is difficult to sort out hardware effects from effects due to skill in compiler writing.\n  * ■ Most of the comparative analysis on RISC has been done on “toy” machines rather than commercial products. Furthermore, most commercially available\n\n\nmachines advertised as RISC possess a mixture of RISC and CISC characteristics. Thus, a fair comparison with a commercial, “pure-play” CISC machine (e.g., VAX, Pentium) is difficult.\n\n\nThe qualitative assessment is, almost by definition, subjective. Several researchers have turned their attention to such an assessment [COLW85a, WALL85], but the results are, at best, ambiguous, and certainly subject to rebuttal [PATT85b] and, of course, counterrebuttal [COLW85b].\n\n\nIn more recent years, the RISC versus CISC controversy has died down to a great extent. This is because there has been a gradual convergence of the technologies. As chip densities and raw hardware speeds increase, RISC systems have become more complex. At the same time, in an effort to squeeze out maximum performance, CISC designs have focused on issues traditionally associated with RISC, such as an increased number of general-purpose registers and increased emphasis on instruction pipeline design."
        }
      ]
    },
    {
      "name": "Instruction-Level Parallelism and Superscalar Processors",
      "sections": [
        {
          "name": "Overview",
          "content": "The term\n   *superscalar*\n   , first coined in 1987 [AGER87], refers to a machine that is designed to improve the performance of the execution of scalar instructions. In most applications, the bulk of the operations are on scalar quantities. Accordingly, the superscalar approach represents the next step in the evolution of high-performance general-purpose processors.\n\n\nThe essence of the superscalar approach is the ability to execute instructions independently and concurrently in different pipelines. The concept can be further exploited by allowing instructions to be executed in an order different from the program order. Figure 16.1 compares, in general terms, the scalar and superscalar approaches. In a traditional scalar organization, there is a single pipelined functional unit for integer operations and one for floating-point operations. Parallelism is achieved by enabling multiple instructions to be at different stages of the pipeline at\n\n\n\n\n![Figure 16.1: Superscalar Organization Compared to Ordinary Scalar Organization. (a) Scalar organization: A single 'Pipelined integer functional unit' connects to an 'Integer register file' and 'Memory'. A single 'Pipelined floating-point functional unit' connects to a 'Floating-point register file' and 'Memory'. (b) Superscalar organization: Multiple 'Pipelined integer functional units' connect to an 'Integer register file' and 'Memory'. Multiple 'Pipelined floating-point functional units' connect to a 'Floating-point register file' and 'Memory'.](images/image_0271.jpeg)\n\n\n(a) Scalar organization\n\n\n(b) Superscalar organization\n\n\nFigure 16.1: Superscalar Organization Compared to Ordinary Scalar Organization. (a) Scalar organization: A single 'Pipelined integer functional unit' connects to an 'Integer register file' and 'Memory'. A single 'Pipelined floating-point functional unit' connects to a 'Floating-point register file' and 'Memory'. (b) Superscalar organization: Multiple 'Pipelined integer functional units' connect to an 'Integer register file' and 'Memory'. Multiple 'Pipelined floating-point functional units' connect to a 'Floating-point register file' and 'Memory'.\n\n\n**Figure 16.1**\n   Superscalar Organization Compared to Ordinary Scalar Organization\n\n\none time. In the superscalar organization, there are multiple functional units, each of which is implemented as a pipeline. Each individual functional unit provides a degree of parallelism by virtue of its pipelined structure. The use of multiple functional units enables the processor to execute streams of instructions in parallel, one stream for each pipeline. It is the responsibility of the hardware, in conjunction with the compiler, to assure that the parallel execution does not violate the intent of the program.\n\n\nMany researchers have investigated superscalar-like processors, and their research indicates that some degree of performance improvement is possible. Table 16.1 presents the reported performance advantages. The differences in the\n\n\n**Table 16.1**\n   Reported Speedups of Superscalar-Like Machines\n\n\n\nReference | Speedup\n[TJAD70] | 1.8\n[KUCK77] | 8\n[WEIS84] | 1.58\n[ACOS86] | 2.7\n[SOHI90] | 1.8\n[SMIT89] | 2.3\n[JOU89b] | 2.2\n[LEE91] | 7\n\n\nresults arise from differences both in the hardware of the simulated machine and in the applications being simulated.\n\n\n\n\n**Superscalar versus Superpipelined**\n\n\nAn alternative approach to achieving greater performance is referred to as superpipelining, a term first coined in 1988 [JOU88]. Superpipelining exploits the fact that many pipeline stages perform tasks that require less than half a clock cycle. Thus, a doubled internal clock speed allows the performance of two tasks in one external clock cycle. We have seen one example of this approach with the MIPS R4000.\n\n\nFigure 16.2 compares the two approaches. The upper part of the diagram illustrates an ordinary pipeline, used as a base for comparison. The base pipeline issues\n\n\n\n\n![Figure 16.2: Comparison of Superscalar and Superpipelined Approaches. The diagram shows three execution paths over 9 base cycles. The 'Simple 4-stage pipeline' takes 4 cycles per instruction. The 'Superpipelined' approach uses a 2-stage pipeline with a doubled internal clock, completing an instruction in 2 base cycles. The 'Superscalar' approach issues two instructions per cycle, completing 4 instructions in 4 cycles.](images/image_0272.jpeg)\n\n\nKey:\n    **Execute**\n\n\n\nFetch | Decode | Execute | Write\n |  | █ | \n█ |  | █ | \n█ | █ | █ | \n█ | █ | █ | █\n█ | █ | █ | █\n█ | █ | █ | █\n█ | █ | █ | █\n█ | █ | █ | █\n█ | █ | █ | █\n\n\nSimple 4-stage pipeline\n\n\nSuperpipelined\n\n\nSuperscalar\n\n\nSuccessive instructions\n\n\nTime in base cycles\n\n\nFigure 16.2: Comparison of Superscalar and Superpipelined Approaches. The diagram shows three execution paths over 9 base cycles. The 'Simple 4-stage pipeline' takes 4 cycles per instruction. The 'Superpipelined' approach uses a 2-stage pipeline with a doubled internal clock, completing an instruction in 2 base cycles. The 'Superscalar' approach issues two instructions per cycle, completing 4 instructions in 4 cycles.\n\n\n**Figure 16.2**\n   Comparison of Superscalar and Superpipelined Approaches\n\n\none instruction per clock cycle and can perform one pipeline stage per clock cycle. The pipeline has four stages: instruction fetch; operation decode; operation execution; and result write back. The execution stage is crosshatched for clarity. Note that although several instructions are executing concurrently, only one instruction is in its execution stage at any one time.\n\n\nThe next part of the diagram shows a\n   **superpipelined**\n   implementation that is capable of performing two pipeline stages per clock cycle. An alternative way of looking at this is that the functions performed in each stage can be split into two nonoverlapping parts and each can execute in half a clock cycle. A superpipeline implementation that behaves in this fashion is said to be of degree 2. Finally, the lowest part of the diagram shows a superscalar implementation capable of executing two instances of each stage in parallel. Higher-degree superpipeline and superscalar implementations are of course possible.\n\n\nBoth the superpipeline and the superscalar implementations depicted in Figure 16.2 have the same number of instructions executing at the same time in the steady state. The superpipelined processor falls behind the superscalar processor at the start of the program and at each branch target.\n\n\n\n\n**Constraints**\n\n\nThe superscalar approach depends on the ability to execute multiple instructions in parallel. The term\n   **instruction-level parallelism**\n   refers to the degree to which, on average, the instructions of a program can be executed in parallel. A combination of compiler-based optimization and hardware techniques can be used to maximize instruction-level parallelism. Before examining the design techniques used in superscalar machines to increase instruction-level parallelism, we need to look at the fundamental limitations to parallelism with which the system must cope. [JOHN91] lists five limitations:\n\n\n  * ■ True data dependency;\n  * ■ Procedural dependency;\n  * ■ Resource conflicts;\n  * ■ Output dependency;\n  * ■ Antidependency.\n\n\nWe examine the first three of these limitations in the remainder of this section. A discussion of the last two must await some of the developments in the next section.\n\n\n**TRUE DATA DEPENDENCY**\n   Consider the following sequence:\n   \n    1\n\n\nADD EAX, ECX ;load register EAX with the con-\n              ;tents of ECX plus the contents\n              ;of EAX\nMOV EBX, EAX ;load EBX with the contents of EAX\nThe second instruction can be fetched and decoded but cannot execute until the first instruction executes. The reason is that the second instruction needs data\n\n\n1\n   \n   For the Intel x86 assembly language, a semicolon starts a comment field.\n\n\n\n\n![Figure 16.3: Effect of Dependencies. A timeline diagram showing instruction execution with various dependencies.](images/image_0273.jpeg)\n\n\nKey:\n\n\n\nIfetch | Decode | Execute | Write\n\n\nThe diagram shows a timeline from 0 to 9 base cycles. Instructions are represented by horizontal bars divided into four stages: Ifetch, Decode, Execute, and Write. The Execute stage is shown with a cross-hatch pattern.\n\n\n  * **No dependency:**\n     Instructions i0 and i1 execute in parallel. i0 starts at cycle 0, i1 starts at cycle 1.\n  * **Data dependency:**\n     Instructions i0 and i1. i0 starts at cycle 0. i1 starts at cycle 2, but its Execute stage (cycles 3-4) overlaps with the Write stage of i0 (cycle 4), causing a delay.\n  * **Procedural dependency:**\n     Instructions i0, i1/branch, i2, i3, i4, i5. i0 starts at cycle 0. i1/branch starts at cycle 1. i2 starts at cycle 3. i3 starts at cycle 3. i4 starts at cycle 4. i5 starts at cycle 4. The Execute stages of i2, i3, i4, and i5 all overlap with the Write stage of i1/branch (cycle 4), causing a delay.\n  * **Resource conflict:**\n     Instructions i0 and i1. i0 starts at cycle 0. i1 starts at cycle 1. Both instructions use the same functional unit, so their Execute stages (cycles 3-4) overlap, causing a delay.\n\n\nTime in base cycles\n\n\nFigure 16.3: Effect of Dependencies. A timeline diagram showing instruction execution with various dependencies.\n\n\n**Figure 16.3**\n   Effect of Dependencies\n\n\nproduced by the first instruction. This situation is referred to as a\n   **true data dependency**\n   (also called\n   **flow dependency**\n   or\n   **read after write [RAW] dependency**\n   ).\n\n\nFigure 16.3 illustrates this\n   **dependency**\n   in a superscalar machine of degree 2. With no dependency, two instructions can be fetched and executed in parallel. If there is a data dependency between the first and second instructions, then the second instruction is delayed as many clock cycles as required to remove the dependency. In general, any instruction must be delayed until all of its input values have been produced.\n\n\nIn a simple pipeline, such as illustrated in the upper part of Figure 16.2, the aforementioned sequence of instructions would cause no delay. However, consider the following, in which one of the loads is from memory rather than from a register:\n\n\nMOV EAX, eff ;load register EAX with the\n             contents of effective memory\n             address eff\nMOV EBX, EAX ;load EBX with the contents of EAX\n\nA typical RISC processor takes two or more cycles to perform a load from memory when the load is a cache hit. It can take tens or even hundreds of cycles for a cache miss on all cache levels, because of the delay of an off-chip memory access. One way to compensate for this delay is for the compiler to reorder instructions so that one or more subsequent instructions that do not depend on the memory load can begin flowing through the pipeline. This scheme is less effective in the case of a superscalar pipeline: The independent instructions executed during the load are likely to be executed on the first cycle of the load, leaving the processor with nothing to do until the load completes.\n\n\n**PROCEDURAL DEPENDENCIES**\n   As was discussed in Chapter 14, the presence of branches in an instruction sequence complicates the pipeline operation. The instructions following a branch (taken or not taken) have a\n   **procedural dependency**\n   on the branch and cannot be executed until the branch is executed. Figure 16.3 illustrates the effect of a branch on a superscalar pipeline of degree 2.\n\n\nAs we have seen, this type of procedural dependency also affects a scalar pipeline. The consequence for a superscalar pipeline is more severe, because a greater magnitude of opportunity is lost with each delay.\n\n\nIf variable-length instructions are used, then another sort of procedural dependency arises. Because the length of any particular instruction is not known, it must be at least partially decoded before the following instruction can be fetched. This prevents the simultaneous fetching required in a superscalar pipeline. This is one of the reasons that superscalar techniques are more readily applicable to a RISC or RISC-like architecture, with its fixed instruction length.\n\n\n**RESOURCE CONFLICT**\n   A\n   **resource conflict**\n   is a competition of two or more instructions for the same resource at the same time. Examples of resources include memories, caches, buses, register-file ports, and functional units (e.g., ALU adder).\n\n\nIn terms of the pipeline, a resource conflict exhibits similar behavior to a data dependency (Figure 16.3). There are some differences, however. For one thing, resource conflicts can be overcome by duplication of resources, whereas a true data dependency cannot be eliminated. Also, when an operation takes a long time to complete, resource conflicts can be minimized by pipelining the appropriate functional unit."
        },
        {
          "name": "Design Issues",
          "content": "**Instruction-Level Parallelism and Machine Parallelism**\n\n\n[JOU89a] makes an important distinction between the two related concepts of instruction-level parallelism and machine parallelism.\n   **Instruction-level parallelism**\n   exists when instructions in a sequence are independent and thus can be executed in parallel by overlapping.\n\n\nAs an example of the concept of instruction-level parallelism, consider the following two code fragments [JOU89b]:\n\n\n\nLoad R1 ← R2 | Add R3 ← R3, \"1\"\nAdd R3 ← R3, \"1\" | Add R4 ← R3, R2\nAdd R4 ← R4, R2 | Store [R4] ← R0\n\n\nThe three instructions on the left are independent, and in theory all three could be executed in parallel. In contrast, the three instructions on the right cannot be executed in parallel because the second instruction uses the result of the first, and the third instruction uses the result of the second.\n\n\nThe degree of instruction-level parallelism is determined by the frequency of true data dependencies and procedural dependencies in the code. These factors, in turn, are dependent on the instruction set architecture and on the application. Instruction-level parallelism is also determined by what [JOU89a] refers to as operation latency: the time until the result of an instruction is available for use as an operand in a subsequent instruction. The latency determines how much of a delay a data or procedural dependency will cause.\n\n\n**Machine parallelism**\n   is a measure of the ability of the processor to take advantage of instruction-level parallelism. Machine parallelism is determined by the number of instructions that can be fetched and executed at the same time (the number of parallel pipelines) and by the speed and sophistication of the mechanisms that the processor uses to find independent instructions.\n\n\nBoth instruction-level and machine parallelism are important factors in enhancing performance. A program may not have enough instruction-level parallelism to take full advantage of machine parallelism. The use of a fixed-length instruction set architecture, as in a RISC, enhances instruction-level parallelism. On the other hand, limited machine parallelism will limit performance no matter what the nature of the program.\n\n\n\n\n**Instruction Issue Policy**\n\n\nAs was mentioned, machine parallelism is not simply a matter of having multiple instances of each pipeline stage. The processor must also be able to identify instruction-level parallelism and orchestrate the fetching, decoding, and execution of instructions in parallel. [JOHN91] uses the term\n   **instruction issue**\n   to refer to the process of initiating instruction execution in the processor's functional units and the term\n   **instruction issue policy**\n   to refer to the protocol used to issue instructions. In general, we can say that instruction issue occurs when instruction moves from the decode stage of the pipeline to the first execute stage of the pipeline.\n\n\nIn essence, the processor is trying to look ahead of the current point of execution to locate instructions that can be brought into the pipeline and executed. Three types of orderings are important in this regard:\n\n\n  * ■ The order in which instructions are fetched;\n  * ■ The order in which instructions are executed;\n  * ■ The order in which instructions update the contents of register and memory locations.\n\n\nThe more sophisticated the processor, the less it is bound by a strict relationship between these orderings. To optimize utilization of the various pipeline elements, the processor will need to alter one or more of these orderings with respect to the ordering to be found in a strict sequential execution. The one constraint on the processor is that the result must be correct. Thus, the processor must accommodate the various dependencies and conflicts discussed earlier.\n\n\nIn general terms, we can group superscalar instruction issue policies into the following categories:\n\n\n  * ■ In-order issue with in-order completion.\n  * ■ In-order issue with\n    **out-of-order**\n    completion.\n  * ■ Out-of-order issue with out-of-order completion.\n\n\n**IN-ORDER ISSUE WITH IN-ORDER COMPLETION**\n   The simplest instruction issue policy is to issue instructions in the exact order that would be achieved by sequential execution (\n   **in-order issue**\n   ) and to write results in that same order (\n   **in-order completion**\n   ). Not even scalar pipelines follow such a simple-minded policy. However, it is useful to consider this policy as a baseline for comparing more sophisticated approaches.\n\n\nFigure 16.4a gives an example of this policy. We assume a superscalar pipeline capable of fetching and decoding two instructions at a time, having three separate functional units (e.g., two integer arithmetic and one floating-point arithmetic), and having two instances of the write-back pipeline stage. The example assumes the following constraints on a six-instruction code fragment:\n\n\n  * ■ I1 requires two cycles to execute.\n  * ■ I3 and I4 conflict for the same functional unit.\n  * ■ I5 depends on the value produced by I4.\n  * ■ I5 and I6 conflict for a functional unit.\n\n\nInstructions are fetched two at a time and passed to the decode unit. Because instructions are fetched in pairs, the next two instructions must wait until the pair of decode pipeline stages has cleared. To guarantee in-order\n   **completion**\n   , when there is a conflict for a functional unit or when a functional unit requires more than one cycle to generate a result, the issuing of instructions temporarily stalls.\n\n\nIn this example, the elapsed time from decoding the first instruction to writing the last results is eight cycles.\n\n\n**IN-ORDER ISSUE WITH OUT-OF-ORDER COMPLETION**\n**Out-of-order completion**\n   is used in scalar RISC processors to improve the performance of instructions that require multiple cycles. Figure 16.4b illustrates its use on a superscalar processor. Instruction I2 is allowed to run to completion prior to I1. This allows I3 to be completed earlier, with the net result of a savings of one cycle.\n\n\nWith out-of-order completion, any number of instructions may be in the execution stage at any one time, up to the maximum degree of machine parallelism across all functional units. Instruction issuing is stalled by a resource conflict, a data dependency, or a procedural dependency.\n\n\nIn addition to the aforementioned limitations, a new dependency, which we referred to earlier as an\n   **output dependency**\n   (also called\n   **write after write [WAW] dependency**\n   ), arises. The following code fragment illustrates this dependency (\n   *op*\n   represents any operation):\n\n\nI1: R3 ← R3 op R5\nI2: R4 ← R3 + 1\nI3: R3 ← R5 + 1\nI4: R7 ← R3 op R4\n\nDecode | Execute | Write | Cycle\nI1 | I2 |  |  |  |  | 1\nI3 | I4 | I1 | I2 |  |  | 2\nI3 | I4 | I1 |  |  | I3 | 3\n | I4 |  |  |  | I1 | 4\nI5 | I6 |  |  |  | I2 | 5\n | I6 |  |  |  |  | 6\n |  |  |  | I3 | I4 | 7\n |  |  |  | I5 | I6 | 8\n\n\n(a) In-order issue and in-order completion\n\n\n\nDecode | Execute | Write | Cycle\nI1 | I2 |  |  |  |  | 1\nI3 | I4 | I1 | I2 |  |  | 2\nI3 | I4 | I1 |  |  | I3 | 3\nI5 | I6 |  |  |  | I2 | 4\n | I6 |  |  |  | I1 | 5\n |  |  |  |  | I3 | 6\n |  |  |  | I5 | I4 | 7\n |  |  |  | I6 | I5 | 8\n\n\n(b) In-order issue and out-of-order completion\n\n\n\nDecode | Window | Execute | Write | Cycle\nI1 | I2 |  |  |  |  |  |  | 1\nI3 | I4 | I1,I2 |  |  |  |  |  | 2\nI5 | I6 | I3,I4 |  | I1 | I2 |  |  | 3\n |  | I4,I5,I6 |  | I1 |  | I3 |  | 4\n |  | I5 |  |  | I6 | I4 |  | 5\n |  |  |  |  | I5 |  |  | 6\n |  |  |  |  |  | I2 |  | 7\n |  |  |  |  |  | I1 | I3 | 8\n |  |  |  |  |  | I4 | I6 | 9\n |  |  |  |  |  | I5 |  | 10\n\n\n(c) Out-of-order issue and out-of-order completion\n\n\n**Figure 16.4**\nInstruction I2 cannot execute before instruction I1, because it needs the result in register R3 produced in I1; this is an example of a true data dependency, as described in Section 16.1. Similarly, I4 must wait for I3, because it uses a result produced by I3. What about the relationship between I1 and I3? There is no data dependency here, as we have defined it. However, if I3 executes to completion prior to I1, then the wrong value of the contents of R3 will be fetched for the execution of I4. Consequently, I3 must complete after I1 to produce the correct output values. To ensure this, the issuing of the third instruction must be stalled if its result might later be overwritten by an older instruction that takes longer to complete.\n\n\nOut-of-order completion requires more complex instruction issue logic than in-order completion. In addition, it is more difficult to deal with instruction interrupts and exceptions. When an interrupt occurs, instruction execution at the current\n\n\npoint is suspended, to be resumed later. The processor must assure that the resumption takes into account that, at the time of interruption, instructions ahead of the instruction that caused the interrupt may already have completed.\n\n\n**OUT-OF-ORDER ISSUE WITH OUT-OF-ORDER COMPLETION**\n   With in-order issue, the processor will only decode instructions up to the point of a dependency or conflict. No additional instructions are decoded until the conflict is resolved. As a result, the processor cannot look ahead of the point of conflict to subsequent instructions that may be independent of those already in the pipeline and that may be usefully introduced into the pipeline.\n\n\nTo allow\n   **out-of-order issue**\n   , it is necessary to decouple the decode and execute stages of the pipeline. This is done with a buffer referred to as an\n   **instruction window**\n   . With this organization, after a processor has finished decoding an instruction, it is placed in the instruction window. As long as this buffer is not full, the processor can continue to fetch and decode new instructions. When a functional unit becomes available in the execute stage, an instruction from the instruction window may be issued to the execute stage. Any instruction may be issued, provided that (1) it needs the particular functional unit that is available, and (2) no conflicts or dependencies block this instruction. Figure 16.5 suggests this organization.\n\n\nThe result of this organization is that the processor has a lookahead capability, allowing it to identify independent instructions that can be brought into the execute stage. Instructions are issued from the instruction window with little regard for their original program order. As before, the only constraint is that the program execution behaves correctly.\n\n\nFigures 16.4c illustrates this policy. During each of the first three cycles, two instructions are fetched into the decode stage. During each cycle, subject to the constraint of the buffer size, two instructions move from the decode stage to the instruction window. In this example, it is possible to issue instruction I6 ahead of I5 (recall that I5 depends on I4, but I6 does not). Thus, one cycle is saved in both the execute and write-back stages, and the end-to-end savings, compared with Figure 16.4b, is one cycle.\n\n\n\n\n![Diagram illustrating the organization for Out-of-Order Issue with Out-of-Order Completion. The diagram shows two main sections: 'In-order front end' and 'Out-of-order execution'. The 'In-order front end' consists of four vertical boxes: Fetch, Decode, Rename, and Dispatch. The 'Out-of-order execution' section consists of five vertical boxes: Issue, Register read, Execute, Write back, and Commit. A 'Buffer of instructions' is shown as a horizontal box above the 'Issue' and 'Commit' boxes. Arrows indicate the flow of instructions: from 'Dispatch' to the 'Buffer of instructions', from the 'Buffer of instructions' to 'Issue', and from 'Commit' back to the 'Buffer of instructions'.](images/image_0274.jpeg)\n\n\nDiagram illustrating the organization for Out-of-Order Issue with Out-of-Order Completion. The diagram shows two main sections: 'In-order front end' and 'Out-of-order execution'. The 'In-order front end' consists of four vertical boxes: Fetch, Decode, Rename, and Dispatch. The 'Out-of-order execution' section consists of five vertical boxes: Issue, Register read, Execute, Write back, and Commit. A 'Buffer of instructions' is shown as a horizontal box above the 'Issue' and 'Commit' boxes. Arrows indicate the flow of instructions: from 'Dispatch' to the 'Buffer of instructions', from the 'Buffer of instructions' to 'Issue', and from 'Commit' back to the 'Buffer of instructions'.\n\n\n**Figure 16.5**\n   Organization for Out-of-Order Issue with Out-of-Order Completion\n\n\nThe instruction window is depicted in Figure 16.4c to illustrate its role. However, this window is not an additional pipeline stage. An instruction being in the window simply implies that the processor has sufficient information about that instruction to decide when it can be issued.\n\n\nThe out-of-order issue, out-of-order completion policy is subject to the same constraints described earlier. An instruction cannot be issued if it violates a dependency or conflict. The difference is that more instructions are available for issuing, reducing the probability that a pipeline stage will have to stall. In addition, a new dependency, which we referred to earlier as an\n   **antidependency**\n   (also called\n   **write after read [WAR] dependency**\n   ), arises. The code fragment considered earlier illustrates this dependency:\n\n\nI1: R3 ← R3 op R5\nI2: R4 ← R3 + 1\nI3: R3 ← R5 + 1\nI4: R7 ← R3 op R4\nInstruction I3 cannot complete execution before instruction I2 begins execution and has fetched its operands. This is so because I3 updates register R3, which is a source operand for I2. The term\n   *antidependency*\n   is used because the constraint is similar to that of a true data dependency, but reversed: Instead of the first instruction producing a value that the second instruction uses, the second instruction destroys a value that the first instruction uses.\n\n\n\n\n![Logo for Online Interactive Simulation, featuring a globe and the text 'Online Interactive Simulation' and 'www'.](images/image_0275.jpeg)\n\n\nLogo for Online Interactive Simulation, featuring a globe and the text 'Online Interactive Simulation' and 'www'.\n\n\n\n\n**Reorder Buffer Simulator**\n\n\n\n\n**Tomasulo's Algorithm Simulator**\n\n\n\n\n**Alternative Simulation of Tomasulo's Algorithm**\n\n\nOne common technique that is used to support out-of-order completion is the reorder buffer. The reorder buffer is temporary storage for results completed out of order that are then committed to the register file in program order. A related concept is Tomasulo's algorithm. Appendix N examines these concepts.\n\n\n\n\n**Register Renaming**\n\n\nWhen out-of-order instruction issuing and/or out-of-order instruction completion are allowed, we have seen that this gives rise to the possibility of WAW dependencies and WAR dependencies. These dependencies differ from RAW data dependencies and resource conflicts, which reflect the flow of data through a program and the sequence of execution. WAW dependencies and WAR dependencies, on the other hand, arise because the values in registers may no longer reflect the sequence of values dictated by the program flow.\n\n\nWhen instructions are issued in sequence and complete in sequence, it is possible to specify the contents of each register at each point in the execution. When out-of-order techniques are used, the values in registers cannot be fully known at each point in time just from a consideration of the sequence of instructions dictated\n\n\nby the program. In effect, values are in conflict for the use of registers, and the processor must resolve those conflicts by occasionally stalling a pipeline stage.\n\n\nAntidependencies and output dependencies are both examples of storage conflicts. Multiple instructions are competing for the use of the same register locations, generating pipeline constraints that retard performance. The problem is made more acute when register optimization techniques are used (as discussed in Chapter 15), because these compiler techniques attempt to maximize the use of registers, hence maximizing the number of storage conflicts.\n\n\nOne method for coping with these types of storage conflicts is based on a traditional resource-conflict solution: duplication of resources. In this context, the technique is referred to as\n   **register renaming**\n   . In essence, registers are allocated dynamically by the processor hardware, and they are associated with the values needed by instructions at various points in time. When a new register value is created (i.e., when an instruction executes that has a register as a destination operand), a new register is allocated for that value. Subsequent instructions that access that value as a source operand in that register must go through a renaming process: the register references in those instructions must be revised to refer to the register containing the needed value. Thus, the same original register reference in several different instructions may refer to different actual registers, if different values are intended.\n\n\nLet us consider how register renaming could be used on the code fragment we have been examining:\n\n\nI1: R3b ← R3a op R5a\nI2: R4b ← R3b + 1\nI3: R3c ← R5a + 1\nI4: R7b ← R3c op R4b\nThe register reference without the subscript refers to the logical register reference found in the instruction. The register reference with the subscript refers to a hardware register allocated to hold a new value. When a new allocation is made for a particular logical register, subsequent instruction references to that logical register as a source operand are made to refer to the most recently allocated hardware register (recent in terms of the program sequence of instructions).\n\n\nIn this example, the creation of register\n   \n    R3_c\n   \n   in instruction I3 avoids the WAR dependency on the second instruction and the WAW on the first instruction, and it does not interfere with the correct value being accessed by I4. The result is that I3 can be issued immediately; without renaming, I3 cannot be issued until the first instruction is complete and the second instruction is issued.\n\n\n\n\n![Logo for Online Interactive Simulation, featuring a globe and the text 'Online Interactive Simulation' and 'www'.](images/image_0276.jpeg)\n\n\nLogo for Online Interactive Simulation, featuring a globe and the text 'Online Interactive Simulation' and 'www'.\n\n\n\n\n**Scoreboarding Simulator**\n\n\nAn alternative to register renaming is scoreboarding. In essence, scoreboarding is a bookkeeping technique that allows instructions to execute whenever they are not dependent on previous instructions and no structural hazards are present. See Appendix N for a discussion.\n\n\n\n\n**Machine Parallelism**\n\n\nIn the preceding discussion, we looked at three hardware techniques that can be used in a superscalar processor to enhance performance: duplication of resources, out-of-order issue, and renaming. One study that illuminates the relationship among these techniques was reported in [SMIT89]. The study made use of a simulation that modeled a machine with the characteristics of the MIPS R2000, augmented with various superscalar features. A number of different program sequences were simulated.\n\n\nFigure 16.6 shows the results. In each of the graphs, the vertical axis corresponds to the mean speedup of the superscalar machine over the scalar machine. The horizontal axis shows the results for four alternative processor organizations. The base machine does not duplicate any of the functional units, but it can issue instructions out of order. The second configuration duplicates the load/store functional unit that accesses a data cache. The third configuration duplicates the ALU, and the fourth configuration duplicates both load/store and ALU. In each graph, results are shown for instruction window sizes of 8, 16, and 32 instructions, which dictates the amount of lookahead the processor can do. The difference between the two graphs is that, in the second, register renaming is allowed. This is equivalent to saying that the first graph reflects a machine that is limited by all dependencies, whereas the second graph corresponds to a machine that is limited only by true dependencies.\n\n\nThe two graphs, combined, yield some important conclusions. The first is that it is probably not worthwhile to add functional units without register renaming. There\n\n\n\n\n![Figure 16.6: Speedups of Various Machine Organizations without Procedural Dependencies. Two bar charts comparing speedup for four configurations (base, +ld/st, +alu, +both) across three window sizes (8, 16, 32) with and without register renaming.](images/image_0277.jpeg)\n\n\nFigure 16.6 consists of two bar charts comparing the speedup of various machine organizations. The y-axis for both charts is 'Speedup', ranging from 0 to 4. The x-axis for both is the machine configuration: 'base', '+ld/st', '+alu', and '+both'. A legend at the top indicates that the three shades of gray represent different window sizes: dark gray for 8, medium gray for 16, and light gray for 32.\n\n\nThe left chart, titled 'Without renaming', shows speedup values that are relatively low and similar across configurations. The right chart, titled 'With renaming', shows significantly higher speedup values, especially for the '+alu' and '+both' configurations.\n\n\n\nConfiguration | Without renaming | With renaming\n8 | 16 | 32 | 8 | 16 | 32\nbase | 1.9 | 2.0 | 2.1 | 2.3 | 2.5 | 2.6\n+ld/st | 1.9 | 2.1 | 2.1 | 2.3 | 2.6 | 2.6\n+alu | 2.2 | 2.4 | 2.4 | 2.9 | 3.3 | 3.7\n+both | 2.3 | 2.5 | 2.6 | 3.0 | 3.7 | 4.1\n\n\nFigure 16.6: Speedups of Various Machine Organizations without Procedural Dependencies. Two bar charts comparing speedup for four configurations (base, +ld/st, +alu, +both) across three window sizes (8, 16, 32) with and without register renaming.\n\n\n**Figure 16.6**\n   Speedups of Various Machine Organizations without Procedural Dependencies\n\n\nis some slight improvement in performance, but at the cost of increased hardware complexity. With register renaming, which eliminates antidependencies and output dependencies, noticeable gains are achieved by adding more functional units. Note, however, that there is a significant difference in the amount of gain achievable between using an instruction window of 8 versus a larger instruction window. This indicates that if the instruction window is too small, data dependencies will prevent effective utilization of the extra functional units; the processor must be able to look quite far ahead to find independent instructions to utilize the hardware more fully.\n\n\n\n\n![Logo for Online Interactive Simulation (OIS) featuring a globe and the text 'www'.](images/image_0278.jpeg)\n\n\nLogo for Online Interactive Simulation (OIS) featuring a globe and the text 'www'.\n\n\n\n\n**Pipeline with Static vs. Dynamic Scheduling—Simulator**\n\n\n\n\n**Branch Prediction**\n\n\nAny high-performance pipelined machine must address the issue of dealing with branches. For example, the Intel 80486 addressed the problem by fetching both the next sequential instruction after a branch and speculatively fetching the branch target instruction. However, because there are two pipeline stages between prefetch and execution, this strategy incurs a two-cycle delay when the branch gets taken.\n\n\nWith the advent of RISC machines, the delayed branch strategy was explored. This allows the processor to calculate the result of conditional branch instructions before any unusable instructions have been prefetched. With this method, the processor always executes the single instruction that immediately follows the branch. This keeps the pipeline full while the processor fetches a new instruction stream.\n\n\nWith the development of superscalar machines, the delayed branch strategy has less appeal. The reason is that multiple instructions need to execute in the delay slot, raising several problems relating to instruction dependencies. Thus, superscalar machines have returned to pre-RISC techniques of\n   **branch prediction**\n   . Some, like the PowerPC 601, use a simple static branch prediction technique. More sophisticated processors, such as the PowerPC 620 and the Pentium 4, use dynamic branch prediction based on branch history analysis.\n\n\n\n\n**Superscalar Execution**\n\n\nWe are now in a position to provide an overview of superscalar execution of programs; this is illustrated in Figure 16.7. The program to be executed consists of a linear sequence of instructions. This is the static program as written by the programmer or generated by the compiler. The instruction fetch stage, which includes branch prediction, is used to form a dynamic stream of instructions. This stream is examined for dependencies, and the processor may remove artificial dependencies. The processor then dispatches the instructions into a window of execution. In this window, instructions no longer form a sequential stream but are structured according to their true data dependencies. The processor executes each instruction in an order determined by the true data dependencies and hardware resource availability. Finally, instructions are conceptually put back into sequential order and their results are recorded.\n\n\n\n\n![Figure 16.7: Conceptual Depiction of Superscalar Processing. The diagram shows a 'Static program' block on the left feeding into a 'Window of execution' stage. This stage is divided into three sub-stages: 'Instruction fetch and branch prediction', 'Instruction dispatch', and 'Instruction issue'. Multiple instructions are shown as horizontal lines moving through these stages. The 'Instruction issue' stage is highlighted with a dashed green border and contains several vertical lines representing parallel execution units. Arrows show instructions being issued to these units. After the window, instructions move to 'Instruction execution' and finally to 'Instruction reorder and commit'.](images/image_0279.jpeg)\n\n\nFigure 16.7: Conceptual Depiction of Superscalar Processing. The diagram shows a 'Static program' block on the left feeding into a 'Window of execution' stage. This stage is divided into three sub-stages: 'Instruction fetch and branch prediction', 'Instruction dispatch', and 'Instruction issue'. Multiple instructions are shown as horizontal lines moving through these stages. The 'Instruction issue' stage is highlighted with a dashed green border and contains several vertical lines representing parallel execution units. Arrows show instructions being issued to these units. After the window, instructions move to 'Instruction execution' and finally to 'Instruction reorder and commit'.\n\n\n**Figure 16.7**\n   Conceptual Depiction of Superscalar Processing\n\n\nThe final step mentioned in the preceding paragraph is referred to as\n   **committing**\n   , or\n   **retiring**\n   , the instruction. This step is needed for the following reason. Because of the use of parallel, multiple pipelines, instructions may complete in an order different from that shown in the static program. Further, the use of branch prediction and speculative execution means that some instructions may complete execution and then must be abandoned because the branch they represent is not taken. Therefore, permanent storage and program-visible registers cannot be updated immediately when instructions complete execution. Results must be held in some sort of temporary storage that is usable by dependent instructions and then made permanent when it is determined that the sequential model would have executed the instruction.\n\n\n\n\n**Superscalar Implementation**\n\n\nBased on our discussion so far, we can make some general comments about the processor hardware required for the superscalar approach. [SMIT95] lists the following key elements:\n\n\n  * ■ Instruction fetch strategies that simultaneously fetch multiple instructions, often by predicting the outcomes of, and fetching beyond, conditional branch instructions. These functions require the use of multiple pipeline fetch and decode stages, and branch prediction logic.\n  * ■ Logic for determining true dependencies involving register values, and mechanisms for communicating these values to where they are needed during execution.\n  * ■ Mechanisms for initiating, or issuing, multiple instructions in parallel.\n  * ■ Resources for parallel execution of multiple instructions, including multiple pipelined functional units and memory hierarchies capable of simultaneously servicing multiple memory references.\n  * ■ Mechanisms for committing the process state in correct order."
        },
        {
          "name": "Intel Core Microarchitecture",
          "content": "Although the concept of superscalar design is generally associated with the RISC architecture, the same superscalar principles can be applied to a CISC machine. Perhaps the most notable example of this is the Intel x86 architecture. The evolution of superscalar concepts in the Intel line is interesting to note. The 386 is a traditional CISC nonpipelined machine. The 486 introduced the first pipelined x86 processor, reducing the average latency of integer operations from between two and four cycles to one cycle, but still limited to executing a single instruction each cycle, with no superscalar elements. The original Pentium had a modest superscalar component, consisting of the use of two separate integer execution units. The Pentium Pro introduced a full-blown superscalar design with out-of-order execution. Subsequent x86 models have refined and enhanced the superscalar design.\n\n\nFigure 16.8 shows the current version of the x86 pipeline architecture. Intel refers to a pipeline architecture as a\n   *microarchitecture*\n   . The microarchitecture\n\n\n\n\n![Block diagram of the Intel Core Microarchitecture showing the instruction pipeline and execution units.](images/image_0280.jpeg)\n\n\nThe diagram illustrates the Intel Core Microarchitecture, showing the flow of instructions from the L1 instruction cache through various stages of the pipeline to the execution units and finally to the L1 data cache and DTLB.\n\n\n**Instruction Pipeline Flow:**\n\n\n  * **L1 instruction cache**\n     feeds into\n     **Instruction fetch and predecode**\n     .\n  * **Instruction fetch and predecode**\n     feeds into\n     **Instruction queue**\n     .\n  * **Instruction queue**\n     feeds into\n     **Decode**\n     .\n  * **Decode**\n     feeds into\n     **Rename/Allocator**\n     .\n  * **Rename/Allocator**\n     feeds into\n     **Retirement unit (Re-order buffer)**\n     .\n  * **Retirement unit (Re-order buffer)**\n     feeds into\n     **Scheduler/Reservation station**\n     .\n\n\n**Execution Units (Ports 0-4):**\n\n\n  * **Port 0:**\n     Integer ALU, branch, MMX/SSE, FPmove.\n  * **Port 1:**\n     Integer ALU, FPAdd, MMX/SSE, FPmove.\n  * **Port 2:**\n     Integer ALU, FPMul, MMX/SSE, FPmove.\n  * **Port 3:**\n     Load unit.\n  * **Port 4:**\n     Store Unit, Memory ordering buffer.\n\n\n**External and Internal Connections:**\n\n\n  * **Microcode ROM**\n     provides input to the\n     **Decode**\n     stage.\n  * **Branch prediction unit**\n     provides input to the\n     **Instruction fetch and predecode**\n     stage and receives input from the\n     **Retirement unit (Re-order buffer)**\n     .\n  * **Shared bus interface unit**\n     connects to the\n     **Shared L2 cache up to 10.7 Gbps FSB**\n     .\n  * **Shared L2 cache up to 10.7 Gbps FSB**\n     connects to the\n     **Shared bus interface unit**\n     and the\n     **L1 data cache and DTLB**\n     .\n  * **L1 data cache and DTLB**\n     connects to the\n     **Shared L2 cache up to 10.7 Gbps FSB**\n     and provides feedback to the\n     **Scheduler/Reservation station**\n     .\n\n\nBlock diagram of the Intel Core Microarchitecture showing the instruction pipeline and execution units.\n\n\nFigure 16.8 Intel Core Microarchitecture\n\n\nunderlies and implements the machine's instruction set architecture. The microarchitecture is referred to as the Intel Core Microarchitecture. It is implemented on each processor core in the Intel Core 2 and Intel Xeon processor families. There is also an Enhanced Intel Core Microarchitecture. One key difference between the two microarchitectures is that the Enhanced Intel Core Microarchitecture provides a third level of cache.\n\n\nTable 16.2 shows some of the parameters and performance characteristics of the cache architecture. All of the caches use a writeback update policy. When an instruction reads data from a memory location, the processor looks for the cache line that contains this data in the caches and main memory in the following order:\n\n\n  * 1. L1 data cache of the initiating core\n  * 2. L1 data cache of other cores and L2 cache\n  * 3. System memory\n\n\nThe cache line is taken from the L1 data cache of another core only if it is modified, ignoring the cache line availability or state in the L2 cache. Table 16.2b\n\n\n**Table 16.2**\n   Cache/Memory Parameters and Performance of Processors Based on Intel Core Microarchitecture\n\n\n\n(a) Cache Parameters\nCache Level | Capacity | Associativity (ways) | Line Size (bytes) | Writeback Update Policy\nL1 data | 32 kB | 8 | 64 | Writeback\nL1 instruction | 32 kB | 8 | N/A | N/A\nL2 (shared)\n      \n       1 | 2, 4 MB | 8 or 16 | 64 | Writeback\nL2 (shared)\n      \n       2 | 3, 6 MB | 12 or 24 | 64 | Writeback\nL3 (shared)\n      \n       2 | 8, 12, 16 MB | 15 | 64 | Writeback\nNotes:\n      \n\n      1. Intel Core Microarchitecture\n      \n      2. Enhanced Intel Core Microarchitecture\n\n\n\n(b) Load/Store Performance\nData Locality | Load | Store\nLatency | Throughput | Latency | Throughput\nL1 data cache | 3 clock cycles | 1 clock cycle | 2 clock cycles | 3 clock cycles\nL1 data cache of the other core in modified state | 14 clock cycles + 5.5 bus cycles | 14 clock cycles + 5.5 bus cycles | 14 clock cycles + 5.5 bus cycles | N/A\nL2 cache | 14 | 3 | 14 | 3\nMemory | 14 clock cycles + 5.5 bus cycles + memory latency | Depends on bus read protocol | 14 clock cycles + 5.5 bus cycles + memory latency | Depends on bus read protocol\n\n\nshows the characteristics of fetching the first four bytes of different localities from the memory cluster. The latency column provides an estimate of access latency. However, the actual latency can vary depending on the load of cache, memory components, and their parameters.\n\n\nThe pipeline of the Intel Core microarchitecture contains:\n\n\n  * ■ An in-order issue front end that fetches instruction streams from memory, with four instruction decoders to supply decoded instructions to the out-of-order execution core. Each instruction is translated into one or more fixed-length RISC instructions, known as\n    **micro-operations**\n    , or\n    **micro-ops**\n    .\n  * ■ An out-of-order superscalar execution core that can issue up to six micro-ops per cycle and reorder micro-ops to execute as soon as sources are ready and execution resources are available.\n  * ■ An in-order retirement unit that ensures the results of execution of micro-ops are processed and architectural states and the processor's register set are updated according to the original program order.\n\n\nIn effect, the Intel Core Microarchitecture implements a CISC instruction set architecture on a RISC microarchitecture. The inner RISC micro-ops pass through a pipeline with at least 14 stages; in some cases, the micro-op requires multiple execution stages, resulting in an even longer pipeline. This contrasts with the five-stage pipeline (Figure 14.21) used on the earlier Intel x86 processors and on the Pentium.\n\n\n\n\n**Front End**\n\n\nThe front end needs to supply decoded instructions (micro-ops) and sustain the stream to a six-issue wide out-of-order engine. It consists of three major components: branch prediction unit (BPU), instruction fetch and predecode unit, and instruction queue and decode unit.\n\n\n**BRANCH PREDICTION UNIT**\n   This unit helps the instruction fetch unit fetch the most likely instruction to be executed by predicting the various branch types: conditional, indirect, direct, call, and return. The BPU uses dedicated hardware for each branch type. Branch prediction enables the processor to begin executing instructions long before the branch outcome is decided.\n\n\nThe microarchitecture uses a dynamic branch prediction strategy based on the history of recent executions of branch instructions. A branch target buffer (BTB) is maintained that caches information about recently encountered branch instructions. Whenever a branch instruction is encountered in the instruction stream, the BTB is checked. If an entry already exists in the BTB, then the instruction unit is guided by the history information for that entry in determining whether to predict that the branch is taken. If a branch is predicted, then the branch destination address associated with this entry is used for prefetching the branch target instruction.\n\n\nOnce the instruction is executed, the history portion of the appropriate entry is updated to reflect the result of the branch instruction. If this instruction is not represented in the BTB, then the address of this instruction is loaded into an entry in the BTB; if necessary, an older entry is deleted.\n\n\nThe description of the preceding two paragraphs fits, in general terms, the branch prediction strategy used on the original Pentium model, as well as the later\n\n\nPentium models, including current Intel models. However, in the case of the Pentium, a relatively simple 2-bit history scheme is used. The later models have much longer pipelines (14 stages for the Intel Core Microarchitecture compared with 5 stages for the Pentium) and therefore the penalty for misprediction is greater. Accordingly, the later models use a more elaborate branch prediction scheme with more history bits to reduce the misprediction rate.\n\n\nConditional branches that do not have a history in the BTB are predicted using a static prediction algorithm, according to the following rules:\n\n\n  * ■ For branch addresses that are not instruction pointer (IP) relative, predict taken if the branch is a return and not taken otherwise.\n  * ■ For IP-relative backward conditional branches, predict taken. This rule reflects the typical behavior of loops.\n  * ■ For IP-relative forward conditional branches, predict not taken.\n\n\n**INSTRUCTION FETCH AND PREDECODE UNIT**\n   The instruction fetch unit comprises the instruction translation lookaside buffer (ITLB), an instruction prefetcher, the instruction cache, and the predecode logic.\n\n\nInstruction fetch is performed from an L1 instruction cache. When an L1 cache miss occurs, the in-order front end feeds new instructions into the L1 cache from the L2 cache 64 bytes at a time. As a default, instructions are fetched sequentially, so that each L2 cache line fetch includes the next instruction to be fetched. Branch prediction via the branch prediction unit may alter this sequential fetch operation. The ITLB translates the linear IP address given it into physical addresses needed to access the L2 cache. Static branch prediction in the front end is used to determine which instructions to fetch next.\n\n\nThe predecode unit accepts the sixteen bytes from the instruction cache or prefetch buffers and carries out the following tasks:\n\n\n  * ■ Determine the length of the instructions.\n  * ■ Decode all prefixes associated with instructions.\n  * ■ Mark various properties of instructions for the decoders (for example, “is branch”).\n\n\nThe predecode unit can write up to six instructions per cycle into the instruction queue. If a fetch contains more than six instructions, the predecoder continues to decode up to six instructions per cycle until all instructions in the fetch are written to the instruction queue. Subsequent fetches can only enter predecoding after the current fetch completes.\n\n\n**INSTRUCTION QUEUE AND DECODE UNIT**\n   Fetched instructions are placed in an instruction queue. From there, the decode unit scans the bytes to determine instruction boundaries; this is a necessary operation because of the variable length of x86 instructions. The decoder translates each machine instruction into from one to four micro-ops, each of which is a 118-bit RISC instruction. Note for comparison that most pure RISC machines have an instruction length of just 32 bits. The longer micro-op length is required to accommodate the more complex x86 instructions. Nevertheless, the micro-ops are easier to manage than the original instructions from which they derive.\n\n\nA few instructions require more than four micro-ops. These instructions are transferred to microcode ROM, which contains the series of micro-ops (five or more) associated with a complex machine instruction. For example, a string instruction may translate into a very large (even hundreds), repetitive sequence of micro-ops. Thus, the microcode ROM is a microprogrammed control unit in the sense discussed in Part Six.\n\n\nThe resulting micro-op sequence is delivered to the rename/allocator module.\n\n\n\n\n**Out-of-Order Execution Logic**\n\n\nThis part of the processor reorders micro-ops to allow them to execute as quickly as their input operands are ready.\n\n\n**ALLOCATE**\n   The allocate stage allocates resources required for execution. It performs the following functions:\n\n\n  * ■ If a needed resource, such as a register, is unavailable for one of the three micro-ops arriving at the allocator during a clock cycle, the allocator stalls the pipeline.\n  * ■ The allocator allocates a reorder buffer (ROB) entry, which tracks the completion status of one of the 126 micro-ops that could be in process at any time.\n    \n     2\n  * ■ The allocator allocates one of the 128 integer or floating-point register entries for the result data value of the micro-op, and possibly a load or store buffer used to track one of the 48 loads or 24 stores in the machine pipeline.\n  * ■ The allocator allocates an entry in one of the two micro-op queues in front of the instruction schedulers.\n\n\nThe ROB is a circular buffer that can hold up to 126 micro-ops and also contains the 128 hardware registers. Each buffer entry consists of the following fields:\n\n\n  * ■\n    **State:**\n    Indicates whether this micro-op is scheduled for execution, has been dispatched for execution, or has completed execution and is ready for retirement.\n  * ■\n    **Memory Address:**\n    The address of the Pentium instruction that generated the micro-op.\n  * ■\n    **Micro-op:**\n    The actual operation.\n  * ■\n    **Alias Register:**\n    If the micro-op references one of the 16 architectural registers, this entry redirects that reference to one of the 128 hardware registers.\n\n\nMicro-ops enter the ROB in order. Micro-ops are then dispatched from the ROB to the Dispatch/Execute unit out of order. The criterion for dispatch is that the appropriate execution unit and all necessary data items required for this microop are available. Finally, micro-ops are retired from the ROB in order. To accomplish in-order retirement, micro-ops are retired oldest first after each micro-op has been designated as ready for retirement.\n\n\n**REGISTER RENAMING**\n   The rename stage remaps references to the 16 architectural registers (8 floating-point registers, plus EAX, EBX, ECX, EDX, ESI, EDI, EBP, and ESP) into a set of 128 physical registers. The stage removes false dependencies\n\n\n2\n   \n   See Appendix N for a discussion of reorder buffers.\n\n\ncaused by a limited number of architectural registers while preserving the true data dependencies (reads after writes).\n\n\n***MICRO-OP QUEUING***\n   After resource allocation and register renaming, micro-ops are placed in one of two micro-op queues, where they are held until there is room in the schedulers. One of the two queues is for memory operations (loads and stores) and the other for micro-ops that do not involve memory references. Each queue obeys a FIFO (first-in-first-out) discipline, but no order is maintained between queues. That is, a micro-op may be read out of one queue out of order with respect to micro-ops in the other queue. This provides greater flexibility to the schedulers.\n\n\n***MICRO-OP SCHEDULING AND DISPATCHING***\n   The schedulers are responsible for retrieving micro-ops from the micro-op queues and dispatching these for execution. Each scheduler looks for micro-ops in whose status indicates that the micro-op has all of its operands. If the execution unit needed by that micro-op is available, then the scheduler fetches the micro-op and dispatches it to the appropriate execution unit. Up to six micro-ops can be dispatched in one cycle. If more than one micro-op is available for a given execution unit, then the scheduler dispatches them in sequence from the queue. This is a sort of FIFO discipline that favors in-order execution, but by this time the instruction stream has been so rearranged by dependencies and branches that it is substantially out of order.\n\n\nFour ports attach the schedulers to the execution units. Port 0 is used for both integer and floating-point instructions, with the exception of simple integer operations and the handling of branch mispredictions, which are allocated to Port 1. In addition, MMX execution units are allocated between these two ports. The remaining ports are for memory loads and stores.\n\n\n\n\n**Integer and Floating-Point Execution Units**\n\n\nThe integer and floating-point register files are the source for pending operations by the execution units. The execution units retrieve values from the register files as well as from the L1 data cache. A separate pipeline stage is used to compute flags (e.g., zero, negative); these are typically the input to a branch instruction.\n\n\nA subsequent pipeline stage performs branch checking. This function compares the actual branch result with the prediction. If a branch prediction turns out to have been wrong, then there are micro-operations in various stages of processing that must be removed from the pipeline. The proper branch destination is then provided to the Branch Predictor during a drive stage, which restarts the whole pipeline from the new target address."
        },
        {
          "name": "ARM Cortex-A8",
          "content": "Recent implementations of the ARM architecture have seen the introduction of superscalar techniques in the instruction pipeline. In this section, we focus on the ARM Cortex-A8, which provides a good example of a RISC-based superscalar design.\n\n\nThe Cortex-A8 is in the ARM family of processors that ARM refers to as application processors. An ARM application processor is an embedded processor running complex operating systems for wireless, consumer and imaging applications. The Cortex-A8 targets a wide variety of mobile and consumer applications including mobile phones, set-top boxes, gaming consoles and automotive navigation/entertainment systems.\n\n\nFigure 16.9 shows a logical view of the Cortex-A8 architecture, emphasizing the flow of instructions among functional units. The main instruction flow is through three functional units that implement a dual, in-order-issue, 13-stage pipeline. The Cortex designers decided to stay with in-order issue to keep additional\n\n\n\n\n![Architectural Block Diagram of ARM Cortex-A8 showing the 13-stage integer pipeline and 10-stage SIMD pipeline.](images/image_0281.jpeg)\n\n\nThe diagram illustrates the Cortex-A8 architecture, divided into two main pipelines: the 13-stage integer pipeline and the 10-stage SIMD pipeline.\n\n\n**13-stage integer pipeline (stages 1-13):**\n\n\n  * **Stages 1-2:**\n     Instruction fetch. Includes I-side L1 RAM, L1 cache interface, Prefetch and branch prediction, and TLB.\n  * **Stages 3-7:**\n     Instruction decode. Includes Decode & sequencer, Dependency check and issue, and the Architectural register file.\n  * **Stages 8-13:**\n     Instruction execute and Load/Store. Includes ALU pipe, MUL pipe 0, ALU pipe 1, Load/store pipe 0 or 1, L1 cache interface, and D-side L1 RAM.\n\n\n**10-stage SIMD pipeline (stages 14-23):**\n\n\n  * **Stages 14-16:**\n     NEON unit. Includes NEON instruction decode, NEON register file, and Load and store data queue.\n  * **Stages 17-22:**\n     SIMD execution pipes. Includes Integer ALU pipe, Integer MUL pipe, Integer shift pipe, non-IEEE FP ADD pipe, non-IEEE FP MUL pipe, IEEE floating-point engine, and Load/store permute pipe.\n  * **Stage 23:**\n     NEON register writeback.\n\n\n**Supporting Units and Data Flow:**\n\n\n  * **L2 cache:**\n     Instruction, data, NEON and preload engine buffers.\n  * **Arbitration and Control:**\n     Arbitration, L2 cache pipeline control, Fill and eviction queue, L2 cache data RAM, and L2 cache tag RAM.\n  * **Bus Interface:**\n     Bus interface unit (BIU) and Write buffer.\n  * **Control Flow:**\n     Branch mispredict and Replay signals.\n\n\n**Stage Groupings:**\n\n\n  * **13-stage integer pipeline:**\n     2 stages, 5 stages, 6 stages.\n  * **10-stage SIMD pipeline:**\n     3 stages, 1 stage, 6 stages.\n\n\nArchitectural Block Diagram of ARM Cortex-A8 showing the 13-stage integer pipeline and 10-stage SIMD pipeline.\n\n\n**Figure 16.9**\n   Architectural Block Diagram of ARM Cortex-A8\n\n\npower required to a minimum. Out-of-order issue and\n   **retire**\n   can require extensive amounts of logic consuming extra power.\n\n\nFigure 16.10 shows the details of the main Cortex-A8 pipeline. There is a separate unit for SIMD (single-instruction-multiple-data) unit that implements a 10-stage pipeline.\n\n\n\n\n**Instruction Fetch Unit**\n\n\nThe instruction fetch unit predicts the instruction stream, fetches instructions from the L1 instruction cache, and places the fetched instructions into a buffer for consumption by the decode pipeline. The instruction fetch unit also includes the L1\n\n\n\n\n![Diagram of the ARM Cortex-A8 Integer Pipeline, showing the Instruction Fetch Unit and Instruction Decode Pipeline.](images/image_0282.jpeg)\n\n\nThe diagram illustrates the first two stages of the Cortex-A8 pipeline:\n\n\n**(a) Instruction fetch pipeline:**\n    This stage is divided into three main sections:\n    **F0**\n    ,\n    **F1**\n    , and\n    **F2**\n    .\n\n  * **F0:**\n      Includes an\n      **AGU**\n      (Address Generation Unit) and a\n      **Branch mispredict**\n      input. The AGU feeds into a\n      **RAM + TLB**\n      block.\n  * **F1:**\n      The output of RAM + TLB feeds into a\n      **12-entry fetch queue**\n      .\n  * **F2:**\n      The output of the 12-entry fetch queue feeds into the\n      **Decode**\n      stage of the decode pipeline.\n\n\n    Additionally, a\n    **BTB GHBB RS**\n    block feeds into the RAM + TLB stage.\n   \n**(b) Instruction decode pipeline:**\n    This stage is divided into five main sections:\n    **D0**\n    ,\n    **D1**\n    ,\n    **D2**\n    ,\n    **D3**\n    , and\n    **D4**\n    .\n\n  * **D0:**\n      Includes\n      **Early decode**\n      and\n      **Decode /seq**\n      blocks.\n  * **D1:**\n      Includes\n      **Decode**\n      and\n      **Decode**\n      blocks.\n  * **D2:**\n      Includes a\n      **Dec queue read/write**\n      block, which is connected to a\n      **Pending and replay queue**\n      below it. A\n      **Replay**\n      input feeds into this block.\n  * **D3:**\n      Includes a\n      **Score board + issue logic**\n      block.\n  * **D4:**\n      Includes a\n      **Final decode**\n      block.\n\n\n    The\n    **Decode /seq**\n    and\n    **Decode**\n    blocks in D0 and D1 feed into the\n    **Dec queue read/write**\n    block in D2.\n   \nDiagram of the ARM Cortex-A8 Integer Pipeline, showing the Instruction Fetch Unit and Instruction Decode Pipeline.\n\n\n\n\n![Diagram of the Instruction Execute and Load/Store pipeline (E0 to E5).](images/image_0283.jpeg)\n\n\n**(c) Instruction execute and load/store pipeline:**\n    This stage is divided into six main sections:\n    **E0**\n    ,\n    **E1**\n    ,\n    **E2**\n    ,\n    **E3**\n    ,\n    **E4**\n    , and\n    **E5**\n    .\n\n  * **E0:**\n      An\n      **Architectural register file**\n      block. It has two inputs:\n      **INST 0**\n      and\n      **INST 1**\n      .\n  * **E1:**\n      A block containing\n      **Shift**\n      ,\n      **ALU**\n      ,\n      **SAT**\n      ,\n      **BP**\n      , and\n      **WB**\n      stages. It also contains\n      **MUL 1**\n      ,\n      **MUL 2**\n      , and\n      **MUL 3**\n      units. A\n      **Branch mispredict**\n      input feeds into the\n      **BP**\n      stage, and a\n      **Replay**\n      input feeds into the\n      **WB**\n      stage.\n  * **E2:**\n      A block containing\n      **Shift**\n      ,\n      **ALU**\n      ,\n      **SAT**\n      ,\n      **BP**\n      , and\n      **WB**\n      stages. It also contains\n      **MUL 1**\n      ,\n      **MUL 2**\n      , and\n      **MUL 3**\n      units. A\n      **Branch mispredict**\n      input feeds into the\n      **BP**\n      stage, and a\n      **Replay**\n      input feeds into the\n      **WB**\n      stage.\n  * **E3:**\n      A block containing\n      **Shift**\n      ,\n      **ALU**\n      ,\n      **SAT**\n      ,\n      **BP**\n      , and\n      **WB**\n      stages. It also contains\n      **MUL 1**\n      ,\n      **MUL 2**\n      , and\n      **MUL 3**\n      units. A\n      **Branch mispredict**\n      input feeds into the\n      **BP**\n      stage, and a\n      **Replay**\n      input feeds into the\n      **WB**\n      stage.\n  * **E4:**\n      A block containing\n      **Shift**\n      ,\n      **ALU**\n      ,\n      **SAT**\n      ,\n      **BP**\n      , and\n      **WB**\n      stages. It also contains\n      **MUL 1**\n      ,\n      **MUL 2**\n      , and\n      **MUL 3**\n      units. A\n      **Branch mispredict**\n      input feeds into the\n      **BP**\n      stage, and a\n      **Replay**\n      input feeds into the\n      **WB**\n      stage.\n  * **E5:**\n      A block containing\n      **Shift**\n      ,\n      **ALU**\n      ,\n      **SAT**\n      ,\n      **BP**\n      , and\n      **WB**\n      stages. It also contains\n      **MUL 1**\n      ,\n      **MUL 2**\n      , and\n      **MUL 3**\n      units. A\n      **Branch mispredict**\n      input feeds into the\n      **BP**\n      stage, and a\n      **Replay**\n      input feeds into the\n      **WB**\n      stage.\n\n\n    The\n    **Architectural register file**\n    feeds into the\n    **Shift**\n    stage of E1 and E2. The\n    **Shift**\n    stage of E1 feeds into the\n    **ALU**\n    stage of E2, and so on. The\n    **ALU/multiply pipe 0**\n    and\n    **ALU pipe 1**\n    are also shown within the E1 and E2 blocks respectively.\n   \nDiagram of the Instruction Execute and Load/Store pipeline (E0 to E5).\n\n\n**Figure 16.10**\n   ARM Cortex-A8 Integer Pipeline\n\n\ninstruction cache. Because there can be several unresolved branches in the pipeline, instruction fetches are speculative, meaning there is no guarantee that they are executed. A branch or exceptional instruction in the code stream can cause a pipeline flush, discarding the currently fetched instructions. The instruction fetch unit can fetch up to four instructions per cycle, and goes through the following stages:\n\n\n**F0:**\n   The address generation unit (AGU) generates a new virtual address. Normally, this address is the next address sequentially from the preceding fetch address. The address can also be a branch target address provided by a branch prediction for a previous instruction. F0 is not counted as part of the 13-stage pipeline, because ARM processors have traditionally defined instruction cache access as the first stage.\n\n\n**F1:**\n   The calculated address is used to fetch instructions from the L1 instruction cache. In parallel, the fetch address is used to access the branch prediction arrays to determine if the next fetch address should be based on a branch prediction.\n\n\n**F3:**\n   Instruction data are placed into the instruction queue. If an instruction results in branch prediction, the new target address is sent to the address generation unit.\n\n\nTo minimize the branch penalties typically associated with a deeper pipeline, the Cortex-A8 processor implements a two-level global history branch predictor, consisting of the branch target buffer (BTB) and the global history buffer (GHB). These data structures are accessed in parallel with instruction fetches. The BTB indicates whether or not the current fetch address will return a branch instruction and its branch target address. It contains 512 entries. On a hit in the BTB a branch is predicted and the GHB is accessed. The GHB consists of 4096 2-bit counters that encode the strength and direction information of branches. The GHB is indexed by 10-bit history of the direction of the last ten branches encountered and 4 bits of the PC. In addition to the dynamic branch predictor, a return stack is used to predict subroutine return addresses. The return stack has eight 32-bit entries that store the link register value in r14 and the ARM or Thumb state of the calling function. When a return-type instruction is predicted taken, the return stack provides the last pushed address and state.\n\n\nThe instruction fetch unit can fetch and queue up to 12 instructions. It issues instructions to the decode unit two at a time. The queue enables the instruction fetch unit to prefetch ahead of the rest of the integer pipeline and build up a backlog of instructions ready for decoding.\n\n\n\n\n**Instruction Decode Unit**\n\n\nThe instruction decode unit decodes and sequences all ARM and Thumb instructions. It has a dual pipeline structure, called\n   *pipe0*\n   and\n   *pipe1*\n   , so that two instructions can progress through the unit at a time. When two instructions are issued from the instruction decode pipeline, pipe0 will always contain the older instruction in program order. This means that if the instruction in pipe0 cannot issue, then the instruction in pipe1 will not issue. All issued instructions progress in order down the execution pipeline with results written back into the register file at the end of the execution pipeline. This in-order instruction issue and retire prevents WAR hazards and keeps tracking of WAW\n\n\nhazards and recovery from flush conditions straightforward. Thus, the main concern of the instruction decode pipeline is the prevention of RAW hazards.\n\n\nEach instruction goes through five stages of processing.\n\n\n**D0:**\n   Thumb instructions are decompressed into 32-bit ARM instructions. A preliminary decode function is performed.\n\n\n**D1:**\n   The instruction decode function is completed.\n\n\n**D2:**\n   This stage writes instructions into and read instructions from the pending/replay queue structure.\n\n\n**D3:**\n   This stage contains the instruction scheduling logic. A scoreboard predicts register availability using static scheduling techniques.\n   \n    3\n   \n   Hazard checking is also done at this stage.\n\n\n**D4:**\n   Performs the final decode for all the control signals required by the integer execute and load/store units.\n\n\nIn the first two stages, the instruction type, the source and destination operands, and resource requirements for the instruction are determined. A few less commonly used instructions are referred to as multicycle instructions. The D1 stage breaks these instructions down into multiple instruction opcodes that are sequenced individually through the execution pipeline.\n\n\nThe pending queue serves two purposes. First, it prevents a stall signal from D3 from rippling any further up the pipeline. Second, by buffering instructions, there should always be two instructions available for the dual pipeline. In the case where only one instruction is issued, the pending queue enables two instructions to proceed down the pipeline together, even if they were originally sent from the fetch unit in different cycles.\n\n\nThe replay operation is designed to deal with the effects of the memory system on instruction timing. Instructions are statically scheduled in the D3 stage based on a prediction of when the source operand will be available. Any stall from the memory system can result in the minimum of an 8-cycle delay. This 8-cycle delay minimum is balanced with the minimum number of possible cycles to receive data from the L2 cache in the case of an L1 load miss. Table 16.3 gives the most common cases that can result in an instruction replay because of a memory system stall.\n\n\nTo deal with these stalls, a recovery mechanism is used to flush all subsequent instructions in the execution pipeline and reissue (replay) them. To support replay, instructions are copied into the replay queue before they are issued and removed as they write back their results and retire. If a replay signal is issued instructions are retrieved from the replay queue and reenter the pipeline.\n\n\nThe decode unit issues two instructions in parallel to the execution unit, unless it encounters an issue restriction. Table 16.4 shows the most common restriction cases.\n\n\n\n\n**Integer Execute Unit**\n\n\nThe instruction execute unit consists of two symmetric arithmetic logic unit (ALU) pipelines, an address generator for load and store instructions, and the multiply pipeline. The execute pipelines also perform register write back. The instruction execute unit:\n\n\n3\n   \n   See Appendix N for a discussion of scoreboarding.\n\n\n**Table 16.3**\n\nReplay Event | Delay | Description\nLoad data miss | 8 cycles | 1. A load instruction misses in the L1 data cache.\n       \n\n        2. A request is then made to the L2 data cache.\n       \n\n        3. If a miss also occurs in the L2 data cache, then a second replay occurs. The number of stall cycles depends on the external system memory timing. The minimum time required to receive the critical word for an L2 cache miss is approximately 25 cycles, but can be much longer because of L3 memory latencies.\nData TLB miss | 24 cycles | 1. A table walk because of a miss in the L1 TLB causes a 24-cycle delay, assuming the translation table entries are found in the L2 cache.\n       \n\n        2. If the translation table entries are not present in the L2 cache, the number of stall cycles depends on the external system memory timing.\nStore buffer full | 8 cycles plus latency to drain fill buffer | 1. A store instruction miss does not result in any stalls unless the store buffer is full.\n       \n\n        2. In the case of a full store buffer, the delay is at least eight cycles. The delay can be more if it takes longer to drain some entries from the store buffer.\nUnaligned load or store request | 8 cycles | 1. If a load instruction address is unaligned and the full access is not contained within a 128-bit boundary, there is a 8-cycle penalty.\n       \n\n        2. If a store instruction address is unaligned and the full access is not contained within a 64-bit boundary, there is a 8-cycle penalty.\n\n\n  * ■ Executes all integer ALU and multiply operations, including flag generation.\n  * ■ Generates the virtual addresses for loads and stores and the base write-back value, when required.\n  * ■ Supplies formatted data for stores and forwards data and flags.\n  * ■ Processes branches and other changes of instruction stream and evaluates instruction condition codes.\n\n\nFor ALU instructions, either pipeline can be used, consisting of the following stages:\n\n\n  * **E0:**\n    Access register file. Up to six registers can be read from the register file for two instructions.\n  * **E1:**\n    The barrel shifter (see Figure 14.25) performs its function, if needed.\n  * **E2:**\n    The ALU unit (see Figure 14.25) performs its function.\n  * **E3:**\n    If needed, this stage completes saturation arithmetic used by some ARM data processing instructions.\n  * **E4:**\n    Any change in control flow, including branch misprediction, exceptions, and memory system replays are prioritized and processed.\n  * **E5:**\n    Results of ARM instructions are written back into the register file.\n\n\n**Table 16.4**\n\nRestriction Type | Description | Example | Cycle | Restriction\nLoad/store resource hazard | There is only one LS pipeline. Only one LS instruction can be issued per cycle. It can be in pipeline 0 or pipeline 1. | LDR r5, [r6]\n      \n      STR r7, [r8]\n      \n      MOV r9, r10 | 1\n      \n      2\n      \n      2 | Wait for LS unit\n      \n      Dual issue possible\nMultiply resource hazard | There is only one multiply pipeline, and it is only available in pipeline 0. | ADD r1, r2, r3\n      \n      MUL r4, r5, r6\n      \n      MUL r7, r8, r9 | 1\n      \n      2\n      \n      3 | Wait for pipeline 0\n      \n      Wait for multiply unit\nBranch resource hazard | There can be only one branch per cycle. It can be in pipeline 0 or pipeline 1. A branch is any instruction that changes the PC. | BX r1\n      \n      BEQ 0x1000\n      \n      ADD r1, r2, r3 | 1\n      \n      2\n      \n      2 | Wait for branch\n      \n      Dual issue possible\nData output hazard | Instructions with the same destination cannot be issued in the same cycle. This can happen with conditional code. | MOVEQ r1, r2\n      \n      MOVNE r1, r3\n      \n      LDR r5, [r6] | 1\n      \n      2\n      \n      2 | Wait because of output dependency\n      \n      Dual issue possible\nData source hazard | Instructions cannot be issued if their data is not available. See the scheduling tables for source requirements and stages results. | ADD r1, r2, r3\n      \n      ADD r4, r1, r6\n      \n      LDR r7, [r4] | 1\n      \n      2\n      \n      4 | Wait for r1\n      \n      Wait two cycles for r4\nMulti-cycle instructions | Multi-cycle instructions must issue in pipeline 0 and can only dual issue in their last iteration. | MOV r1, r2\n      \n      LDM r3, {r4-r7}\n      \n      LDM (cycle 2)\n      \n      LDM (cycle 3)\n      \n\n      ADD r8, r9, r10 | 1\n      \n      2\n      \n      3\n      \n      4\n      \n\n      4 | Wait for pipeline 0, transfer r4\n      \n      Transfer r5, r6\n      \n      Transfer r7\n      \n      Dual issue possible on last transfer\n\n\nInstructions that invoke the multiply unit (see Figure 14.25) are routed to pipe0; the multiply operation is performed in stages E1 through E3, and the multiply accumulate operation in stage E4.\n\n\nThe load/store pipeline runs parallel to the integer pipeline. The stages are as follows:\n\n\n**E1:**\n   The memory address is generated from the base and index register.\n\n\n**E2:**\n   The address is applied to the cache arrays.\n\n\n**E3:**\n   In the case of a load, data are returned and formatted for forwarding to the ALU or MUL unit. In the case of a store, the data are formatted and ready to be written into the cache.\n\n\n**E4:**\n   Performs updates to the L2 cache, if required.\n\n\n**E5:**\n   Results of ARM instructions are written back into the register file.\n\n\nTable 16.5 shows a sample code segment and indicates how the processor might schedule it.\n\n\n**Table 16.5**\n\nCycle | Program Counter | Instruction | Timing Description\n1 | 0x00000ed0 | BX r14 | Dual issue pipeline 0\n1 | 0x00000ee4 | CMP r0,#0 | Dual issue in pipeline 1\n2 | 0x00000ee8 | MOV r3,#3 | Dual issue pipeline 0\n2 | 0x00000eec | MOV r0,#0 | Dual issue in pipeline 1\n3 | 0x00000ef0 | STREQ r3,[r1,#0] | Dual issue in pipeline 0, r3 not needed until E3\n3 | 0x00000ef4 | CMP r2,#4 | Dual issue in pipeline 1\n4 | 0x00000ef8 | LDRLS pc,[pc,r2,LSL #2] | Single issue pipeline 0, + 1 cycle for load to pc, no extra cycle for shift since LSL #2\n5 | 0x00000f2c | MOV r0,#1 | Dual issue with 2nd iteration of load in pipeline 1\n6 | 0x00000f30 | B { pc } + 8 | #0xf38 dual issue pipeline 0\n6 | 0x00000f38 | STR r0,[r1,#0] | Dual issue pipeline 1\n7 | 0x00000f3c | LDR pc,[r13,#4] | Single issue pipeline 0, + 1 cycle for load to pc\n8 | 0x00000f7c | ADD r2,r4,#0xc | Dual issue with 2nd iteration of load in pipeline 1\n9 | 0x00000f80 | LDR r0,[r6,#4] | Dual issue pipeline 0\n9 | 0x00000f84 | MOV r1,#0xa | Dual issue pipeline 1\n12 | 0x00000f88 | LDR r0,[r0,#0] | Single issue pipeline 0: r0 produced in E3, required in E1, so + 2 cycle stall\n13 | 0x00000f8c | STR r0,[r4,#0] | Single issue pipeline 0 due to LS resource hazard, no extra delay for r0 since produced in E3 and consumed in E3\n14 | 0x00000f90 | LDR r0,[r4,#0xc] | Single issue pipeline 0 due to LS resource hazard\n15 | 0x00000f94 | LDMFD r13!,{r4-r6,r14} | Load multiple: loads r4 in 1st cycle, r5 and r6 in 2nd cycle, r14 in 3rd cycle, 3 cycles total\n17 | 0x00000f98 | B { pc } + 0xda8 | #0xf40 dual issue in pipeline 1 with 3rd cycle of LDM\n18 | 0x00000f40 | ADD r0,r0,#2 ARM | Single issue in pipeline 0\n19 | 0x00000f44 | ADD r0,r1,r0 ARM | Single issue in pipeline 0, no dual issue due to hazard on r0 produced in E2 and required in E2\n\n\n\n\n**SIMD and Floating-Point Pipeline**\n\n\nAll SIMD and floating-point instructions pass through the integer pipeline and are processed in a separate 10-stage pipeline (Figure 16.11). This unit, referred to as the NEON unit, handles packed SIMD instructions, and provides two types of floating-point support. If implemented, a vector floating-point (VFP) coprocessor performs floating-point operations in compliance with IEEE 754. If the coprocessor is not present, then separate multiply and add pipelines implement the floating-point operations.\n\n\n\n\n![Figure 16.11: ARM Cortex-A8 NEON and Floating-Point Pipeline. The diagram shows a multi-stage pipeline. On the left, the 'Instruction decode' block contains: '16-entry Inst queue + Inst Dec' -> 'Dec queue + Rd/Wr check' -> 'Score-board + Issue logic' -> 'REg read + M3 fwding muxes'. Below it, the 'Load and store with alignment' block contains: 'Mux L1/MCR' -> '8-Entry store queue' -> 'Load Align' -> 'Mux with NRF'. The main pipeline stages are: 1. Integer ALU, MAC, SHIFT pipes: DUP -> MUL 1 -> MUL 2 -> ACC 1 -> ACC 2 -> WB; Shift 1 -> Shift 2 -> Shift 3 -> (empty) -> (empty) -> WB; FMT -> ALU -> ABS -> (empty) -> (empty) -> WB. 2. Non-IEEE FMUL pipe: FDUP -> FMUL 1 -> FMUL 2 -> FMUL 3 -> FMUL 4 -> WB. 3. Non-IEEE FADD pipe: FFMUL -> FADD 1 -> FADD 2 -> FADD 3 -> FADD 4 -> WB. 4. IEEE single/double precision VFP: VFP -> WB. 5. Load/store and permute: PERM 1 -> PERM 2 -> Store Align -> 8-entry store queue -> (empty) -> WB. A 'NEON register writeback' line at the top connects to the WB stages of the Integer ALU, MAC, SHIFT pipes and the Non-IEEE FMUL pipe.](images/image_0284.jpeg)\n\n\nFigure 16.11: ARM Cortex-A8 NEON and Floating-Point Pipeline. The diagram shows a multi-stage pipeline. On the left, the 'Instruction decode' block contains: '16-entry Inst queue + Inst Dec' -> 'Dec queue + Rd/Wr check' -> 'Score-board + Issue logic' -> 'REg read + M3 fwding muxes'. Below it, the 'Load and store with alignment' block contains: 'Mux L1/MCR' -> '8-Entry store queue' -> 'Load Align' -> 'Mux with NRF'. The main pipeline stages are: 1. Integer ALU, MAC, SHIFT pipes: DUP -> MUL 1 -> MUL 2 -> ACC 1 -> ACC 2 -> WB; Shift 1 -> Shift 2 -> Shift 3 -> (empty) -> (empty) -> WB; FMT -> ALU -> ABS -> (empty) -> (empty) -> WB. 2. Non-IEEE FMUL pipe: FDUP -> FMUL 1 -> FMUL 2 -> FMUL 3 -> FMUL 4 -> WB. 3. Non-IEEE FADD pipe: FFMUL -> FADD 1 -> FADD 2 -> FADD 3 -> FADD 4 -> WB. 4. IEEE single/double precision VFP: VFP -> WB. 5. Load/store and permute: PERM 1 -> PERM 2 -> Store Align -> 8-entry store queue -> (empty) -> WB. A 'NEON register writeback' line at the top connects to the WB stages of the Integer ALU, MAC, SHIFT pipes and the Non-IEEE FMUL pipe.\n\n\nFigure 16.11 ARM Cortex-A8 NEON and Floating-Point Pipeline"
        },
        {
          "name": "ARM Cortex-M3",
          "content": "The preceding section looked at the rather complex pipeline organization of the Cortex-A8, an application processor. As a useful contrast, this section examines the considerably simpler pipeline organization of the Cortex-M3. The Cortex-M series is designed for the microcontroller domain. As such, the Cortex-M processors need to be as simple and efficient as possible.\n\n\nFigure 16.12 provides a block diagram overview of the Cortex-M3 processor. This figure provides more detail than that shown in Figure 1.16. Key elements include:\n\n\n  * ■\n    **Processor core:**\n    Includes a three-stage pipeline, a register bank, and a memory interface.\n  * ■\n    **Memory protection unit:**\n    Protects critical data used by the operating system from user applications, separating processing tasks by disallowing access to each other's data, disabling access to memory regions, allowing memory regions to be defined as read-only, and detecting unexpected memory accesses that could potentially break the system.\n  * ■\n    **Nested vectored interrupt controller (NVIC):**\n    Provides configurable interrupt handling abilities to the processor. It facilitates low-latency exception and interrupt handling, and controls power management.\n  * ■\n    **Wake-up interrupt controller (NVIC):**\n    Provides configurable interrupt handling abilities to the processor. It facilitates low-latency exception and interrupt handling, and controls power management.\n  * ■\n    **Flash patch and breakpoint unit:**\n    Implements breakpoints and code patches.\n\n\n\n\n![ARM Cortex-M3 Block Diagram showing internal components and their interconnections.](images/image_0285.jpeg)\n\n\nThe diagram illustrates the internal architecture of the ARM Cortex-M3 processor. The main components are:\n\n\n  * **Cortex-M3 processor**\n     (outer container)\n  * **Cortex-M3 processor core**\n     (inner container, containing Fetch, Decode, Execute stages, Register bank, and Memory interface)\n  * **Wake-up interrupt controller**\n     (optional)\n  * **Nested vectored interrupt controller**\n  * **Embedded trace macrocell**\n     (optional)\n  * **Debug access port**\n     (optional)\n  * **Flash patch and breakpoint**\n     (optional)\n  * **Data watchpoint and trace**\n     (optional)\n  * **Serial wire viewer**\n     (optional)\n  * **Bus matrix**\n  * **Code interface**\n  * **SRAM and peripheral interface**\n\n\nConnections include bidirectional links between the core and interrupt controllers, the core and memory protection unit, and various debug components to the bus matrix. The bus matrix connects to external buses via the code and peripheral interfaces. Optional components are marked with a dashed border and a dagger symbol (†).\n\n\nARM Cortex-M3 Block Diagram showing internal components and their interconnections.\n\n\nFigure 16.12 ARM Cortex-M3 Block Diagram\n\n\n  * ■\n    **Data watchpoint and trace (DWT):**\n    Implements watchpoints, data tracing, and system profiling.\n  * ■\n    **Serial wire viewer:**\n    Can export a stream of software-generated messages, data trace, and profiling information through a single pin.\n  * ■\n    **Debug access port:**\n    Provides an interface for external debug access to the processor.\n  * ■\n    **Embedded trace macrocell:**\n    Is an application-driven trace source that supports\n    \n     printf()\n    \n    style debugging to trace operating system and application events, and generates diagnostic system information.\n  * ■\n    **Bus matrix:**\n    Connects the core and debug interfaces to external buses on the microcontroller.\n\n\n\n\n**Pipeline Structure**\n\n\nThe Cortex-M3 pipeline has three stages (Figure 16.12). We examine these in turn.\n\n\nDuring the fetch stage, one 32-bit word is fetched at a time and loaded into a 3-word buffer. The 32-bit word may consist of:\n\n\n  * ■ two Thumb instructions,\n  * ■ one word-aligned Thumb-2 instruction, or\n\n\n  * ■ the upper/lower halfword of a halfword-aligned Thumb-2 instruction with\n      * — one Thumb instruction, or\n  * — the lower/upper halfword of another halfword-aligned Thumb-2 instruction.\n\n\nAll fetch addresses from the core are word aligned. If a Thumb-2 instruction is halfword aligned, two fetches are necessary to fetch the Thumb-2 instruction. However, the three-entry prefetch buffer ensures that a stall cycle is only necessary for the first halfword Thumb-2 instruction fetched.\n\n\nThis decode stage performs three key functions:\n\n\n  * ■\n    **Instruction decode and register read:**\n    Decodes Thumb and Thumb-2 instructions.\n  * ■\n    **Address generation:**\n    The address generation unit (AGU) generates main memory addresses for the load/store unit.\n  * ■\n    **Branch:**\n    Performs branch based on immediate offset in branch instruction or a return based on the contents of the link register (register R14).\n\n\nFinally, there is a single execute stage for instruction execution, which includes ALU, load/store, and branch instructions.\n\n\n\n\n**Dealing with Branches**\n\n\nTo keep the processor as simple as possible, the Cortex-M3 processor does not use branch prediction, but instead use the simple techniques of branch forwarding and branch speculation, defined as follows:\n\n\n  * ■\n    **Branch forwarding:**\n    The term\n    *forwarding*\n    refers to presenting an instruction address to be fetched from memory. The processor forwards certain branch types, by which the memory transaction of the branch is presented at least one cycle earlier than when the opcode reaches execute. Branch forwarding increases the performance of the core, because branches are a significant part of embedded controller applications. Branches affected are PC relative with immediate offset, or use link register (LR) as the target register.\n  * ■\n    **Branch speculation:**\n    For conditional branches, the instruction address is presented speculatively, so that the instruction is fetched from memory before it is known if the instruction will be executed.\n\n\nThe Cortex-M3 processor prefetches instruction ahead of execution using the fetch buffer. It also speculatively prefetches from branch target addresses. Specifically, when a conditional branch instruction is encountered, the decode stage also includes a speculative instruction fetch that could lead to faster execution. The processor fetches the branch destination instruction during the decode stage itself. Later, during the execute stage, the branch is resolved and it is known which instruction is to be executed next.\n\n\nIf the branch is not to be taken, the next sequential instruction is already available. If the branch is to be taken, the branch instruction is made available at the same time as the decision is made, restricting idle time to just one cycle.\n\n\nFigure 16.13 clarifies the manner in which branches are handled, which can be described as follows:\n\n\n  * 1. The decode stage forwards addresses from unconditional branches and speculatively forwards addresses from conditional branches when it is possible to calculate the address.\n  * 2. If the ALU determines that a branch is not taken, this information is fed back to empty the instruction cache.\n  * 3. A load instruction to the program counter results in a branch address to be forwarded for fetching.\n\n\nAs can be seen, the manner in which branches are handled is considerably simpler for the Cortex-M than the Cortex-A, requiring less processor logic and processing.\n\n\n\n\n![Diagram of the ARM Cortex-M3 Pipeline showing Fetch, Decode, and Execute stages with branch handling logic.](images/image_0286.jpeg)\n\n\nThe diagram illustrates the ARM Cortex-M3 pipeline, divided into three main stages: Fetch, Decode, and Execute, separated by vertical dashed lines.\n\n\n  * **Fetch Stage:**\n     Contains a 'Fetch' block.\n  * **Decode Stage:**\n     Contains an 'Instruction decode and register read' block, an 'AGU' (Address Generation Unit) block above it, and a 'Branch' block below it. The 'Instruction decode and register read' block has arrows pointing to the 'Address phase and write back' block in the Execute stage and the 'Multiply and divide' block. The 'Branch' block has an arrow pointing to the 'ALU and branch' block in the Execute stage.\n  * **Execute Stage:**\n     Contains an 'Address phase and write back' block, a 'Data phase load/store and branch' block, a 'Multiply and divide' block, a 'Shift' block, and an 'ALU and branch' block. The 'Address phase and write back' block has an arrow pointing to the 'Data phase load/store and branch' block. The 'Multiply and divide' block has an arrow pointing to the 'WR' (Write Register) block. The 'Shift' block has an arrow pointing to the 'ALU and branch' block. The 'ALU and branch' block has an arrow pointing to the 'WR' block.\n\n\nBelow the pipeline, three feedback paths are shown:\n\n\n  * **Branch forwarding and speculation:**\n     An arrow from the 'ALU and branch' block in the Execute stage back to the 'Instruction decode and register read' block in the Decode stage.\n  * **ALU branch not forwarded/speculated:**\n     An arrow from the 'ALU and branch' block in the Execute stage back to the 'Fetch' block in the Fetch stage.\n  * **LSU branch result:**\n     An arrow from the 'Data phase load/store and branch' block in the Execute stage back to the 'Fetch' block in the Fetch stage.\n\n\nDiagram of the ARM Cortex-M3 Pipeline showing Fetch, Decode, and Execute stages with branch handling logic.\n\n\nAGU = address generation unit\n\n\n**Figure 16.13**\n   ARM Cortex-M3 Pipeline"
        }
      ]
    },
    {
      "name": "Parallel Processing",
      "sections": [
        {
          "name": "Multiple Processor Organizations",
          "content": "**Types of Parallel Processor Systems**\n\n\nA taxonomy first introduced by Flynn [FLYN72] is still the most common way of categorizing systems with parallel processing capability. Flynn proposed the following categories of computer systems:\n\n\n  * ■\n    **Single instruction, single data (SISD) stream:**\n    A single processor executes a single instruction stream to operate on data stored in a single memory. Uniprocessors fall into this category.\n  * ■\n    **Single instruction, multiple data (SIMD) stream:**\n    A single machine instruction controls the simultaneous execution of a number of processing elements on a lockstep basis. Each processing element has an associated data memory, so that instructions are executed on different sets of data by different processors. Vector and array processors fall into this category, and are discussed in Section 18.7.\n  * ■\n    **Multiple instruction, single data (MISD) stream:**\n    A sequence of data is transmitted to a set of processors, each of which executes a different instruction sequence. This structure is not commercially implemented.\n  * ■\n    **Multiple instruction, multiple data (MIMD) stream:**\n    A set of processors simultaneously execute different instruction sequences on different data sets. SMPs, clusters, and NUMA systems fit into this category.\n\n\nWith the MIMD organization, the processors are general purpose; each is able to process all of the instructions necessary to perform the appropriate data transformation. MIMDs can be further subdivided by the means in which the processors communicate (Figure 17.1). If the processors share a common memory, then each processor accesses programs and data stored in the shared memory, and processors communicate with each other via that memory. The most common form of such system is known as a\n   **symmetric multiprocessor (SMP)**\n   , which we examine in Section 17.2. In an SMP, multiple processors share a single memory or pool of memory by means of a shared bus or other interconnection mechanism; a distinguishing feature is that the memory access time to any region of memory is approximately the same for each processor. A more recent development is the\n   **nonuniform memory access (NUMA)**\n   organization, which is described in Section 17.5. As the name suggests, the memory access time to different regions of memory may differ for a NUMA processor.\n\n\nA collection of independent uniprocessors or SMPs may be interconnected to form a\n   **cluster**\n   . Communication among the computers is either via fixed paths or via some network facility.\n\n\n\n\n**Parallel Organizations**\n\n\nFigure 17.2 illustrates the general organization of the taxonomy of Figure 17.1. Figure 17.2a shows the structure of an SISD. There is some sort of control unit (CU) that provides an instruction stream (IS) to a processing unit (PU). The processing\n\n\n\n\n![](images/image_0287.jpeg)\n\n\n**Processor organizations**\n\n\ngraph TD\n    PO[Processor organizations] --> SISD[Single instruction, single data stream (SISD)]\n    PO --> SIMD[Single instruction, multiple data stream (SIMD)]\n    PO --> MISD[Multiple instruction, single data stream (MISD)]\n    PO --> MIMD[Multiple instruction, multiple data stream (MIMD)]\n    SISD --> U[Uniprocessor]\n    SIMD --> VP[Vector processor]\n    SIMD --> AP[Array processor]\n    MIMD --> SM[Shared memory (tightly coupled)]\n    MIMD --> DM[Distributed memory (loosely coupled)]\n    SM --> SMP[Symmetric multiprocessor (SMP)]\n    SM --> NUMA[Nonuniform memory access (NUMA)]\n    DM --> Clusters\n\n\n**Figure 17.1**\n   A Taxonomy of Parallel Processor Architectures\n\n\n\n\n![Diagram (a) SISD: A single control unit (CU) sends an instruction stream (IS) to a single processing unit (PU), which then sends a data stream (DS) to a single memory unit (MU). Diagram (b) SIMD (with distributed memory): A single control unit (CU) sends an instruction stream (IS) to multiple processing units (PU1, PU2, ..., PUn). Each PU has its own local memory (LM1, LM2, ..., LMn) and sends a data stream (DS) to it. The PUs are connected to a shared bus. Diagram (c) MIMD (with shared memory): Multiple control units (CU1, CU2, ..., CUn) send instruction streams (IS) to multiple processing units (PU1, PU2, ..., PUn). All PUs share a single 'Shared memory' block and send data streams (DS) to it. Diagram (d) MIMD (with distributed memory): Multiple control units (CU1, CU2, ..., CUn) send instruction streams (IS) to multiple processing units (PU1, PU2, ..., PUn). Each PU has its own local memory (LM1, LM2, ..., LMn) and sends a data stream (DS) to it. The PUs are connected to a central 'Interconnection network'.](images/image_0288.jpeg)\n\n\n(a) SISD\n\n\n(b) SIMD (with distributed memory)\n\n\n(c) MIMD (with shared memory)\n\n\n(d) MIMD (with distributed memory)\n\n\nCU = Control unit      SISD = Single instruction,\n    \n\n    IS = Instruction stream      = single data stream\n    \n\n    PU = Processing unit      SIMD = Single instruction,\n    \n\n    DS = Data stream      multiple data stream\n    \n\n    MU = Memory unit      MIMD = Multiple instruction,\n    \n\n    LM = Local memory      multiple data stream\n\n\nDiagram (a) SISD: A single control unit (CU) sends an instruction stream (IS) to a single processing unit (PU), which then sends a data stream (DS) to a single memory unit (MU). Diagram (b) SIMD (with distributed memory): A single control unit (CU) sends an instruction stream (IS) to multiple processing units (PU1, PU2, ..., PUn). Each PU has its own local memory (LM1, LM2, ..., LMn) and sends a data stream (DS) to it. The PUs are connected to a shared bus. Diagram (c) MIMD (with shared memory): Multiple control units (CU1, CU2, ..., CUn) send instruction streams (IS) to multiple processing units (PU1, PU2, ..., PUn). All PUs share a single 'Shared memory' block and send data streams (DS) to it. Diagram (d) MIMD (with distributed memory): Multiple control units (CU1, CU2, ..., CUn) send instruction streams (IS) to multiple processing units (PU1, PU2, ..., PUn). Each PU has its own local memory (LM1, LM2, ..., LMn) and sends a data stream (DS) to it. The PUs are connected to a central 'Interconnection network'.\n\n\n**Figure 17.2**\n   Alternative Computer Organizations\n\n\nunit operates on a single data stream (DS) from a memory unit (MU). With an SIMD, there is still a single control unit, now feeding a single instruction stream to multiple PUs. Each PU may have its own dedicated memory (illustrated in Figure 17.2b), or there may be a shared memory. Finally, with the MIMD, there are multiple control units, each feeding a separate instruction stream to its own PU. The MIMD may be a shared-memory multiprocessor (Figure 17.2c) or a distributed-memory multicomputer (Figure 17.2d).\n\n\nThe design issues relating to SMPs, clusters, and NUMAs are complex, involving issues relating to physical organization, interconnection structures, interprocessor communication, operating system design, and application software techniques. Our concern here is primarily with organization, although we touch briefly on operating system design issues."
        },
        {
          "name": "Symmetric Multiprocessors",
          "content": "Until fairly recently, virtually all single-user personal computers and most workstations contained a single general-purpose microprocessor. As demands for performance increase and as the cost of microprocessors continues to drop, vendors have introduced systems with an SMP organization. The term\n   *SMP*\n   refers to a computer hardware architecture and also to the operating system behavior that reflects that architecture. An SMP can be defined as a standalone computer system with the following characteristics:\n\n\n  * 1. There are two or more similar processors of comparable capability.\n  * 2. These processors share the same main memory and I/O facilities and are interconnected by a bus or other internal connection scheme, such that memory access time is approximately the same for each processor.\n  * 3. All processors share access to I/O devices, either through the same channels or through different channels that provide paths to the same device.\n  * 4. All processors can perform the same functions (hence the term\n    *symmetric*\n    ).\n  * 5. The system is controlled by an integrated operating system that provides interaction between processors and their programs at the job, task, file, and data element levels.\n\n\nPoints 1 to 4 should be self-explanatory. Point 5 illustrates one of the contrasts with a loosely coupled multiprocessing system, such as a cluster. In the latter, the physical unit of interaction is usually a message or complete file. In an SMP, individual data elements can constitute the level of interaction, and there can be a high degree of cooperation between processes.\n\n\nThe operating system of an SMP schedules processes or threads across all of the processors. An SMP organization has a number of potential advantages over a uniprocessor organization, including the following:\n\n\n  * ■\n    **Performance:**\n    If the work to be done by a computer can be organized so that some portions of the work can be done in parallel, then a system with multiple processors will yield greater performance than one with a single processor of the same type (Figure 17.3).\n\n\n\n\n![Figure 17.3: Multiprogramming and Multiprocessing. (a) Interleaving (multiprogramming, one processor): Three processes (Process 1, Process 2, Process 3) share a single processor over time. Each process has segments of 'Blocked' (dark green) and 'Running' (light green) states. (b) Interleaving and overlapping (multiprocessing, two processors): The same three processes are shown, but they are distributed across two processors, allowing for more concurrent execution. A legend at the bottom shows a dark green bar for 'Blocked' and a light green bar for 'Running'.](images/image_0289.jpeg)\n\n\nTime →\n\n\nProcess 1\n\n\nProcess 2\n\n\nProcess 3\n\n\n(a) Interleaving (multiprogramming, one processor)\n\n\nProcess 1\n\n\nProcess 2\n\n\nProcess 3\n\n\n(b) Interleaving and overlapping (multiprocessing, two processors)\n\n\nBlocked      Running\n\n\nFigure 17.3: Multiprogramming and Multiprocessing. (a) Interleaving (multiprogramming, one processor): Three processes (Process 1, Process 2, Process 3) share a single processor over time. Each process has segments of 'Blocked' (dark green) and 'Running' (light green) states. (b) Interleaving and overlapping (multiprocessing, two processors): The same three processes are shown, but they are distributed across two processors, allowing for more concurrent execution. A legend at the bottom shows a dark green bar for 'Blocked' and a light green bar for 'Running'.\n\n\n**Figure 17.3**\n   Multiprogramming and Multiprocessing\n\n\n  * ■\n    **Availability:**\n    In a symmetric multiprocessor, because all processors can perform the same functions, the failure of a single processor does not halt the machine. Instead, the system can continue to function at reduced performance.\n  * ■\n    **Incremental growth:**\n    A user can enhance the performance of a system by adding an additional processor.\n  * ■\n    **Scaling:**\n    Vendors can offer a range of products with different price and performance characteristics based on the number of processors configured in the system.\n\n\nIt is important to note that these are potential, rather than guaranteed, benefits. The operating system must provide tools and functions to exploit the parallelism in an SMP system.\n\n\nAn attractive feature of an SMP is that the existence of multiple processors is transparent to the user. The operating system takes care of scheduling of threads or processes on individual processors and of synchronization among processors.\n\n\n\n\n**Organization**\n\n\nFigure 17.4 depicts in general terms the organization of a multiprocessor system. There are two or more processors. Each processor is self-contained, including a control unit, ALU, registers, and, typically, one or more levels of cache. Each processor has access to a shared main memory and the I/O devices through some form of interconnection mechanism. The processors can communicate with each other through memory (messages and status information left in common data areas). It may also be possible for processors to exchange signals directly. The memory is often organized\n\n\n\n\n![Figure 17.4: Generic Block Diagram of a Tightly Coupled Multiprocessor. The diagram shows a central 'Interconnection network' block. Above it, three 'Processor' blocks are connected to the top of the network. To the right, three 'I/O' blocks are connected to the right side of the network. Below the network, a single 'Main memory' block is connected to the bottom of the network. Bidirectional arrows indicate communication between the processors, I/O devices, and main memory through the interconnection network.](images/image_0290.jpeg)\n\n\nFigure 17.4: Generic Block Diagram of a Tightly Coupled Multiprocessor. The diagram shows a central 'Interconnection network' block. Above it, three 'Processor' blocks are connected to the top of the network. To the right, three 'I/O' blocks are connected to the right side of the network. Below the network, a single 'Main memory' block is connected to the bottom of the network. Bidirectional arrows indicate communication between the processors, I/O devices, and main memory through the interconnection network.\n\n\n**Figure 17.4**\n   Generic Block Diagram of a Tightly Coupled Multiprocessor\n\n\nso that multiple simultaneous accesses to separate blocks of memory are possible. In some configurations, each processor may also have its own private main memory and I/O channels in addition to the shared resources.\n\n\nThe most common organization for personal computers, workstations, and servers is the time-shared bus. The time-shared bus is the simplest mechanism for constructing a multiprocessor system (Figure 17.5). The structure and interfaces are basically the same as for a single-processor system that uses a bus interconnection. The bus consists of control, address, and data lines. To facilitate DMA transfers from I/O subsystems to processors, the following features are provided:\n\n\n  * ■\n    **Addressing:**\n    It must be possible to distinguish modules on the bus to determine the source and destination of data.\n  * ■\n    **Arbitration:**\n    Any I/O module can temporarily function as “master.” A mechanism is provided to arbitrate competing requests for bus control, using some sort of priority scheme.\n  * ■\n    **Time-sharing:**\n    When one module is controlling the bus, other modules are locked out and must, if necessary, suspend operation until bus access is achieved.\n\n\nThese uniprocessor features are directly usable in an SMP organization. In this latter case, there are now multiple processors as well as multiple I/O processors all attempting to gain access to one or more memory modules via the bus.\n\n\n\n\n![Diagram of a Symmetric Multiprocessor (SMP) organization. Multiple processors are connected to a shared bus. Each processor consists of a Processor block containing an L1 cache, connected to an L2 cache, which is then connected to the shared bus. The shared bus connects to a Main memory block and an I/O subsystem. The I/O subsystem is connected to three I/O adapter blocks.](images/image_0291.jpeg)\n\n\nThe diagram illustrates a Symmetric Multiprocessor (SMP) organization. At the top, three identical processor units are shown, with an ellipsis between the second and third. Each processor unit is a vertical stack: a top teal block labeled 'Processor' containing a white sub-block labeled 'L1 cache', and a bottom teal block labeled 'L2 cache'. Lines connect the L2 cache of each processor to a thick horizontal black bar labeled 'Shared bus'. Below the shared bus, a vertical line connects it to a teal block labeled 'Main memory'. To the right of the main memory, the text 'I/O subsystem' is written. From the I/O subsystem, three horizontal lines extend to the right, each connecting to a teal block labeled 'I/O adapter'.\n\n\nDiagram of a Symmetric Multiprocessor (SMP) organization. Multiple processors are connected to a shared bus. Each processor consists of a Processor block containing an L1 cache, connected to an L2 cache, which is then connected to the shared bus. The shared bus connects to a Main memory block and an I/O subsystem. The I/O subsystem is connected to three I/O adapter blocks.\n\n\n**Figure 17.5**\n   Symmetric Multiprocessor Organization\n\n\nThe bus organization has several attractive features:\n\n\n  * ■\n    **Simplicity:**\n    This is the simplest approach to multiprocessor organization. The physical interface and the addressing, arbitration, and time-sharing logic of each processor remain the same as in a single-processor system.\n  * ■\n    **Flexibility:**\n    It is generally easy to expand the system by attaching more processors to the bus.\n  * ■\n    **Reliability:**\n    The bus is essentially a passive medium, and the failure of any attached device should not cause failure of the whole system.\n\n\nThe main drawback to the bus organization is performance. All memory references pass through the common bus. Thus, the bus cycle time limits the speed of the system. To improve performance, it is desirable to equip each processor with a cache memory. This should reduce the number of bus accesses dramatically. Typically, workstation and PC SMPs have two levels of cache, with the L1 cache internal (same chip as the processor) and the L2 cache either internal or external. Some processors now employ a L3 cache as well.\n\n\nThe use of caches introduces some new design considerations. Because each local cache contains an image of a portion of memory, if a word is altered in one\n\n\ncache, it could conceivably invalidate a word in another cache. To prevent this, the other processors must be alerted that an update has taken place. This problem is known as the\n   *cache coherence*\n   problem and is typically addressed in hardware rather than by the operating system. We address this issue in Section 17.4.\n\n\n\n\n**Multiprocessor Operating System Design Considerations**\n\n\nAn SMP operating system manages processor and other computer resources so that the user perceives a single operating system controlling system resources. In fact, such a configuration should appear as a single-processor multiprogramming system. In both the SMP and uniprocessor cases, multiple jobs or processes may be active at one time, and it is the responsibility of the operating system to schedule their execution and to allocate resources. A user may construct applications that use multiple processes or multiple threads within processes without regard to whether a single processor or multiple processors will be available. Thus, a multiprocessor operating system must provide all the functionality of a multiprogramming system plus additional features to accommodate multiple processors. Among the key design issues:\n\n\n  * ■\n    **Simultaneous concurrent processes:**\n    OS routines need to be reentrant to allow several processors to execute the same IS code simultaneously. With multiple processors executing the same or different parts of the OS, OS tables and management structures must be managed properly to avoid deadlock or invalid operations.\n  * ■\n    **Scheduling:**\n    Any processor may perform scheduling, so conflicts must be avoided. The scheduler must assign ready processes to available processors.\n  * ■\n    **Synchronization:**\n    With multiple active processes having potential access to shared address spaces or shared I/O resources, care must be taken to provide effective synchronization. Synchronization is a facility that enforces mutual exclusion and event ordering.\n  * ■\n    **Memory management:**\n    Memory management on a multiprocessor must deal with all of the issues found on uniprocessor machines, as is discussed in Chapter 8. In addition, the operating system needs to exploit the available hardware parallelism, such as multiported memories, to achieve the best performance. The paging mechanisms on different processors must be coordinated to enforce consistency when several processors share a page or segment and to decide on page replacement.\n  * ■\n    **Reliability and fault tolerance:**\n    The operating system should provide graceful degradation in the face of processor failure. The scheduler and other portions of the operating system must recognize the loss of a processor and restructure management tables accordingly."
        },
        {
          "name": "Cache Coherence and the MESI Protocol",
          "content": "In contemporary multiprocessor systems, it is customary to have one or two levels of cache associated with each processor. This organization is essential to achieve reasonable performance. It does, however, create a problem known as the\n   *cache coherence*\n\n\nproblem. The essence of the problem is this: Multiple copies of the same data can exist in different caches simultaneously, and if processors are allowed to update their own copies freely, an inconsistent view of memory can result. In Chapter 4 we defined two common write policies:\n\n\n  * ■\n    **Write back:**\n    Write operations are usually made only to the cache. Main memory is only updated when the corresponding cache line is evicted from the cache.\n  * ■\n    **Write through:**\n    All write operations are made to main memory as well as to the cache, ensuring that main memory is always valid.\n\n\nIt is clear that a write-back policy can result in inconsistency. If two caches contain the same line, and the line is updated in one cache, the other cache will unknowingly have an invalid value. Subsequent reads to that invalid line produce invalid results. Even with the write-through policy, inconsistency can occur unless other caches monitor the memory traffic or receive some direct notification of the update.\n\n\nIn this section, we will briefly survey various approaches to the cache coherence problem and then focus on the approach that is most widely used: the MESI (modified/exclusive/shared/invalid) protocol. A version of this protocol is used on both the x86 architecture.\n\n\nFor any cache coherence protocol, the objective is to let recently used local variables get into the appropriate cache and stay there through numerous reads and write, while using the protocol to maintain consistency of shared variables that might be in multiple caches at the same time. Cache coherence approaches have generally been divided into software and hardware approaches. Some implementations adopt a strategy that involves both software and hardware elements. Nevertheless, the classification into software and hardware approaches is still instructive and is commonly used in surveying cache coherence strategies.\n\n\n\n\n**Software Solutions**\n\n\nSoftware cache coherence schemes attempt to avoid the need for additional hardware circuitry and logic by relying on the compiler and operating system to deal with the problem. Software approaches are attractive because the overhead of detecting potential problems is transferred from run time to compile time, and the design complexity is transferred from hardware to software. On the other hand, compile-time software approaches generally must make conservative decisions, leading to inefficient cache utilization.\n\n\nCompiler-based coherence mechanisms perform an analysis on the code to determine which data items may become unsafe for caching, and they mark those items accordingly. The operating system or hardware then prevents noncacheable items from being cached.\n\n\nThe simplest approach is to prevent any shared data variables from being cached. This is too conservative, because a shared data structure may be exclusively used during some periods and may be effectively read-only during other periods. It is only during periods when at least one process may update the variable and at least one other process may access the variable that cache coherence is an issue.\n\n\nMore efficient approaches analyze the code to determine safe periods for shared variables. The compiler then inserts instructions into the generated code to enforce cache coherence during the critical periods. A number of techniques have been developed for performing the analysis and for enforcing the results; see [LILJ93] and [STEN90] for surveys.\n\n\n\n\n**Hardware Solutions**\n\n\nHardware-based solutions are generally referred to as cache coherence protocols. These solutions provide dynamic recognition at run time of potential inconsistency conditions. Because the problem is only dealt with when it actually arises, there is more effective use of caches, leading to improved performance over a software approach. In addition, these approaches are transparent to the programmer and the compiler, reducing the software development burden.\n\n\nHardware schemes differ in a number of particulars, including where the state information about data lines is held, how that information is organized, where coherence is enforced, and the enforcement mechanisms. In general, hardware schemes can be divided into two categories:\n   **directory protocols**\n   and\n   **snoopy protocols**\n   .\n\n\n**DIRECTORY PROTOCOLS**\n   Directory protocols collect and maintain information about where copies of lines reside. Typically, there is a centralized controller that is part of the main memory controller, and a directory that is stored in main memory. The directory contains global state information about the contents of the various local caches. When an individual cache controller makes a request, the centralized controller checks and issues necessary commands for data transfer between memory and caches or between caches. It is also responsible for keeping the state information up to date; therefore, every local action that can affect the global state of a line must be reported to the central controller.\n\n\nTypically, the controller maintains information about which processors have a copy of which lines. Before a processor can write to a local copy of a line, it must request exclusive access to the line from the controller. Before granting this exclusive access, the controller sends a message to all processors with a cached copy of this line, forcing each processor to invalidate its copy. After receiving acknowledgments back from each such processor, the controller grants exclusive access to the requesting processor. When another processor tries to read a line that is exclusively granted to another processor, it will send a miss notification to the controller. The controller then issues a command to the processor holding that line that requires the processor to do a write back to main memory. The line may now be shared for reading by the original processor and the requesting processor.\n\n\nDirectory schemes suffer from the drawbacks of a central bottleneck and the overhead of communication between the various cache controllers and the central controller. However, they are effective in large-scale systems that involve multiple buses or some other complex interconnection scheme.\n\n\n**SNOOPY PROTOCOLS**\n   Snoopy protocols distribute the responsibility for maintaining cache coherence among all of the cache controllers in a multiprocessor. A cache must recognize when a line that it holds is shared with other\n\n\ncaches. When an update action is performed on a shared cache line, it must be announced to all other caches by a broadcast mechanism. Each cache controller is able to “snoop” on the network to observe these broadcasted notifications, and react accordingly.\n\n\nSnoopy protocols are ideally suited to a bus-based multiprocessor, because the shared bus provides a simple means for broadcasting and snooping. However, because one of the objectives of the use of local caches is to avoid bus accesses, care must be taken that the increased bus traffic required for broadcasting and snooping does not cancel out the gains from the use of local caches.\n\n\nTwo basic approaches to the snoopy protocol have been explored: write invalidate and write update (or write broadcast). With a write-invalidate protocol, there can be multiple readers but only one writer at a time. Initially, a line may be shared among several caches for reading purposes. When one of the caches wants to perform a write to the line, it first issues a notice that invalidates that line in the other caches, making the line exclusive to the writing cache. Once the line is exclusive, the owning processor can make cheap local writes until some other processor requires the same line.\n\n\nWith a write-update protocol, there can be multiple writers as well as multiple readers. When a processor wishes to update a shared line, the word to be updated is distributed to all others, and caches containing that line can update it.\n\n\nNeither of these two approaches is superior to the other under all circumstances. Performance depends on the number of local caches and the pattern of memory reads and writes. Some systems implement adaptive protocols that employ both write-invalidate and write-update mechanisms.\n\n\nThe write-invalidate approach is the most widely used in commercial multiprocessor systems, such as the x86 architecture. It marks the state of every cache line (using two extra bits in the cache tag) as modified, exclusive, shared, or invalid. For this reason, the write-invalidate protocol is called MESI. In the remainder of this section, we will look at its use among local caches across a multiprocessor. For simplicity in the presentation, we do not examine the mechanisms involved in coordinating among both level 1 and level 2 locally as well as at the same time coordinating across the distributed multiprocessor. This would not add any new principles but would greatly complicate the discussion.\n\n\n\n\n**The MESI Protocol**\n\n\nTo provide cache consistency on an SMP, the data cache often supports a protocol known as MESI. For MESI, the data cache includes two status bits per tag, so that each line can be in one of four states:\n\n\n  * ■\n    **Modified:**\n    The line in the cache has been modified (different from main memory) and is available only in this cache.\n  * ■\n    **Exclusive:**\n    The line in the cache is the same as that in main memory and is not present in any other cache.\n  * ■\n    **Shared:**\n    The line in the cache is the same as that in main memory and may be present in another cache.\n  * ■\n    **Invalid:**\n    The line in the cache does not contain valid data.\n\n\n**Table 17.1**\n\n | M\n      \n\n\n       Modified | E\n      \n\n\n       Exclusive | S\n      \n\n\n       Shared | I\n      \n\n\n       Invalid\nThis cache line valid? | Yes | Yes | Yes | No\nThe memory copy is ... | out of date | valid | valid | —\nCopies exist in other caches? | No | No | Maybe | Maybe\nA write to this line ... | does not go to bus | does not go to bus | goes to bus and updates cache | goes directly to bus\n\n\nTable 17.1 summarizes the meaning of the four states. Figure 17.6 displays a state diagram for the MESI protocol. Keep in mind that each line of the cache has its own state bits and therefore its own realization of the state diagram. Figure 17.6a shows the transitions that occur due to actions initiated by the processor attached to this cache. Figure 17.6b shows the transitions that occur due to events that are snooped on the common bus. This presentation of separate state diagrams for processor-initiated and bus-initiated actions helps to clarify the logic of the MESI\n\n\n\n\n![Figure 17.6: MESI State Transition Diagram. (a) Line in cache at initiating processor. (b) Line in snooping cache. The diagram shows four states: Invalid, Shared, Modified, and Exclusive. Transitions are labeled with actions like RMS, RME, WH, WM, SHW, SHR, and self-loops for RH.](images/image_0292.jpeg)\n\n\n(a) Line in cache at initiating processor\n\n\n(b) Line in snooping cache\n\n\nFigure 17.6: MESI State Transition Diagram. (a) Line in cache at initiating processor. (b) Line in snooping cache. The diagram shows four states: Invalid, Shared, Modified, and Exclusive. Transitions are labeled with actions like RMS, RME, WH, WM, SHW, SHR, and self-loops for RH.\n\n\n\nRH\n      \n      = Read hit | Image: Self-loop arrow\n      \n      Dirty line copyback\nRMS\n      \n      = Read miss, shared | Image: Arrow with circle and plus\n      \n      Invalidate transaction\nRME\n      \n      = Read miss, exclusive | Image: Arrow with circle and X\n      \n      Read-with-intent-to-modify\nWH\n      \n      = Write hit | Image: Arrow with circle and up\n      \n      Cache line fill\nWM\n      \n      = Write miss | \nSHR\n      \n      = Snoop hit on read | \nSHW\n      \n      = Snoop hit on write or read-with-intent-to-modify | \n\n\n**Figure 17.6**\nprotocol. At any time a cache line is in a single state. If the next event is from the attached processor, then the transition is dictated by Figure 17.6a and if the next event is from the bus, the transition is dictated by Figure 17.6b. Let us look at these transitions in more detail.\n\n\n**READ MISS**\n   When a read miss occurs in the local cache, the processor initiates a memory read to read the line of main memory containing the missing address. The processor inserts a signal on the bus that alerts all other processor/cache units to snoop the transaction. There are a number of possible outcomes:\n\n\n  * ■ If one other cache has a clean (unmodified since read from memory) copy of the line in the exclusive state, it returns a signal indicating that it shares this line. The responding processor then transitions the state of its copy from exclusive to shared, and the initiating processor reads the line from main memory and transitions the line in its cache from invalid to shared.\n  * ■ If one or more caches have a clean copy of the line in the shared state, each of them signals that it shares the line. The initiating processor reads the line and transitions the line in its cache from invalid to shared.\n  * ■ If one other cache has a modified copy of the line, then that cache blocks the memory read and provides the line to the requesting cache over the shared bus. The responding cache then changes its line from modified to shared.\n    \n     1\n    \n    The line sent to the requesting cache is also received and processed by the memory controller, which stores the block in memory.\n  * ■ If no other cache has a copy of the line (clean or modified), then no signals are returned. The initiating processor reads the line and transitions the line in its cache from invalid to exclusive.\n\n\n**READ HIT**\n   When a read hit occurs on a line currently in the local cache, the processor simply reads the required item. There is no state change: The state remains modified, shared, or exclusive.\n\n\n**WRITE MISS**\n   When a write miss occurs in the local cache, the processor initiates a memory read to read the line of main memory containing the missing address. For this purpose, the processor issues a signal on the bus that means\n   *read-with-intent-to-modify*\n   (RWITM). When the line is loaded, it is immediately marked modified. With respect to other caches, two possible scenarios precede the loading of the line of data.\n\n\nFirst, some other cache may have a modified copy of this line (state = modify). In this case, the alerted processor signals the initiating processor that another processor has a modified copy of the line. The initiating processor surrenders the bus and waits. The other processor gains access to the bus, writes the modified cache\n\n\n1\n   \n   In some implementations, the cache with the modified line signals the initiating processor to retry. Meanwhile, the processor with the modified copy seizes the bus, writes the modified line back to main memory, and transitions the line in its cache from modified to shared. Subsequently, the requesting processor tries again and finds that one or more processors have a clean copy of the line in the shared state, as described in the preceding point.\n\n\nline back to main memory, and transitions the state of the cache line to invalid (because the initiating processor is going to modify this line). Subsequently, the initiating processor will again issue a signal to the bus of RWITM and then read the line from main memory, modify the line in the cache, and mark the line in the modified state.\n\n\nThe second scenario is that no other cache has a modified copy of the requested line. In this case, no signal is returned, and the initiating processor proceeds to read in the line and modify it. Meanwhile, if one or more caches have a clean copy of the line in the shared state, each cache invalidates its copy of the line, and if one cache has a clean copy of the line in the exclusive state, it invalidates its copy of the line.\n\n\n**WRITE HIT**\n   When a write hit occurs on a line currently in the local cache, the effect depends on the current state of that line in the local cache:\n\n\n  * ■\n    **Shared:**\n    Before performing the update, the processor must gain exclusive ownership of the line. The processor signals its intent on the bus. Each processor that has a shared copy of the line in its cache transitions the sector from shared to invalid. The initiating processor then performs the update and transitions its copy of the line from shared to modified.\n  * ■\n    **Exclusive:**\n    The processor already has exclusive control of this line, and so it simply performs the update and transitions its copy of the line from exclusive to modified.\n  * ■\n    **Modified:**\n    The processor already has exclusive control of this line and has the line marked as modified, and so it simply performs the update.\n\n\n**L1-L2 CACHE CONSISTENCY**\n   We have so far described cache coherency protocols in terms of the cooperate activity among caches connected to the same bus or other SMP interconnection facility. Typically, these caches are L2 caches, and each processor also has an L1 cache that does not connect directly to the bus and that therefore cannot engage in a snoopy protocol. Thus, some scheme is needed to maintain data integrity across both levels of cache and across all caches in the SMP configuration.\n\n\nThe strategy is to extend the MESI protocol (or any cache coherence protocol) to the L1 caches. Thus, each line in the L1 cache includes bits to indicate the state. In essence, the objective is the following: for any line that is present in both an L2 cache and its corresponding L1 cache, the L1 line state should track the state of the L2 line. A simple means of doing this is to adopt the write-through policy in the L1 cache; in this case the write through is to the L2 cache and not to the memory. The L1 write-through policy forces any modification to an L1 line out to the L2 cache and therefore makes it visible to other L2 caches. The use of the L1 write-through policy requires that the L1 content must be a subset of the L2 content. This in turn suggests that the associativity of the L2 cache should be equal to or greater than that of the L1 associativity. The L1 write-through policy is used in the IBM S/390 SMP.\n\n\nIf the L1 cache has a write-back policy, the relationship between the two caches is more complex. There are several approaches to maintaining, a topic beyond our scope."
        },
        {
          "name": "Multithreading and Chip Multiprocessors",
          "content": "The most important measure of performance for a processor is the rate at which it executes instructions. This can be expressed as\n\n\n\\text{MIPS rate} = f \\times \\text{IPC}\n\n\nwhere\n   \n    f\n   \n   is the processor clock frequency, in MHz, and\n   \n    \\text{IPC}\n   \n   (instructions per cycle) is the average number of instructions executed per cycle. Accordingly, designers have pursued the goal of increased performance on two fronts: increasing clock frequency and increasing the number of instructions executed or, more properly, the number of instructions that complete during a processor cycle. As we have seen in earlier chapters, designers have increased\n   \n    \\text{IPC}\n   \n   by using an instruction pipeline and then by using multiple parallel instruction pipelines in a superscalar architecture. With pipelined and multiple-pipeline designs, the principal problem is to maximize the utilization of each pipeline stage. To improve throughput, designers have created ever more complex mechanisms, such as executing some instructions in a different order from the way they occur in the instruction stream and beginning execution of instructions that may never be needed. But as was discussed in Section 2.2, this approach may be reaching a limit due to complexity and power consumption concerns.\n\n\nAn alternative approach, which allows for a high degree of instruction-level parallelism without increasing circuit complexity or power consumption, is called multithreading. In essence, the instruction stream is divided into several smaller streams, known as threads, such that the threads can be executed in parallel.\n\n\nThe variety of specific multithreading designs, realized in both commercial systems and experimental systems, is vast. In this section, we give a brief survey of the major concepts.\n\n\n\n\n**Implicit and Explicit Multithreading**\n\n\nThe concept of thread used in discussing multithreaded processors may or may not be the same as the concept of software threads in a multiprogrammed operating system. It will be useful to define terms briefly:\n\n\n  * ■\n    **Process:**\n    An instance of a program running on a computer. A process embodies two key characteristics:\n      * —\n      **Resource ownership:**\n      A process includes a virtual address space to hold the process image; the process image is the collection of program, data, stack, and attributes that define the process. From time to time, a process may be allocated control or ownership of resources, such as main memory, I/O channels, I/O devices, and files.\n  * —\n      **Scheduling/execution:**\n      The execution of a process follows an execution path (trace) through one or more programs. This execution may be interleaved with that of other processes. Thus, a process has an execution state (Running, Ready, etc.) and a dispatching priority and is the entity that is scheduled and dispatched by the operating system.\n\n\n  * ■\n    **Process switch:**\n    An operation that switches the processor from one process to another, by saving all the process control data, registers, and other information for the first and replacing them with the process information for the second.\n    \n     2\n  * ■\n    **Thread:**\n    A dispatchable unit of work within a process. It includes a processor context (which includes the program counter and stack pointer) and its own data area for a stack (to enable subroutine branching). A thread executes sequentially and is interruptible so that the processor can turn to another thread.\n  * ■\n    **Thread switch:**\n    The act of switching processor control from one thread to another within the same process. Typically, this type of switch is much less costly than a process switch.\n\n\nThus, a thread is concerned with scheduling and execution, whereas a process is concerned with both scheduling/execution and resource ownership. The multiple threads within a process share the same resources. This is why a thread switch is much less time consuming than a process switch. Traditional operating systems, such as earlier versions of unix, did not support threads. Most modern operating systems, such as Linux, other versions of unix, and Windows, do support thread. A distinction is made between user-level threads, which are visible to the application program, and kernel-level threads, which are visible only to the operating system. Both of these may be referred to as explicit threads, defined in software.\n\n\nAll of the commercial processors and most of the experimental processors so far have used explicit multithreading. These systems concurrently execute instructions from different explicit threads, either by interleaving instructions from different threads on shared pipelines or by parallel execution on parallel pipelines. Implicit multithreading refers to the concurrent execution of multiple threads extracted from a single sequential program. These implicit threads may be defined either statically by the compiler or dynamically by the hardware. In the remainder of this section we consider explicit multithreading.\n\n\n\n\n**Approaches to Explicit Multithreading**\n\n\nAt minimum, a multithreaded processor must provide a separate program counter for each thread of execution to be executed concurrently. The designs differ in the amount and type of additional hardware used to support concurrent thread execution. In general, instruction fetching takes place on a thread basis. The processor treats each thread separately and may use a number of techniques for optimizing single-thread execution, including branch prediction, register renaming, and superscalar techniques. What is achieved is thread-level parallelism, which may provide for greatly improved performance when married to instruction-level parallelism.\n\n\nBroadly speaking, there are four principal approaches to multithreading:\n\n\n  * ■\n    **Interleaved multithreading:**\n    This is also known as\n    **fine-grained multithreading**\n    . The processor deals with two or more thread contexts at a time, switching from one thread to another at each clock cycle. If a thread is blocked because\n\n\n2\n   \n   The term\n   *context switch*\n   is often found in OS literature and textbooks. Unfortunately, although most of the literature uses this term to mean what is here called a process switch, other sources use it to mean a thread switch. To avoid ambiguity, the term is not used in this book.\n\n\n  * of data dependencies or memory latencies, that thread is skipped and a ready thread is executed.\n  * ■\n    **Blocked multithreading:**\n    This is also known as\n    **coarse-grained multithreading**\n    . The instructions of a thread are executed successively until an event occurs that may cause delay, such as a cache miss. This event induces a switch to another thread. This approach is effective on an in-order processor that would stall the pipeline for a delay event such as a cache miss.\n  * ■\n    **Simultaneous multithreading (SMT):**\n    Instructions are simultaneously issued from multiple threads to the execution units of a superscalar processor. This combines the wide superscalar instruction issue capability with the use of multiple thread contexts.\n  * ■\n    **Chip multiprocessing:**\n    In this case, multiple cores are implemented on a single chip and each core handles separate threads. The advantage of this approach is that the available logic area on a chip is used effectively without depending on ever-increasing complexity in pipeline design. This is referred to as multi-core; we examine this topic separately in Chapter 18.\n\n\nFor the first two approaches, instructions from different threads are not executed simultaneously. Instead, the processor is able to rapidly switch from one thread to another, using a different set of registers and other context information. This results in a better utilization of the processor's execution resources and avoids a large penalty due to cache misses and other latency events. The SMT approach involves true simultaneous execution of instructions from different threads, using replicated execution resources. Chip multiprocessing also enables simultaneous execution of instructions from different threads.\n\n\nFigure 17.7, based on one in [UNGE02], illustrates some of the possible pipeline architectures that involve multithreading and contrasts these with approaches that do not use multithreading. Each horizontal row represents the potential issue slot or slots for a single execution cycle; that is, the width of each row corresponds to the maximum number of instructions that can be issued in a single clock cycle.\n   \n    3\n   \n   The vertical dimension represents the time sequence of clock cycles. An empty (shaded) slot represents an unused execution slot in one pipeline. A no-op is indicated by N.\n\n\nThe first three illustrations in Figure 17.7 show different approaches with a scalar (i.e., single-issue) processor:\n\n\n  * ■\n    **Single-threaded scalar:**\n    This is the simple pipeline found in traditional RISC and CISC machines, with no multithreading.\n  * ■\n    **Interleaved multithreaded scalar:**\n    This is the easiest multithreading approach to implement. By switching from one thread to another at each clock cycle, the pipeline stages can be kept fully occupied, or close to fully occupied. The hardware must be capable of switching from one thread context to another between cycles.\n\n\n3\n   \n   Issue slots are the position from which instructions can be issued in a given clock cycle. Recall from Chapter 16 that instruction issue is the process of initiating instruction execution in the processor's functional units. This occurs when an instruction moves from the decode stage of the pipeline to the first execute stage of the pipeline.\n\n\n\n\n![Figure 17.7: Approaches to Executing Multiple Threads. The diagram illustrates ten different execution models for multiple threads (A, B, C, D) using a grid of cells where teal cells represent active thread execution and white cells represent idle or switched-out threads. (a) Single-threaded scalar: One thread (A) executes for several cycles, then switches to another thread (A) after a latency cycle. (b) Interleaved multithreading scalar: Threads are interleaved with zero cycles between switches. (c) Blocked multithreading scalar: A thread (A) executes for several cycles, then switches to another thread (B) after a latency cycle. (d) Superscalar: Multiple threads (A, B, C, D) execute simultaneously in parallel. (e) Interleaved multithreading superscalar: Threads are interleaved with zero cycles between switches. (f) Blocked multithreading superscalar: Threads are interleaved with a latency cycle between switches. (g) VLIW: A single thread (A) executes for several cycles, then switches to another thread (A) after a latency cycle. (h) Interleaved multithreading VLIW: Threads are interleaved with zero cycles between switches. (i) Blocked multithreading VLIW: Threads are interleaved with a latency cycle between switches. (j) Simultaneous multithreading (SMT): Multiple threads (A, B, C, D) execute simultaneously in parallel. (k) Chip multiprocessor (multicore): Multiple threads (A, B, C, D) execute simultaneously in parallel across different cores.](images/image_0293.jpeg)\n\n\n(a) Single-threaded scalar\n\n\n(b) Interleaved multithreading scalar\n\n\n(c) Blocked multithreading scalar\n\n\n(d) Superscalar\n\n\n(e) Interleaved multithreading superscalar\n\n\n(f) Blocked multithreading superscalar\n\n\n(g) VLIW\n\n\n(h) Interleaved multithreading VLIW\n\n\n(i) Blocked multithreading VLIW\n\n\n(j) Simultaneous multithreading (SMT)\n\n\n(k) Chip multiprocessor (multicore)\n\n\nFigure 17.7: Approaches to Executing Multiple Threads. The diagram illustrates ten different execution models for multiple threads (A, B, C, D) using a grid of cells where teal cells represent active thread execution and white cells represent idle or switched-out threads. (a) Single-threaded scalar: One thread (A) executes for several cycles, then switches to another thread (A) after a latency cycle. (b) Interleaved multithreading scalar: Threads are interleaved with zero cycles between switches. (c) Blocked multithreading scalar: A thread (A) executes for several cycles, then switches to another thread (B) after a latency cycle. (d) Superscalar: Multiple threads (A, B, C, D) execute simultaneously in parallel. (e) Interleaved multithreading superscalar: Threads are interleaved with zero cycles between switches. (f) Blocked multithreading superscalar: Threads are interleaved with a latency cycle between switches. (g) VLIW: A single thread (A) executes for several cycles, then switches to another thread (A) after a latency cycle. (h) Interleaved multithreading VLIW: Threads are interleaved with zero cycles between switches. (i) Blocked multithreading VLIW: Threads are interleaved with a latency cycle between switches. (j) Simultaneous multithreading (SMT): Multiple threads (A, B, C, D) execute simultaneously in parallel. (k) Chip multiprocessor (multicore): Multiple threads (A, B, C, D) execute simultaneously in parallel across different cores.\n\n\n**Figure 17.7**\n   Approaches to Executing Multiple Threads\n\n\n  * ■\n    **Blocked multithreaded scalar:**\n    In this case, a single thread is executed until a latency event occurs that would stop the pipeline, at which time the processor switches to another thread.\n\n\nFigure 17.7c shows a situation in which the time to perform a thread switch is one cycle, whereas Figure 17.7b shows that thread switching occurs in zero cycles.\n\n\nIn the case of interleaved multithreading, it is assumed that there are no control or data dependencies between threads, which simplifies the pipeline design and therefore should allow a thread switch with no delay. However, depending on the specific design and implementation, block multithreading may require a clock cycle to perform a thread switch, as illustrated in Figure 17.7. This is true if a fetched instruction triggers the thread switch and must be discarded from the pipeline [UNGE03].\n\n\nAlthough interleaved multithreading appears to offer better processor utilization than blocked multithreading, it does so at the sacrifice of single-thread performance. The multiple threads compete for cache resources, which raises the probability of a cache miss for a given thread.\n\n\nMore opportunities for parallel execution are available if the processor can issue multiple instructions per cycle. Figures 17.7d through 17.7i illustrate a number of variations among processors that have hardware for issuing four instructions per cycle. In all these cases, only instructions from a single thread are issued in a single cycle. The following alternatives are illustrated:\n\n\n  * ■\n    **Superscalar:**\n    This is the basic superscalar approach with no multithreading. Until relatively recently, this was the most powerful approach to providing parallelism within a processor. Note that during some cycles, not all of the available issue slots are used. During these cycles, less than the maximum number of instructions is issued; this is referred to as\n    *horizontal loss*\n    . During other instruction cycles, no issue slots are used; these are cycles when no instructions can be issued; this is referred to as\n    *vertical loss*\n    .\n  * ■\n    **Interleaved multithreading superscalar:**\n    During each cycle, as many instructions as possible are issued from a single thread. With this technique, potential delays due to thread switches are eliminated, as previously discussed. However, the number of instructions issued in any given cycle is still limited by dependencies that exist within any given thread.\n  * ■\n    **Blocked multithreaded superscalar:**\n    Again, instructions from only one thread may be issued during any cycle, and blocked multithreading is used.\n  * ■\n    **Very long instruction word (VLIW):**\n    A VLIW architecture, such as IA-64, places multiple instructions in a single word. Typically, a VLIW is constructed by the compiler, which places operations that may be executed in parallel in the same word. In a simple VLIW machine (Figure 17.7g), if it is not possible to completely fill the word with instructions to be issued in parallel, no-ops are used.\n  * ■\n    **Interleaved multithreading VLIW:**\n    This approach should provide similar efficiencies to those provided by interleaved multithreading on a superscalar architecture.\n  * ■\n    **Blocked multithreaded VLIW:**\n    This approach should provide similar efficiencies to those provided by blocked multithreading on a superscalar architecture.\n\n\nThe final two approaches illustrated in Figure 17.7 enable the parallel, simultaneous execution of multiple threads:\n\n\n  * ■\n    **Simultaneous multithreading:**\n    Figure 17.7j shows a system capable of issuing 8 instructions at a time. If one thread has a high degree of instruction-level parallelism, it may on some cycles be able fill all of the horizontal slots. On other cycles, instructions from two or more threads may be issued. If sufficient\n\n\n  * ■\n    **Chip multiprocessor (multicore):**\n    Figure 17.7k shows a chip containing four cores, each of which has a two-issue superscalar processor. Each core is assigned a thread, from which it can issue up to two instructions per cycle. We discuss multicore computers in Chapter 18.\n\n\nComparing Figures 17.7j and 17.7k, we see that a chip multiprocessor with the same instruction issue capability as an SMT cannot achieve the same degree of instruction-level parallelism. This is because the chip multiprocessor is not able to hide latencies by issuing instructions from other threads. On the other hand, the chip multiprocessor should outperform a superscalar processor with the same instruction issue capability, because the horizontal losses will be greater for the superscalar processor. In addition, it is possible to use multithreading within each of the cores on a chip multiprocessor, and this is done on some contemporary machines."
        },
        {
          "name": "Clusters",
          "content": "An important and relatively recent development computer system design is clustering. Clustering is an alternative to symmetric multiprocessing as an approach to providing high performance and high availability and is particularly attractive for server applications. We can define a cluster as a group of interconnected, whole computers working together as a unified computing resource that can create the illusion of being one machine. The term\n   *whole computer*\n   means a system that can run on its own, apart from the cluster; in the literature, each computer in a cluster is typically referred to as a\n   *node*\n   .\n\n\n[BREW97] lists four benefits that can be achieved with clustering. These can also be thought of as objectives or design requirements:\n\n\n  * ■\n    **Absolute scalability:**\n    It is possible to create large clusters that far surpass the power of even the largest standalone machines. A cluster can have tens, hundreds, or even thousands of machines, each of which is a multiprocessor.\n  * ■\n    **Incremental scalability:**\n    A cluster is configured in such a way that it is possible to add new systems to the cluster in small increments. Thus, a user can start out with a modest system and expand it as needs grow, without having to go through a major upgrade in which an existing small system is replaced with a larger system.\n  * ■\n    **High availability:**\n    Because each node in a cluster is a standalone computer, the failure of one node does not mean loss of service. In many products, fault tolerance is handled automatically in software.\n  * ■\n    **Superior price/performance:**\n    By using commodity building blocks, it is possible to put together a cluster with equal or greater computing power than a single large machine, at much lower cost.\n\n\n\n\n**Cluster Configurations**\n\n\nIn the literature, clusters are classified in a number of different ways. Perhaps the simplest classification is based on whether the computers in a cluster share access to the same disks. Figure 17.8a shows a two-node cluster in which the only interconnection\n\n\n\n\n![Diagram (a) Standby server with no shared disk. Two server nodes are shown. Each node contains two processors (P) and three I/O units (I/O) and one memory unit (M). The I/O units are connected to a disk subsystem. A high-speed message link connects the two nodes.](images/image_0294.jpeg)\n\n\nDiagram (a) Standby server with no shared disk. Two server nodes are shown. Each node contains two processors (P) and three I/O units (I/O) and one memory unit (M). The I/O units are connected to a disk subsystem. A high-speed message link connects the two nodes.\n\n\n(a) Standby server with no shared disk\n\n\n\n\n![Diagram (b) Shared Disk. Two server nodes are shown. Each node contains two processors (P) and three I/O units (I/O) and one memory unit (M). The I/O units are connected to a shared RAID disk subsystem. A high-speed message link connects the two nodes.](images/image_0295.jpeg)\n\n\nDiagram (b) Shared Disk. Two server nodes are shown. Each node contains two processors (P) and three I/O units (I/O) and one memory unit (M). The I/O units are connected to a shared RAID disk subsystem. A high-speed message link connects the two nodes.\n\n\n(b) Shared Disk\n\n\n**Figure 17.8**\nis by means of a high-speed link that can be used for message exchange to coordinate cluster activity. The link can be a LAN that is shared with other computers that are not part of the cluster or the link can be a dedicated interconnection facility. In the latter case, one or more of the computers in the cluster will have a link to a LAN or WAN so that there is a connection between the server cluster and remote client systems. Note that in the figure, each computer is depicted as being a multiprocessor. This is not necessary but does enhance both performance and availability.\n\n\nIn the simple classification depicted in Figure 17.8, the other alternative is a shared-disk cluster. In this case, there generally is still a message link between nodes. In addition, there is a disk subsystem that is directly linked to multiple computers within the cluster. In this figure, the common disk subsystem is a RAID system. The use of RAID or some similar redundant disk technology is common in clusters so that the high availability achieved by the presence of multiple computers is not compromised by a shared disk that is a single point of failure.\n\n\nA clearer picture of the range of cluster options can be gained by looking at functional alternatives. Table 17.2 provides a useful classification along functional lines, which we now discuss.\n\n\n**Table 17.2**\n\nClustering Method | Description | Benefits | Limitations\nPassive Standby | A secondary server takes over in case of primary server failure. | Easy to implement. | High cost because the secondary server is unavailable for other processing tasks.\nActive Secondary: | The secondary server is also used for processing tasks. | Reduced cost because secondary servers can be used for processing. | Increased complexity.\nSeparate Servers | Separate servers have their own disks. Data is continuously copied from primary to secondary server. | High availability. | High network and server overhead due to copying operations.\nServers Connected to Disks | Servers are cabled to the same disks, but each server owns its disks. If one server fails, its disks are taken over by the other server. | Reduced network and server overhead due to elimination of copying operations. | Usually requires disk mirroring or RAID technology to compensate for risk of disk failure.\nServers Share Disks | Multiple servers simultaneously share access to disks. | Low network and server overhead. Reduced risk of downtime caused by disk failure. | Requires lock manager software. Usually used with disk mirroring or RAID technology.\n\n\nA common, older method, known as\n   **passive standby**\n   , is simply to have one computer handle all of the processing load while the other computer remains inactive, standing by to take over in the event of a failure of the primary. To coordinate the machines, the active, or primary, system periodically sends a “heartbeat” message to the standby machine. Should these messages stop arriving, the standby assumes that the primary server has failed and puts itself into operation. This approach increases availability but does not improve performance. Further, if the only information that is exchanged between the two systems is a heartbeat message, and if the two systems do not share common disks, then the standby provides a functional backup but has no access to the databases managed by the primary.\n\n\nThe passive standby is generally not referred to as a cluster. The term\n   *cluster*\n   is reserved for multiple interconnected computers that are all actively doing processing while maintaining the image of a single system to the outside world. The term\n   **active secondary**\n   is often used in referring to this configuration. Three classifications of clustering can be identified: separate servers, shared nothing, and shared memory.\n\n\nIn one approach to clustering, each computer is a\n   **separate server**\n   with its own disks and there are no disks shared between systems (Figure 17.8a). This arrangement provides high performance as well as high availability. In this case, some type of management or scheduling software is needed to assign incoming client requests to servers so that the load is balanced and high utilization is achieved. It is desirable to have a failover capability, which means that if a computer fails while executing an application, another computer in the cluster can pick up and complete\n\n\nthe application. For this to happen, data must constantly be copied among systems so that each system has access to the current data of the other systems. The overhead of this data exchange ensures high availability at the cost of a performance penalty.\n\n\nTo reduce the communications overhead, most clusters now consist of servers connected to common disks (Figure 17.8b). In one variation on this approach, called\n   **shared nothing**\n   , the common disks are partitioned into volumes, and each volume is owned by a single computer. If that computer fails, the cluster must be reconfigured so that some other computer has ownership of the volumes of the failed computer.\n\n\nIt is also possible to have multiple computers share the same disks at the same time (called the\n   **shared disk**\n   approach), so that each computer has access to all of the volumes on all of the disks. This approach requires the use of some type of locking facility to ensure that data can only be accessed by one computer at a time.\n\n\n\n\n**Operating System Design Issues**\n\n\nFull exploitation of a cluster hardware configuration requires some enhancements to a single-system operating system.\n\n\n**FAILURE MANAGEMENT**\n   How failures are managed by a cluster depends on the clustering method used (Table 17.2). In general, two approaches can be taken to dealing with failures: highly available clusters and fault-tolerant clusters. A highly available cluster offers a high probability that all resources will be in service. If a failure occurs, such as a system goes down or a disk volume is lost, then the queries in progress are lost. Any lost query, if retried, will be serviced by a different computer in the cluster. However, the cluster operating system makes no guarantee about the state of partially executed transactions. This would need to be handled at the application level.\n\n\nA fault-tolerant cluster ensures that all resources are always available. This is achieved by the use of redundant shared disks and mechanisms for backing out uncommitted transactions and committing completed transactions.\n\n\nThe function of switching applications and data resources over from a failed system to an alternative system in the cluster is referred to as\n   **failover**\n   . A related function is the restoration of applications and data resources to the original system once it has been fixed; this is referred to as\n   **failback**\n   . Failback can be automated, but this is desirable only if the problem is truly fixed and unlikely to recur. If not, automatic failback can cause subsequently failed resources to bounce back and forth between computers, resulting in performance and recovery problems.\n\n\n**LOAD BALANCING**\n   A cluster requires an effective capability for balancing the load among available computers. This includes the requirement that the cluster be incrementally scalable. When a new computer is added to the cluster, the load-balancing facility should automatically include this computer in scheduling applications. Middleware mechanisms need to recognize that services can appear on different members of the cluster and may migrate from one member to another.\n\n\n**PARALLELIZING COMPUTATION**\n   In some cases, effective use of a cluster requires executing software from a single application in parallel. [KAPP00] lists three general approaches to the problem:\n\n\n  * ■\n    **Parallelizing compiler:**\n    A parallelizing compiler determines, at compile time, which parts of an application can be executed in parallel. These are then split off to be assigned to different computers in the cluster. Performance depends on the nature of the problem and how well the compiler is designed. In general, such compilers are difficult to develop.\n  * ■\n    **Parallelized application:**\n    In this approach, the programmer writes the application from the outset to run on a cluster, and uses message passing to move data, as required, between cluster nodes. This places a high burden on the programmer but may be the best approach for exploiting clusters for some applications.\n  * ■\n    **Parametric computing:**\n    This approach can be used if the essence of the application is an algorithm or program that must be executed a large number of times, each time with a different set of starting conditions or parameters. A good example is a simulation model, which will run a large number of different scenarios and then develop statistical summaries of the results. For this approach to be effective, parametric processing tools are needed to organize, run, and manage the jobs in an effective manner.\n\n\n\n\n**Cluster Computer Architecture**\n\n\nFigure 17.9 shows a typical cluster architecture. The individual computers are connected by some high-speed LAN or switch hardware. Each computer is capable of operating independently. In addition, a middleware layer of software is installed in each computer to enable cluster operation. The cluster middleware provides a unified system image to the user, known as a single-system image. The middleware is also responsible for providing high availability, by means of load balancing and\n\n\n\n\n![Diagram of Cluster Computer Architecture showing five nodes connected to a high-speed network switch, with layers of software above them.](images/image_0296.jpeg)\n\n\nThe diagram illustrates a cluster computer architecture. At the bottom, a horizontal bar represents the 'High-speed network/switch'. Above it, five vertical blocks represent individual nodes. Each node is labeled 'PC/workstation' at the top. Below this label, each node contains two stacked boxes: 'Comm SW' (Communication Software) and 'Net. interface HW' (Network Interface Hardware). Above these nodes, a large horizontal bar represents the 'Cluster middleware (Single system image and availability infrastructure)'. Above this middleware layer, there are two groups of software layers. The left group consists of a single box labeled 'Sequential applications'. The right group consists of two stacked boxes: 'Parallel programming environment' at the bottom and 'Parallel applications' at the top.\n\n\nDiagram of Cluster Computer Architecture showing five nodes connected to a high-speed network switch, with layers of software above them.\n\n\n**Figure 17.9**\n   Cluster Computer Architecture [BUY99]\n\n\nresponding to failures in individual components. [HWAN99] lists the following as desirable cluster middleware services and functions:\n\n\n  * ■\n    **Single entry point:**\n    A user logs onto the cluster rather than to an individual computer.\n  * ■\n    **Single file hierarchy:**\n    The user sees a single hierarchy of file directories under the same root directory.\n  * ■\n    **Single control point:**\n    There is a default workstation used for cluster management and control.\n  * ■\n    **Single virtual networking:**\n    Any node can access any other point in the cluster, even though the actual cluster configuration may consist of multiple interconnected networks. There is a single virtual network operation.\n  * ■\n    **Single memory space:**\n    Distributed shared memory enables programs to share variables.\n  * ■\n    **Single job-management system:**\n    Under a cluster job scheduler, a user can submit a job without specifying the host computer to execute the job.\n  * ■\n    **Single user interface:**\n    A common graphic interface supports all users, regardless of the workstation from which they enter the cluster.\n  * ■\n    **Single I/O space:**\n    Any node can remotely access any I/O peripheral or disk device without knowledge of its physical location.\n  * ■\n    **Single process space:**\n    A uniform process-identification scheme is used. A process on any node can create or communicate with any other process on a remote node.\n  * ■\n    **Checkpointing:**\n    This function periodically saves the process state and intermediate computing results, to allow rollback recovery after a failure.\n  * ■\n    **Process migration:**\n    This function enables load balancing.\n\n\nThe last four items on the preceding list enhance the availability of the cluster. The remaining items are concerned with providing a single system image.\n\n\nReturning to Figure 17.9, a cluster will also include software tools for enabling the efficient execution of programs that are capable of parallel execution.\n\n\n\n\n**Blade Servers**\n\n\nA common implementation of the cluster approach is the blade server. A blade server is a server architecture that houses multiple server modules (“blades”) in a single chassis. It is widely used in data centers to save space and improve system management. Either self-standing or rack mounted, the chassis provides the power supply, and each blade has its own processor, memory, and hard disk.\n\n\nAn example of the application is shown in Figure 17.10. The trend at large data centers, with substantial banks of blade servers, is the deployment of 10-Gbps ports on individual servers to handle the massive multimedia traffic provided by these servers. Such arrangements are stressing the on-site Ethernet switches needed to interconnect large numbers of servers. A 100-Gbps rate provides the bandwidth required to handle the increased traffic load. The 100-Gbps\n\n\n\n\n![Diagram of a 100-Gbps Ethernet configuration for a massive blade server site. The network is organized in a three-tier hierarchy. The top tier consists of three 'Eth switch' boxes. The middle tier consists of two 'Eth switch' boxes. The bottom tier consists of three stacks of blade servers, each with its own 'Eth switch' box. Connections are as follows: Top-tier switches connect to middle-tier switches with 100GbE links. Middle-tier switches connect to bottom-tier switches with 100GbE links. Bottom-tier switches connect to blade servers with 10GbE & 40GbE links. An arrow labeled 'N × 100GbE' points from the top-tier switches to 'Additional blade server racks' represented by three dots. An arrow labeled '100GbE' points from the middle-tier switches to the bottom-tier switches. An arrow labeled '10GbE & 40GbE' points from the bottom-tier switches to the blade server stacks. Ellipses are used to indicate multiple switches and server racks.](images/image_0297.jpeg)\n\n\nDiagram of a 100-Gbps Ethernet configuration for a massive blade server site. The network is organized in a three-tier hierarchy. The top tier consists of three 'Eth switch' boxes. The middle tier consists of two 'Eth switch' boxes. The bottom tier consists of three stacks of blade servers, each with its own 'Eth switch' box. Connections are as follows: Top-tier switches connect to middle-tier switches with 100GbE links. Middle-tier switches connect to bottom-tier switches with 100GbE links. Bottom-tier switches connect to blade servers with 10GbE & 40GbE links. An arrow labeled 'N × 100GbE' points from the top-tier switches to 'Additional blade server racks' represented by three dots. An arrow labeled '100GbE' points from the middle-tier switches to the bottom-tier switches. An arrow labeled '10GbE & 40GbE' points from the bottom-tier switches to the blade server stacks. Ellipses are used to indicate multiple switches and server racks.\n\n\n**Figure 17.10**\n   Example 100-Gbps Ethernet Configuration for Massive Blade Server Site\n\n\nEthernet switches are deployed in switch uplinks inside the data center as well as providing interbuilding, intercampus, wide area connections for enterprise networks.\n\n\n\n\n**Clusters Compared to SMP**\n\n\nBoth clusters and symmetric multiprocessors provide a configuration with multiple processors to support high-demand applications. Both solutions are commercially available, although SMP schemes have been around far longer.\n\n\nThe main strength of the SMP approach is that an SMP is easier to manage and configure than a cluster. The SMP is much closer to the original single-processor model for which nearly all applications are written. The principal change required in going from a uniprocessor to an SMP is to the scheduler function. Another benefit of the SMP is that it usually takes up less physical space and draws less power than a comparable cluster. A final important benefit is that the SMP products are well established and stable.\n\n\nOver the long run, however, the advantages of the cluster approach are likely to result in clusters dominating the high-performance server market. Clusters are far superior to SMPs in terms of incremental and absolute scalability. Clusters are also superior in terms of availability, because all components of the system can readily be made highly redundant."
        },
        {
          "name": "Nonuniform Memory Access",
          "content": "In terms of commercial products, the two common approaches to providing a multiple-processor system to support applications are SMPs and clusters. For some years, another approach, known as nonuniform memory access (NUMA), has been the subject of research and commercial NUMA products are now available.\n\n\nBefore proceeding, we should define some terms often found in the NUMA literature.\n\n\n  * ■\n    **Uniform memory access (UMA):**\n    All processors have access to all parts of main memory using loads and stores. The memory access time of a processor to all regions of memory is the same. The access times experienced by different processors are the same. The SMP organization discussed in Sections 17.2 and 17.3 is UMA.\n  * ■\n    **Nonuniform memory access (NUMA):**\n    All processors have access to all parts of main memory using loads and stores. The memory access time of a processor differs depending on which region of main memory is accessed. The last statement is true for all processors; however, for different processors, which memory regions are slower and which are faster differ.\n  * ■\n    **Cache-coherent NUMA (CC-NUMA):**\n    A NUMA system in which cache coherence is maintained among the caches of the various processors.\n\n\nA NUMA system without cache coherence is more or less equivalent to a cluster. The commercial products that have received much attention recently are CC-NUMA systems, which are quite distinct from both SMPs and clusters. Usually, but unfortunately not always, such systems are in fact referred to in the commercial literature as CC-NUMA systems. This section is concerned only with CC-NUMA systems.\n\n\n\n\n**Motivation**\n\n\nWith an SMP system, there is a practical limit to the number of processors that can be used. An effective cache scheme reduces the bus traffic between any one processor and main memory. As the number of processors increases, this bus traffic also increases. Also, the bus is used to exchange cache-coherence signals, further adding to the burden. At some point, the bus becomes a performance bottleneck. Performance degradation seems to limit the number of processors in an SMP configuration to somewhere between 16 and 64 processors. For example, Silicon Graphics' Power Challenge SMP is limited to 64 R10000 processors in a single system; beyond this number performance degrades substantially.\n\n\nThe processor limit in an SMP is one of the driving motivations behind the development of cluster systems. However, with a cluster, each node has its own private main memory; applications do not see a large global memory. In effect, coherency is maintained in software rather than hardware. This memory granularity affects performance and, to achieve maximum performance, software must be tailored to this environment. One approach to achieving large-scale multiprocessing while retaining the flavor of SMP is NUMA.\n\n\nThe objective with NUMA is to maintain a transparent system wide memory while permitting multiple multiprocessor nodes, each with its own bus or other internal interconnect system.\n\n\n\n\n**Organization**\n\n\nFigure 17.11 depicts a typical CC-NUMA organization. There are multiple independent nodes, each of which is, in effect, an SMP organization. Thus, each node contains multiple processors, each with its own L1 and L2 caches, plus main memory. The node is the basic building block of the overall CC-NUMA organization. For example, each Silicon Graphics Origin node includes two MIPS R10000 processors;\n\n\n\n\n![Diagram of CC-NUMA Organization showing multiple nodes connected via an interconnect network.](images/image_0298.jpeg)\n\n\nThe diagram illustrates a CC-NUMA (Cache Coherent Non-Uniform Memory Access) organization. It features three distinct nodes, each containing multiple processors, local caches, and main memory. An interconnect network links these nodes together.\n\n\n  * **Node 1 (Top):**\n     Contains processors labeled\n     **Processor 1-1**\n     and\n     **Processor 1-m**\n     . Each processor has an\n     **L1 Cache**\n     . Below the processors are\n     **L2 Cache**\n     blocks. A horizontal bus connects these components. On the bus are a\n     **Main Memory 1**\n     block and an\n     **I/O**\n     block. A\n     **Directory**\n     block is also connected to the bus.\n  * **Node 2 (Middle):**\n     Contains processors labeled\n     **Processor 2-1**\n     and\n     **Processor 2-m**\n     . Each processor has an\n     **L1 Cache**\n     . Below the processors are\n     **L2 Cache**\n     blocks. A horizontal bus connects these components. On the bus are an\n     **I/O**\n     block and a\n     **Main Memory 2**\n     block. A\n     **Directory**\n     block is also connected to the bus.\n  * **Node 3 (Bottom):**\n     Contains processors labeled\n     **Processor N-1**\n     and\n     **Processor N-m**\n     . Each processor has an\n     **L1 Cache**\n     . Below the processors are\n     **L2 Cache**\n     blocks. A horizontal bus connects these components. On the bus are a\n     **Main memory N**\n     block and a\n     **Directory**\n     block. An\n     **I/O**\n     block is also connected to the bus.\n\n\nA central\n    **Interconnect Network**\n    (represented by a large rounded rectangle) is connected to the\n    **I/O**\n    blocks of all three nodes, facilitating communication between them.\n\n\nDiagram of CC-NUMA Organization showing multiple nodes connected via an interconnect network.\n\n\n**Figure 17.11**\n   CC-NUMA Organization\n\n\neach Sequent NUMA-Q node includes four Pentium II processors. The nodes are interconnected by means of some communications facility, which could be a switching mechanism, a ring, or some other networking facility.\n\n\nEach node in the CC-NUMA system includes some main memory. From the point of view of the processors, however, there is only a single addressable memory, with each location having a unique system wide address. When a processor initiates a memory access, if the requested memory location is not in that processor's cache, then the L2 cache initiates a fetch operation. If the desired line is in the local portion of the main memory, the line is fetched across the local bus. If the desired line is in a remote portion of the main memory, then an automatic request is sent out to fetch that line across the interconnection network, deliver it to the local bus, and then deliver it to the requesting cache on that bus. All of this activity is automatic and transparent to the processor and its cache.\n\n\nIn this configuration, cache coherence is a central concern. Although implementations differ as to details, in general terms we can say that each node must maintain some sort of directory that gives it an indication of the location of various portions of memory and also cache status information. To see how this scheme works, we give an example taken from [PFIS98]. Suppose that processor 3 on node 2 (P2-3) requests a memory location 798, which is in the memory of node 1. The following sequence occurs:\n\n\n  * 1. P2-3 issues a read request on the snoopy bus of node 2 for location 798.\n  * 2. The directory on node 2 sees the request and recognizes that the location is in node 1.\n  * 3. Node 2's directory sends a request to node 1, which is picked up by node 1's directory.\n  * 4. Node 1's directory, acting as a surrogate of P2-3, requests the contents of 798, as if it were a processor.\n  * 5. Node 1's main memory responds by putting the requested data on the bus.\n  * 6. Node 1's directory picks up the data from the bus.\n  * 7. The value is transferred back to node 2's directory.\n  * 8. Node 2's directory places the data back on node 2's bus, acting as a surrogate for the memory that originally held it.\n  * 9. The value is picked up and placed in P2-3's cache and delivered to P2-3.\n\n\nThe preceding sequence explains how data are read from a remote memory using hardware mechanisms that make the transaction transparent to the processor. On top of these mechanisms, some form of cache coherence protocol is needed. Various systems differ on exactly how this is done. We make only a few general remarks here. First, as part of the preceding sequence, node 1's directory keeps a record that some remote cache has a copy of the line containing location 798. Then, there needs to be a cooperative protocol to take care of modifications. For example, if a modification is done in a cache, this fact can be broadcast to other nodes. Each node's directory that receives such a broadcast can then determine if any local cache has that line and, if so, cause it to be purged. If the actual memory location is at the node receiving the broadcast notification, then that node's\n\n\ndirectory needs to maintain an entry indicating that that line of memory is invalid and remains so until a write back occurs. If another processor (local or remote) requests the invalid line, then the local directory must force a write back to update memory before providing the data.\n\n\n\n\n**NUMA Pros and Cons**\n\n\nThe main advantage of a CC-NUMA system is that it can deliver effective performance at higher levels of parallelism than SMP, without requiring major software changes. With multiple NUMA nodes, the bus traffic on any individual node is limited to a demand that the bus can handle. However, if many of the memory accesses are to remote nodes, performance begins to break down. There is reason to believe that this performance breakdown can be avoided. First, the use of L1 and L2 caches is designed to minimize all memory accesses, including remote ones. If much of the software has good temporal locality, then remote memory accesses should not be excessive. Second, if the software has good spatial locality, and if virtual memory is in use, then the data needed for an application will reside on a limited number of frequently used pages that can be initially loaded into the memory local to the running application. The Sequent designers report that such spatial locality does appear in representative applications [LOVE96]. Finally, the virtual memory scheme can be enhanced by including in the operating system a page migration mechanism that will move a virtual memory page to a node that is frequently using it; the Silicon Graphics designers report success with this approach [WHIT97].\n\n\nEven if the performance breakdown due to remote access is addressed, there are two other disadvantages for the CC-NUMA approach [PFIS98]. First, a CC-NUMA does not transparently look like an SMP; software changes will be required to move an operating system and applications from an SMP to a CC-NUMA system. These include page allocation, already mentioned, process allocation, and load balancing by the operating system. A second concern is that of availability. This is a rather complex issue and depends on the exact implementation of the CC-NUMA system; the interested reader is referred to [PFIS98].\n\n\n\n\n![Logo for Online Interactive Simulation, featuring a globe and the text 'www'.](images/image_0299.jpeg)\n\n\nLogo for Online Interactive Simulation, featuring a globe and the text 'www'.\n\n\n**Vector Processor Simulator**"
        },
        {
          "name": "Cloud Computing",
          "content": "Cloud computing was introduced in Chapter 1, where the three service models were discussed. Here we go into greater detail.\n\n\n\n\n**Cloud Computing Elements**\n\n\nNIST SP-800-145 (\n   *The NIST Definition of Cloud Computing*\n   ) specifies that\n   **cloud computing**\n   is composed of five essential characteristics, three service models, and\n\n\nfour deployment models. Figure 17.12 illustrates the relationship among these concepts. The essential characteristics of cloud computing include the following:\n\n\n  * ■\n    **Broad network access:**\n    Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, laptops, and tablets) as well as other traditional or cloud-based software services.\n  * ■\n    **Rapid elasticity:**\n    Cloud computing gives you the ability to expand and reduce resources according to your specific service requirement. For example, you may need a large number of server resources for the duration of a specific task. You can then release these resources upon completion of the task.\n  * ■\n    **Measured service:**\n    Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.\n  * ■\n    **On-demand self-service:**\n    A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically\n\n\n\n\n![Figure 17.12: Cloud Computing Elements. A diagram showing the relationship between Essential characteristics, Service models, and Deployment models.](images/image_0300.jpeg)\n\n\nThe diagram illustrates the relationship between three key elements of cloud computing, organized into three horizontal layers:\n\n\n  * **Essential characteristics:**\n     The top layer, containing four boxes: \"Broad network access\", \"Rapid elasticity\", \"Measured service\", and \"On-demand self-service\". Below these is a larger box labeled \"Resource pooling\".\n  * **Service models:**\n     The middle layer, containing three nested boxes: \"Software as a service (SaaS)\" (outermost), \"Platform as a service (PaaS)\" (middle), and \"Infrastructure as a service (IaaS)\" (innermost).\n  * **Deployment models:**\n     The bottom layer, containing four cloud-shaped icons labeled \"Public\", \"Private\", \"Hybrid\", and \"Community\".\n\n\nFigure 17.12: Cloud Computing Elements. A diagram showing the relationship between Essential characteristics, Service models, and Deployment models.\n\n\n**Figure 17.12**\n   Cloud Computing Elements\n\n\nwithout requiring human interaction with each service provider. Because the service is on demand, the resources are not permanent parts of your IT infrastructure.\n\n\n  * ■\n    **Resource pooling:**\n    The provider’s computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. There is a degree of location independence in that the customer generally has no control or knowledge over the exact location of the provided resources, but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter). Examples of resources include storage, processing, memory, network bandwidth, and virtual machines. Even private clouds tend to pool resources between different parts of the same organization.\n\n\nNIST defines three\n   **service models**\n   , which can be viewed as nested service alternatives (Figure 17.13). These were defined in Chapter 1, and can be briefly summarized as follows:\n\n\n  * ■\n    **Software as a service (SaaS):**\n    Provides service to customers in the form of software, specifically application software, running on and accessible in the cloud.\n\n\n\n\n![Figure 17.13: Cloud Service Models. The diagram shows three nested models: (a) SaaS, (b) PaaS, and (c) IaaS. Each model is represented by three concentric rounded rectangles. (a) SaaS: The outermost rectangle is labeled 'Cloud application software (provided by cloud, visible to subscriber)'. The middle rectangle is labeled 'Cloud platform (visible only to provider)'. The innermost rectangle is labeled 'Cloud infrastructure (visible only to provider)'. (b) PaaS: The outermost rectangle is labeled 'Cloud application software (developed by subscriber)'. The middle rectangle is labeled 'Cloud platform (visible to subscriber)'. The innermost rectangle is labeled 'Cloud infrastructure (visible only to provider)'. (c) IaaS: The outermost rectangle is labeled 'Cloud application software (developed by subscriber)'. The middle rectangle is labeled 'Cloud platform (visible to subscriber)'. The innermost rectangle is labeled 'Cloud infrastructure (visible to subscriber)'.](images/image_0301.jpeg)\n\n\nFigure 17.13: Cloud Service Models. The diagram shows three nested models: (a) SaaS, (b) PaaS, and (c) IaaS. Each model is represented by three concentric rounded rectangles. (a) SaaS: The outermost rectangle is labeled 'Cloud application software (provided by cloud, visible to subscriber)'. The middle rectangle is labeled 'Cloud platform (visible only to provider)'. The innermost rectangle is labeled 'Cloud infrastructure (visible only to provider)'. (b) PaaS: The outermost rectangle is labeled 'Cloud application software (developed by subscriber)'. The middle rectangle is labeled 'Cloud platform (visible to subscriber)'. The innermost rectangle is labeled 'Cloud infrastructure (visible only to provider)'. (c) IaaS: The outermost rectangle is labeled 'Cloud application software (developed by subscriber)'. The middle rectangle is labeled 'Cloud platform (visible to subscriber)'. The innermost rectangle is labeled 'Cloud infrastructure (visible to subscriber)'.\n\n\n**Figure 17.13**\n   Cloud Service Models\n\n\n  * ■\n    **Platform as a service (PaaS):**\n    Provides service to customers in the form of a platform on which the customer's applications can run.\n  * ■\n    **Infrastructure as a service (IaaS):**\n    Provides the customer access to the underlying cloud infrastructure.\n\n\nNIST defines four\n   **deployment models**\n   :\n\n\n  * ■\n    **Public cloud:**\n    The cloud infrastructure is made available to the general public or a large industry group and is owned by an organization selling cloud services. The cloud provider is responsible both for the cloud infrastructure and for the control of data and operations within the cloud. The major advantage of the public cloud is cost. A subscribing organization only pays for the services and resources it needs and can adjust these as needed. Further, the subscriber has greatly reduced management overhead. The principal concern is security. However, there are a number of public cloud providers that have demonstrated strong security controls and, in fact, such providers may have more resources and expertise to devote to security that would be available in a private cloud.\n  * ■\n    **Private cloud:**\n    A private cloud is a cloud infrastructure implemented within the internal IT environment of the organization. The organization may choose to manage the cloud in house or contract the management function to a third party. Additionally, the cloud servers and storage devices may exist on premise or off premise. A key motivation for opting for a private cloud is security. A private cloud infrastructure offers tighter controls over the geographic location of data storage and other aspects of security.\n  * ■\n    **Community cloud:**\n    A community cloud shares characteristics of private and public clouds. Like a private cloud, a community cloud is not open to any subscriber. Like a public cloud, the cloud resources are shared among a number of independent organizations. The organizations that share the community cloud have similar requirements and, typically, a need to exchange data with each other. One example of an industry that is employing the community cloud concept is the health care industry. A community cloud can be implemented to comply with government privacy and other regulations. The community participants can exchange data in a controlled fashion. The cloud infrastructure may be managed by the participating organizations or a third party and may exist on premise or off premise. In this deployment model, the costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized.\n  * ■\n    **Hybrid cloud:**\n    The cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities but are bound together by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load balancing between clouds). With a hybrid cloud solution, sensitive information can be placed in a private area of the cloud, and less sensitive data can take advantage of the cost benefits of the public cloud.\n\n\nFigure 17.14 illustrates the typical cloud service context. An enterprise maintains workstations within an enterprise LAN or set of LANs, which are connected by a router through a network or the Internet to the cloud service provider. The cloud service provider maintains a massive collection of servers, which it manages with a variety\n\n\n\n\n![Diagram illustrating the Cloud Computing Context. It shows an Enterprise cloud user on the left, connected to a LAN switch, which is connected to a Router. This Router is connected to a cloud representing the Network or Internet. Below the cloud, another Router is connected to multiple LAN switches, which are connected to a large number of Servers. The entire cloud infrastructure is labeled as the Cloud service provider.](images/image_0302.jpeg)\n\n\nThe diagram illustrates the Cloud Computing Context. On the left, an\n    **Enterprise cloud user**\n    is shown with three laptops and two desktop computers. These devices are connected to a\n    **LAN switch**\n    , which is connected to a\n    **Router**\n    . This Router is connected to a cloud labeled\n    **Network or Internet**\n    . Below the cloud, another\n    **Router**\n    is connected to multiple\n    **LAN switches**\n    , which are connected to a large number of\n    **Servers**\n    . The entire cloud infrastructure is labeled as the\n    **Cloud service provider**\n    .\n\n\nDiagram illustrating the Cloud Computing Context. It shows an Enterprise cloud user on the left, connected to a LAN switch, which is connected to a Router. This Router is connected to a cloud representing the Network or Internet. Below the cloud, another Router is connected to multiple LAN switches, which are connected to a large number of Servers. The entire cloud infrastructure is labeled as the Cloud service provider.\n\n\n**Figure 17.14**\n   Cloud Computing Context\n\n\nof network management, redundancy, and security tools. In the figure, the cloud infrastructure is shown as a collection of blade servers, which is a common architecture.\n\n\n\n\n**Cloud Computing Reference Architecture**\n\n\nNIST SP 500-292 (\n   *NIST Cloud Computing Reference Architecture*\n   ) establishes a reference architecture, described as follows:\n\n\nThe NIST cloud computing reference architecture focuses on the requirements of “what” cloud services provide, not a “how to” design solution and implementation. The reference architecture is intended to facilitate the understanding of the operational intricacies in cloud computing. It does not represent the system architecture of a specific cloud computing system; instead it is a tool for describing, discussing, and developing a system-specific architecture using a common framework of reference.\n\n\nNIST developed the reference architecture with the following objectives in mind:\n\n\n  * ■ To illustrate and understand the various cloud services in the context of an overall cloud computing conceptual model.\n  * ■ To provide a technical reference for consumers to understand, discuss, categorize, and compare cloud services.\n  * ■ To facilitate the analysis of candidate standards for security, interoperability, and portability and reference implementations.\n\n\nThe reference architecture, depicted in Figure 17.15, defines five major actors in terms of the roles and responsibilities:\n\n\n  * ■\n    **Cloud consumer:**\n    A person or organization that maintains a business relationship with, and uses service from, cloud providers.\n  * ■\n    **Cloud provider (CP):**\n    A person, organization, or entity responsible for making a service available to interested parties.\n  * ■\n    **Cloud auditor:**\n    A party that can conduct independent assessment of cloud services, information system operations, performance, and security of the cloud implementation.\n  * ■\n    **Cloud broker:**\n    An entity that manages the use, performance, and delivery of cloud services, and negotiates relationships between CPs and cloud consumers.\n  * ■\n    **Cloud carrier:**\n    An intermediary that provides connectivity and transport of cloud services from CPs to cloud consumers.\n\n\n\n\n![NIST Cloud Computing Reference Architecture diagram showing the relationships between Cloud consumer, Cloud provider, Cloud broker, and Cloud carrier.](images/image_0303.jpeg)\n\n\nThe diagram illustrates the NIST Cloud Computing Reference Architecture, showing the interactions between five major actors and the internal structure of the Cloud provider.\n\n\n**Actors and their roles:**\n\n\n  * **Cloud consumer:**\n     Interacts with the Cloud provider and Cloud broker.\n  * **Cloud auditor:**\n     Interacts with the Cloud provider and Cloud broker, performing security, privacy, and performance audits.\n  * **Cloud provider:**\n     The central entity, structured into layers and management functions.\n       * **Service orchestration:**\n       Includes the Service layer (SaaS, PaaS, IaaS) and the Resource abstraction and control layer.\n  * **Cloud service management:**\n       Includes Business support, Provisioning/configuration, and Portability/interoperability.\n  * **Physical resource layer:**\n       Includes Hardware and Facility.\n  * **Cloud broker:**\n     Interacts with the Cloud provider and Cloud consumer, performing service intermediation, aggregation, and arbitrage.\n  * **Cloud carrier:**\n     Provides connectivity and transport between the Cloud provider and Cloud consumer.\n\n\n**Security and Privacy:**\n    Two vertical columns labeled \"Security\" and \"Privacy\" run through the Cloud provider and Cloud broker sections, indicating cross-cutting concerns.\n\n\nNIST Cloud Computing Reference Architecture diagram showing the relationships between Cloud consumer, Cloud provider, Cloud broker, and Cloud carrier.\n\n\nFigure 17.15 NIST Cloud Computing Reference Architecture\n\n\nThe roles of the cloud consumer and provider have already been discussed. To summarize, a\n   **cloud provider**\n   can provide one or more of the cloud services to meet IT and business requirements of\n   **cloud consumers**\n   . For each of the three service models (SaaS, PaaS, IaaS), the CP provides the storage and processing facilities needed to support that service model, together with a cloud interface for cloud service consumers. For SaaS, the CP deploys, configures, maintains, and updates the operation of the software applications on a cloud infrastructure so that the services are provisioned at the expected service levels to cloud consumers. The consumers of SaaS can be organizations that provide their members with access to software applications, end users who directly use software applications, or software application administrators who configure applications for end users.\n\n\nFor PaaS, the CP manages the computing infrastructure for the platform and runs the cloud software that provides the components of the platform, such as runtime software execution stack, databases, and other middleware components. Cloud consumers of PaaS can employ the tools and execution resources provided by CPs to develop, test, deploy, and manage the applications hosted in a cloud environment.\n\n\nFor IaaS, the CP acquires the physical computing resources underlying the service, including the servers, networks, storage, and hosting infrastructure. The IaaS cloud consumer in turn uses these computing resources, such as a virtual computer, for their fundamental computing needs.\n\n\nThe\n   **cloud carrier**\n   is a networking facility that provides connectivity and transport of cloud services between cloud consumers and CPs. Typically, a CP will set up service level agreements (SLAs) with a cloud carrier to provide services consistent with the level of SLAs offered to cloud consumers, and may require the cloud carrier to provide dedicated and secure connections between cloud consumers and CPs.\n\n\nA\n   **cloud broker**\n   is useful when cloud services are too complex for a cloud consumer to easily manage. A cloud broker can offer three areas of support:\n\n\n  * ■\n    **Service intermediation:**\n    These are value-added services, such as identity management, performance reporting, and enhanced security.\n  * ■\n    **Service aggregation:**\n    The broker combines multiple cloud services to meet consumer needs not specifically addressed by a single CP, or to optimize performance or minimize cost.\n  * ■\n    **Service arbitrage:**\n    This is similar to service aggregation except that the services being aggregated are not fixed. Service arbitrage means a broker has the flexibility to choose services from multiple agencies. The cloud broker, for example, can use a credit-scoring service to measure and select an agency with the best score.\n\n\nA\n   **cloud auditor**\n   can evaluate the services provided by a CP in terms of security controls, privacy impact, performance, and so on. The auditor is an independent entity that can assure that the CP conforms to a set of standards."
        }
      ]
    },
    {
      "name": "Multicore Computers",
      "sections": [
        {
          "name": "Hardware Performance Issues",
          "content": "As we discuss in Chapter 2, microprocessor systems have experienced a steady increase in execution performance for decades. This increase is due to a number of factors, including increase in clock frequency, increase in transistor density, and refinements in the organization of the processor on the chip.\n\n\n\n\n**Increase in Parallelism and Complexity**\n\n\nThe organizational changes in processor design have primarily been focused on exploiting ILP, so that more work is done in each clock cycle. These changes include, in chronological order (Figure 18.1):\n\n\n  * ■\n    **Pipelining:**\n    Individual instructions are executed through a pipeline of stages so that while one instruction is executing in one stage of the pipeline, another instruction is executing in another stage of the pipeline.\n  * ■\n    **Superscalar:**\n    Multiple pipelines are constructed by replicating execution resources. This enables parallel execution of instructions in parallel pipelines, so long as hazards are avoided.\n\n\n\n\n![Diagram (a) Superscalar organization showing a single core with separate instruction and data caches, and a shared L2 cache.](images/image_0304.jpeg)\n\n\nDiagram (a) illustrates the Superscalar organization. It features a single core with the following components: Issue logic (containing Program counter and Single-thread register file), Instruction fetch unit, L1 instruction cache, Execution units and queues, L1 data cache, and L2 cache.\n\n\nDiagram (a) Superscalar organization showing a single core with separate instruction and data caches, and a shared L2 cache.\n\n\n(a) Superscalar\n\n\n\n\n![Diagram (b) Simultaneous multithreading organization showing multiple threads sharing a single core's resources.](images/image_0305.jpeg)\n\n\nDiagram (b) illustrates the Simultaneous multithreading (SMT) organization. It features a single core with multiple threads (PC 1 to PC n and Register 1 to Register n) sharing the following components: Issue logic, Instruction fetch unit, L1 instruction cache, Execution units and queues, L1 data cache, and L2 cache.\n\n\nDiagram (b) Simultaneous multithreading organization showing multiple threads sharing a single core's resources.\n\n\n(b) Simultaneous multithreading\n\n\n\n\n![Diagram (c) Multicore organization showing multiple cores sharing a single L2 cache.](images/image_0306.jpeg)\n\n\nDiagram (c) illustrates the Multicore organization. It features multiple cores (Core 1 to Core n) sharing a single L2 cache. Each core contains an Issue logic block (labeled as superscalar or SMT) and two L1 caches (L1-I and L1-D).\n\n\nDiagram (c) Multicore organization showing multiple cores sharing a single L2 cache.\n\n\n(c) Multicore\n\n\n**Figure 18.1**\n  * ■\n    **Simultaneous multithreading (SMT):**\n    Register banks are expanded so that multiple threads can share the use of pipeline resources.\n\n\nWith each of these innovations, designers have over the years attempted to increase the performance of the system by adding complexity. In the case of pipelining, simple three-stage pipelines were replaced by pipelines with five stages. Intel's Pentium 4 \"Prescott\" core had 31 stages for some instructions.\n\n\nThere is a practical limit to how far this trend can be taken, because with more stages, there is the need for more logic, more interconnections, and more control signals.\n\n\nWith superscalar organization, increased performance can be achieved by increasing the number of parallel pipelines. Again, there are diminishing returns\n\n\nas the number of pipelines increases. More logic is required to manage hazards and to stage instruction resources. Eventually, a single thread of execution reaches the point where hazards and resource dependencies prevent the full use of the multiple pipelines available. Also, compiled binary code rarely exposes enough ILP to take advantage of more than about six parallel pipelines.\n\n\nThis same point of diminishing returns is reached with SMT, as the complexity of managing multiple threads over a set of pipelines limits the number of threads and number of pipelines that can be effectively utilized. SMT's advantage lies in the fact that two (or more) program streams can be searched for available ILP.\n\n\nThere is a related set of problems dealing with the design and fabrication of the computer chip. The increase in complexity to deal with all of the logical issues related to very long pipelines, multiple superscalar pipelines, and multiple SMT register banks means that increasing amounts of the chip area are occupied with coordinating and signal transfer logic. This increases the difficulty of designing, fabricating, and debugging the chips. The increasingly difficult engineering challenge related to processor logic is one of the reasons that an increasing fraction of the processor chip is devoted to the simpler memory logic. Power issues, discussed next, provide another reason.\n\n\n\n\n**Power Consumption**\n\n\nTo maintain the trend of higher performance as the number of transistors per chip rises, designers have resorted to more elaborate processor designs (pipelining, superscalar, SMT) and to high clock frequencies. Unfortunately, power requirements have grown exponentially as chip density and clock frequency have risen. This was shown in Figure 2.2.\n\n\nOne way to control power density is to use more of the chip area for cache memory. Memory transistors are smaller and have a power density an order of magnitude lower than that of logic (see Figure 18.2). As chip transistor density has increased, the percentage of chip area devoted to memory has grown, and is now often half the chip area. Even so, there is still a considerable amount of chip area devoted to processing logic.\n\n\n\n\n![Figure 18.2: Power and Memory Considerations. A line graph showing Power density (watts/cm²) on a logarithmic y-axis (1 to 100) versus Feature size (μm) on the x-axis (0.25 to 0.10). The 'Logic' curve shows a steady increase from approximately 25 watts/cm² at 0.25 μm to 100 watts/cm² at 0.10 μm. The 'Memory' curve shows a more gradual increase from approximately 2 watts/cm² at 0.25 μm to 15 watts/cm² at 0.10 μm.](images/image_0307.jpeg)\n\n\nFeature size (\n       \n        \\mu\\text{m}\n       \n       ) | Logic Power density (watts/cm\n       \n        2\n       \n       ) | Memory Power density (watts/cm\n       \n        2\n       \n       )\n0.25 | ~25 | ~2\n0.18 | ~40 | ~3\n0.13 | ~60 | ~8\n0.10 | 100 | ~15\n\n\nFigure 18.2: Power and Memory Considerations. A line graph showing Power density (watts/cm²) on a logarithmic y-axis (1 to 100) versus Feature size (μm) on the x-axis (0.25 to 0.10). The 'Logic' curve shows a steady increase from approximately 25 watts/cm² at 0.25 μm to 100 watts/cm² at 0.10 μm. The 'Memory' curve shows a more gradual increase from approximately 2 watts/cm² at 0.25 μm to 15 watts/cm² at 0.10 μm.\n\n\n**Figure 18.2**\n   Power and Memory Considerations\n\n\nHow to use all those logic transistors is a key design issue. As discussed earlier in this section, there are limits to the effective use of such techniques as superscalar and SMT. In general terms, the experience of recent decades has been encapsulated in a rule of thumb known as\n   **Pollack's rule**\n   [POLL99], which states that performance increase is roughly proportional to square root of increase in complexity. In other words, if you double the logic in a processor core, then it delivers only 40% more performance. In principle, the use of multiple cores has the potential to provide near-linear performance improvement with the increase in the number of cores—but only for software that can take advantage.\n\n\nPower considerations provide another motive for moving toward a multicore organization. Because the chip has such a huge amount of cache memory, it becomes unlikely that any one thread of execution can effectively use all that memory. Even with SMT, multithreading is done in a relatively limited fashion and cannot therefore fully exploit a gigantic cache, whereas a number of relatively independent threads or processes has a greater opportunity to take full advantage of the cache memory."
        },
        {
          "name": "Software Performance Issues",
          "content": "A detailed examination of the software performance issues related to multicore organization is beyond our scope. In this section, we first provide an overview of these issues, and then look at an example of an application designed to exploit multicore capabilities.\n\n\n\n\n**Software on Multicore**\n\n\nThe potential performance benefits of a multicore organization depend on the ability to effectively exploit the parallel resources available to the application. Let us focus first on a single application running on a multicore system. Recall from Chapter 2 that Amdahl's law states that:\n\n\n\\begin{aligned}\\text{Speed up} &= \\frac{\\text{time to execute program on a single processor}}{\\text{time to execute program on } N \\text{ parallel processors}} \\\\ &= \\frac{1}{(1 - f) + \\frac{f}{N}}\\end{aligned}\\tag{18.1}\n\n\nThe law assumes a program in which a fraction\n   \n    (1 - f)\n   \n   of the execution time involves code that is inherently sequential and a fraction\n   \n    f\n   \n   that involves code that is infinitely parallelizable with no scheduling overhead.\n\n\nThis law appears to make the prospect of a multicore organization attractive. But as Figure 18.3a shows, even a small amount of serial code has a noticeable impact. If only 10% of the code is inherently serial (\n   \n    f = 0.9\n   \n   ), running the program on a multicore system with eight processors yields a performance gain of only a factor of 4.7. In addition, software typically incurs overhead as a result of communication and distribution of work among multiple processors and as a result of cache\n\n\n\n\n![Figure 18.3(a): Speedup with 0%, 2%, 5%, and 10% sequential portions. The graph shows relative speedup increasing linearly with the number of processors for 0% sequential portion, and then leveling off as the sequential portion increases.](images/image_0308.jpeg)\n\n\nFigure 18.3(a) is a line graph showing the relationship between the number of processors (x-axis, 1 to 8) and relative speedup (y-axis, 0 to 8). Four curves are plotted, representing different percentages of sequential portions: 0%, 2%, 5%, and 10%. The 0% curve shows the highest speedup, reaching approximately 7.5 at 8 processors. The 10% curve shows the lowest speedup, reaching approximately 4.5 at 8 processors.\n\n\n\nNumber of processors | 0% sequential portion | 2% sequential portion | 5% sequential portion | 10% sequential portion\n1 | 1.0 | 1.0 | 1.0 | 1.0\n2 | 2.0 | 1.9 | 1.8 | 1.7\n3 | 3.0 | 2.8 | 2.6 | 2.4\n4 | 4.0 | 3.7 | 3.4 | 3.1\n5 | 5.0 | 4.6 | 4.2 | 3.8\n6 | 6.0 | 5.5 | 5.0 | 4.5\n7 | 7.0 | 6.4 | 5.8 | 5.2\n8 | 8.0 | 7.3 | 6.6 | 5.9\n\n\n(a) Speedup with 0%, 2%, 5%, and 10% sequential portions\n\n\nFigure 18.3(a): Speedup with 0%, 2%, 5%, and 10% sequential portions. The graph shows relative speedup increasing linearly with the number of processors for 0% sequential portion, and then leveling off as the sequential portion increases.\n\n\n\n\n![Figure 18.3(b): Speedup with overheads. The graph shows relative speedup increasing with the number of processors, peaking around 4-5 processors, and then slightly decreasing due to overheads.](images/image_0309.jpeg)\n\n\nFigure 18.3(b) is a line graph showing the relationship between the number of processors (x-axis, 1 to 8) and relative speedup (y-axis, 0 to 2.5). Five curves are plotted, representing different overhead percentages: 5%, 10%, 15%, and 20%. All curves start at a relative speedup of 1.0 for 1 processor. The 5% curve reaches the highest peak of approximately 2.25 at 5 processors. The 20% curve reaches the lowest peak of approximately 1.7 at 5 processors.\n\n\n\nNumber of processors | 5% overhead | 10% overhead | 15% overhead | 20% overhead\n1 | 1.0 | 1.0 | 1.0 | 1.0\n2 | 1.6 | 1.5 | 1.4 | 1.3\n3 | 1.9 | 1.8 | 1.7 | 1.6\n4 | 2.1 | 2.0 | 1.9 | 1.8\n5 | 2.25 | 2.1 | 2.0 | 1.9\n6 | 2.1 | 2.0 | 1.9 | 1.8\n7 | 2.0 | 1.9 | 1.8 | 1.7\n8 | 1.9 | 1.8 | 1.7 | 1.6\n\n\n(b) Speedup with overheads\n\n\nFigure 18.3(b): Speedup with overheads. The graph shows relative speedup increasing with the number of processors, peaking around 4-5 processors, and then slightly decreasing due to overheads.\n\n\n**Figure 18.3**\n   Performance Effect of Multiple Cores\n\n\ncoherence overhead. This overhead results in a curve where performance peaks and then begins to degrade because of the increased burden of the overhead of using multiple processors (e.g., coordination and OS management). Figure 18.3b, from [MCD005], is a representative example.\n\n\nHowever, software engineers have been addressing this problem and there are numerous applications in which it is possible to effectively exploit a multicore system. [MCD005] analyzes the effectiveness of multicore systems on a set of database applications, in which great attention was paid to reducing the serial fraction within hardware architectures, operating systems, middleware, and the database\n\n\napplication software. Figure 18.4 shows the result. As this example shows, database management systems and database applications are one area in which multicore systems can be used effectively. Many kinds of servers can also effectively use the parallel multicore organization, because servers typically handle numerous relatively independent transactions in parallel.\n\n\nIn addition to general-purpose server software, a number of classes of applications benefit directly from the ability to scale throughput with the number of cores. [MCD006] lists the following examples:\n\n\n  * ■\n    **Multithreaded native applications (thread-level parallelism):**\n    Multithreaded applications are characterized by having a small number of highly threaded processes.\n  * ■\n    **Multiprocess applications (process-level parallelism):**\n    Multiprocess applications are characterized by the presence of many single-threaded processes.\n  * ■\n    **Java applications:**\n    Java applications embrace threading in a fundamental way. Not only does the Java language greatly facilitate multithreaded applications, but the Java Virtual Machine is a multithreaded process that provides scheduling and memory management for Java applications.\n  * ■\n    **Multi-instance applications (application-level parallelism):**\n    Even if an individual application does not scale to take advantage of a large number of threads, it is still possible to gain from multicore architecture by running multiple instances of the application in parallel. If multiple application instances require some degree of isolation, virtualization technology (for the hardware of the operating system) can be used to provide each of them with its own separate and secure domain.\n\n\n\n\n![Figure 18.4: Scaling of Database Workloads on Multiple-Processor Hardware. The graph plots Scaling (Y-axis, 0 to 64) against the Number of CPUs (X-axis, 0 to 64). Four data series are shown: Oracle DSS 4-way join, TMC data mining, DB2 DSS scan & aggs, and Oracle ad hoc insurance OLTP. A dashed line represents 'Perfect scaling'. The workloads show increasing scaling efficiency as the number of CPUs increases, with Oracle DSS 4-way join demonstrating the highest scaling performance.](images/image_0310.jpeg)\n\n\nNumber of CPUs | Oracle DSS 4-way join | TMC data mining | DB2 DSS scan & aggs | Oracle ad hoc insurance OLTP\n0 | 0 | 0 | 0 | 0\n16 | 12 | 10 | 8 | 6\n32 | 24 | 20 | 16 | 12\n48 | 36 | 32 | 28 | 24\n64 | 48 | 44 | 40 | 36\n\n\nFigure 18.4: Scaling of Database Workloads on Multiple-Processor Hardware. The graph plots Scaling (Y-axis, 0 to 64) against the Number of CPUs (X-axis, 0 to 64). Four data series are shown: Oracle DSS 4-way join, TMC data mining, DB2 DSS scan & aggs, and Oracle ad hoc insurance OLTP. A dashed line represents 'Perfect scaling'. The workloads show increasing scaling efficiency as the number of CPUs increases, with Oracle DSS 4-way join demonstrating the highest scaling performance.\n\n\n**Figure 18.4**\n   Scaling of Database Workloads on Multiple-Processor Hardware\n\n\nBefore turning to an example, we elaborate on the topic of thread-level parallelism by introducing the concept of\n   **threading granularity**\n   , which can be defined as the minimal unit of work that can be beneficially parallelized. In general, the finer the granularity the system enables, the less constrained is the programmer in parallelizing a program. Consequently, finer grain threading systems allow parallelization in more situations than coarse-grained ones. The choice of the target granularity of an architecture involves an inherent tradeoff. On the one hand, the finer grain systems are preferable because of the flexibility they afford to the programmer. On the other hand, the finer the threading granularity, the more significant part of the execution is taken by the threading system overhead.\n\n\n\n\n**Application Example: Valve Game Software**\n\n\nValve is an entertainment and technology company that has developed a number of popular games as well as the Source engine, one of the most widely played game engines available. Source is an animation engine used by Valve for its games and licensed to other game developers.\n\n\nValve has reprogrammed the Source engine software to use multithreading to exploit the scalability of multicore processor chips from Intel and AMD [REIM06]. The revised Source engine code provides more powerful support for Valve games such as Half Life 2.\n\n\nFrom Valve's perspective, threading granularity options are defined as follows [HARR06]:\n\n\n  * ■\n    **Coarse-grained threading:**\n    Individual modules, called systems, are assigned to individual processors. In the Source engine case, this means putting rendering on one processor, AI (artificial intelligence) on another, physics on another, and so on. This is straightforward. In essence, each major module is single threaded and the principal coordination involves synchronizing all the threads with a timeline thread.\n  * ■\n    **Fine-grained threading:**\n    Many similar or identical tasks are spread across multiple processors. For example, a loop that iterates over an array of data can be split up into a number of smaller parallel loops in individual threads that can be scheduled in parallel.\n  * ■\n    **Hybrid threading:**\n    This involves the selective use of fine-grain threading for some systems and single threading for other systems.\n\n\nValve found that through coarse threading, it could achieve up to twice the performance across two processors compared to executing on a single processor. But this performance gain could only be achieved with contrived cases. For real-world gameplay, the improvement was on the order of a factor of 1.2. Valve also found that effective use of fine-grain threading was difficult. The time per work unit can be variable, and managing the timeline of outcomes and consequences involved complex programming.\n\n\nValve found that a hybrid threading approach was the most promising and would scale the best as multicore systems with eight or sixteen processors became available. Valve identified systems that operate very effectively when assigned to a single processor permanently. An example is sound mixing, which has little user interaction, is not constrained by the frame configuration of windows, and works on\n\n\nits own set of data. Other modules, such as scene rendering, can be organized into a number of threads so that the module can execute on a single processor but achieve greater performance as it is spread out over more and more processors.\n\n\nFigure 18.5 illustrates the thread structure for the rendering module. In this hierarchical structure, higher-level threads spawn lower-level threads as needed. The rendering module relies on a critical part of the Source engine, the world list, which is a database representation of the visual elements in the game's world. The first task is to determine what are the areas of the world that need to be rendered. The next task is to determine what objects are in the scene as viewed from multiple angles. Then comes the processor-intensive work. The rendering module has to work out the rendering of each object from multiple points of view, such as the player's view, the view of TV monitors, and the point of view of reflections in water.\n\n\nSome of the key elements of the threading strategy for the rendering module are listed in [LEON07] and include the following:\n\n\n  * ■ Construct scene-rendering lists for multiple scenes in parallel (e.g., the world and its reflection in water).\n  * ■ Overlap graphics simulation.\n  * ■ Compute character bone transformations for all characters in all scenes in parallel.\n  * ■ Allow multiple threads to draw in parallel.\n\n\n\n\n![A hierarchical diagram showing the thread structure for a rendering module. The root node is 'Render', which branches into 'Skybox', 'Main view', 'Monitor', and 'Etc.'. 'Main view' further branches into 'Scene list', which then branches into 'For each object'. 'For each object' branches into 'Particles', 'Character', and 'Etc.'. 'Particles' branches into 'Sim and draw', and 'Character' branches into 'Bone setup' and 'Draw'.](images/image_0311.jpeg)\n\n\ngraph TD\n    Render[Render] --> Skybox[**Skybox**]\n    Render --> MainView[**Main view**]\n    Render --> Monitor[**Monitor**]\n    Render --> Etc1[**Etc.**]\n    MainView --> SceneList[**Scene list**]\n    SceneList --> ForEachObject[**For each object**]\n    ForEachObject --> Particles[**Particles**]\n    ForEachObject --> Character[**Character**]\n    ForEachObject --> Etc2[**Etc.**]\n    Particles --> SimAndDraw[**Sim and draw**]\n    Character --> BoneSetup[**Bone setup**]\n    Character --> Draw[**Draw**]\n  \nA hierarchical diagram showing the thread structure for a rendering module. The root node is 'Render', which branches into 'Skybox', 'Main view', 'Monitor', and 'Etc.'. 'Main view' further branches into 'Scene list', which then branches into 'For each object'. 'For each object' branches into 'Particles', 'Character', and 'Etc.'. 'Particles' branches into 'Sim and draw', and 'Character' branches into 'Bone setup' and 'Draw'.\n\n\n**Figure 18.5**\n   Hybrid Threading for Rendering Module\n\n\nThe designers found that simply locking key databases, such as the world list, for a thread was too inefficient. Over 95% of the time, a thread is trying to read from a data set, and only 5% of the time at most is spent in writing to a data set. Thus, a concurrency mechanism known as the single-writer-multiple-readers model works effectively."
        },
        {
          "name": "Multicore Organization",
          "content": "At a top level of description, the main variables in a multicore organization are as follows:\n\n\n  * ■ The number of core processors on the chip\n  * ■ The number of levels of cache memory\n  * ■ How cache memory is shared among cores\n  * ■ Whether simultaneous multithreading (SMT) is employed\n  * ■ The types of cores\n\n\nWe explore all but the last of these considerations in this section, deferring a discussion of types of cores to the next section.\n\n\n\n\n**Levels of Cache**\n\n\nFigure 18.6 shows four general organizations for multicore systems. Figure 18.6a is an organization found in some of the earlier multicore computer chips and is still seen in some embedded chips. In this organization, the only on-chip cache is L1 cache, with each core having its own dedicated L1 cache. Almost invariably, the L1 cache is divided into instruction and data caches for performance reasons, while L2 and higher-level caches are unified. An example of this organization is the ARM11 MPCore.\n\n\nThe organization of Figure 18.6b is also one in which there is no on-chip cache sharing. In this, there is enough area available on the chip to allow for L2 cache. An example of this organization is the AMD Opteron. Figure 18.6c shows a similar allocation of chip space to memory, but with the use of a shared L2 cache. The Intel Core Duo has this organization. Finally, as the amount of cache memory available on the chip continues to grow, performance considerations dictate splitting off a separate, shared L3 cache (Figure 18.6d), with dedicated L1 and L2 caches for each core processor. The Intel Core i7 is an example of this organization.\n\n\nThe use of a shared higher-level cache on the chip has several advantages over exclusive reliance on dedicated caches:\n\n\n  * 1. Constructive interference can reduce overall miss rates. That is, if a thread on one core accesses a main memory location, this brings the line containing the referenced location into the shared cache. If a thread on another core soon thereafter accesses the same memory block, the memory locations will already be available in the shared on-chip cache.\n  * 2. A related advantage is that data shared by multiple cores is not replicated at the shared cache level.\n\n\n\n\n![Figure 18.6: Multicore Organization Alternatives. The figure shows four configurations: (a) Dedicated L1 cache, (b) Dedicated L2 cache, (c) Shared L2 cache, and (d) Shared L3 cache. Each configuration shows multiple CPU cores (CPU Core 1 to CPU Core n) with their respective L1 caches (L1-D and L1-I) and connections to main memory and I/O.](images/image_0312.jpeg)\n\n\nFigure 18.6 illustrates four multicore organization alternatives:\n\n\n  * (a) Dedicated L1 cache: Each core has its own L1-D and L1-I caches. These connect to a shared L2 cache, which then connects to main memory and I/O.\n  * (b) Dedicated L2 cache: Each core has its own L1-D and L1-I caches, and its own L2 cache. These connect to main memory and I/O.\n  * (c) Shared L2 cache: Each core has its own L1-D and L1-I caches, which connect to a shared L2 cache. This shared L2 cache then connects to main memory and I/O.\n  * (d) Shared L3 cache: Each core has its own L1-D and L1-I caches, and its own L2 cache. These connect to a shared L3 cache, which then connects to main memory and I/O.\n\n\nFigure 18.6: Multicore Organization Alternatives. The figure shows four configurations: (a) Dedicated L1 cache, (b) Dedicated L2 cache, (c) Shared L2 cache, and (d) Shared L3 cache. Each configuration shows multiple CPU cores (CPU Core 1 to CPU Core n) with their respective L1 caches (L1-D and L1-I) and connections to main memory and I/O.\n\n\n**Figure 18.6**\n   Multicore Organization Alternatives\n\n\n  * 3. With proper line replacement algorithms, the amount of shared cache allocated to each core is dynamic, so that threads that have less locality (larger working sets) can employ more cache.\n  * 4. Inter-core communication is easy to implement, via shared memory locations.\n  * 5. The use of a shared higher-level cache confines the cache coherency problem to the lower cache levels, which may provide some additional performance advantage.\n\n\nA potential advantage to having only dedicated L2 caches on the chip is that each core enjoys more rapid access to its private L2 cache. This is advantageous for threads that exhibit strong locality.\n\n\nAs both the amount of memory available and the number of cores grow, the use of a shared L3 cache combined with dedicated percore L2 caches seems likely to provide better performance than simply a massive shared L2 cache or very large dedicated L2 caches with no on-chip L3. An example of this latter arrangement is the Xeon E5-2600/4600 chip processor (Figure 7.1)\n\n\nNot shown is the arrangement where L1s are local to each core, L2s are shared among 2 to 4 cores, and L3 is global across all cores. This arrangement is likely to become more common over time.\n\n\n\n\n**Simultaneous Multithreading**\n\n\nAnother organizational design decision in a multicore system is whether the individual cores will implement\n   **simultaneous multithreading (SMT)**\n   . For example, the Intel Core Duo uses pure superscalar cores, whereas the Intel Core i7 uses SMT cores. SMT has the effect of scaling up the number of hardware-level threads that the multicore system supports. Thus, a multicore system with four cores and SMT that supports four simultaneous threads in each core appears the same to the application level as a multicore system with 16 cores. As software is developed to more fully exploit parallel resources, an SMT approach appears to be more attractive than a purely superscalar approach."
        },
        {
          "name": "Heterogeneous Multicore Organization",
          "content": "The quest to make optimal use of the silicon real estate on a processor chip is never ending. As clock speeds and logic densities increase, designers must balance many design elements in attempts to maximize performance and minimize power consumption. We have so far examined a number of such approaches, including the following:\n\n\n  * 1. Increase the percentage of the chip devoted to cache memory.\n  * 2. Increase the number of levels of cache memory.\n  * 3. Change the length (increase or decrease) and functional components of the instruction pipeline.\n  * 4. Employ simultaneous multithreading.\n  * 5. Use multiple cores.\n\n\nA typical case for the use of multiple cores is a chip with multiple identical cores, known as\n   **homogenous multicore organization**\n   . To achieve better results, in terms of performance and/or power consumption, an increasingly popular design choice is\n   **heterogeneous multicore organization**\n   , which refers to a processor chip that includes more than one kind of core. In this section, we look at two approaches to heterogeneous multicore organization.\n\n\n\n\n**Different Instruction Set Architectures**\n\n\nThe approach that has received the most industry attention is the use of cores that have distinct ISAs. Typically, this involves mixing conventional cores, referred to in this context as CPUs, with specialized cores optimized for certain types of data or applications. Most often, the additional cores are optimized to deal with vector and matrix data processing.\n\n\n**CPU/GPU MULTICORE**\n   The most prominent trend in terms of heterogeneous multicore design is the use of both CPUs and graphics processing units (GPUs) on the same chip. GPUs are discussed in detail in the following chapter. Briefly, GPUs are characterized by the ability to support thousands of parallel execution threads. Thus, GPUs are well matched to applications that process large amounts\n\n\nof vector and matrix data. Initially aimed at improving the performance of graphics applications, thanks to easy-to-adopt programming models such as CUDA (Compute Unified Device Architecture), these new processors are increasingly being applied to improve the performance of general-purpose and scientific applications that involve large numbers of repetitive operations on structured data.\n\n\nTo deal with the diversity of target applications in today's computing environment, multicore containing both GPUs and CPUs has the potential to enhance performance. This heterogeneous mix, however, presents issues of coordination and correctness.\n\n\nFigure 18.7 is a typical multicore processor organization. Multiple CPUs and GPUs share on-chip resources, such as the last-level cache (LLC), interconnection network, and memory controllers. Most critical is the way in which cache management policies provide effective sharing of the LLC. The differences in cache sensitivity and memory access rate between CPUs and GPUs create significant challenges to the efficient sharing of the LLC.\n\n\nTable 18.1 illustrates the potential performance benefit of combining CPUs and GPUs for scientific applications. This table shows the basic operating parameters of an AMD chip, the A10 5800K [ALTS12]. For floating-point calculations, the CPU's performance at 121.6 GFLOPS is dwarfed by the GPU, which offers 614 GFLOPS to applications that can utilize the resource effectively.\n\n\nWhether it is scientific applications or traditional graphics processing, the key to leveraging the added GPU processors is to consider the time needed to transfer a block of data to the GPU, process it, then return the results to the main application thread. In earlier implementations of chips that incorporated GPUs, physical memory is partitioned between CPU and GPU. If an application thread is running on a CPU that demands GPU processing, the CPU explicitly copies the data to the GPU memory. The GPU completes the computation and then copies the result back to CPU memory. Issues of cache coherence across CPU and GPU memory caches do not arise because the memory is partitioned. On the other hand, the physical handling of data back and forth results in a performance penalty.\n\n\nA number of research and development efforts are underway to improve performance over that described in the preceding paragraph, of which the most notable\n\n\n\n\n![Diagram of a heterogeneous multicore chip organization. The chip contains multiple CPU and GPU cores. Each core has its own local cache. All cores are connected to a central on-chip interconnection network. The network is also connected to DRAM controllers and last-level caches. The diagram shows a repeating pattern of CPU and GPU units, each with a local cache, connected to the interconnection network, which is further connected to DRAM controllers and last-level caches.](images/image_0313.jpeg)\n\n\nThe diagram illustrates a heterogeneous multicore chip organization. At the top, there are two rows of processing units. The first row contains CPU units, each with a local 'Cache' below it. The second row contains GPU units, also each with a local 'Cache' below it. Both rows are connected to a central 'On-chip interconnection network' via double-headed arrows. Below this network, there are two rows of support components. The first row contains 'DRAM controller' units, and the second row contains 'Last-level cache' units. Double-headed arrows connect the interconnection network to each DRAM controller and each last-level cache. Ellipses (dots) are used to indicate that there are multiple units of each type (CPU, GPU, DRAM controller, and last-level cache) in the chip.\n\n\nDiagram of a heterogeneous multicore chip organization. The chip contains multiple CPU and GPU cores. Each core has its own local cache. All cores are connected to a central on-chip interconnection network. The network is also connected to DRAM controllers and last-level caches. The diagram shows a repeating pattern of CPU and GPU units, each with a local cache, connected to the interconnection network, which is further connected to DRAM controllers and last-level caches.\n\n\n**Figure 18.7**\n   Heterogenous Multicore Chip Elements\n\n\n**Table 18.1**\n\n | CPU | GPU\nClock frequency (GHz) | 3.8 | 0.8\nCores | 4 | 384\nFLOPS/core | 8 | 2\nGFLOPS | 121.6 | 614.4\n\n\nFLOPS = floating-point operations per second.\n\n\nFLOPS/core = number of parallel floating-point operations that can be performed.\n\n\nis the initiative by the Heterogeneous System Architecture (HSA) Foundation. Key features of the HSA approach include the following:\n\n\n  * 1. The entire virtual memory space is visible to both CPU and GPU. Both CPU and GPU can access and allocate any location in the system's virtual memory space.\n  * 2. The virtual memory system brings in pages to physical main memory as needed.\n  * 3. A coherent memory policy ensures that CPU and GPU caches both see an up-to-date view of data.\n  * 4. A unified programming interface that enables users to exploit the parallel capabilities of the GPUs within programs that rely on CPU execution as well.\n\n\nThe overall objective is to allow programmers to write applications that exploit the serial power of CPUs and the parallel-processing power of GPUs seamlessly with efficient coordination at the OS and hardware level. As mentioned, this is an ongoing area of research and development.\n\n\n**CPU/DSP MULTICORE**\n   Another common example of a heterogeneous multicore chip is a mixture of CPUs and digital signal processors (DSPs). A DSP provides ultra-fast instruction sequences (shift and add; multiply and add), which are commonly used in math-intensive digital signal processing applications. DSPs are used to process analog data from sources such as sound, weather satellites, and earthquake monitors. Signals are converted into digital data and analyzed using various algorithms such as Fast Fourier Transform. DSP cores are widely used in myriad devices, including cellphones, sound cards, fax machines, modems, hard disks, and digital TVs.\n\n\nAs a good representative example, Figure 18.8 shows a recent version of Texas Instruments (TI) K2H SoC platform [TI12]. This heterogeneous multicore processor delivers power-efficient processing solutions for high-end imaging applications. TI lists the performance as delivering up to 352 GMACS, 198 GFLOPS, and 19,600 MIPS. GMACS stands for giga (billions of) multiply-accumulate operations per second, a common measure of DSP performance. Target applications for these systems include industrial automation, video surveillance, high-end inspection systems, industrial printers/scanners, and currency/counterfeit detection.\n\n\n\n\n![Block diagram of the Texas Instruments 66AK2H12 Heterogenous Multicore Chip. The chip is divided into two main sections: the Memory subsystem and the TeraNet fabric. The Memory subsystem includes 72-bit DDR3 EMIF, 6-MB MSM SRAM, C66x DSP cores (8x), ARM Cortex-A15 cores (4x), and various control blocks like Debug & trace, Boot ROM, Semaphore, Power management, PLL, and EDMA (5x). The TeraNet fabric connects these to external interfaces: EMIF16, GPIO x32, 3x I2C, USB 3.0, 2x UART, 3x SPI, PCIe x2, SRIO x4, and a 5-port Ethernet switch. The Ethernet switch connects to four 1GBE ports and a Network coprocessor, which includes a Queue manager, Packet DMA, Security accelerator, and Packet accelerator. A 2x HyperLink connection is shown on the left.](images/image_0314.jpeg)\n\n\n**Memory subsystem**\n\n\n  * 72-bit DDR3 EMIF\n  * 72-bit DDR3 EMIF\n  * Debug & trace\n  * Boot ROM\n  * Semaphore\n  * Power management\n  * PLL\n  * EDMA (5x)\n  * 6-MB MSM SRAM (MSMC)\n  * C66x DSP (8x)\n       * 32-kB L1 P-cache\n  * 32-kB L1 D-cache\n  * ARM Cortex-A15 (4x)\n       * 32-kB L1 P-cache\n  * 32-kB L1 D-cache\n  * 4MB L2 cache\n\n\n**TeraNet**\n\n\n**External Interfaces**\n\n\n  * 2x HyperLink\n  * EMIF16\n  * GPIO x32\n  * 3x I2C\n  * USB 3.0\n  * 2x UART\n  * 3x SPI\n  * PCIe x2\n  * SRIO x4\n  * 5-port Ethernet switch\n       * 1GBE (4x)\n  * Network coprocessor\n         * Queue manager\n  * Packet DMA\n  * Security accelerator\n  * Packet accelerator\n\n\nBlock diagram of the Texas Instruments 66AK2H12 Heterogenous Multicore Chip. The chip is divided into two main sections: the Memory subsystem and the TeraNet fabric. The Memory subsystem includes 72-bit DDR3 EMIF, 6-MB MSM SRAM, C66x DSP cores (8x), ARM Cortex-A15 cores (4x), and various control blocks like Debug & trace, Boot ROM, Semaphore, Power management, PLL, and EDMA (5x). The TeraNet fabric connects these to external interfaces: EMIF16, GPIO x32, 3x I2C, USB 3.0, 2x UART, 3x SPI, PCIe x2, SRIO x4, and a 5-port Ethernet switch. The Ethernet switch connects to four 1GBE ports and a Network coprocessor, which includes a Queue manager, Packet DMA, Security accelerator, and Packet accelerator. A 2x HyperLink connection is shown on the left.\n\n\n**Figure 18.8**\n   Texas Instruments 66AK2H12 Heterogenous Multicore Chip\n\n\nThe TI chip includes four ARM Cortex-A15 cores and eight TI C66x DSP cores.\n\n\nEach DSP core contains 32 kB of L1 data cache and 32 kB of L1 program (instruction) cache. In addition, each DSP has 1 MB of dedicated SRAM memory that can be configured as all L2 cache, all main memory, or a mix of the two. The portion configured as main memory functions as a “local” main memory, referred to simply as\n   *SRAM*\n   . This local main memory can be used for temporary data, avoiding the need for traffic between cache and off-chip memory. The L2 cache of each of\n\n\nthe eight DSP cores is dedicated rather than shared with the other DSP cores. This is typical for a multicore DSP organization: Each DSP works on a separate block of data in parallel, so there is little need for data sharing.\n\n\nEach ARM Cortex-A15 CPU core has 32-kB L1 data and program caches, and the four cores share a 4-MB L2 cache.\n\n\nThe 6-MB multicore shared memory (MSM) is always configured as all SRAM. That is, it behaves like main memory rather than cache. It can be configured to feed directly the L1 DSP and CPU caches, or to feed the L2 DSP and CPU caches. This configuration decision depends on the expected application profile. The multicore shared memory controller (MSMC) manages traffic among ARM cores, DSP, DMA, other mastering peripherals, and the external memory interface (EMIF). MSMC controls access to the MSM, which is accessible by all the cores and the mastering peripherals on the device.\n\n\n\n\n**Equivalent Instruction Set Architectures**\n\n\nAnother recent approach to heterogeneous multicore organization is the use of multiple cores that have equivalent ISAs but vary in performance or power efficiency. The leading example of this is ARM's big.Little architecture, which we examine in this section.\n\n\nFigure 18.9 illustrates this architecture. The figure shows a multicore processor chip containing two high-performance Cortex-A15 cores and two lower-performance, lower-power-consuming Cortex-A7 cores. The A7 cores handle less computation-intensive tasks, such as background processing, playing music, sending texts, and making phone calls. The A15 cores are invoked for high intensity tasks, such as for video, gaming, and navigation.\n\n\nThe big.Little architecture is aimed at the smartphone and tablet market. These are devices whose performance demands from users are increasing at a much faster rate than the capacity of batteries or the power savings from semiconductor process advances. The usage pattern for smartphones and tablets is quite dynamic. Periods of processing-intensive tasks, such as gaming and web browsing, alternate\n\n\n\n\n![Diagram of the big.Little Chip Components architecture. The diagram shows a central CCI-400 (cache coherent interconnect) bar at the bottom. Above it are two groups of cores: a left group with two Cortex-A15 cores and a right group with two Cortex-A7 cores. Each group has its own L2 cache below the cores. Above the L2 caches are GIC-400 global interrupt controllers, which send interrupts to the cores. To the right of the Cortex-A7 group is an I/O coherent master block. The CCI-400 connects to 'Memory controller ports' on the left and a 'System port' on the right.](images/image_0315.jpeg)\n\n\ngraph TD\n    GIC400[GIC-400 global interrupt controller]\n    subgraph CoreGroup1 [Left Core Group]\n        direction TB\n        A15L1[Cortex-A15 core]\n        A15R1[Cortex-A15 core]\n        A15L2[L2]\n    end\n    subgraph CoreGroup2 [Right Core Group]\n        direction TB\n        A7L1[Cortex-A7 core]\n        A7R1[Cortex-A7 core]\n        A7L2[L2]\n    end\n    IOMaster[I/O coherent master]\n    CCI400[CCI-400 cache coherent interconnect]\n    GIC400 <-->|Interrupts| A15L1\n    GIC400 <-->|Interrupts| A15R1\n    GIC400 <-->|Interrupts| A7L1\n    GIC400 <-->|Interrupts| A7R1\n    A15L1 <--> A15L2\n    A15R1 <--> A15L2\n    A7L1 <--> A7L2\n    A7R1 <--> A7L2\n    A15L2 <--> CCI400\n    A7L2 <--> CCI400\n    IOMaster <--> CCI400\n    CCI400 <-->|Memory controller ports| MemPorts[Memory controller ports]\n    CCI400 <-->|System port| SysPort[System port]\n  \nDiagram of the big.Little Chip Components architecture. The diagram shows a central CCI-400 (cache coherent interconnect) bar at the bottom. Above it are two groups of cores: a left group with two Cortex-A15 cores and a right group with two Cortex-A7 cores. Each group has its own L2 cache below the cores. Above the L2 caches are GIC-400 global interrupt controllers, which send interrupts to the cores. To the right of the Cortex-A7 group is an I/O coherent master block. The CCI-400 connects to 'Memory controller ports' on the left and a 'System port' on the right.\n\n\n**Figure 18.9**\n   big.Little Chip Components\n\n\nwith typically longer periods of low processing-intensity tasks, such as texting, e-mail, and audio. The big.Little architecture takes advantage of this variation in required performance. The A15 is designed for maximum performance within the mobile power budget. The A7 processor is designed for maximum efficiency and high enough performance to address all but the most intense periods of work.\n\n\n**A7 AND A15 CHARACTERISTICS**\n   The A7 is far simpler and less powerful than the A15. But its simplicity requires far fewer transistors than does the A15's complexity—and fewer transistors require less energy to operate. The differences between the A7 and A15 cores are seen most clearly by examining their instruction pipelines, as shown in Figure 18.10.\n\n\n\n\n![Figure 18.10: Cortex A-7 and A-15 Pipelines. (a) Cortex A-7 Pipeline: A simple 3-stage pipeline with Fetch, Decode, and Issue stages, followed by six parallel execution units (Integer, Multiply, Floating-point/NEON, Dual issue, Load/Store) and a Write back stage. (b) Cortex A-15 Pipeline: A complex 4-stage pipeline with Fetch, Decode, Rename, & Dispatch, a Loop cache, and a multi-stage execution engine with multiple queues and issue ports feeding into various functional units (Integer, Multiply, Floating-point/NEON, Branch, Load, Store) before Write back.](images/image_0316.jpeg)\n\n\n(a) Cortex A-7 Pipeline\n\n\n(b) Cortex A-15 Pipeline\n\n\nFigure 18.10: Cortex A-7 and A-15 Pipelines. (a) Cortex A-7 Pipeline: A simple 3-stage pipeline with Fetch, Decode, and Issue stages, followed by six parallel execution units (Integer, Multiply, Floating-point/NEON, Dual issue, Load/Store) and a Write back stage. (b) Cortex A-15 Pipeline: A complex 4-stage pipeline with Fetch, Decode, Rename, & Dispatch, a Loop cache, and a multi-stage execution engine with multiple queues and issue ports feeding into various functional units (Integer, Multiply, Floating-point/NEON, Branch, Load, Store) before Write back.\n\n\n**Figure 18.10**\n   Cortex A-7 and A-15 Pipelines\n\n\nThe A7 is an in-order CPU with a pipeline length of 8 to 10 stages. It has a single queue for all of its execution units, and two instructions can be sent to its five execution units per clock cycle. The A15, on the other hand, is an out-of-order processor with a pipeline length of 15 to 24 stages. Each of its eight execution units has its own multistage queue, and three instructions can be processed per clock cycle.\n\n\nThe energy consumed by the execution of an instruction is partially related to the number of pipeline stages it must traverse. Therefore, a significant difference in energy consumption between Cortex-A15 and Cortex-A7 comes from the different pipeline complexity. Across a range of benchmarks, the Cortex-A15 delivers roughly twice the performance of the Cortex-A7 per unit MHz, and the Cortex-A7 is roughly three times as energy efficient as the Cortex-A15 in completing the same workloads [JEFF12]. The performance tradeoff is illustrated in Figure 18.11 [STEV13].\n\n\n**SOFTWARE PROCESSING MODELS**\n   The big.Little architecture can be configured to use one of two software processing models: migration and multiprocessing (MP). The software models differ mainly in the way they allocate work to big or Little cores during runtime execution of a workload.\n\n\nIn the migration model, big and Little cores are paired. To the OS kernel scheduler, each big/Little pair is visible as a single core. Power management software is responsible for migrating software contexts between the two cores. This model is a natural extension to the dynamic voltage and frequency scaling (DVFS) operating points provided by current mobile platforms to allow the OS to match the performance of the platform to the performance required by the application. In today's smartphone SoCs, DVFS drivers like\n   \n    cpu_freq\n   \n   sample the OS performance at regular and frequent intervals, and the DVFS governor decides whether to shift to a higher or lower operating point or remain at the current operating point. As shown in Figure 18.11, both the A7 and the A15 can execute at four distinct operating\n\n\n\n\n![Figure 18.11: Cortex-A7 and A15 Performance Comparison. A line graph showing Power vs. Performance. The Cortex-A15 (black line) has four operating points: 'Highest Cortex-A15 operating point' at the top right, 'Lowest Cortex-A15 operating point' at the bottom left, and two intermediate points. The Cortex-A7 (green line) has four operating points: 'Highest Cortex-A7 operating point' at the top right, 'Lowest Cortex-A7 operating point' at the bottom left, and two intermediate points. The A15 curve is significantly steeper and higher than the A7 curve, indicating higher performance per unit power.](images/image_0317.jpeg)\n\n\nOperating Point | Core | Performance (Relative) | Power (Relative)\nHighest Cortex-A15 operating point | A15 | 1.0 | 1.0\nLowest Cortex-A15 operating point | A15 | 0.2 | 0.1\nHighest Cortex-A7 operating point | A7 | 0.3 | 0.05\nLowest Cortex-A7 operating point | A7 | 0.1 | 0.01\n\n\nFigure 18.11: Cortex-A7 and A15 Performance Comparison. A line graph showing Power vs. Performance. The Cortex-A15 (black line) has four operating points: 'Highest Cortex-A15 operating point' at the top right, 'Lowest Cortex-A15 operating point' at the bottom left, and two intermediate points. The Cortex-A7 (green line) has four operating points: 'Highest Cortex-A7 operating point' at the top right, 'Lowest Cortex-A7 operating point' at the bottom left, and two intermediate points. The A15 curve is significantly steeper and higher than the A7 curve, indicating higher performance per unit power.\n\n\n**Figure 18.11**\n   Cortex-A7 and A15 Performance Comparison\n\n\npoints. The DVFS software can effectively dial in to one of the operating points on the curve, setting a specific CPU clock frequency and voltage level.\n\n\nThese operating points affect the voltage and frequency of a single CPU cluster; however, in a big.Little system there are two CPU clusters with independent voltage and frequency domains. This allows the big cluster to act as a logical extension of the DVFS operating points provided by the Little processor cluster. In a big.Little system under a migration mode of control, when Cortex-A7 is executing, the DVFS driver can tune the performance of the CPU cluster to higher levels. Once Cortex-A7 is at its highest operating point, if more performance is required, a task migration can be invoked that picks up the OS and applications and moves them to the Cortex-A15. In today's smartphone SoCs, DVFS drivers like\n   \n    cpu_freq\n   \n   sample the OS performance at regular and frequent intervals, and the DVFS governor decides whether to shift to a higher or lower operating point or remain at the current operating point.\n\n\nThe migration model is simple but requires that one of the CPUs in each pair is always idle. The MP model allows any mixture of A15 and A7 cores to be powered on and executing simultaneously. Whether a big processor needs to be powered on is determined by performance requirements of tasks currently executing. If there are demanding tasks, then a big processor can be powered on to execute them. Low demand tasks can execute on a Little processor. Finally, any processors that are not being used can be powered down. This ensures that cores, big or Little, are only active when they are needed, and that the appropriate core is used to execute any given workload.\n\n\nThe MP model is somewhat more complicated to implement but is also more efficient of resources. It assigns tasks appropriately and enables more cores to be running simultaneously when the demand warrants it.\n\n\n\n\n**Cache Coherence and the MOESI Model**\n\n\nTypically, a heterogeneous multicore processor will feature dedicated L2 cache assigned to the different processor types. We see that in the general depiction of a CPU/GPU scheme of Figure 18.7. Because the CPU and GPU are engaged in quite different tasks, it makes sense that each has its own L2 cache, shared among the similar CPUs. We also see this in the big.Little architecture (Figure 18.9), in which the A7 cores share an L2 cache and the A15 cores share a separate L2 cache.\n\n\nWhen multiple caches exist, there is a need for a cache-coherence scheme to avoid access to invalid data. Cache coherency may be addressed with software-based techniques. In the case where the cache contains stale data, the cached copy may be invalidated and reread from memory when needed again. When memory contains stale data due to a write-back cache containing dirty data, the cache may be cleaned by forcing write back to memory. Any other cached copies that may exist in other caches must be invalidated. This software burden consumes too many resources in a SoC chip, leading to the use of hardware cache-coherent implementations, especially in heterogeneous multicore processors.\n\n\nAs described in Chapter 17, there are two main approaches to hardware-implemented cache coherence: directory protocols and snoopy protocols. ARM has developed a hardware coherence capability called ACE (Advanced Extensible\n\n\nInterface Coherence Extensions) that can be configured to implement either directory or snoopy approach, or even a combination. ACE has been designed to support a wide range of coherent masters with differing capabilities. ACE supports coherency between dissimilar processors such as the Cortex-A15 and Cortex-A7 processors, enabling ARM big.Little technology. It supports I/O coherency for un-cached masters, supports masters with differing cache line sizes, differing internal cache state models, and masters with write-back or write-through caches. As another example, ACE is implemented in the memory subsystem memory controller (MSMC) in the TI SoC chip of Figure 18.8. MSMC supports hardware cache coherence between the ARM CorePac L1/L2 caches and EDMA/IO peripherals for shared SRAM and DDR spaces. This feature allows the sharing of MSMC SRAM and DDR data spaces by these masters on the chip, without having to use explicit software cache maintenance techniques.\n\n\nACE makes use of a five-state cache model. In each cache, each line is either Valid or Invalid. If a line is Valid, it can be in one of four states, defined by two dimensions. A line may contain data that are Shared or Unique. A Shared line contains data from a region of external (main) memory that is potentially sharable. A Unique line contains data from a region of memory that is dedicated to the core owning this cache. And the line is either Clean or Dirty, generally meaning either memory contains the latest, most up-to-date data and the cache line is merely a copy of memory, or if it's Dirty then the cache line is the latest, most up-to-date data and it must be written back to memory at some stage. The one exception to the above description is when multiple caches share a line and it's dirty. In this case, all caches must contain the latest data value at all times, but only one may be in the Shared/Dirty state, the others being held in the Shared/Clean state. The Shared/Dirty state is thus used to indicate which cache has responsibility for writing the data back to memory, and Shared/Clean is more accurately described as meaning data is shared but there is no need to write it back to memory.\n\n\nThe ACE states correspond to a cache coherency model with five states, known as MOESI (Figure 18.12). Table 18.2 compares the MOESI model with the MESI model described in Chapter 17.\n\n\n\n\n![Figure 18.12: ARM ACE Cache Line States. A 2D state transition diagram showing five states: Modified, Owned, Exclusive, Shared, and Invalid. The horizontal axis represents 'Shared' status (Unique, Shared, Invalid) and the vertical axis represents 'Dirty' status (Dirty, Clean).](images/image_0318.jpeg)\n\n\nThe diagram illustrates the ARM ACE Cache Line States as a 2D grid. The horizontal axis represents the 'Shared' dimension, with three states: Unique (left), Shared (middle), and Invalid (right). The vertical axis represents the 'Dirty' dimension, with two states: Dirty (top) and Clean (bottom). The states are arranged as follows:\n\n\n\nShared \\ Dirty | Dirty | Clean\nUnique | Modified | Exclusive\nShared | Owned | Shared\nInvalid | Invalid\n\n\nFigure 18.12: ARM ACE Cache Line States. A 2D state transition diagram showing five states: Modified, Owned, Exclusive, Shared, and Invalid. The horizontal axis represents 'Shared' status (Unique, Shared, Invalid) and the vertical axis represents 'Dirty' status (Dirty, Clean).\n\n\n**Figure 18.12**\n   ARM ACE Cache Line States\n\n\n**Table 18.2**\n\n(a) MESIM\n | Modified | Exclusive | Shared | Invalid\nClean/Dirty | Dirty | Clean | Clean | N/A\nUnique? | Yes | Yes | No | N/A\nCan write? | Yes | Yes | No | N/A\nCan forward? | Yes | Yes | Yes | N/A\nComments | Must write back to share or replace | Transitions to M on write | Shared implies clean, can forward | Cannot read\n\n\n\n(b) MOESI\n | Modified | Owned | Exclusive | Shared | Invalid\nClean/Dirty | Dirty | Dirty | Clean | Either | N/A\nUnique? | Yes | Yes | Yes | No | N/A\nCan write? | Yes | Yes | Yes | No | N/A\nCan forward? | Yes | Yes | Yes | No | N/A\nComments | Can share without write back | Must write back to transition | Transitions to M on write | Shared, can be dirty or clean | Cannot read"
        },
        {
          "name": "Intel Core i7-990X",
          "content": "Intel has introduced a number of multicore products in recent years. In this section, we look at the Intel Core i7-990X.\n\n\nThe general structure of the Intel Core i7-990X is shown in Figure 18.13. Each core has its own\n   **dedicated L2 cache**\n   and the six cores share a 12-MB\n   **L3 cache**\n   . One mechanism Intel uses to make its caches more effective is prefetching, in which the hardware examines memory access patterns and attempts to fill the caches speculatively with data that's likely to be requested soon.\n\n\nThe Core i7-990X chip supports two forms of external communications to other chips. The\n   **DDR3 memory controller**\n   brings the memory controller for the DDR main memory\n   \n    1\n   \n   onto the chip. The interface supports three channels that are 8 bytes wide for a total bus width of 192 bits, for an aggregate data rate of up to 32 GB/s. With the memory controller on the chip, the Front Side Bus is eliminated.\n\n\nThe\n   **QuickPath Interconnect**\n   (QPI) is a cache-coherent, point-to-point link-based electrical interconnect specification for Intel processors and chipsets. It enables high-speed communications among connected processor chips. The QPI link operates at 6.4 GT/s (transfers per second). At 16 bits per transfer, that adds up to 12.8 GB/s, and since QPI links involve dedicated bidirectional pairs, the total bandwidth is 25.6 GB/s. Section 3.5 covers QPI in some detail.\n\n\n1\n   \n   The DDR synchronous RAM memory is discussed in Chapter 5.\n\n\n\n\n![Block diagram of the Intel Core i7-990X architecture. The diagram shows six cores (Core 0 to Core 5) arranged in a 2x3 grid. Each core has a 32 kB L1-I and 32 kB L1-D cache. Each core also has a 256 kB L2 Cache. A shared 12 MB L3 Cache is located below the cores. Below the L3 Cache are two blocks: DDR3 Memory Controllers on the left and QuickPath Interconnect on the right. Bidirectional arrows indicate data flow between the cores/L3 Cache and the DDR3 Memory Controllers at 3 x 8B @ 1.33 GT/s, and between the cores/L3 Cache and the QuickPath Interconnect at 4 x 20B @ 6.4 GT/s.](images/image_0319.jpeg)\n\n\nBlock diagram of the Intel Core i7-990X architecture. The diagram shows six cores (Core 0 to Core 5) arranged in a 2x3 grid. Each core has a 32 kB L1-I and 32 kB L1-D cache. Each core also has a 256 kB L2 Cache. A shared 12 MB L3 Cache is located below the cores. Below the L3 Cache are two blocks: DDR3 Memory Controllers on the left and QuickPath Interconnect on the right. Bidirectional arrows indicate data flow between the cores/L3 Cache and the DDR3 Memory Controllers at 3 x 8B @ 1.33 GT/s, and between the cores/L3 Cache and the QuickPath Interconnect at 4 x 20B @ 6.4 GT/s.\n\n\nFigure 18.13 Intel Core i7-990X Block Diagram"
        },
        {
          "name": "ARM Cortex-A15 MPCore",
          "content": "We have already seen two examples of heterogeneous multicore processors using ARM cores, in Section 18.4: the big.Little architecture, which uses a combination of ARM Cortex-A7 and Cortex-A15 cores; and the Texas Instruments DSP SoC architecture, which combines Cortex-A15 cores with TI DSP cores. In this section, we introduce the Cortex-A15 MPCore multicore chip, which is a homogeneous multicore processor using multiple A15 cores. The A15 MPCore is a high-performance chip targeted at applications including mobile computing, high-end digital home servers, and wireless infrastructure.\n\n\nFigure 18.14 presents a block diagram of the Cortex-A15 MPCore. The key elements of the system are as follows:\n\n\n  * ■\n    **Generic interrupt controller (GIC):**\n    Handles interrupt detection and interrupt prioritization. The GIC distributes interrupts to individual cores.\n  * ■\n    **Debug unit and interface:**\n    The debug unit enables an external debug host to: stop program execution; examine and alter process and coprocessor state; examine and alter memory and input/output peripheral state; and restart the processor.\n  * ■\n    **Generic timer:**\n    Each core has its own private timer that can generate interrupts.\n  * ■\n    **Trace:**\n    Supports performance monitoring and program trace tools.\n  * ■\n    **Core:**\n    A single ARM Cortex-A15 core.\n  * ■\n    **L1 cache:**\n    Each core has its own dedicated L1 data cache and L1 instruction cache.\n  * ■\n    **L2 cache:**\n    The shared L2 memory system services L1 instruction and data cache misses from each core.\n  * ■\n    **Snoop control unit (SCU):**\n    Responsible for maintaining L1/L2 cache coherency.\n\n\n\n\n![Figure 18.14: ARM Cortex-A15 MPCore Chip Block Diagram. The diagram shows a multi-core architecture with a Generic Interrupt Controller (GIC) at the top. The GIC has a configurable number of hardware interrupt lines and per-CPU private fast interrupt (FIQ) lines. Below the GIC are four CPU clusters, each containing a Timer, CPU interface, and Watchdog (Wdog). Each cluster sends an IRQ signal to the GIC. Each cluster also contains a CPU/VFP block and an L1 cache. The CPU/VFP blocks are connected to the GIC via IRQ lines. The L1 caches are connected to an Instruction and data 64-bit bus and a Coherency control bus. The Coherency control bus is connected to a Snoop control unit (SCU). The SCU is connected to a Read/write 64-bit bus and an Optional 2nd R/W 64-bit bus.](images/image_0320.jpeg)\n\n\nFigure 18.14: ARM Cortex-A15 MPCore Chip Block Diagram. The diagram shows a multi-core architecture with a Generic Interrupt Controller (GIC) at the top. The GIC has a configurable number of hardware interrupt lines and per-CPU private fast interrupt (FIQ) lines. Below the GIC are four CPU clusters, each containing a Timer, CPU interface, and Watchdog (Wdog). Each cluster sends an IRQ signal to the GIC. Each cluster also contains a CPU/VFP block and an L1 cache. The CPU/VFP blocks are connected to the GIC via IRQ lines. The L1 caches are connected to an Instruction and data 64-bit bus and a Coherency control bus. The Coherency control bus is connected to a Snoop control unit (SCU). The SCU is connected to a Read/write 64-bit bus and an Optional 2nd R/W 64-bit bus.\n\n\n**Figure 18.14**\n   ARM Cortex-A15 MPCore Chip Block Diagram\n\n\n\n\n**Interrupt Handling**\n\n\nThe GIC collates interrupts from a large number of sources. It provides\n\n\n  * ■ Masking of interrupts\n  * ■ Prioritization of the interrupts\n  * ■ Distribution of the interrupts to the target A15 cores\n  * ■ Tracking the status of interrupts\n  * ■ Generation of interrupts by software\n\n\nThe GIC is a single functional unit that is placed in the system alongside A15 cores. This enables the number of interrupts supported in the system to be independent of the A15 core design. The GIC is memory mapped; that is, control registers for the GIC are defined relative to a main memory base address. The GIC is accessed by the A15 cores using a private interface through the SCU.\n\n\nThe GIC is designed to satisfy two functional requirements:\n\n\n  * ■ Provide a means of routing an interrupt request to a single CPU or multiple CPUs, as required.\n  * ■ Provide a means of interprocessor communication so that a thread on one CPU can cause activity by a thread on another CPU.\n\n\nAs an example that makes use of both requirements, consider a multithreaded application that has threads running on multiple processors. Suppose the application allocates some virtual memory. To maintain consistency, the operating system must update memory translation tables on all processors. The OS could update the tables on the processor where the virtual memory allocation took place, and then issue an interrupt to all the other processors running this application. The other processors could then use this interrupt's ID to determine that they need to update their memory translation tables.\n\n\nThe GIC can route an interrupt to one or more CPUs in the following three ways:\n\n\n  * ■ An interrupt can be directed to a specific processor only.\n  * ■ An interrupt can be directed to a defined group of processors. The MPCore views the first processor to accept the interrupt, typically the least loaded, as being best positioned to handle the interrupt.\n  * ■ An interrupt can be directed to all processors.\n\n\nFrom the point of view of software running on a particular CPU, the OS can generate an interrupt to all but self, to self, or to specific other CPUs. For communication between threads running on different CPUs, the interrupt mechanism is typically combined with shared memory for message passing. Thus, when a thread is interrupted by an interprocessor communication interrupt, it reads from the appropriate block of shared memory to retrieve a message from the thread that triggered the interrupt. A total of 16 interrupt IDs per CPU are available for interprocessor communication.\n\n\nFrom the point of view of an A15 core, an interrupt can be:\n\n\n  * ■\n    **Inactive:**\n    An Inactive interrupt is one that is nonasserted, or which in a multi-processing environment has been completely processed by that CPU but can still be either Pending or Active in some of the CPUs to which it is targeted, and so might not have been cleared at the interrupt source.\n  * ■\n    **Pending:**\n    A Pending interrupt is one that has been asserted, and for which processing has not started on that CPU.\n  * ■\n    **Active:**\n    An Active interrupt is one that has been started on that CPU, but processing is not complete. An Active interrupt can be pre-empted when a new interrupt of higher priority interrupts A15 core interrupt processing.\n\n\nInterrupts come from the following sources:\n\n\n  * ■\n    **Interprocessor interrupts (IPIs):**\n    Each CPU has private interrupts, ID0-ID15, that can only be triggered by software. The priority of an IPI depends on the receiving CPU, not the sending CPU.\n\n\n  * ■\n    **Private timer and/or watchdog interrupts:**\n    These use interrupt IDs 29 and 30.\n  * ■\n    **Legacy FIQ line:**\n    In legacy IRQ mode, the legacy FIQ pin, on a per CPU basis, bypasses the Interrupt Distributor logic and directly drives interrupt requests into the CPU.\n  * ■\n    **Hardware interrupts:**\n    Hardware interrupts are triggered by programmable events on associated interrupt input lines. CPUs can support up to 224 interrupt input lines. Hardware interrupts start at ID32.\n\n\nFigure 18.15 is a block diagram of the GIC. The GIC is configurable to support between 0 and 255 hardware interrupt inputs. The GIC maintains a list of interrupts, showing their priority and status. The Interrupt Distributor transmits to each CPU Interface the highest Pending interrupt for that interface. It receives back the information that the interrupt has been acknowledged, and can then change the status of the corresponding interrupt. The CPU Interface also transmits End of Interrupt (EOI) information, which enables the Interrupt Distributor to update the status of this interrupt from Active to Inactive.\n\n\n\n\n**Cache Coherency**\n\n\nThe MPCore's Snoop Control Unit (SCU) is designed to resolve most of the traditional bottlenecks related to access to shared data and the scalability limitation introduced by coherence traffic.\n\n\n\n\n![Block diagram of the Generic Interrupt Controller (GIC) showing the flow from interrupt inputs through the Interrupt Interface, Interrupt List, and Prioritization and Selection blocks to the CPU interfaces.](images/image_0321.jpeg)\n\n\nThe diagram illustrates the Generic Interrupt Controller (GIC) architecture. It consists of the following main components and data flows:\n\n\n  * **Interrupt Interface:**\n     Located on the left, it receives multiple external interrupt signals (indicated by arrows) and provides a \"Private bus read/write\" connection to a \"Decoder\" block above it.\n  * **Decoder:**\n     A small block that interacts with the Interrupt Interface and the Interrupt List.\n  * **Interrupt List:**\n     A large central table with columns for \"Priority\" and \"Status\". It contains multiple rows representing different interrupt sources. The \"Interrupt interface\" block is connected to the \"Priority\" column, and the \"Decoder\" is connected to the \"Status\" column.\n  * **Prioritization and selection:**\n     A block on the right that receives input from the \"Interrupt List\" and sends \"Top priority interrupts\" to the CPU interfaces.\n  * **CPU Interfaces (A15 Core 0, A15 Core 1, A15 Core 2, A15 Core 3):**\n     Four blocks at the bottom right, each containing an \"Interrupt number\" and \"Priority\" field. They receive \"Top priority interrupts\" from the \"Prioritization and selection\" block. A bracket labeled \"IRQ request to each CPU interface\" groups these four blocks.\n  * **Control Signals:**\n     A multi-line connection at the top right labeled \"Core acknowledge and end of interrupt (EOI) information from CPU interface\" provides feedback from the CPU interfaces back to the \"Prioritization and selection\" block.\n\n\nBlock diagram of the Generic Interrupt Controller (GIC) showing the flow from interrupt inputs through the Interrupt Interface, Interrupt List, and Prioritization and Selection blocks to the CPU interfaces.\n\n\n**Figure 18.15**\n   Generic Interrupt Controller Block Diagram\n\n\n**L1 CACHE COHERENCY**\n   The L1 cache coherency scheme is based on the MESI protocol described in Chapter 17. The SCU monitors operations with shared data to optimize MESI state migration. The SCU introduces three types of optimization: direct data intervention, duplicated tag RAMs, and migratory lines.\n\n\n**Direct data intervention (DDI)**\n   enables copying clean data from one CPU L1 data cache to another CPU L1 data cache without accessing external memory. This reduces read after read activity from the Level 1 cache to the Level 2 cache. Thus, a local L1 cache miss is resolved in a remote L1 cache rather than from access to the shared L2 cache.\n\n\nRecall that main memory location of each line within a cache is identified by a tag for that line. The tags can be implemented as a separate block of RAM of the same length as the number of lines in the cache. In the SCU,\n   **duplicated tag RAMs**\n   are duplicated versions of L1 tag RAMs used by the SCU to check for data availability before sending coherency commands to the relevant CPUs. Coherency commands are sent only to CPUs that must update their coherent data cache. This reduces the power consumption and performance impact from snooping into and manipulating each processor's cache on each memory update. Having tag data available locally lets the SCU limit cache manipulations to processors that have cache lines in common.\n\n\nThe\n   **migratory lines**\n   feature enables moving dirty data from one CPU to another without writing to L2 and reading the data back in from external memory. The operation can be described as follows. In a typical MESI protocol, one processor has a modified line and another processor attempts to read that line, the following actions occur:\n\n\n  * 1. The line contents are transferred from the modified line to the processor that initiated the read.\n  * 2. The line contents are written back to main memory.\n  * 3. The line is put in the shared state in both caches.\n\n\n\n\n**L2 Cache Coherency**\n\n\nThe SCU uses hybrid MESI and MOESI protocols to maintain coherency between the individual L1 data caches and the L2 cache. The L2 memory system contains a snoop tag array that is a duplicate copy of each of the L1 data cache directories. The snoop tag array reduces the amount of snoop traffic between the L2 memory system and the L1 memory system. Any line that resides in the snoop tag array in the Modified/Exclusive state belongs to the L1 memory system. Any access that hits against a line in this state must be serviced by the L1 memory system and passed to the L2 memory system. If the line is invalid or in the shared state in the snoop tag array, then the L2 cache can supply the data. The SCU contains buffers that can handle direct cache-to-cache transfers between cores without reading or writing any data on the ACE. Lines can migrate back and forth without any change to the MOESI state of the line in the L2 cache. Shareable transactions on the ACP are also coherent, so the snoop tag arrays are queried as a result of ACP transactions. For reads where the shareable line resides in one of the L1 data caches in the Modified/Exclusive state, the line is transferred from the L1 memory system to the L2 memory system and passed back on the ACP."
        },
        {
          "name": "IBM zEnterprise EC12 Mainframe",
          "content": "In this section, we look at a mainframe computer organization that uses multicore processor chips. The example we use is the IBM zEnterprise EC12 mainframe computer [SHUM13, DOBO13], which began shipping in late 2010. Section 7.8 provides a general overview of the EC12, together with a discussion of its I/O structure.\n\n\n\n\n**Organization**\n\n\nThe principal building block of the mainframe is the multichip module (MCM). The MCM is a 103-layer glass ceramic substrate (size 96–96 mm) containing eight chips and 7356 connections. The total number of transistors is over 23 billion. The MCM plugs into a card that is part of the book packaging. The book itself is plugged into the mid-plane system board to provide interconnectivity among the books.\n\n\nThe key components of an MCM are shown in Figure 18.16:\n\n\n\n\n![Diagram of the IBM EC12 Processor Node Structure showing the internal components of a Multichip Module (MCM) and its external connections.](images/image_0322.jpeg)\n\n\nThe diagram illustrates the internal and external structure of an IBM EC12 Processor Node. The central component is the\n    **MCM**\n    (Multichip Module), which contains eight\n    **PU**\n    (Processor Unit) chips, each with 6 cores. These are arranged in two rows of four: PU2, PU1, PU0 in the top row and PU3, PU4, PU5 in the bottom row. Two\n    **SC**\n    (Storage Control) chips, SC1 and SC0, are positioned in the center, connected to all PU chips. The MCM is connected to external components as follows:\n\n\n  * **Top row connections:**\n     MCU2, HCA2, MCU1, HCA1, MCU0, HCA0.\n  * **Bottom row connections:**\n     MCU3, HCA3, MCU4, HCA4, MCU5, HCA5.\n  * **Fabric Book Connectivity (FBC):**\n     FBC0, FBC1, FBC2 on the left and FBC0, FBC1, FBC2 on the right.\n\n\nLegend:\n\n\n  * FBC = fabric book connectivity\n  * HCA = host channel adapter\n  * MCM = multichip module\n  * MCU = memory control unit\n  * PU = processor unit\n  * SC = storage control\n\n\nDiagram of the IBM EC12 Processor Node Structure showing the internal components of a Multichip Module (MCM) and its external connections.\n\n\n**Figure 18.16**\n   IBM EC12 Processor Node Structure\n\n\n  * ■\n    **Processor unit (PU):**\n    There are six 5.5-GHz processor PU chips, each containing four processor cores plus three levels of cache. The PUs have external connections to main memory via memory control units and to I/O via host channel adapters. Thus, each MCM includes 24 cores.\n  * ■\n    **Storage control (SC):**\n    The two SC chips contain an additional level of cache plus interconnection logic for connecting to three other MCMs.\n\n\nThe microprocessor core features a wide superscalar, out-of-order pipeline that can decode three z/Architecture CISC instructions per clock cycle (\n   \n    < 0.18\n   \n   ns) and execute up to seven operations per cycle. The instruction execution path is predicted by branch direction and target prediction logic. Each core includes two integer units, two load/store units, one binary floating-point unit, and one decimal floating-point unit.\n\n\n\n\n**Cache Structure**\n\n\nThe EC12 incorporates a four-level cache structure. We look at each level in turn (Figure 18.17).\n\n\nEach core has a dedicated 160-kB\n   **L1 cache**\n   , divided into a 96-kB data cache and a 64-kB instruction cache. The L1 cache is designed as a write-through cache to L2, that is, altered data are also stored to the next level of memory. These caches are 8-way set associative.\n\n\nEach core also has a dedicated 2-MB L2, split equally into 1-MB data cache and 1-MB instruction cache. The L2 caches are write-through to L3, and 8-way set associative.\n\n\nEach 4-core processor unit chip includes a 24-MB\n   **L3 cache**\n   shared by all six cores. Because L1 and L2 caches are write-through, the L3 cache must process every\n\n\n\n\n![Diagram of the IBM EC12 Cache Hierarchy showing the relationship between Processor Units (PUs), Memory Control Modules (MCMs), Storage Control (SC) chips, and the four levels of cache (L1, L2, L3, L4).](images/image_0323.jpeg)\n\n\nThe diagram illustrates the IBM EC12 Cache Hierarchy. It shows two Processor Units (PU0 and PU5) within a Memory Control Module (MCM). Each PU contains 6 cores, each with its own L1 and L2 caches. The L1 caches are 64-kB instruction caches and 96-kB data caches, while the L2 caches are 1-MB instruction caches and 1-MB data caches. Both PUs share a 48-MB L3 cache. The MCM also contains two Storage Control (SC) chips, SC0 and SC1, each with a 192-MB L4 cache. Lines indicate the interconnection between the L3 caches and the L4 caches, and between the L4 caches and the SC chips.\n\n\nDiagram of the IBM EC12 Cache Hierarchy showing the relationship between Processor Units (PUs), Memory Control Modules (MCMs), Storage Control (SC) chips, and the four levels of cache (L1, L2, L3, L4).\n\n\n**Figure 18.17**\n   IBM EC12 Cache Hierarchy\n\n\nstore generated by the six cores on its chip. This feature maintains data availability during a core failure. The L3 cache is 12-way set associative. The EC12 implements embedded DRAM (eDRAM) as L3 cache memory on the chip. While this eDRAM memory is slower than static RAM (SRAM) normally used to implement cache memory, you can put a lot more of it onto a given area. For many workloads, having more memory closer to the core is more important than having fast memory.\n\n\nFinally, all 6 PUs on an MCM share a 160-MB\n   **L4 cache**\n   , which is split into one 92-MB cache on each SC chip. The principal motivation for incorporating a level 4 cache is that the very high clock speed of the core processors results in a significant mismatch with main memory speed. The fourth cache layer is needed to keep the cores running efficiently. The large shared L3 and L4 caches are suited to transaction-processing workloads exhibiting a high degree of data sharing and task swapping. The L4 cache is 24-way set associative. The SC chip, which houses the L4 cache, also acts as a cross-point switch for L4-to-L4 traffic to up to three remote books\n   \n    2\n   \n   by three bidirectional data buses. The L4 cache is the coherence manager, meaning that all memory fetches must be in the L4 cache before that data can be used by the processor.\n\n\nAll four caches use a line size of 256 bytes.\n\n\nThe EC12 is an interesting study in design trade-offs and the difficulty in exploiting the increasingly powerful processors available with current technology. The large L4 cache is intended to drive the need for access to main memory down to the bare minimum. However, the distance to the off-chip L4 cache costs a number of instruction cycles. Thus, the on-chip area devoted to cache is as large as possible, even to the point of having fewer cores than possible on the chip. The L1 caches are small, to minimize distance from the core and ensure that access can occur in one cycle. Each L2 cache is dedicated to a single core, in an attempt to maximize the amount of cached data that can be accessed without resort to a shared cache. The L3 cache is shared by all four cores on a chip and is as large as possible, to minimize the need to go to the L4 cache.\n\n\nBecause all of the books of the zEnterprise 196 share the workload, the four L4 caches on the four books form a single pool of L4 cache memory. Thus, access to L4 means not only going off-chip but perhaps off-book, further increasing access delay. This means relatively large distances exist between the higher-level caches in the processors and the L4 cache content. Still, accessing L4 cache data on another book is faster than accessing DRAM on the other book, which is why the L4 caches work this way.\n\n\nTo overcome the delays that are inherent to the book design and to save cycles to access the off-book L4 content, the designers try to keep instructions and data as close to the cores as possible by directing as much work as possible of a given logical partition workload to the cores located in the same book as the L4 cache. This is achieved by having the system resource manager/scheduler and the z/OS dispatcher work together to keep as much work as possible within the boundaries of as few cores and L4 cache space (which is best within a book boundary) as can be achieved without affecting throughput and response times. Preventing the resource manager/scheduler and the dispatcher from assigning workloads to processors where they might run less efficiently contributes to overcoming latency in a high-frequency processor design such as the EC12.\n\n\n2\n   \n   Recall from Chapter 7 that a EC12 book consists of an MCM, memory cards, and I/O cage connections."
        }
      ]
    },
    {
      "name": "General-Purpose Graphic Processing Units",
      "sections": [
        {
          "name": "Cuda Basics",
          "content": "**CUDA (Compute Unified Device Architecture)**\n   is a parallel computing platform and programming model created by NVIDIA and implemented by the graphics processing units (GPUs) that they produce. To adequately describe the GPGPU architecture, several CUDA software terms and concepts need to be covered first. This is by no means a comprehensive introduction to the CUDA programming language, particularly since the focus of this chapter and book is on computer architecture. However, it\n\n\nis difficult to describe the hardware portion of the GPGPU system without first laying the foundation with CUDA software terminology and its programming framework. These concepts will carry over into the GPU/GPGPU architecture domain.\n\n\nCUDA C is a C/C++ based language. A CUDA program can be divided into three general sections: (1) code to be run on the host (CPU); (2) code to be run on the device (GPU); and (3) the code related to the transfer of data between the host and the device. The code to be run on the host is of course serial code that can't, or isn't worth, parallelizing. The data-parallel code to be run on the GPU is called a\n   **kernel**\n   , while a\n   **thread**\n   is a single instance of this kernel function. The kernel typically will have few to no branching statements. Branching statements in the kernel result in serial execution of the threads in the GPU hardware. More about this will be covered in Section 19.3.\n\n\nThe programmer defines the number of threads launched when the kernel function is called. The total number of threads defined is typically in the thousands to maximize the utilization of the\n   **GPU processor cores**\n   (also known as\n   **CUDA cores**\n   ), as well as maximize the available speedup. Additionally, the programmer specifies how these threads are to be bundled. To be more specific, threads are uniformly bundled in\n   **blocks**\n   , and the number of blocks (also known as\n   **thread blocks**\n   ) per kernel launch is called a\n   **grid**\n   (see Figure 19.1). Table 19.1 gives a summary of the CUDA terms just defined.\n\n\n\n\n![Diagram illustrating the relationship among Threads, Blocks, and a Grid. A Grid (top) contains 3x2 Blocks. A Block (bottom) contains 3x2 Threads.](images/image_0324.jpeg)\n\n\nThe diagram illustrates the hierarchical relationship between a Grid, Blocks, and Threads in CUDA. At the top level is a\n    **Grid**\n    , which is a 2D array of\n    **Blocks**\n    . The Grid shown contains 3 columns and 2 rows of blocks, labeled\n    **Block(0, 0)**\n    ,\n    **Block(1, 0)**\n    ,\n    **Block(2, 0)**\n    in the top row, and\n    **Block(0, 1)**\n    ,\n    **Block(1, 1)**\n    ,\n    **Block(2, 1)**\n    in the bottom row. Each block is represented by a box containing wavy lines, indicating the presence of multiple threads. Below the Grid, a dashed line points to a single\n    **Block (1,1)**\n    , which is a 2D array of\n    **Threads**\n    . This block contains 3 columns and 2 rows of threads, labeled\n    **Thread (0, 0)**\n    ,\n    **Thread (1, 0)**\n    ,\n    **Thread (2, 0)**\n    in the top row, and\n    **Thread (0, 1)**\n    ,\n    **Thread (1, 1)**\n    ,\n    **Thread (2, 1)**\n    in the bottom row. Each thread is represented by a box containing a single wavy line.\n\n\nDiagram illustrating the relationship among Threads, Blocks, and a Grid. A Grid (top) contains 3x2 Blocks. A Block (bottom) contains 3x2 Threads.\n\n\n**Figure 19.1**\n   Relationship among Threads, Blocks, and a Grid\n\n\n**Table 19.1**\n\nCUDA Term | Definition | Equivalent GPU Hardware Component\nKernel | Parallel code in the form of a function to be run on GPU | Not applicable\nThread | An instance of the kernel on the GPU | GPU/CUDA processor core\nBlock | A group of threads assigned to a particular SM | CUDA multiprocessor (SM)\nGrid | The GPU | GPU\n\n\nFigure 19.1 illustrates a two-dimensional\n   **grid**\n   of two-dimensional thread blocks. Both the grid and block dimensions can be either one, two, or three dimensions. They need not have the same dimensions. For example, the grid could be set to one dimension, and the thread block could be set to three dimensions. However, as we will see shortly, this configuration can't fully utilize the GPU processors, because a block is assigned to only one of the several GPU\n   **streaming multiprocessors (SMs)**\n   . A block is never split between SMs. Thus, all but one set of GPU processor cores will be idle, while one SM is bearing the full processing load. Additionally, there is a maximum number of threads that an SM will accept. If this number is surpassed, then the code won't compile. Therefore, it is up to the programmer to use the specification data of the GPU to be used, and distribute the load as uniformly as possible. At minimum, the number of thread blocks launched should be no less than the number of SMs on the GPU. However, finding the optimum configuration can be a very time consuming and daunting process."
        },
        {
          "name": "GPU versus CPU",
          "content": "This section compares the complementary architectures of the GPU and the CPU. Because the GPU and CPU are orthogonally optimized to one another, their combination into a heterogeneous GPGPU system provides superior cost and performance gains for certain applications, compared to a pure CPU approach.\n\n\n\n\n**Basic Differences between CPU and GPU Architectures**\n\n\nBecause the GPU and the CPU are designed and optimized for two significantly different types of applications, their architectures differ significantly. This can be seen by comparing the relative amount of die area (transistor count) that is dedicated to cache, control logic, and processing logic for the two types of processor technologies (see Figure 19.2). In the CPU, as discussed in Chapter 18, the control logic and cache memory make up the majority of the CPU's real estate. This is as expected for an architecture which is tuned to process sequential code as quickly as possible. On the other hand, a GPU uses a massively parallel SIMD (single instruction multiple data) architecture to perform mainly mathematical operations. As such, a GPU doesn't require the same complex capabilities of the CPU's control logic (i.e., out of order execution, branch prediction, data hazards, etc.). Nor does it require large amounts of cache memory. GPUs simply run the same thread of code on large amounts of data\n\n\n\n\n![Figure 19.2: CPU versus GPU Silicon Area/Transistor Dedication. The diagram compares the internal structure of a CPU and a GPU. The CPU section shows a 'Control' block (1x1), a 2x2 grid of 'ALU' blocks, a 'Cache' block (1x1), and a 'DRAM' block (1x1). The GPU section shows a large 16x16 grid of small processing blocks, with a 'DRAM' block (1x1) at the bottom. The GPU's grid is significantly larger than the CPU's components, illustrating its higher transistor count and parallel processing nature.](images/image_0325.jpeg)\n\n\nFigure 19.2: CPU versus GPU Silicon Area/Transistor Dedication. The diagram compares the internal structure of a CPU and a GPU. The CPU section shows a 'Control' block (1x1), a 2x2 grid of 'ALU' blocks, a 'Cache' block (1x1), and a 'DRAM' block (1x1). The GPU section shows a large 16x16 grid of small processing blocks, with a 'DRAM' block (1x1) at the bottom. The GPU's grid is significantly larger than the CPU's components, illustrating its higher transistor count and parallel processing nature.\n\n\n**Figure 19.2**\n   CPU versus GPU Silicon Area/Transistor Dedication\n\n\nand are able to hide memory latency by managing the execution of more threads than available processor cores.\n\n\n\n\n**Performance and Performance per Watt Comparison**\n\n\nThe video game market has driven the need for ever-increasing real-time graphics realism. This translates into more parallel GPU processor cores with greater floating-point capabilities. As a result, the GPU is designed to maximize the number of floating-point operations per second (FLOPs) it can perform. Additionally, newer NVIDIA architectures, such as the Kepler and Maxwell architectures, have focused on increasing the performance per watt ratio (FLOPs/watt) over previous GPU architectures by decreasing the power required by each GPU processor core. This was accomplished with Kepler by decreasing its processor cores' clock, while increasing the number of on-chip transistors (following Moore's Law) allowing for a positive net gain of 3x the performance per watt over the Fermi architecture. Additionally, the Maxwell architecture has improved execution efficiency. This trend of increasing FLOPs that a GPU can perform versus a multicore CPU has diverged at an exponential rate (see Figure 19.3 [NVID14]), thus creating a large performance gap. Similar can be said about the performance per watt gap between these two different processing technologies."
        },
        {
          "name": "GPU Architecture Overview",
          "content": "The historical evolution of the GPU architecture can be divided up into three major phases or eras. The first phase would cover the early days of the GPU architecture (early 1980s to late 1990s), where the GPU was composed of fixed, nonprogrammable, specialized processing stages (e.g., vertex, raster, shader, etc.). Additionally, the continued technology advancements during this period, allowing for a dramatic decrease in the size and cost of a graphics system, in turn brought graphics processors to the PC in the mid- to late-1990s. The second phase would cover the iterative modification of the resulting Phase I GPU architecture from a fixed, specialized, hardware pipeline to a fully programmable processor (approximately during the early to mid-2000s). The general, final modification, introduced by NVIDIA in 2006, facilitated the use of its new GPGPU language, CUDA. The third phase picks up where the second one leaves off and covers how the GPU/GPGPU architecture makes an excellent and affordable highly parallelized SIMD coprocessor for\n\n\n\n\n![Line graph showing Theoretical GFLOPS for NVIDIA GPU single precision, NVIDIA GPU double precision, Intel CPU single precision, and Intel CPU double precision from Sep-02 to Aug-13. NVIDIA GPU single precision shows exponential growth, while others remain relatively flat or show slow linear growth.](images/image_0326.jpeg)\n\n\nThe graph illustrates the theoretical floating-point performance of GPUs and CPUs over a period of approximately 11 years. The y-axis represents Theoretical GFLOPS, ranging from 0 to 5500 in increments of 500. The x-axis shows time in months, with labels every four months from Sep-02 to Aug-13. Four data series are plotted: NVIDIA GPU single precision (light blue line with circles), NVIDIA GPU double precision (dark blue line with diamonds), Intel CPU single precision (black line with circles), and Intel CPU double precision (gray line with diamonds). The NVIDIA GPU single precision series shows a dramatic increase, starting near 100 GFLOPS in 2002 and reaching approximately 5400 GFLOPS by 2013. The NVIDIA GPU double precision series also shows growth, starting near 100 GFLOPS and reaching about 1400 GFLOPS. In contrast, the Intel CPU series remain relatively flat, with single precision staying below 1000 GFLOPS and double precision staying below 500 GFLOPS throughout the period.\n\n\n\nTime | NVIDIA GPU single precision (GFLOPS) | NVIDIA GPU double precision (GFLOPS) | Intel CPU single precision (GFLOPS) | Intel CPU double precision (GFLOPS)\nSep-02 | ~100 | ~100 | ~100 | ~100\nJan-04 | ~150 | ~150 | ~150 | ~150\nMay-05 | ~250 | ~250 | ~250 | ~250\nOct-06 | ~550 | ~250 | ~250 | ~250\nFeb-08 | ~950 | ~250 | ~250 | ~250\nJul-09 | ~1350 | ~550 | ~300 | ~250\nNov-10 | ~1550 | ~700 | ~450 | ~350\nApr-12 | ~3100 | ~1300 | ~550 | ~350\nAug-13 | ~5400 | ~1400 | ~800 | ~400\n\n\nLine graph showing Theoretical GFLOPS for NVIDIA GPU single precision, NVIDIA GPU double precision, Intel CPU single precision, and Intel CPU double precision from Sep-02 to Aug-13. NVIDIA GPU single precision shows exponential growth, while others remain relatively flat or show slow linear growth.\n\n\n**Figure 19.3**\n   Floating-Point Operations per Second for CPU and GPU\n\n\naccelerating the run times of some nongraphics-related programs, along with how a GPGPU language (CUDA in this case) maps to this architecture. The focus of this chapter follows this third phase or era of the GPU.\n\n\nThe first NVIDIA GPU with added GPGPU support hardware was the GeForce 8800 GTX. To enable the GPU to be used by programmers for general-purpose parallel computing applications, a true cache hierarchy and a user-addressable shared memory was added. Additionally, arrays of the programmable GPU processor cores are equally divided up into scalable SMs. The benefit of such an architecture is the scalability of GPU processor cores, as well as SMs in new generations or different models of GPUs without requiring modification to the CUDA programming language.\n\n\n\n\n**Baseline GPU Architecture**\n\n\nAs previously mentioned, NVIDIA has progressed through several generations of GPU processing technologies (i.e., Tesla, Fermi, Kepler, and Maxwell), each of which has a small to moderate difference in its microarchitecture over its predecessor. The naming convention for the SM has been slightly modified for the newer generations\n\n\nof GPU technologies, such as SMX for Kepler and SMM for Maxwell. This helps signify a relatively significant change to the SM architecture from its predecessor (it also helps with the new product's promotional marketing!). With that being said, from a CUDA programming perspective, all of these processing technologies still have identical top-level architectures.\n\n\nFor the remainder of this chapter, we will use NVIDIA's Fermi architecture as the example baseline architecture. The Fermi architecture was chosen due to its fairly representative GPU architecture and lower CUDA core/SM count, which simplifies the mapping between the GPU hardware and the CUDA software. This example architecture is composed of 16 SMs, where each SM contains a group of 32 CUDA cores. Therefore, the Fermi GPU has a total of\n   \n    16 \\text{ SMs} \\times 32 \\text{ CUDA cores/SM}\n   \n   , or 512 CUDA cores.\n\n\n\n\n**Full Chip Layout**\n\n\nFigure 19.4 illustrates the general layout of the NVIDIA Fermi architecture GPU. As can be seen in this figure, the L2 cache is centrally located to the 16 SMs (8 SMs above and below). Each SM is represented by the 2 adjacent columns and 16 rows of rectangles (GPU processor cores), along with a column of 16 load/store units and a column of 4 special function units (SFUs). A more detailed illustration of the SM module is shown in Figure 19.5 [NIVD09]. The rectangles at the head and foot of the SMs in Figure 19.4 are where the registers and L1/shared memory are located. Each of the six DRAM I/O interfaces has a 64-bit memory interface (the DRAM interface circuitry is shown in dark blue rectangles on the outermost left and right sides). Thus, collectively, there is a 384-bit interface to the GPU's GDDR5 (graphic double data\n\n\n\n\n![Figure 19.4: NVIDIA Fermi Architecture. A schematic diagram showing the layout of the GPU chip. It features a central L2 cache surrounded by 16 Streaming Multiprocessors (SMs). Each SM is represented by a grid of 2 columns and 16 rows of processor cores. Each SM also includes a column of 16 load/store units and a column of 4 special function units (SFUs). The chip is flanked by six DRAM I/O interfaces on the left and right sides, each with a 64-bit memory interface.](images/image_0327.jpeg)\n\n\nThe diagram illustrates the NVIDIA Fermi GPU architecture. It is a rectangular grid representing the chip layout. The central area is a large horizontal block labeled 'L2 cache'. Surrounding this cache are 16 Streaming Multiprocessors (SMs), arranged in two rows of eight. Each SM is depicted as a grid of 2 columns and 16 rows of small rectangles, representing the GPU processor cores. To the left and right of each SM's core grid are vertical columns of smaller rectangles: a column of 16 load/store units and a column of 4 special function units (SFUs). The entire chip is bordered by six vertical blocks on the left and right, each labeled 'DRAM', representing the memory interfaces. The overall layout shows a highly symmetric and modular design.\n\n\nFigure 19.4: NVIDIA Fermi Architecture. A schematic diagram showing the layout of the GPU chip. It features a central L2 cache surrounded by 16 Streaming Multiprocessors (SMs). Each SM is represented by a grid of 2 columns and 16 rows of processor cores. Each SM also includes a column of 16 load/store units and a column of 4 special function units (SFUs). The chip is flanked by six DRAM I/O interfaces on the left and right sides, each with a 64-bit memory interface.\n\n\n**Figure 19.4**\n   NVIDIA Fermi Architecture\n\n\n\n\n![Figure 19.5: Single SM Architecture. This diagram illustrates the internal components of a Streaming Multiprocessor (SM). At the top is the Instruction cache, followed by two Warp schedulers and two Dispatch units. Below these is a Register file (32k x 32-bit). The main processing area consists of four groups: two groups of 8 CUDA cores each, a group of 16 Load/Store (Ld/St) units, and a group of 4 Special Function Units (SFU). A dashed box labeled 'CUDA core' provides a detailed view of a single core, showing a Dispatch port, Operand collector, FP unit, Int unit, and a Result queue. Below the processing groups is an Interconnect network, followed by a 64-kB shared memory/L1 cache, and finally a Uniform cache at the bottom.](images/image_0328.jpeg)\n\n\nFigure 19.5: Single SM Architecture. This diagram illustrates the internal components of a Streaming Multiprocessor (SM). At the top is the Instruction cache, followed by two Warp schedulers and two Dispatch units. Below these is a Register file (32k x 32-bit). The main processing area consists of four groups: two groups of 8 CUDA cores each, a group of 16 Load/Store (Ld/St) units, and a group of 4 Special Function Units (SFU). A dashed box labeled 'CUDA core' provides a detailed view of a single core, showing a Dispatch port, Operand collector, FP unit, Int unit, and a Result queue. Below the processing groups is an Interconnect network, followed by a 64-kB shared memory/L1 cache, and finally a Uniform cache at the bottom.\n\n\n**Figure 19.5**\n   Single SM Architecture\n\n\nrate, a DDR memory designed specifically for graphic processing) DRAM, allowing for support of up to a total of 6 GB of SM off-chip memory (i.e., global, constant, texture, and local). More specifics about these different memory types will be discussed in the next section. Also, illustrated in Figure 19.4 is the host interface, which can be found on the left-hand side of the GPU layout diagram. The host interface allows for PCIe connectivity between the GPU and the CPU. Lastly, the GigaThread global scheduler, in orange and located next to the host interface, is responsible for the distribution of thread blocks to all of the SM's warp schedulers (see Figure 19.5).\n\n\n\n\n**Streaming Multiprocessor Architecture Details**\n\n\nThe right-hand side of Figure 19.5 breaks down the NVIDIA Fermi architecture into its basic components for a single SM. These components are\n\n\n  * ■ GPU processor cores (total of 32 CUDA cores)\n  * ■ Warp scheduler and dispatch port\n\n\n  * ■ Sixteen load/store units\n  * ■ Four SFUs\n  * ■\n    \n     32k \\times 32\n    \n    -bit registers\n  * ■ Shared memory and L1 cache (64 kB in total)\n\n\n**DUAL WARP SCHEDULER**\n   As covered previously, the GigaThread global scheduler unit on the GPU chip distributes the thread blocks to the SMs. The dual warp scheduler will then break up each thread block it is processing into\n   **warps**\n   , where a warp is a bundle of 32 threads that start at the same starting address and their thread IDs are consecutive. Once a warp is issued, each thread will have its own instruction address counter and register set. This allows for independent branching and execution of each thread in the SM.\n\n\nThe GPU is most efficient when it is processing as many warps as possible to keep the CUDA cores maximally utilized. As illustrated in Figure 19.6, maximum SM hardware utilization will occur when the dual warp schedulers and instruction dispatch units are able to issue two warps every two clock cycles (Fermi architecture). As explained next, structural hazards are the main source of an SM falling short of achieving this maximum processing rate, while off-chip memory access latency can be more easily hidden.\n\n\nEach divided column of 16 CUDA cores (\n   \n    \\times 2\n   \n   ), 16 load/store units, and 4 SFUs (see Figure 19.5) is eligible to be assigned half a warp (16 threads) to process from each of the two warp scheduler/dispatch units per clock cycle, given that the component column isn't experiencing a structural hazard. Structural hazards are caused by limited SFUs, double-precision multiplication, and branching. However, the warp schedulers have a built-in scoreboard to track warps that are available for execution, as well as structural hazards. This allows for the SM to both work around structural hazards and help hide off-chip memory access latency as optimally as possible.\n\n\n\n\n![Diagram illustrating the Dual Warp Schedulers and Instruction Dispatch Units Run Example. The diagram shows two parallel columns of hardware units. Each column has a 'WARP scheduler' at the top, followed by an 'Instruction dispatch unit'. Below the dispatch units, instructions for two warps are shown. The left column shows instructions for Warp 8 (11, 12) and Warp 14 (42, 95, 96). The right column shows instructions for Warp 9 (11, 12) and Warp 15 (33, 34, 96). A vertical arrow on the left labeled 'Time' indicates the progression of instructions over time. The diagram demonstrates how two warps are issued per clock cycle by the dual dispatch units.](images/image_0329.jpeg)\n\n\nDiagram illustrating the Dual Warp Schedulers and Instruction Dispatch Units Run Example. The diagram shows two parallel columns of hardware units. Each column has a 'WARP scheduler' at the top, followed by an 'Instruction dispatch unit'. Below the dispatch units, instructions for two warps are shown. The left column shows instructions for Warp 8 (11, 12) and Warp 14 (42, 95, 96). The right column shows instructions for Warp 9 (11, 12) and Warp 15 (33, 34, 96). A vertical arrow on the left labeled 'Time' indicates the progression of instructions over time. The diagram demonstrates how two warps are issued per clock cycle by the dual dispatch units.\n\n\n**Figure 19.6**\n   Dual Warp Schedulers and Instruction Dispatch Units Run Example\n\n\nTherefore, it is important for the programmer to set the thread block size greater than the total number of CUDA cores in an SM, but less than the maximum allowable threads per block, and to make sure the thread block size (in the\n   \n    x\n   \n   and/or\n   \n    y\n   \n   dimensions) is a multiple of 32 (warp size) to achieve near-optimal utilization of the SMs.\n\n\n**CUDA CORES**\n   As mentioned in the CUDA Basics section, the NVIDIA GPU processor cores are also known as CUDA cores (see Figure 19.5). Also defined earlier, and as can be seen in Figure 19.4, there are a total of 32 CUDA cores dedicated to each SM in the Fermi architecture. Each CUDA core has two separate pipelines or data paths: an integer (INT) unit pipeline and a floating-point (FP) unit pipeline (see Figure 19.5). Only one of these data paths can be used during a single clock period. The INT unit is capable of 32-bit, 64-bit, and extended precision for integer and logic/bitwise operations. The FP unit can perform a single-precision FP operation, while a double-precision FP operation requires two CUDA cores. Therefore, threads that perform only double-precision FP operations will take twice as long to run compared to a single-precision FP thread. This performance impact of double-precision FP arithmetic is addressed in the Kepler architecture by the inclusion of dedicated double-precision units in each SM, as well as a majority of single-precision units. Fortunately, the management of thread-level FP single- and double-precision operations is hidden from the CUDA programmer. However, the programmer should be aware of the potential performance impact that can be incurred between using the two precision types based on the GPU used.\n\n\nThe Fermi architecture added an improvement to the CUDA core's FP unit over its predecessors. It upgraded from the IEEE 754-1985 floating-point arithmetic standard to the IEEE 754-2008 standard. This was accomplished by improving on the accuracy of the multiply-add instruction (MAD) with a fused multiply-add (FMA) instruction. The FMA instruction is valid for both single- and double-precision arithmetic. The Fermi architecture performs only a single rounding at the end of an FMA instruction. Not only is the accuracy of the result improved, but also performing an FMA instruction is compressed into a single processor clock cycle. Therefore, 32 single-precision or 16 double-precision FMA operations can occur in a single processor clock cycle per SM.\n\n\n**SPECIAL FUNCTION UNITS**\n   Each SM has four SFUs. The SFU performs transcendental operations, such as cosine, sine, reciprocal, and square root, in a single clock cycle. Since there are only 4 SFUs in an SM and 32 parallel threads of a single instruction in a warp, it takes 8 clock cycles to complete a warp that requires the SFUs. However, the CUDA processors, along with the load and store units, can still be utilized at the same time.\n\n\n**LOAD AND STORE UNITS**\n   Each of the 16 load and store units of the SM calculates the source and destination addresses for a single thread per clock cycle. The addresses are for the cache or DRAM that the threads wish to write data to, or read data from.\n\n\n**REGISTERS, SHARED MEMORY, AND L1 CACHE**\n   As illustrated in Figure 19.5, each SM has its own (on-chip) dedicated set of registers and shared memory/L1 cache block. Details and benefits as to these low latency, on-chip memories are described below.\n\n\n**Table 19.2**\n\nMemory Type | Relative Access Times | Access Type | Scope | Data Lifetime\nRegisters | Fastest. On-chip | R/W | Single thread | Thread\nShared | Fast. On-chip | R/W | All threads in a block | Block\nLocal | 100\n      \n       \\times\n      \n      to 150\n      \n       \\times\n      \n      slower than shared and register. Off-chip | R/W | Single thread | Thread\nGlobal | 100\n      \n       \\times\n      \n      to 150\n      \n       \\times\n      \n      slower than shared and register. Off-chip. | R/W | All threads and host | Application\nConstant | 100\n      \n       \\times\n      \n      to 150\n      \n       \\times\n      \n      slower than shared and register. Off-chip | R | All threads and host | Application\nTexture | 100\n      \n       \\times\n      \n      to 150\n      \n       \\times\n      \n      slower than shared and register. Off-chip | R | All threads and host | Application\n\n\nAlthough the Fermi architecture has an impressive\n   \n    32k \\times 32\n   \n   -bit registers per SM, each thread has a maximum of\n   \n    64 \\times 32\n   \n   -bit registers allocated to it as defined by CUDA compute capability version 2.x, which is a function of the maximum number of active warps allowed per SM, as well as the number of registers per SM. As shown in Table 19.2, the registers, along with shared memory, have the fastest access times of only several nanoseconds (ns). If there is any temporary register spillage, the data will first get moved to L1 cache before being sent to L2 cache, then long access latency local memory (see Figure 19.7a). The use of L1 cache helps prevent data read/write hazards from occurring. The lifetime of the data in the registers assigned to a thread is therefore only as long as the life of the thread.\n\n\nThe addressable, on-chip shared memory dedicated to the GPU processor cores of an SM is a unique configuration when compared to contemporary multicore microprocessors, such as the CPU. These contemporary architectures, as covered in Chapter 18 and illustrated in Figure 18.6, have a dedicated on-chip L1 cache and a set of registers per core. However, they typically do not have on-chip addressable memory. Instead, dedicated memory management hardware regulates the movement of data between the cache and main memory without control from the programmer. This is significantly different from the GPU architecture (see Figure 19.5).\n\n\nAs discussed at the beginning of this chapter, shared memory was added to the GPU architecture specifically to assist with GPGPU applications. Optimizing the use of shared memory can significantly improve the speedup and performance of a GPGPU application by eliminating unneeded long latency accesses to off-chip memory. Despite the shared memory being small in size for each SM (48 kB at maximum configuration), it has a very low access latency of 100\n   \n    \\times\n   \n   to 150\n   \n    \\times\n   \n   less than global memory (see Table 19.2). Thus, there are three major ways that shared memory can accelerate the parallel processing tasks: (1) multiple repeated use of shared memory data by all threads of a block (e.g., blocks of data used for matrix-matrix multiplication); (2) select threads of a block (based on specific IDs) are used to transfer data from the global memory to the shared memory, thus redundant\n\n\n\n\n![Figure 19.7: Fermi Memory Architecture. (a) SM memory architecture showing Core 0, Core 1, and Core 31 connected to a 128 kB register file, x kB shared memory, (64-x) kB L1 data cache, and 64 kB L1 instruction cache. (b) Overall memory architecture showing SM 0, SM 1, and SM 15 connected to a 768 kB L2 cache, which is connected to DRAM.](images/image_0330.jpeg)\n\n\n(a) SM memory architecture\n\n\n(b) Overall memory architecture\n\n\nFigure 19.7: Fermi Memory Architecture. (a) SM memory architecture showing Core 0, Core 1, and Core 31 connected to a 128 kB register file, x kB shared memory, (64-x) kB L1 data cache, and 64 kB L1 instruction cache. (b) Overall memory architecture showing SM 0, SM 1, and SM 15 connected to a 768 kB L2 cache, which is connected to DRAM.\n\n\n**Figure 19.7**\n   Fermi Memory Architecture\n\n\nreads and writes to the same memory locations are removed; and (3) the user can optimize data accesses to global memory by making sure the accesses are coalesced, when possible. All of these points also aid in reducing off-chip memory bandwidth constraint issues. The lifetime of the data in an SM's shared memory is as long as the life of the thread block being processed on it. So, once all of the threads of the block have completed, the data in the SM's shared memory is no longer valid.\n\n\nAlthough the use of shared memory will give the optimum run times, in some applications the memory accesses are not known during the programming phase. This is where having more L1 cache available (maximum setting of 48 kB) will give the optimal results. Additionally, the L1 cache helps with aiding register spills,\n\n\ninstead of going straight to local (off-chip) DRAM memory. The two-level cache hierarchy—single L1 cache per SM, and the across chip, SM shared L2 cache—gives the same benefits as those found in conventional multicore microprocessors.\n\n\n\n\n**Importance of Knowing and Programming to Your Memory Types**\n\n\nIt is important for the programmer to understand the nuances of the various GPU memories, particularly the sizes available for each memory type, their relative access times, and accessibility limitations, to enable correct and efficient code development using CUDA. As one can see from the CUDA Basics section covered at the beginning of the chapter, the SM level memories just covered, and the terminology and parameters listed in Table 19.2, a much different approach is required for GPGPU programming than program development targeted for a CPU, where the specific data storage hardware used (other than file I/O) is hidden from the programmer.\n\n\nFor example, with the GPU architecture, each thread assigned to a CUDA core has its own set of registers, such that one thread cannot access another thread's registers, whether in the same SM or not. The only way threads within a particular SM can cooperate with each other (via data sharing) is through the shared memory (see Figure 19.8). This is typically accomplished by the programmer assigning only certain threads of an SM to write to specific locations of its shared memory, thus preventing write hazards or wasted cycles (e.g., many threads reading the same data\n\n\n\n\n![Diagram illustrating the CUDA Representation of a GPU's Basic Architecture. The diagram shows a (Device) Grid containing two blocks: Block (0,0) and Block (1,0). Each block contains shared memory, registers, and two threads: Thread (0,0) and Thread (1,0). The threads access shared memory and registers. The blocks access global memory and constant memory. The host interacts with global memory and constant memory.](images/image_0331.jpeg)\n\n\nThe diagram illustrates the CUDA Representation of a GPU's Basic Architecture. It shows a (Device) Grid containing two blocks: Block (0,0) and Block (1,0). Each block contains shared memory, registers, and two threads: Thread (0,0) and Thread (1,0). The threads access shared memory and registers. The blocks access global memory and constant memory. The host interacts with global memory and constant memory.\n\n\nDiagram illustrating the CUDA Representation of a GPU's Basic Architecture. The diagram shows a (Device) Grid containing two blocks: Block (0,0) and Block (1,0). Each block contains shared memory, registers, and two threads: Thread (0,0) and Thread (1,0). The threads access shared memory and registers. The blocks access global memory and constant memory. The host interacts with global memory and constant memory.\n\n\n**Figure 19.8**\n   CUDA Representation of a GPU's Basic Architecture\n\n\nfrom global memory and writing it to the same shared memory address). Before all of the threads of a particular SM are allowed to read from the shared memory that has just been written to, synchronization of all the threads of that SM needs to take place to prevent a read-after-write (RAW) data hazard.\n   \n    1"
        },
        {
          "name": "Intel’s Gen8 GPU",
          "content": "As another example of a GPGPU architecture, this section provides an overview of the Gen8 processor graphics architecture [INTE14, PEDD14].\n\n\nThe fundamental building block of the Gen8 architecture is the execution unit (EU) shown in Figure 19.9. The EU is a simultaneous multithreading (SMT) architecture with seven threads. Recall from Chapters 17 and 18 that in an SMT architecture, register banks are expanded so that multiple threads can share the use of pipeline resources. The EU has seven threads and is implemented as a superscalar pipeline architecture. Each thread includes 128 general-purpose registers. Within each EU, the primary computation units are two SIMD floating-point units that support both floating-point and integer computation. Each SIMD FPU can complete simultaneous add and multiply floating-point instructions every cycle. There is also a branch unit dedicated to branch instructions and a send unit for memory operations.\n\n\nEach register stores 32 bytes, accessible as an SIMD 8-element vector of 32-bit data elements. Thus each Gen8 thread has 4 kB of general-purpose register file (GRF), for a total of 28 kB of GRF per EU. Flexible addressing modes permit registers to be addressed together to build effectively wider registers, or even to represent strided rectangular block data structures.\n   \n    2\n   \n   Per thread architectural state is maintained in a separate dedicated architecture register file (ARF).\n\n\n\n\n![Diagram of the Intel Gen8 Execution Unit (EU). The diagram shows a central vertical stack of seven 'Superscalar pipeline' blocks. To the left of this stack is a vertical bar labeled 'Instruction fetch'. To the right is a vertical bar labeled 'Thread arbiter'. Arrows indicate data flow from the instruction fetch to each pipeline stage, and from each pipeline stage to the thread arbiter. From the thread arbiter, arrows point to three output units: 'Send', 'Branch', and a stack of four units labeled 'SIMD FPU'.](images/image_0332.jpeg)\n\n\nDiagram of the Intel Gen8 Execution Unit (EU). The diagram shows a central vertical stack of seven 'Superscalar pipeline' blocks. To the left of this stack is a vertical bar labeled 'Instruction fetch'. To the right is a vertical bar labeled 'Thread arbiter'. Arrows indicate data flow from the instruction fetch to each pipeline stage, and from each pipeline stage to the thread arbiter. From the thread arbiter, arrows point to three output units: 'Send', 'Branch', and a stack of four units labeled 'SIMD FPU'.\n\n\n**Figure 19.9**\n   Intel Gen8 Execution Unit\n\n\n1\n   \n   See Chapter 16 for a discussion of RAW hazards.\n\n\n2\n   \n   The term\n   *strided*\n   refers to a sequence of memory reads and writes to addresses, each of which is separated from the last by a constant interval called the\n   *stride length*\n   . Strided references are often generated by loops through an array, and (if the data is large enough that access-time is significant) it can be worthwhile to tune for better locality by inverting double loops or by partially unrolling the outer loop of a loop nest.\n\n\n\n\n![Diagram of an Intel Gen8 Subslice architecture. The subslice contains 8 Execution Units (EUs) arranged in two columns of four. Above the EUs is a Local thread dispatcher and an Instruction cache. Below the EUs are a Sampler (with L1 and L2 caches) and a Data port. Arrows indicate data flow between the dispatcher, EUs, and the sampler/data port.](images/image_0333.jpeg)\n\n\nThe diagram illustrates the internal structure of an Intel Gen8 Subslice. At the top, a label indicates \"Subslice: 8 EUs\". Below this, a \"Local thread dispatcher\" is shown, which feeds into the \"Instruction cache\". The main body of the subslice consists of eight \"EU\" (Execution Unit) blocks, organized into two vertical columns of four. Each EU block contains internal logic and a small local cache. Horizontal arrows indicate data flow between the EUs within each column. Vertical arrows show data flow between the EUs and the \"Sampler\" and \"Data port\" units at the bottom. The \"Sampler\" unit includes an \"L1\" cache and an \"L2 sampler cache\". The \"Data port\" unit is shown with a large double-headed arrow, indicating bidirectional data transfer. A large downward arrow at the top points into the subslice, and a large upward arrow at the bottom points out of the subslice.\n\n\nDiagram of an Intel Gen8 Subslice architecture. The subslice contains 8 Execution Units (EUs) arranged in two columns of four. Above the EUs is a Local thread dispatcher and an Instruction cache. Below the EUs are a Sampler (with L1 and L2 caches) and a Data port. Arrows indicate data flow between the dispatcher, EUs, and the sampler/data port.\n\n\n**Figure 19.10**\n   Intel Gen8 Subslice\n\n\nThe EU can issue up to four different instructions simultaneously from different threads. The thread arbiter dispatches each instruction to one of the four functional units for execution.\n\n\nEUs are organized into a subslice (Figure 19.10), which may contain up to eight EUs. Each subslice contains its own local thread dispatcher unit and its own supporting instruction caches. Thus, a single subslice has dedicated hardware resources and register files for a total of 56 simultaneous threads.\n\n\nA subslice also includes a unit called the sampler, with its own local L1 and L2 cache. The sampler is used for sampling texture and image surfaces. The sampler includes logic to support dynamic decompression of block compression texture formats. The sampler also includes fixed-function logic that enables address conversion of image\n   \n    (u,v)\n   \n   coordinates and address clamping modes such as mirror, wrap, border, and clamp. The sampler supports a variety of sampling filtering modes such as point, bilinear, trilinear, and anisotropic. The data port provides efficient read/write operations that attempt to take advantage of cache line size to consolidate read operations from different threads.\n\n\nTo create product variants, subslices may be clustered into groups called slices (Figure 19.11). Currently, up to three subslices may be organized into a single slice for a total of 24 EUs. In addition to the subslices, the slice includes logic for thread dispatch routing, other function logic to optimize graphic data processing, a shared\n\n\n\n\n![Diagram of the Intel Gen8 Slice architecture. A Slice contains 24 Execution Units (EUs) organized into three Subslices of 8 EUs each. Each Subslice includes an Instruction cache, a Local thread dispatcher, and a Sampler L1 with a 1.2 sampler cache. Each EU has its own L1 instruction and data caches. The Subslices are connected to a central L3 data cache and a Shared local memory. The entire Slice is managed by Function logic and Fixed-function units.](images/image_0334.jpeg)\n\n\nThe diagram illustrates the internal structure of an Intel Gen8 GPU Slice. At the top, a large arrow points down into the Slice, which is labeled \"Slice: 24 EUs\". The Slice is divided into three main functional blocks: \"Function logic\" on the left, \"L3 data cache\" in the center, and \"Shared local memory\" on the right. Above these is a \"Fixed-function units\" block. The Slice contains three \"Subslice: 8 EUs\" units. Each Subslice has an \"Instruction cache\" and a \"Local thread dispatcher\" at the top. Below these are eight \"EU\" (Execution Unit) blocks arranged in two columns of four. Each EU has its own L1 instruction and data caches. At the bottom of each Subslice are a \"Sampler L1\" and a \"1.2 sampler cache\", followed by a \"Data port\". Arrows indicate data flow from the EUs to the L3 data cache and Shared local memory, and from the L3 data cache and Shared local memory back to the EUs. The Function logic and Fixed-function units also have connections to the L3 data cache and Shared local memory.\n\n\nDiagram of the Intel Gen8 Slice architecture. A Slice contains 24 Execution Units (EUs) organized into three Subslices of 8 EUs each. Each Subslice includes an Instruction cache, a Local thread dispatcher, and a Sampler L1 with a 1.2 sampler cache. Each EU has its own L1 instruction and data caches. The Subslices are connected to a central L3 data cache and a Shared local memory. The entire Slice is managed by Function logic and Fixed-function units.\n\n\n**Figure 19.11**\n   Intel Gen8 Slice\n\n\nL3 cache, and a smaller shared local memory structure. The latter is visible (addressable memory) to the EUs and is useful for sharing temporary variables.\n\n\nTo enhance performance a technique known as\n   **cache banking**\n   is used for the shared L3 data cache. To achieve high bandwidth, the cache is divided into equal-size memory modules, called banks, which can be accessed simultaneously. Any memory read or write request made of\n   \n    n\n   \n   addresses that fall in\n   \n    n\n   \n   distinct memory banks can therefore be serviced simultaneously, yielding an overall bandwidth that is\n   \n    n\n   \n   times as high as the bandwidth of a single module. However, if two addresses of a memory request fall in the same memory bank, there is a bank conflict and the access has to be serialized. The hardware splits a memory request with bank conflicts into as many separate conflict-free requests as necessary, decreasing throughput by a factor equal to the number of separate memory requests. If the number of separate memory requests is\n   \n    n\n   \n   , the initial memory request is said to cause\n   \n    n\n   \n   -way bank conflicts. To get maximum performance, it is therefore important to understand how memory addresses map to memory banks in order to schedule the memory requests so as to minimize bank conflicts.\n\n\nFinally, an SoC product architect can create product families or a specific product within a family by placing a single slice or multiple slices on an SoC chip. These slices are combined with additional front-end logic to manage command\n\n\nsubmission, as well as fixed-function logic to support 3D rendering and media pipelines. Additionally, the entire Gen8 compute architecture interfaces to the rest of the SoC components via a dedicated unit called the graphics technology interface (GTI).\n\n\nAn example of such an SoC is the Intel Core M Processor with Intel HD Graphics 5300 Gen8 (Figure 19.12). In addition to the GPU portion, the chip contains multiple CPU cores, an LLC cache and a system agent. The system agent includes controllers for DRAM memory, display, and PCIe devices. The Processor Graphics Gen8, CPUs, LLC cache, and system agent are interconnected with a ring structure, such as we saw for the Xeon processor (Figure 7.16)."
        },
        {
          "name": "When to Use a GPU as a Coprocessor",
          "content": "We end this chapter with a brief discussion on determining candidate GPGPU applications from a software design perspective, as well as some related software tools to assist with this process.\n\n\nWhat differentiates a program that would benefit from running a portion of its code on a GPU (thus, a heterogeneous computing platform) versus a program that wouldn't? As has been illustrated and discussed in this chapter, the GPU is made up of hundreds to thousands of processor cores and has an SIMD architecture. Therefore, programs that have a highly parallelizable portion(s) of code, which can be replicated into thousands of lightweight threads to work on large data sets concurrently, are the best candidates for accelerating their run time on a GPGPU system. Here, a lightweight thread is defined as an instance of a relatively small, massively parallelizable snippet of code, which has no or very little branching. Typically, the original serial code is in the form of a large iteration for-loop, or several embedded for-loops, which perform calculations on equations that have no data dependency between iterations (e.g., matrix arithmetic). Additionally, when initially profiling the entire program with tools similar to the GNU command line based\n   \n    gprof\n   \n   or NVIDIA's\n   \n    nvprof\n   \n   visual based profiler (either profiler preferably run against typical representative data), the section(s) to be parallelized must make up a fair percentage of the program's total run time. This requirement will both maximize the speedup that can be obtained (Amdahl's law) and minimize the impact that data transfer time between the CPU and the GPU will have on the overall speedup.\n\n\nOnce a candidate massively parallelizable code segment has been identified, it then needs to be converted from serial code to parallel code or a CUDA kernel. If a parallelizing compiler were available that could automatically do this conversion without input from the user and also give a near-optimal, correct solution, then that would save a great deal of time, money, and effort. Unfortunately, such a tool does not yet exist. This leaves two options: (1) convert the code through complex planning and programming in CUDA, OpenCL, or similar; or (2) use a compiler directive language, such as OpenACC, hiCUDA, or similar. Although using a compiler directive language to place parallelizing \"hints\" in the code for the compiler can save a great deal of programming time, it is still an iterative process and the optimum run time obtained is not guaranteed. However, this method has seen a\n\n\n\n\n![Block diagram of the Intel Core M Processor SoC architecture. The diagram shows the integration of the CPU, GPU, and System Agent. The CPU consists of multiple CPU cores connected to LLC cache slices via a SoC ring interconnect. The GPU (Intel Processor Graphics Gen8) is connected to the SoC ring interconnect via the GTI interface. The System Agent contains the Display controller, Memory controller, and PCIe interface, also connected to the SoC ring interconnect. The GPU architecture is detailed, showing a Slice of 24 EUs, each containing 8 Subslices of 8 EUs each, with Local thread dispatchers and Sampler L1 L2 cache units. The GPU also includes Atomics, Barriers, L3 data cache, and Shared local memory.](images/image_0335.jpeg)\n\n\n**Intel Core M Processor**\n\n\n**Intel Processor Graphics Gen8**\n\n\n**Slice: 24 EUs**\n\n\n**Subslice: 8 EUs**\n\n\n**Fixed function units**\n\n\n**GTI**\n\n\n**CPU core**\n\n\n**SoC ring interconnect**\n\n\n**LLC cache slice**\n\n\n**System agent**\n\n\n**Display controller**\n\n\n**Memory controller**\n\n\n**PCIe**\n\n\n**Atomics, Barriers**\n\n\n**L3 data cache**\n\n\n**Shared local memory**\n\n\nBlock diagram of the Intel Core M Processor SoC architecture. The diagram shows the integration of the CPU, GPU, and System Agent. The CPU consists of multiple CPU cores connected to LLC cache slices via a SoC ring interconnect. The GPU (Intel Processor Graphics Gen8) is connected to the SoC ring interconnect via the GTI interface. The System Agent contains the Display controller, Memory controller, and PCIe interface, also connected to the SoC ring interconnect. The GPU architecture is detailed, showing a Slice of 24 EUs, each containing 8 Subslices of 8 EUs each, with Local thread dispatchers and Sampler L1 L2 cache units. The GPU also includes Atomics, Barriers, L3 data cache, and Shared local memory.\n\n\n**Figure 19.12**\n   Intel Core M Processor SoC\n\n\ngrowing interest over the past several years, and the newer versions of the CUDA compiler support the OpenACC language. Yet, a well-planned/engineered and coded CUDA program will almost always give the best runtimes to date."
        }
      ]
    },
    {
      "name": "Control Unit Operation",
      "sections": [
        {
          "name": "Micro-Operations",
          "content": "We have seen that the operation of a computer, in executing a program, consists of a sequence of instruction cycles, with one machine instruction per cycle. Of course, we must remember that this sequence of instruction cycles is not necessarily the same as the\n   *written sequence*\n   of instructions that make up the program, because of the existence of branching instructions. What we are referring to here is the\n   *execution time sequence*\n   of instructions.\n\n\nWe have further seen that each instruction cycle is made up of a number of smaller units. One subdivision that we found convenient is fetch, indirect, execute, and interrupt, with only fetch and execute cycles always occurring.\n\n\nTo design a control unit, however, we need to break down the description further. In our discussion of pipelining in Chapter 14, we began to see that a further decomposition is possible. In fact, we will see that each of the smaller cycles involves a series of steps, each of which involves the processor registers. We will refer to these steps as\n   **micro-operations**\n   . The prefix\n   *micro*\n   refers to the fact that each step is very simple and accomplishes very little. Figure 20.1 depicts the relationship among the various concepts we have been discussing. To summarize, the execution of a program consists of the sequential execution of instructions. Each instruction is executed during an instruction cycle made up of shorter subcycles (e.g., fetch, indirect, execute, interrupt). The execution of each subcycle involves one or more shorter operations, that is, micro-operations.\n\n\nMicro-operations are the functional, or atomic, operations of a processor. In this section, we will examine micro-operations to gain an understanding of how the events of any instruction cycle can be described as a sequence of such micro-operations. A simple example will be used. In the remainder of this chapter, we then show how the concept of micro-operations serves as a guide to the design of the control unit.\n\n\n\n\n**The Fetch Cycle**\n\n\nWe begin by looking at the fetch cycle, which occurs at the beginning of each instruction cycle and causes an instruction to be fetched from memory. For purposes of discussion, we assume the organization depicted in Figure 14.6 (\n   *Data Flow, Fetch Cycle*\n   ). Four registers are involved:\n\n\n  * ■\n    **Memory address register (MAR)**\n    : Is connected to the address lines of the system bus. It specifies the address in memory for a read or write operation.\n  * ■\n    **Memory buffer register (MBR)**\n    : Is connected to the data lines of the system bus. It contains the value to be stored in memory or the last value read from memory.\n\n\n\n\n![Hierarchical diagram of Program Execution](images/image_0336.jpeg)\n\n\nThe diagram illustrates the hierarchical structure of program execution. At the top level is a green box labeled \"Program execution\". Below it, three gray boxes labeled \"Instruction cycle\" are connected by lines. An ellipsis \"...\" is positioned between the second and third \"Instruction cycle\" boxes. Each \"Instruction cycle\" box has lines connecting to four subcycles: \"Fetch\", \"Indirect\", \"Execute\", and \"Interrupt\". Each of these subcycle boxes has lines connecting to three micro-operation boxes labeled \"\n    \n     \\mu\n    \n    OP\".\n\n\nHierarchical diagram of Program Execution\n\n\n**Figure 20.1**\n   Constituent Elements of a Program Execution\n\n\n  * ■\n    **Program counter (PC):**\n    Holds the address of the next instruction to be fetched.\n  * ■\n    **Instruction register (IR):**\n    Holds the last instruction fetched.\n\n\nLet us look at the sequence of events for the fetch cycle from the point of view of its effect on the processor registers. An example appears in Figure 20.2. At the beginning of the fetch cycle, the address of the next instruction to be executed is in the program counter (PC); in this case, the address is 1100100. The first step is to move that address to the memory address register (MAR) because this is the only register connected to the address lines of the system bus. The second step is to bring in the instruction. The desired address (in the MAR) is placed on the address bus, the control unit issues a READ command on the control bus, and the result appears on the data bus and is copied into the memory buffer register (MBR). We also need to increment the PC by the instruction length to get ready for the next instruction. Because these two actions (read word from memory, increment PC) do not interfere with each other, we can do them simultaneously to save time. The third step is to move the contents of the MBR to the instruction register (IR). This frees up the MBR for use during a possible indirect cycle.\n\n\nThus, the simple fetch cycle actually consists of three steps and four micro-operations. Each micro-operation involves the movement of data into or out of a register. So long as these movements do not interfere with one another, several of them can take place during one step, saving time. Symbolically, we can write this sequence of events as follows:\n\n\n\n   \\begin{aligned} t_1: \\text{MAR} &\\leftarrow (\\text{PC}) \\\\ t_2: \\text{MBR} &\\leftarrow \\text{Memory} \\\\ &\\quad \\text{PC} \\leftarrow (\\text{PC}) + I \\\\ t_3: \\text{IR} &\\leftarrow (\\text{MBR}) \\end{aligned}\n  \nwhere\n   \n    I\n   \n   is the instruction length. We need to make several comments about this sequence. We assume that a clock is available for timing purposes and that it emits regularly spaced clock pulses. Each clock pulse defines a time unit. Thus, all time units are\n\n\n\n(a) Beginning (before\n      \n       t_1\n      \n      ) | (b) After first step\ntMAR\n         \n\n\n\n\n\n          MBR\n         \n\n\n\n\n\n          PC\n         \n\n          000000001100100\n         \n\n\n\n          IR\n         \n\n\n\n\n\n          AC | tMAR |  | MBR |  | PC | 000000001100100 | IR |  | AC |  | MAR\n         \n\n          000000001100100\n         \n\n\n\n          MBR\n         \n\n\n\n\n\n          PC\n         \n\n          000000001100100\n         \n\n\n\n          IR\n         \n\n\n\n\n\n          AC | MAR | 000000001100100 | MBR |  | PC | 000000001100100 | IR |  | AC | \ntMAR | \nMBR | \nPC | 000000001100100\nIR | \nAC | \nMAR | 000000001100100\nMBR | \nPC | 000000001100100\nIR | \nAC | \n(c) After second step | (d) After third step\nMAR\n         \n\n          000000001100100\n         \n\n\n\n          MBR\n         \n\n          000100000100000\n         \n\n\n\n          PC\n         \n\n          000000001100101\n         \n\n\n\n          IR\n         \n\n\n\n\n\n          AC | MAR | 000000001100100 | MBR | 000100000100000 | PC | 000000001100101 | IR |  | AC |  | MAR\n         \n\n          000000001100100\n         \n\n\n\n          MBR\n         \n\n          000100000100000\n         \n\n\n\n          PC\n         \n\n          000000001100101\n         \n\n\n\n          IR\n         \n\n          000100000100000\n         \n\n\n\n          AC | MAR | 000000001100100 | MBR | 000100000100000 | PC | 000000001100101 | IR | 000100000100000 | AC | \nMAR | 000000001100100\nMBR | 000100000100000\nPC | 000000001100101\nIR | \nAC | \nMAR | 000000001100100\nMBR | 000100000100000\nPC | 000000001100101\nIR | 000100000100000\nAC | \n\n\n**Figure 20.2**\n   Sequence of Events, Fetch Cycle\n\n\nof equal duration. Each micro-operation can be performed within the time of a single time unit. The notation\n   \n    (t_1, t_2, t_3)\n   \n   represents successive time units. In words, we have\n\n\n  * ■\n    **First time unit:**\n    Move contents of PC to MAR.\n  * ■\n    **Second time unit:**\n    Move contents of memory location specified by MAR to MBR. Increment by\n    \n     I\n    \n    the contents of the PC.\n  * ■\n    **Third time unit:**\n    Move contents of MBR to IR.\n\n\nNote that the second and third micro-operations both take place during the second time unit. The third micro-operation could have been grouped with the fourth without affecting the fetch operation:\n\n\n\n   \\begin{aligned} t_1: \\text{MAR} &\\leftarrow (\\text{PC}) \\\\ t_2: \\text{MBR} &\\leftarrow \\text{Memory} \\\\ t_3: \\text{PC} &\\leftarrow (\\text{PC}) + I \\\\ \\text{IR} &\\leftarrow (\\text{MBR}) \\end{aligned}\n  \nThe groupings of micro-operations must follow two simple rules:\n\n\n  * 1. The proper sequence of events must be followed. Thus\n    \n     (\\text{MAR} \\leftarrow (\\text{PC}))\n    \n    must precede\n    \n     (\\text{MBR} \\leftarrow \\text{Memory})\n    \n    because the memory read operation makes use of the address in the MAR.\n  * 2. Conflicts must be avoided. One should not attempt to read to and write from the same register in one time unit, because the results would be unpredictable. For example, the micro-operations\n    \n     (\\text{MBR} \\leftarrow \\text{Memory})\n    \n    and\n    \n     (\\text{IR} \\leftarrow \\text{MBR})\n    \n    should not occur during the same time unit.\n\n\nA final point worth noting is that one of the micro-operations involves an addition. To avoid duplication of circuitry, this addition could be performed by the ALU. The use of the ALU may involve additional micro-operations, depending on the functionality of the ALU and the organization of the processor. We defer a discussion of this point until later in this chapter.\n\n\nIt is useful to compare events described in this and the following subsections to Figure 3.5 (\n   *Example of Program Execution*\n   ). Whereas micro-operations are ignored in that figure, this discussion shows the micro-operations needed to perform the subcycles of the instruction cycle.\n\n\n\n\n**The Indirect Cycle**\n\n\nOnce an instruction is fetched, the next step is to fetch source operands. Continuing our simple example, let us assume a one-address instruction format, with direct and indirect addressing allowed. If the instruction specifies an indirect address, then an indirect cycle must precede the execute cycle. The data flow differs somewhat from that indicated in Figure 14.7 (\n   *Data Flow, Indirect Cycle*\n   ) and includes the following micro-operations:\n\n\n\n   \\begin{aligned} t_1: \\text{MAR} &\\leftarrow (\\text{IR}(\\text{Address})) \\\\ t_2: \\text{MBR} &\\leftarrow \\text{Memory} \\\\ t_3: \\text{IR}(\\text{Address}) &\\leftarrow (\\text{MBR}(\\text{Address})) \\end{aligned}\n  \nThe address field of the instruction is transferred to the MAR. This is then used to fetch the address of the operand. Finally, the address field of the IR is updated from the MBR, so that it now contains a direct rather than an indirect address.\n\n\nThe IR is now in the same state as if indirect addressing had not been used, and it is ready for the execute cycle. We skip that cycle for a moment, to consider the interrupt cycle.\n\n\n\n\n**The Interrupt Cycle**\n\n\nAt the completion of the execute cycle, a test is made to determine whether any enabled interrupts have occurred. If so, the interrupt cycle occurs. The nature of this cycle varies greatly from one machine to another. We present a very simple sequence of events, as illustrated in Figure 14.8 (\n   *Data Flow, Interrupt Cycle*\n   ). We have\n\n\nt1: MBR ← (PC)\nt2: MAR ← Save_Address\n     PC ← Routine_Address\nt3: Memory ← (MBR)\nIn the first step, the contents of the PC are transferred to the MBR, so that they can be saved for return from the interrupt. Then the MAR is loaded with the address at which the contents of the PC are to be saved, and the PC is loaded with the address of the start of the interrupt-processing routine. These two actions may each be a single micro-operation. However, because most processors provide multiple types and/or levels of interrupts, it may take one or more additional micro-operations to obtain the Save_Address and the Routine_Address before they can be transferred to the MAR and PC, respectively. In any case, once this is done, the final step is to store the MBR, which contains the old value of the PC, into memory. The processor is now ready to begin the next instruction cycle.\n\n\n\n\n**The Execute Cycle**\n\n\nThe fetch, indirect, and interrupt cycles are simple and predictable. Each involves a small, fixed sequence of micro-operations and, in each case, the same micro-operations are repeated each time around.\n\n\nThis is not true of the execute cycle. Because of the variety of opcodes, there are a number of different sequences of micro-operations that can occur. The control unit examines the opcode and generates a sequence of micro-operations based on the value of the opcode. This is referred to as instruction decoding.\n\n\nLet us consider several hypothetical examples.\n\n\nFirst, consider an add instruction:\n\n\nADD R1, X\nwhich adds the contents of the location X to register R1. The following sequence of micro-operations might occur:\n\n\nt1: MAR ← (IR(address))\nt2: MBR ← Memory\nt3: R1 ← (R1) + (MBR)\nWe begin with the IR containing the ADD instruction. In the first step, the address portion of the IR is loaded into the MAR. Then the referenced memory location is read. Finally, the contents of R1 and MBR are added by the ALU. Again, this is a simplified example. Additional micro-operations may be required to extract\n\n\nthe register reference from the IR and perhaps to stage the ALU inputs or outputs in some intermediate registers.\n\n\nLet us look at two more complex examples. A common instruction is increment and skip if zero:\n\n\nISZ X\n\n\nThe content of location X is incremented by 1. If the result is 0, the next instruction is skipped. A possible sequence of micro-operations is\n\n\nt_1: MAR \\leftarrow (IR(address))\nt_2: MBR \\leftarrow Memory\nt_3: MBR \\leftarrow (MBR) + 1\nt_4: Memory \\leftarrow (MBR)\n    If ((MBR) = 0) then (PC \\leftarrow (PC) + I)\nThe new feature introduced here is the conditional action. The PC is incremented if (MBR) = 0. This test and action can be implemented as one micro-operation. Note also that this micro-operation can be performed during the same time unit during which the updated value in MBR is stored back to memory.\n\n\nFinally, consider a subroutine call instruction. As an example, consider a branch-and-save-address instruction:\n\n\nBSA X\n\n\nThe address of the instruction that follows the BSA instruction is saved in location X, and execution continues at location X + I. The saved address will later be used for return. This is a straightforward technique for supporting subroutine calls. The following micro-operations suffice:\n\n\nt_1: MAR \\leftarrow (IR(address))\n      MBR \\leftarrow (PC)\nt_2: PC \\leftarrow (IR(address))\n      Memory \\leftarrow (MBR)\nt_3: PC \\leftarrow (PC) + I\nThe address in the PC at the start of the instruction is the address of the next instruction in sequence. This is saved at the address designated in the IR. The latter address is also incremented to provide the address of the instruction for the next instruction cycle.\n\n\n\n\n**The Instruction Cycle**\n\n\nWe have seen that each phase of the instruction cycle can be decomposed into a sequence of elementary micro-operations. In our example, there is one sequence each for the fetch, indirect, and interrupt cycles, and, for the execute cycle, there is one sequence of micro-operations for each opcode.\n\n\nTo complete the picture, we need to tie sequences of micro-operations together, and this is done in Figure 20.3. We assume a new 2-bit register called the\n   *instruction cycle code*\n   (ICC). The ICC designates the state of the processor in terms of which portion of the cycle it is in:\n\n\n00: Fetch\n\n\n01: Indirect\n\n\n\n\n![Flowchart for Instruction Cycle showing the sequence of operations based on the Instruction Cycle Count (ICC) and interrupt status.](images/image_0337.jpeg)\n\n\ngraph TD\n    Start(( )) --> ICC{ICC?}\n    ICC -- \"11 (interrupt)\" --> Setup[Setup interrupt]\n    Setup --> ICC11[ICC = 11]\n    ICC -- \"10 (execute)\" --> Opcode{Opcode}\n    Opcode --> Execute[Execute instruction]\n    Execute --> Interrupt{Interrupt for enabled interrupt?}\n    Interrupt -- \"Yes\" --> ICC11\n    Interrupt -- \"No\" --> ICC00[ICC = 00]\n    ICC -- \"01 indirect\" --> Read[Read address]\n    Read --> ICC10[ICC = 10]\n    ICC -- \"00 (fetch)\" --> Fetch[Fetch instruction]\n    Fetch --> Indirect{Indirect addressing?}\n    Indirect -- \"No\" --> ICC10\n    Indirect -- \"Yes\" --> ICC01[ICC = 01]\n    ICC11 --> Start\n    ICC00 --> Start\n    ICC10 --> Start\n    ICC01 --> Start\n  \nThe flowchart illustrates the Instruction Cycle. It begins with a decision point 'ICC?'. If the ICC is 11 (interrupt), it goes to 'Setup interrupt' and then sets ICC = 11. If the ICC is 10 (execute), it goes to 'Opcode' and then 'Execute instruction'. After execution, it checks 'Interrupt for enabled interrupt?'. If yes, it sets ICC = 11; if no, it sets ICC = 00. If the ICC is 01 (indirect), it goes to 'Read address' and sets ICC = 10. If the ICC is 00 (fetch), it goes to 'Fetch instruction' and then checks 'Indirect addressing?'. If no, it sets ICC = 10; if yes, it sets ICC = 01. Finally, the process loops back to the start.\n\n\nFlowchart for Instruction Cycle showing the sequence of operations based on the Instruction Cycle Count (ICC) and interrupt status.\n\n\n**Figure 20.3**\n   Flowchart for Instruction Cycle\n\n\n10: Execute\n\n\n11: Interrupt\n\n\nAt the end of each of the four cycles, the ICC is set appropriately. The indirect cycle is always followed by the execute cycle. The interrupt cycle is always followed by the fetch cycle (see Figure 14.4,\n   *The Instruction Cycle*\n   ). For both the fetch and execute cycles, the next cycle depends on the state of the system.\n\n\nThus, the flowchart of Figure 20.3 defines the complete sequence of micro-operations, depending only on the instruction sequence and the interrupt pattern. Of course, this is a simplified example. The flowchart for an actual processor would be more complex. In any case, we have reached the point in our discussion in which the operation of the processor is defined as the performance of a sequence of micro-operations. We can now consider how the control unit causes this sequence to occur."
        },
        {
          "name": "Control of the Processor",
          "content": "**Functional Requirements**\n\n\nAs a result of our analysis in the preceding section, we have decomposed the behavior or functioning of the processor into elementary operations, called micro-operations. By reducing the operation of the processor to its most fundamental level, we are able to define exactly what it is that the control unit must cause to happen. Thus, we can define the\n   *functional requirements*\n   for the control unit: those functions that the control unit must perform. A definition of these functional requirements is the basis for the design and implementation of the control unit.\n\n\nWith the information at hand, the following three-step process leads to a characterization of the control unit:\n\n\n  * 1. Define the basic elements of the processor.\n  * 2. Describe the micro-operations that the processor performs.\n  * 3. Determine the functions that the control unit must perform to cause the micro-operations to be performed.\n\n\nWe have already performed steps 1 and 2. Let us summarize the results. First, the basic functional elements of the processor are the following:\n\n\n  * ■ ALU\n  * ■ Registers\n  * ■ Internal data paths\n  * ■ External data paths\n  * ■ Control unit\n\n\nSome thought should convince you that this is a complete list. The ALU is the functional essence of the computer. Registers are used to store data internal to the processor. Some registers contain status information needed to manage instruction sequencing (e.g., a program status word). Others contain data that go to or come from the ALU, memory, and I/O modules. Internal data paths are used to move data between registers and between register and ALU. External data paths link registers to memory and I/O modules, often by means of a system bus. The control unit causes operations to happen within the processor.\n\n\nThe execution of a program consists of operations involving these processor elements. As we have seen, these operations consist of a sequence of micro-operations. Upon review of Section 20.1, the reader should see that all micro-operations fall into one of the following categories:\n\n\n  * ■ Transfer data from one register to another.\n  * ■ Transfer data from a register to an external interface (e.g., system bus).\n  * ■ Transfer data from an external interface to a register.\n  * ■ Perform an arithmetic or logic operation, using registers for input and output.\n\n\nAll of the micro-operations needed to perform one instruction cycle, including all of the micro-operations to execute every instruction in the instruction set, fall into one of these categories.\n\n\nWe can now be somewhat more explicit about the way in which the control unit functions. The control unit performs two basic tasks:\n\n\n  * ■\n    **Sequencing:**\n    The control unit causes the processor to step through a series of micro-operations in the proper sequence, based on the program being executed.\n  * ■\n    **Execution:**\n    The control unit causes each micro-operation to be performed.\n\n\nThe preceding is a functional description of what the control unit does. The key to how the control unit operates is the use of control signals.\n\n\n\n\n**Control Signals**\n\n\nWe have defined the elements that make up the processor (ALU, registers, data paths) and the micro-operations that are performed. For the control unit to perform its function, it must have inputs that allow it to determine the state of the system and outputs that allow it to control the behavior of the system. These are the external specifications of the control unit. Internally, the control unit must have the logic required to perform its sequencing and execution functions. We defer a discussion of the internal operation of the control unit to Section 20.3 and Chapter 21. The remainder of this section is concerned with the interaction between the control unit and the other elements of the processor.\n\n\nFigure 20.4 is a general model of the control unit, showing all of its inputs and outputs. The inputs are:\n\n\n  * ■\n    **Clock:**\n    This is how the control unit “keeps time.” The control unit causes one micro-operation (or a set of simultaneous micro-operations) to be performed for each clock pulse. This is sometimes referred to as the processor cycle time, or the clock cycle time.\n  * ■\n    **Instruction register:**\n    The opcode and addressing mode of the current instruction are used to determine which micro-operations to perform during the execute cycle.\n  * ■\n    **Flags:**\n    These are needed by the control unit to determine the status of the processor and the outcome of previous ALU operations. For example, for the increment-and-skip-if-zero (ISZ) instruction, the control unit will increment the PC if the zero flag is set.\n  * ■\n    **Control signals from control bus:**\n    The control bus portion of the system bus provides signals to the control unit.\n\n\n\n\n![Block Diagram of the Control Unit](images/image_0338.jpeg)\n\n\nThe diagram illustrates the Control Unit as a central rectangular block. It has several inputs and outputs:\n\n\n  * **Inputs to the Control Unit:**\n  * An arrow from the\n       **Instruction register**\n       (a box above the unit) pointing down into the unit.\n  * Three arrows from the\n       **Flags**\n       (represented by a vertical line with dots) pointing into the unit.\n  * An arrow from the\n       **Clock**\n       pointing into the unit.\n  * **Outputs from the Control Unit:**\n  * An arrow labeled\n       **Control signals within CPU**\n       pointing from the unit to the right.\n  * An arrow labeled\n       **Control signals to control bus**\n       pointing from the unit to a vertical bar on the right labeled\n       **Control bus**\n       .\n  * **Inputs to the Control Unit from the Control Bus:**\n  * An arrow labeled\n       **Control signals from control bus**\n       pointing from the\n       **Control bus**\n       into the unit.\n\n\nBlock Diagram of the Control Unit\n\n\n**Figure 20.4**\n   Block Diagram of the Control Unit\n\n\nThe outputs are as follows:\n\n\n  * ■\n    **Control signals within the processor:**\n    These are two types: those that cause data to be moved from one register to another, and those that activate specific ALU functions.\n  * ■\n    **Control signals to control bus:**\n    These are also of two types: control signals to memory, and control signals to the I/O modules.\n\n\nThree types of control signals are used: those that activate an ALU function; those that activate a data path; and those that are signals on the external system bus or other external interface. All of these signals are ultimately applied directly as binary inputs to individual logic gates.\n\n\nLet us consider again the fetch cycle to see how the control unit maintains control. The control unit keeps track of where it is in the instruction cycle. At a given point, it knows that the fetch cycle is to be performed next. The first step is to transfer the contents of the PC to the MAR. The control unit does this by activating the control signal that opens the gates between the bits of the PC and the bits of the MAR. The next step is to read a word from memory into the MBR and increment the PC. The control unit does this by sending the following control signals simultaneously:\n\n\n  * ■ A control signal that opens gates, allowing the contents of the MAR onto the address bus;\n  * ■ A memory read control signal on the control bus;\n  * ■ A control signal that opens the gates, allowing the contents of the data bus to be stored in the MBR;\n  * ■ Control signals to logic that add 1 to the contents of the PC and store the result back to the PC.\n\n\nFollowing this, the control unit sends a control signal that opens gates between the MBR and the IR.\n\n\nThis completes the fetch cycle except for one thing: The control unit must decide whether to perform an indirect cycle or an execute cycle next. To decide this, it examines the IR to see if an indirect memory reference is made.\n\n\nThe indirect and interrupt cycles work similarly. For the execute cycle, the control unit begins by examining the opcode and, on the basis of that, decides which sequence of micro-operations to perform for the execute cycle.\n\n\n\n\n**A Control Signals Example**\n\n\nTo illustrate the functioning of the control unit, let us examine a simple example. Figure 20.5 illustrates the example. This is a simple processor with a single accumulator (AC). The data paths between elements are indicated. The control paths for signals emanating from the control unit are not shown, but the terminations of control signals are labeled\n   \n    C_i\n   \n   and indicated by a circle. The control unit receives inputs from the clock, the IR, and flags. With each clock cycle, the control unit\n\n\n\n\n![Figure 20.5: Data Paths and Control Signals. This diagram illustrates the internal data paths and control signals of a processor. It shows several registers: Memory Buffer Register (MBR), Memory Address Register (MAR), Program Counter (PC), Instruction Register (IR), and Accumulator (AC). The ALU (Arithmetic Logic Unit) is also shown, along with a Control unit and a Clock. Data paths are indicated by solid lines with arrows, and control signals are indicated by lines ending in circles. The MBR and MAR are connected to the system bus (C5, C11, C12, C0). The PC and IR are connected to each other and to the system bus (C1, C3, C4, C8, C13). The ALU and AC are connected to the system bus (C6, C7, C9, C10). The Control unit receives inputs from the PC, IR, and ALU (Flags) and sends control signals to the MBR, MAR, PC, IR, ALU, and the system bus. A clock signal is also shown.](images/image_0339.jpeg)\n\n\nFigure 20.5: Data Paths and Control Signals. This diagram illustrates the internal data paths and control signals of a processor. It shows several registers: Memory Buffer Register (MBR), Memory Address Register (MAR), Program Counter (PC), Instruction Register (IR), and Accumulator (AC). The ALU (Arithmetic Logic Unit) is also shown, along with a Control unit and a Clock. Data paths are indicated by solid lines with arrows, and control signals are indicated by lines ending in circles. The MBR and MAR are connected to the system bus (C5, C11, C12, C0). The PC and IR are connected to each other and to the system bus (C1, C3, C4, C8, C13). The ALU and AC are connected to the system bus (C6, C7, C9, C10). The Control unit receives inputs from the PC, IR, and ALU (Flags) and sends control signals to the MBR, MAR, PC, IR, ALU, and the system bus. A clock signal is also shown.\n\n\n**Figure 20.5**\n   Data Paths and Control Signals\n\n\nreads all of its inputs and emits a set of control signals. Control signals go to three separate destinations:\n\n\n  * ■\n    **Data paths:**\n    The control unit controls the internal flow of data. For example, on instruction fetch, the contents of the memory buffer register are transferred to the IR. For each path to be controlled, there is a switch (indicated by a circle in the figure). A control signal from the control unit temporarily opens the gate to let data pass.\n  * ■\n    **ALU:**\n    The control unit controls the operation of the ALU by a set of control signals. These signals activate various logic circuits and gates within the ALU.\n  * ■\n    **System bus:**\n    The control unit sends control signals out onto the control lines of the system bus (e.g., memory READ).\n\n\nThe control unit must maintain knowledge of where it is in the instruction cycle. Using this knowledge, and by reading all of its inputs, the control unit emits a sequence of control signals that causes micro-operations to occur. It uses the clock pulses to time the sequence of events, allowing time between events for signal levels to stabilize. Table 20.1 indicates the control signals that are needed for some of the micro-operation sequences described earlier. For simplicity, the data and control paths for incrementing the PC and for loading the fixed addresses into the PC and MAR are not shown.\n\n\nIt is worth pondering the minimal nature of the control unit. The control unit is the engine that runs the entire computer. It does this based only on knowing the instructions to be executed and the nature of the results of arithmetic and logical operations (e.g., positive, overflow, etc.). It never gets to see the data being processed or the actual results produced. And it controls everything with a few control signals to points within the processor and a few control signals to the system bus.\n\n\n**Table 20.1**\n\n | Micro-operations | Active Control Signals\nFetch: | t_1\n      \n      :\n      \n       MAR \\leftarrow (PC) | C_2\nt_2\n      \n      :\n      \n       MBR \\leftarrow \\text{Memory}\n      \n\n\n       PC \\leftarrow (PC) + 1 | C_5, C_R\nt_3\n      \n      :\n      \n       IR \\leftarrow (MBR) | C_4\nIndirect: | t_1\n      \n      :\n      \n       MAR \\leftarrow (IR(\\text{Address})) | C_8\nt_2\n      \n      :\n      \n       MBR \\leftarrow \\text{Memory} | C_5, C_R\nt_3\n      \n      :\n      \n       IR(\\text{Address}) \\leftarrow (MBR(\\text{Address})) | C_4\nInterrupt: | t_1\n      \n      :\n      \n       MBR \\leftarrow (PC) | C_1\nt_2\n      \n      :\n      \n       MAR \\leftarrow \\text{Save-address}\n      \n\n\n       PC \\leftarrow \\text{Routine-address} | \nt_3\n      \n      :\n      \n       \\text{Memory} \\leftarrow (MBR) | C_{12}, C_W\n\n\nC_R\n   \n   = Read control signal to system bus.\n\n\nC_W\n   \n   = Write control signal to system bus.\n\n\n\n\n**Internal Processor Organization**\n\n\nFigure 20.5 indicates the use of a variety of data paths. The complexity of this type of organization should be clear. More typically, some sort of internal bus arrangement, as was suggested in Figure 14.2 (\n   *Internal Structure of the CPU*\n   ), will be used.\n\n\nUsing an internal processor bus, Figure 20.5 can be rearranged as shown in Figure 20.6. A single internal bus connects the ALU and all processor registers. Gates and control signals are provided for movement of data onto and off the bus from each register. Additional control signals control data transfer to and from the system (external) bus and the operation of the ALU.\n\n\nTwo new registers, labeled Y and Z, have been added to the organization. These are needed for the proper operation of the ALU. When an operation involving two operands is performed, one can be obtained from the internal bus, but the other must be obtained from another source. The AC could be used for this purpose, but this limits the flexibility of the system and would not work with a processor with multiple general-purpose registers. Register Y provides temporary storage for the other input. The ALU is a combinatorial circuit (see Chapter 11) with no internal storage. Thus, when control signals activate an ALU function, the input to the ALU is transformed to the output. Therefore, the output of the ALU cannot be directly connected to the bus, because this output would feed back to the input. Register Z provides temporary output storage. With this arrangement, an operation to add a value from memory to the AC would have the following steps:\n\n\nt_1: MAR \\leftarrow (IR(\\text{address}))\nt_2: MBR \\leftarrow \\text{Memory}\nt_3: Y \\leftarrow (MBR)\nt_4: Z \\leftarrow (AC) + (Y)\nt_5: AC \\leftarrow (Z)\nOther organizations are possible, but, in general, some sort of internal bus or set of internal buses is used. The use of common data paths simplifies the\n\n\n\n\n![Diagram of a CPU with an internal bus. The components are arranged vertically: Control unit, IR (Instruction Register), PC (Program Counter), MAR (Memory Address Register), MBR (Memory Buffer Register), AC (Accumulator), Y (temporary register), ALU (Arithmetic Logic Unit), and Z (overflow flag). The Control unit has bidirectional arrows with the IR and PC. The PC has a unidirectional arrow to the MAR. The MAR has a unidirectional arrow to the MBR. The MBR has bidirectional arrows with the AC and the Internal CPU bus. The AC has a unidirectional arrow to the Y register. The Y register has a unidirectional arrow to the ALU. The ALU has a unidirectional arrow to the Z register. The Internal CPU bus is a vertical bar on the right with bidirectional connections to the MAR, MBR, AC, Y, and Z registers. Labels 'Address lines' and 'Data lines' point to the bus connections from the MAR and MBR respectively.](images/image_0340.jpeg)\n\n\nDiagram of a CPU with an internal bus. The components are arranged vertically: Control unit, IR (Instruction Register), PC (Program Counter), MAR (Memory Address Register), MBR (Memory Buffer Register), AC (Accumulator), Y (temporary register), ALU (Arithmetic Logic Unit), and Z (overflow flag). The Control unit has bidirectional arrows with the IR and PC. The PC has a unidirectional arrow to the MAR. The MAR has a unidirectional arrow to the MBR. The MBR has bidirectional arrows with the AC and the Internal CPU bus. The AC has a unidirectional arrow to the Y register. The Y register has a unidirectional arrow to the ALU. The ALU has a unidirectional arrow to the Z register. The Internal CPU bus is a vertical bar on the right with bidirectional connections to the MAR, MBR, AC, Y, and Z registers. Labels 'Address lines' and 'Data lines' point to the bus connections from the MAR and MBR respectively.\n\n\nFigure 20.6 CPU with Internal Bus\n\n\ninterconnection layout and the control of the processor. Another practical reason for the use of an internal bus is to save space.\n\n\n\n\n**The Intel 8085**\n\n\nTo illustrate some of the concepts introduced thus far in this chapter, let us consider the Intel 8085. Its organization is shown in Figure 20.7. Several key components that may not be self-explanatory are:\n\n\n  * ■\n    **Incrementer/decrementer address latch:**\n    Logic that can add 1 to or subtract 1 from the contents of the stack pointer or program counter. This saves time by avoiding the use of the ALU for this purpose.\n  * ■\n    **Interrupt control:**\n    This module handles multiple levels of interrupt signals.\n  * ■\n    **Serial I/O control:**\n    This module interfaces to devices that communicate 1 bit at a time.\n\n\nTable 20.2 describes the external signals into and out of the 8085. These are linked to the external system bus. These signals are the interface between the 8085 processor and the rest of the system (Figure 20.8).\n\n\n\n\n![Intel 8085 CPU Block Diagram](images/image_0341.jpeg)\n\n\nThe diagram illustrates the internal architecture of the Intel 8085 CPU. At the top, external control signals are shown:\n    **INTA**\n    ,\n    **RST 6.5**\n    ,\n    **TRAP**\n    ,\n    **INTR**\n    ,\n    **RST 5.5**\n    ,\n    **RST 7.5**\n    ,\n    **SID**\n    , and\n    **SOD**\n    . These connect to the\n    **Interrupt control**\n    block and the\n    **Serial I/O control**\n    block, which are connected to the\n    **8-bit internal data bus**\n    .\n\n\nThe\n    **8-bit internal data bus**\n    connects to several internal components:\n\n\n  * **Registers:**\n**(8) Accumulator**\n     ,\n     **(8) temp. reg.**\n     ,\n     **(8) flags**\n     ,\n     **(8) instruction register**\n     , and the\n     **register array**\n     . The register array contains\n     **B reg.**\n     ,\n     **C reg.**\n     ,\n     **D reg.**\n     ,\n     **E reg.**\n     ,\n     **H reg.**\n     ,\n     **L reg.**\n     ,\n     **stack pointer**\n     ,\n     **program counter**\n     ,\n     **incrementer/**\n     , and\n     **decrementer address latch**\n     .\n  * **ALU (Arithmetic Logic Unit):**\n     Receives inputs from the Accumulator, temp. reg., and flags. Its output feeds back into the Accumulator and the instruction decoder.\n  * **Timing and control module:**\n     A central block that generates control signals. It has inputs for\n     **Power supply**\n     (+5V, GND),\n     **X1**\n     , and\n     **X2**\n     . It outputs\n     **Clk Out**\n     ,\n     **Ready**\n     ,\n     **RD**\n     ,\n     **WR**\n     ,\n     **ALE**\n     ,\n     **S\n      \n       0**\n     ,\n     **S\n      \n       1**\n     ,\n     **IO/M**\n     ,\n     **Hold**\n     ,\n     **Reset in**\n     ,\n     **HLDA**\n     , and\n     **Reset out**\n     .\n  * **Address Buffers:**\n     Two\n     **(8) address buffer**\n     blocks. The first receives input from the instruction decoder and the register array, and outputs to the\n     **A\n      \n       15\n      \n      –A\n      \n       8\n      \n      address bus**\n     . The second receives input from the internal data bus and outputs to the\n     **AD\n      \n       7\n      \n      –AD\n      \n       0\n      \n      address/data bus**\n     .\n\n\nIntel 8085 CPU Block Diagram\n\n\n**Figure 20.7**\n   Intel 8085 CPU Block Diagram\n\n\nThe control unit is identified as having two components labeled (1) instruction decoder and machine cycle encoding and (2) timing and control. A discussion of the first component is deferred until the next section. The essence of the control unit is the timing and control module. This module includes a clock and accepts as inputs the current instruction and some external control signals. Its output consists of control signals to the other components of the processor plus control signals to the external system bus.\n\n\nThe timing of processor operations is synchronized by the clock and controlled by the control unit with control signals. Each instruction cycle is divided into from one to five\n   *machine cycles*\n   ; each machine cycle is in turn divided into from three to five\n   *states*\n   . Each state lasts one clock cycle. During a state, the processor performs one or a set of simultaneous micro-operations as determined by the control signals.\n\n\nThe number of machine cycles is fixed for a given instruction but varies from one instruction to another. Machine cycles are defined to be equivalent to bus accesses. Thus, the number of machine cycles for an instruction depends on the number of times the processor must communicate with external devices. For example, if an instruction consists of two 8-bit portions, then two machine cycles are required to fetch the instruction. If that instruction involves a 1-byte memory or I/O operation, then a third machine cycle is required for execution.\n\n\n**Table 20.2**\n\nAddress and Data Signals\nHigh Address (A15–A8) | The high-order 8 bits of a 16-bit address.\nAddress/Data (AD7–AD0) | The lower-order 8 bits of a 16-bit address or 8 bits of data. This multiplexing saves on pins.\nSerial Input Data (SID) | A single-bit input to accommodate devices that transmit serially (one bit at a time).\nSerial Output Data (SOD) | A single-bit output to accommodate devices that receive serially.\nTiming and Control Signals\nCLK (OUT) | The system clock. The CLK signal goes to peripheral chips and synchronizes their timing.\nX1, X2 | These signals come from an external crystal or other device to drive the internal clock generator.\nAddress Latch Enabled (ALE) | Occurs during the first clock state of a machine cycle and causes peripheral chips to store the address lines. This allows the address module (e.g., memory, I/O) to recognize that it is being addressed.\nStatus (S0, S1) | Control signals used to indicate whether a read or write operation is taking place.\nIO/M | Used to enable either I/O or memory modules for read and write operations.\nRead Control (RD) | Indicates that the selected memory or I/O module is to be read and that the data bus is available for data transfer.\nWrite Control (WR) | Indicates that data on the data bus is to be written into the selected memory or I/O location.\nMemory and I/O Initiated Symbols\nHold | Requests the CPU to relinquish control and use of the external system bus. The CPU will complete execution of the instruction presently in the IR and then enter a hold state, during which no signals are inserted by the CPU to the control, address, or data buses. During the hold state, the bus may be used for DMA operations.\nHold Acknowledge (HOLDA) | This control unit output signal acknowledges the HOLD signal and indicates that the bus is now available.\nREADY | Used to synchronize the CPU with slower memory or I/O devices. When an addressed device asserts READY, the CPU may proceed with an input (DBIN) or output (WR) operation. Otherwise, the CPU enters a wait state until the device is ready.\nInterrupt-Related Signals\nTRAP | Restart Interrupts (RST 7.5, 6.5, 5.5)\nInterrupt Request (INTR) | These five lines are used by an external device to interrupt the CPU. The CPU will not honor the request if it is in the hold state or if the interrupt is disabled. An interrupt is honored only at the completion of an instruction. The interrupts are in descending order of priority.\nInterrupt Acknowledge | Acknowledges an interrupt.\n\n\n\n\n**CPU Initialization**\n\n\n\n\n**RESET IN**\n\n\nCauses the contents of the PC to be set to zero. The CPU resumes execution at location zero.\n\n\n\n\n**RESET OUT**\n\n\nAcknowledges that the CPU has been reset. The signal can be used to reset the rest of the system.\n\n\n\n\n**Voltage and Ground**\n\n\n\n\n**VCC**\n\n\n+5-volt power supply\n\n\n\n\n**VSS**\n\n\nElectrical ground\n\n\nFigure 20.9 gives an example of 8085 timing, showing the value of external control signals. Of course, at the same time, the control unit generates internal control signals that control internal data transfers. The diagram shows the instruction cycle for an OUT instruction. Three machine cycles (\n   \n    M_1\n   \n   ,\n   \n    M_2\n   \n   ,\n   \n    M_3\n   \n   ) are needed. During the first, the OUT instruction is fetched. The second machine cycle fetches the second half of the instruction, which contains the number of the I/O device selected for output. During the third cycle, the contents of the AC are written out to the selected device over the data bus.\n\n\nThe Address Latch Enabled (ALE) pulse signals the start of each machine cycle from the control unit. The ALE pulse alerts external circuits. During timing state\n   \n    T_1\n   \n   of machine cycle\n   \n    M_1\n   \n   , the control unit sets the IO/M signal to indicate that this is a memory operation. Also, the control unit causes the contents of the PC\n\n\n\n\n![Pin configuration diagram for the Intel 8085 microprocessor. The diagram shows a 40-pin DIP package with pins numbered 1 to 40. Pin 1 is at the top left, and pin 40 is at the top right. The left side of the chip shows input signals: X1, X2, Reset out, SOD, SID, Trap, RST 7.5, RST 6.5, RST 5.5, INTR, INTA, AD0 through AD7, and Vss. The right side shows output and bidirectional signals: Vcc, HOLD, HLDA, CLK (out), Reset in, Ready, IO/M, S1, Vpp, RD, WR, S0, A15, A14, A13, A12, A11, A10, A9, and A8. Bidirectional signals are indicated by arrows pointing in both directions.](images/image_0342.jpeg)\n\n\nPin configuration diagram for the Intel 8085 microprocessor. The diagram shows a 40-pin DIP package with pins numbered 1 to 40. Pin 1 is at the top left, and pin 40 is at the top right. The left side of the chip shows input signals: X1, X2, Reset out, SOD, SID, Trap, RST 7.5, RST 6.5, RST 5.5, INTR, INTA, AD0 through AD7, and Vss. The right side shows output and bidirectional signals: Vcc, HOLD, HLDA, CLK (out), Reset in, Ready, IO/M, S1, Vpp, RD, WR, S0, A15, A14, A13, A12, A11, A10, A9, and A8. Bidirectional signals are indicated by arrows pointing in both directions.\n\n\n**Figure 20.8**\n   Intel 8085 Pin Configuration\n\n\n\n\n![Timing Diagram for Intel 8085 OUT Instruction. The diagram shows the sequence of events for the OUT Byte instruction across three machine cycles: M1, M2, and M3. It tracks the 3-MHz CLK, address lines A15-A8, data lines AD7-AD0, control signals ALE, RD, WR, and IOM, and the bus outputs PC out, PC+1->PC, INSTR->IR, X, PC out, PC+1->PC, byte->Z,W, WZ out, and A ->Port. The diagram is divided into four phases: Instruction fetch, Memory read, and Output write. The OUT Byte instruction is shown spanning M1, M2, and M3 cycles.](images/image_0343.jpeg)\n\n\nThe timing diagram for the Intel 8085 OUT instruction is organized into three machine cycles (M\n    \n     1\n    \n    , M\n    \n     2\n    \n    , M\n    \n     3\n    \n    ) and four states (T\n    \n     1\n    \n    , T\n    \n     2\n    \n    , T\n    \n     3\n    \n    , T\n    \n     4\n    \n    ) per cycle. The diagram tracks the following signals:\n\n\n  * **3-MHz CLK:**\n     A square wave clock signal.\n  * **A\n      \n       15\n      \n      - A\n      \n       8\n      \n      :**\n     Address bus lines, showing the address\n     \n      PC_H\n     \n     during the first two machine cycles and\n     **IO PORT**\n     during the third.\n  * **AD\n      \n       7\n      \n      - AD\n      \n       0\n      \n      :**\n     Data bus lines, showing\n     \n      PC_H\n     \n     ,\n     **INSTR**\n     , and\n     **ACCUM**\n     data.\n  * **ALE:**\n     Address Latch Enable signal, active during T\n     \n      1\n     \n     of each cycle.\n  * **\\overline{RD}\n      \n      :**\n     Read Control signal, active during T\n     \n      2\n     \n     and T\n     \n      3\n     \n     of M\n     \n      2\n     \n     and M\n     \n      3\n     \n     .\n  * **\\overline{WR}\n      \n      :**\n     Write Control signal, active during T\n     \n      4\n     \n     of M\n     \n      3\n     \n     .\n  * **\\overline{IOM}\n      \n      :**\n     Input/Output Mode signal, active during T\n     \n      1\n     \n     of M\n     \n      3\n     \n     .\n  * **Bus outputs:**\n\n      PC\\ out\n     \n     ,\n     \n      PC+1 \\to PC\n     \n     ,\n     \n      INSTR \\to IR\n     \n     ,\n     **X**\n     ,\n     \n      PC\\ out\n     \n     ,\n     \n      PC+1 \\to PC\n     \n     ,\n     \n      byte \\to Z,W\n     \n     ,\n     \n      WZ\\ out\n     \n     , and\n     \n      A \\to Port\n     \n     .\n\n\nThe diagram is divided into three phases by horizontal arrows:\n\n\n  * **Instruction fetch:**\n     Covers the first two machine cycles (M\n     \n      1\n     \n     and M\n     \n      2\n     \n     ).\n  * **Memory read:**\n     Covers the third machine cycle (M\n     \n      3\n     \n     ).\n  * **Output write:**\n     Covers the fourth machine cycle (M\n     \n      4\n     \n     ).\n\n\nTiming Diagram for Intel 8085 OUT Instruction. The diagram shows the sequence of events for the OUT Byte instruction across three machine cycles: M1, M2, and M3. It tracks the 3-MHz CLK, address lines A15-A8, data lines AD7-AD0, control signals ALE, RD, WR, and IOM, and the bus outputs PC out, PC+1->PC, INSTR->IR, X, PC out, PC+1->PC, byte->Z,W, WZ out, and A ->Port. The diagram is divided into four phases: Instruction fetch, Memory read, and Output write. The OUT Byte instruction is shown spanning M1, M2, and M3 cycles.\n\n\n**Figure 20.9**\n   Timing Diagram for Intel 8085 OUT Instruction\n\n\nto be placed on the address bus (A\n   \n    15\n   \n   through A\n   \n    8\n   \n   ) and the address/data bus (AD\n   \n    7\n   \n   through AD\n   \n    0\n   \n   ). With the falling edge of the ALE pulse, the other modules on the bus store the address.\n\n\nDuring timing state T\n   \n    2\n   \n   , the addressed memory module places the contents of the addressed memory location on the address/data bus. The control unit sets the Read Control (RD) signal to indicate a read, but it waits until T\n   \n    3\n   \n   to copy the data from the bus. This gives the memory module time to put the data on the bus and for the signal levels to stabilize. The final state, T\n   \n    4\n   \n   , is a\n   *bus idle*\n   state during which the processor decodes the instruction. The remaining machine cycles proceed in a similar fashion."
        },
        {
          "name": "Hardwired Implementation",
          "content": "We have discussed the control unit in terms of its inputs, output, and functions. We now turn to the topic of control unit implementation. A wide variety of techniques have been used. Most of these fall into one of two categories:\n\n\n  * ■ Hardwired implementation\n  * ■ Microprogrammed implementation\n\n\nIn a\n   **hardwired implementation**\n   , the control unit is essentially a state machine circuit. Its input logic signals are transformed into a set of output logic signals, which\n\n\nare the control signals. This approach is examined in this section. Microprogrammed implementation is the subject of Chapter 21.\n\n\n\n\n**Control Unit Inputs**\n\n\nFigure 20.4 depicts the control unit as we have so far discussed it. The key inputs are the IR, the clock, flags, and control bus signals. In the case of the flags and control bus signals, each individual bit typically has some meaning (e.g., overflow). The other two inputs, however, are not directly useful to the control unit.\n\n\nFirst consider the IR. The control unit makes use of the opcode and will perform different actions (issue a different combination of control signals) for different instructions. To simplify the control unit logic, there should be a unique logic input for each opcode. This function can be performed by a\n    *decoder*\n    , which takes an encoded input and produces a single output. In general, a decoder will have\n    \n     n\n    \n    binary inputs and\n    \n     2^n\n    \n    binary outputs. Each of the\n    \n     2^n\n    \n    different input patterns will activate a single unique output. Table 20.3 is an example for\n    \n     n = 4\n    \n    . The decoder for a control unit will typically have to be more complex than that, to account for variable-length opcodes. An example of the digital logic used to implement a decoder is presented in Chapter 11.\n\n\nThe clock portion of the control unit issues a repetitive sequence of pulses. This is useful for measuring the duration of micro-operations. Essentially, the period of the clock pulses must be long enough to allow the propagation of signals along\n\n\n**Table 20.3**\n   A Decoder with 4 Inputs and 16 Outputs\n\n\ndata paths and through processor circuitry. However, as we have seen, the control unit emits different control signals at different time units within a single instruction cycle. Thus, we would like a counter as input to the control unit, with a different control signal being used for\n   \n    T_1, T_2\n   \n   , and so forth. At the end of an instruction cycle, the control unit must feed back to the counter to reinitialize it at\n   \n    T_1\n   \n   .\n\n\nWith these two refinements, the control unit can be depicted as in Figure 20.10.\n\n\n\n\n**Control Unit Logic**\n\n\nTo define the hardwired implementation of a control unit, all that remains is to discuss the internal logic of the control unit that produces output control signals as a function of its input signals.\n\n\nEssentially, what must be done is, for each control signal, to derive a Boolean expression of that signal as a function of the inputs. This is best explained by example. Let us consider again our simple example illustrated in Figure 20.5. We saw in Table 20.1 the micro-operation sequences and control signals needed to control three of the four phases of the instruction cycle.\n\n\nLet us consider a single control signal,\n   \n    C_5\n   \n   . This signal causes data to be read from the external data bus into the MBR. We can see that it is used twice in Table 20.1. Let us define two new control signals,\n   \n    P\n   \n   and\n   \n    Q\n   \n   , that have the following interpretation:\n\n\n\nPQ = 00 | Fetch Cycle\nPQ = 01 | Indirect Cycle\nPQ = 10 | Execute Cycle\nPQ = 11 | Interrupt Cycle\n\n\nThen the following Boolean expression defines\n   \n    C_5\n   \n   :\n\n\nC_5 = \\bar{P} \\cdot \\bar{Q} \\cdot T_2 + \\bar{P} \\cdot Q \\cdot T_2\n\n\n\n\n![Diagram of a Control Unit with Decoded Inputs. An Instruction register feeds into a Decoder. The Decoder outputs control signals I0, I1, ..., Ik to the Control unit. A Timing generator, driven by a Clock, outputs time signals T1, T2, ..., Tn to the Control unit. The Control unit also receives inputs from Flags. The Control unit outputs control signals C0, C1, ..., Cm.](images/image_0344.jpeg)\n\n\nThe diagram illustrates the architecture of a control unit. At the top, an 'Instruction register' provides input to a 'Decoder' block. The 'Decoder' block has multiple outputs, labeled\n    \n     I_0, I_1, \\dots, I_k\n    \n    , which are fed into the 'Control unit'. To the left of the 'Control unit', a 'Timing generator' block is shown. It receives a 'Clock' signal as input and produces a sequence of time signals,\n    \n     T_1, T_2, \\dots, T_n\n    \n    , which are also fed into the 'Control unit'. The 'Control unit' block has several inputs: the decoded instruction signals\n    \n     I_0\n    \n    through\n    \n     I_k\n    \n    , the timing signals\n    \n     T_1\n    \n    through\n    \n     T_n\n    \n    , and a set of 'Flags' represented by three dots. The 'Control unit' block has multiple outputs, labeled\n    \n     C_0, C_1, \\dots, C_m\n    \n    .\n\n\nDiagram of a Control Unit with Decoded Inputs. An Instruction register feeds into a Decoder. The Decoder outputs control signals I0, I1, ..., Ik to the Control unit. A Timing generator, driven by a Clock, outputs time signals T1, T2, ..., Tn to the Control unit. The Control unit also receives inputs from Flags. The Control unit outputs control signals C0, C1, ..., Cm.\n\n\n**Figure 20.10**\n   Control Unit with Decoded Inputs\n\n\nThat is, the control signal\n   \n    C_5\n   \n   will be asserted during the second time unit of both the fetch and indirect cycles.\n\n\nThis expression is not complete.\n   \n    C_5\n   \n   is also needed during the execute cycle. For our simple example, let us assume that there are only three instructions that read from memory: LDA, ADD, and AND. Now we can define\n   \n    C_5\n   \n   as\n\n\nC_5 = \\bar{P} \\cdot \\bar{Q} \\cdot T_2 + \\bar{P} \\cdot Q \\cdot T_2 + P \\cdot \\bar{Q} \\cdot (LDA + ADD + AND) \\cdot T_2\n\n\nThis same process could be repeated for every control signal generated by the processor. The result would be a set of Boolean equations that define the behavior of the control unit and hence of the processor.\n\n\nTo tie everything together, the control unit must control the state of the instruction cycle. As was mentioned, at the end of each subcycle (fetch, indirect, execute, interrupt), the control unit issues a signal that causes the timing generator to reinitialize and issue\n   \n    T_1\n   \n   . The control unit must also set the appropriate values of\n   \n    P\n   \n   and\n   \n    Q\n   \n   to define the next subcycle to be performed.\n\n\nThe reader should be able to appreciate that in a modern complex processor, the number of Boolean equations needed to define the control unit is very large. The task of implementing a combinatorial circuit that satisfies all of these equations becomes extremely difficult. The result is that a far simpler approach, known as\n   *microprogramming*\n   , is usually used. This is the subject of the next chapter."
        }
      ]
    },
    {
      "name": "Microprogrammed Control",
      "sections": [
        {
          "name": "Basic Concepts",
          "content": "**Microinstructions**\n\n\nThe control unit seems a reasonably simple device. Nevertheless, to implement a control unit as an interconnection of basic logic elements is no easy task. The design must include logic for sequencing through micro-operations, for executing micro-operations, for interpreting opcodes, and for making decisions based on ALU flags. It is difficult to design and test such a piece of hardware. Furthermore, the design is relatively inflexible. For example, it is difficult to change the design if one wishes to add a new machine instruction.\n\n\nAn alternative, which has been used in many CISC processors, is to implement a\n   **microprogrammed control unit**\n   .\n\n\nConsider Table 21.1. In addition to the use of control signals, each micro-operation is described in symbolic notation. This notation looks suspiciously like a programming language. In fact it is a language, known as a\n   **microprogramming language**\n   . Each line describes a set of micro-operations occurring at one time and is known as a\n   **microinstruction**\n   . A sequence of instructions is known as a\n   **microprogram**\n   , or\n   *firmware*\n   . This latter term reflects the fact that a microprogram is midway between hardware and software. It is easier to design in firmware than hardware, but it is more difficult to write a firmware program than a software program.\n\n\nHow can we use the concept of microprogramming to implement a control unit? Consider that for each micro-operation, all that the control unit is allowed to do is generate a set of control signals. Thus, for any micro-operation, each control line emanating from the control unit is either on or off. This condition can, of course, be represented by a binary digit for each control line. So we could construct a\n   *control word*\n   in which each bit represents one control line. Then each micro-operation would be represented by a different pattern of 1s and 0s in the control word.\n\n\nSuppose we string together a sequence of control words to represent the sequence of micro-operations performed by the control unit. Next, we must recognize that the sequence of micro-operations is not fixed. Sometimes we have an indirect cycle; sometimes we do not. So let us put our control words in a memory, with each word having a unique address. Now add an address field to each control word, indicating the location of the next control word to be executed if a certain condition is true (e.g., the indirect bit in a memory-reference instruction is 1). Also, add a few bits to specify the condition.\n\n\n**Table 21.1**\n   Machine Instruction Set for Wilkes Example\n\n\n\nOrder | Effect of Order\nAn | C(Acc) + C(n)\n      \n      to\n      \n       Acc_1\nSn | C(Acc) - C(n)\n      \n      to\n      \n       Acc_1\nHn | C(n)\n      \n      to\n      \n       Acc_2\nVn | C(Acc_2) \\times C(n)\n      \n      to\n      \n       Acc\n      \n      , where\n      \n       C(n) \\ge 0\nTn | C(Acc_1)\n      \n      to\n      \n       n\n      \n      , 0 to\n      \n       Acc\nUn | C(Acc_1)\n      \n      to\n      \n       n\nRn | C(Acc) \\times 2^{(n+1)}\n      \n      to\n      \n       Acc\nLn | C(Acc) \\times 2^{n+1}\n      \n      to\n      \n       Acc\nGn | IF\n      \n       C(Acc) < 0\n      \n      , transfer control to\n      \n       n\n      \n      ; if\n      \n       C(Acc) \\ge 0\n      \n      , ignore (i.e., proceed serially)\nIn | Read next character on input mechanism into\n      \n       n\nOn | Send\n      \n       C(n)\n      \n      to output mechanism\n\n\nNotation:\n   \n    Acc\n   \n   = accumulator\n\n\nAcc_1\n   \n   = most significant half of accumulator\n\n\nAcc_2\n   \n   = least significant half of accumulator\n\n\nn\n   \n   = storage location\n   \n    n\n\n\nC(X)\n   \n   = contents of\n   \n    X\n   \n   (\n   \n    X\n   \n   = register or storage location)\n\n\nThe result is known as a\n   **horizontal microinstruction**\n   , an example of which is shown in Figure 21.1a. The format of the microinstruction or control word is as follows. There is one bit for each internal processor control line and one bit for each system bus control line. There is a condition field indicating the condition under which there should be a branch, and there is a field with the address of the microinstruction to be executed next when a branch is taken. Such a microinstruction is interpreted as follows:\n\n\n  * 1. To execute this microinstruction, turn on all the control lines indicated by a 1 bit; leave off all control lines indicated by a 0 bit. The resulting control signals will cause one or more micro-operations to be performed.\n  * 2. If the condition indicated by the condition bits is false, execute the next microinstruction in sequence.\n  * 3. If the condition indicated by the condition bits is true, the next microinstruction to be executed is indicated in the address field.\n\n\nFigure 21.2 shows how these control words or microinstructions could be arranged in a\n   **control memory**\n   . The microinstructions in each routine are to be executed sequentially. Each routine ends with a branch or jump instruction indicating where to go next. There is a special execute cycle routine whose only purpose is to signify that one of the machine instruction routines (AND, ADD, and so on) is to be executed next, depending on the current opcode.\n\n\nThe control memory of Figure 21.2 is a concise description of the complete operation of the control unit. It defines the sequence of micro-operations to be\n\n\n\n\n![Diagram (a) showing the format of a horizontal microinstruction. It is a single row of bits divided into four fields: a long field for Microinstruction address, a short field for Jump condition, a field for System bus control signals, and a field for Internal CPU control signals. The Jump condition field is further divided into sub-fields: Unconditional, Zero, Overflow, and Indirect bit.](images/image_0345.jpeg)\n\n\nMicroinstruction address\n\n\nJump condition\n\n\n  * —Unconditional\n  * —Zero\n  * —Overflow\n  * —Indirect bit\n\n\nSystem bus control signals\n\n\nInternal CPU control signals\n\n\nDiagram (a) showing the format of a horizontal microinstruction. It is a single row of bits divided into four fields: a long field for Microinstruction address, a short field for Jump condition, a field for System bus control signals, and a field for Internal CPU control signals. The Jump condition field is further divided into sub-fields: Unconditional, Zero, Overflow, and Indirect bit.\n\n\n(a) Horizontal microinstruction\n\n\n\n\n![Diagram (b) showing the format of a vertical microinstruction. It is a single row of bits divided into two main sections. The first section is for Microinstruction address and Jump condition. The second section is a large block labeled Function codes, which is further divided into multiple sub-fields.](images/image_0346.jpeg)\n\n\nMicroinstruction address\n\n\nJump condition\n\n\nFunction codes\n\n\nDiagram (b) showing the format of a vertical microinstruction. It is a single row of bits divided into two main sections. The first section is for Microinstruction address and Jump condition. The second section is a large block labeled Function codes, which is further divided into multiple sub-fields.\n\n\n(b) Vertical microinstruction\n\n\n**Figure 21.1**\n   Typical Microinstruction Formats\n\n\n\n\n![Figure 21.2: Organization of Control Memory. A vertical stack of memory blocks. The top block contains dots and is labeled 'Fetch cycle routine'. Below it is a block labeled 'Jump to indirect or execute'. The next block is labeled 'Indirect cycle routine'. Then 'Interrupt cycle routine'. Below that is 'Execute cycle beginning', which contains a block labeled 'Jump to opcode routine'. Below that is 'AND routine', containing a block labeled 'Jump to fetch or interrupt'. Below that is 'ADD routine', containing a block labeled 'Jump to fetch or interrupt'. At the bottom is a block labeled 'IOF routine', containing a block labeled 'Jump to fetch or interrupt'. Vertical dots separate the main stack from the bottom block.](images/image_0347.jpeg)\n\n\nFigure 21.2: Organization of Control Memory. A vertical stack of memory blocks. The top block contains dots and is labeled 'Fetch cycle routine'. Below it is a block labeled 'Jump to indirect or execute'. The next block is labeled 'Indirect cycle routine'. Then 'Interrupt cycle routine'. Below that is 'Execute cycle beginning', which contains a block labeled 'Jump to opcode routine'. Below that is 'AND routine', containing a block labeled 'Jump to fetch or interrupt'. Below that is 'ADD routine', containing a block labeled 'Jump to fetch or interrupt'. At the bottom is a block labeled 'IOF routine', containing a block labeled 'Jump to fetch or interrupt'. Vertical dots separate the main stack from the bottom block.\n\n\n**Figure 21.2**\n   Organization of Control Memory\n\n\nperformed during each cycle (fetch, indirect, execute, interrupt), and it specifies the sequencing of these cycles. If nothing else, this notation would be a useful device for documenting the functioning of a control unit for a particular computer. But it is more than that. It is also a way of implementing the control unit.\n\n\n\n\n**Microprogrammed Control Unit**\n\n\nThe control memory of Figure 21.2 contains a program that describes the behavior of the control unit. It follows that we could implement the control unit by simply executing that program.\n\n\nFigure 21.3 shows the key elements of such an implementation. The set of microinstructions is stored in the\n   *control memory*\n   . The\n   *control address register*\n   contains the address of the next microinstruction to be read. When a microinstruction is read from the control memory, it is transferred to a\n   *control buffer register*\n   . The left-hand portion of that register (see Figure 21.1a) connects to the control lines emanating from the control unit. Thus,\n   *reading*\n   a microinstruction from the control memory is the same as\n   *executing*\n   that microinstruction. The third element shown in the figure is a sequencing unit that loads the control address register and issues a read command.\n\n\n\n\n![Figure 21.3: Control Unit Microarchitecture. A block diagram showing the flow of data between three main components: Sequencing logic, Control address register, and Control memory. The Sequencing logic block has an arrow pointing to the Control address register. The Control address register has an arrow pointing down to the Control memory block. The Control memory block has an arrow pointing down to the Control buffer register. A 'Read' signal line originates from the Sequencing logic block and points to the Control memory block.](images/image_0348.jpeg)\n\n\ngraph TD\n    SL[Sequencing logic] --> CAR[Control address register]\n    CAR --> CM[Control memory]\n    CM --> CBR[Control buffer register]\n    SL -- Read --> CM\n  \nFigure 21.3: Control Unit Microarchitecture. A block diagram showing the flow of data between three main components: Sequencing logic, Control address register, and Control memory. The Sequencing logic block has an arrow pointing to the Control address register. The Control address register has an arrow pointing down to the Control memory block. The Control memory block has an arrow pointing down to the Control buffer register. A 'Read' signal line originates from the Sequencing logic block and points to the Control memory block.\n\n\nFigure 21.3 Control Unit Microarchitecture\n\n\nLet us examine this structure in greater detail, as depicted in Figure 21.4. Comparing this with Figure 21.3, we see that the control unit still has the same inputs (IR, ALU flags, clock) and outputs (control signals). The control unit functions as follows:\n\n\n  * 1. To execute an instruction, the sequencing logic unit issues a READ command to the control memory.\n  * 2. The word whose address is specified in the control address register is read into the control buffer register.\n  * 3. The content of the control buffer register generates control signals and next-address information for the sequencing logic unit.\n  * 4. The sequencing logic unit loads a new address into the control address register based on the next-address information from the control buffer register and the ALU flags.\n\n\nAll this happens during one clock pulse.\n\n\nThe last step just listed needs elaboration. At the conclusion of each microinstruction, the sequencing logic unit loads a new address into the control address register. Depending on the value of the ALU flags and the control buffer register, one of three decisions is made:\n\n\n  * ■\n    **Get the next instruction:**\n    Add 1 to the control address register.\n  * ■\n    **Jump to a new routine based on a jump microinstruction:**\n    Load the address field of the control buffer register into the control address register.\n  * ■\n    **Jump to a machine instruction routine:**\n    Load the control address register based on the opcode in the IR.\n\n\nFigure 21.4 shows two modules labeled\n   *decoder*\n   . The upper decoder translates the opcode of the IR into a control memory address. The lower decoder is not used for horizontal microinstructions but is used for\n   **vertical microinstructions**\n   (Figure 21.1b). As was mentioned, in a horizontal microinstruction every bit in the\n\n\n\n\n![Block diagram of a Microprogrammed Control Unit. The diagram shows a 'Control unit' block containing several components: an 'Instruction register' at the top, a 'Decoder' below it, a 'Control address register' below the decoder, a large 'Control memory' block, a 'Control buffer register' below the memory, and another 'Decoder' at the bottom. External inputs 'ALU Flags' and 'Clock' enter the 'Sequencing logic' block, which is also inside the 'Control unit'. The 'Sequencing logic' block has arrows pointing to the 'Control address register' and the 'Control memory' (labeled 'Read'). The 'Control memory' has an arrow pointing to the 'Control buffer register'. The bottom 'Decoder' has two output arrows: 'Control signals within CPU' and 'Control signals to system bus'.](images/image_0349.jpeg)\n\n\nBlock diagram of a Microprogrammed Control Unit. The diagram shows a 'Control unit' block containing several components: an 'Instruction register' at the top, a 'Decoder' below it, a 'Control address register' below the decoder, a large 'Control memory' block, a 'Control buffer register' below the memory, and another 'Decoder' at the bottom. External inputs 'ALU Flags' and 'Clock' enter the 'Sequencing logic' block, which is also inside the 'Control unit'. The 'Sequencing logic' block has arrows pointing to the 'Control address register' and the 'Control memory' (labeled 'Read'). The 'Control memory' has an arrow pointing to the 'Control buffer register'. The bottom 'Decoder' has two output arrows: 'Control signals within CPU' and 'Control signals to system bus'.\n\n\n**Figure 21.4**\n   Functioning of Microprogrammed Control Unit\n\n\ncontrol field attaches to a control line. In a vertical microinstruction, a code is used for each action to be performed [e.g.,\n   \n    MAR \\leftarrow (PC)\n   \n   ], and the decoder translates this code into individual control signals. The advantage of vertical microinstructions is that they are more compact (fewer bits) than horizontal microinstructions, at the expense of a small additional amount of logic and time delay.\n\n\n\n\n**Wilkes Control**\n\n\nAs was mentioned, Wilkes first proposed the use of a microprogrammed control unit in 1951 [WILK51]. This proposal was subsequently elaborated into a more detailed design [WILK53]. It is instructive to examine this seminal proposal.\n\n\nThe configuration proposed by Wilkes is depicted in Figure 21.5. The heart of the system is a matrix partially filled with diodes. During a machine cycle, one row of the matrix is activated with a pulse. This generates signals at those points where a diode is present (indicated by a dot in the diagram). The first part of the row generates the control signals that control the operation of the processor. The second part generates the\n\n\n\n\n![Diagram of Wilkes's Microprogrammed Control Unit showing the flow from the instruction register through two registers (II and I) to an address decoder, which then selects a row in a control memory matrix. The matrix outputs control signals and a conditional signal.](images/image_0350.jpeg)\n\n\nThe diagram illustrates Wilkes's Microprogrammed Control Unit. It starts with an 'instruction register' at the top left. Its output is split: one path goes through a gate (represented by a circle with a line) to 'Register II', and the other path goes through a gate to 'Register I'. A 'Clock' signal is shown with arrows pointing to gates between the instruction register and Register II, and between Register II and Register I. The output of Register I goes into an 'Address decoder' block. The 'Address decoder' also receives 'Control signals' as input. The output of the address decoder is a set of horizontal lines representing a row in a control memory matrix. The matrix is a grid of dots (representing control signals) and vertical lines. A bracket at the bottom of the matrix groups the vertical lines as 'Control signals'. A specific vertical line on the right side of the matrix is labeled 'Conditional signal'. A feedback loop at the top right shows a line from the matrix returning to the instruction register.\n\n\nDiagram of Wilkes's Microprogrammed Control Unit showing the flow from the instruction register through two registers (II and I) to an address decoder, which then selects a row in a control memory matrix. The matrix outputs control signals and a conditional signal.\n\n\n**Figure 21.5**\n   Wilkes's Microprogrammed Control Unit\n\n\naddress of the row to be pulsed in the next machine cycle. Thus, each row of the matrix is one microinstruction, and the layout of the matrix is the control memory.\n\n\nAt the beginning of the cycle, the address of the row to be pulsed is contained in Register I. This address is the input to the decoder, which, when activated by a clock pulse, activates one row of the matrix. Depending on the control signals, either the opcode in the instruction register or the second part of the pulsed row is passed into Register II during the cycle. Register II is then gated to Register I by a clock pulse. Alternating clock pulses are used to activate a row of the matrix and to transfer from Register II to Register I. The two-register arrangement is needed because the decoder is simply a combinatorial circuit; with only one register, the output would become the input during a cycle, causing an unstable condition.\n\n\nThis scheme is very similar to the horizontal microprogramming approach described earlier (Figure 21.1a). The main difference is this: In the previous description, the control address register could be incremented by one to get the next address. In the Wilkes scheme, the next address is contained in the microinstruction. To permit branching, a row must contain two address parts, controlled by a conditional signal (e.g., flag), as shown in the figure.\n\n\nHaving proposed this scheme, Wilkes provides an example of its use to implement the control unit of a simple machine. This example, the first known design of a microprogrammed processor, is worth repeating here because it illustrates many of the contemporary principles of microprogramming.\n\n\nThe processor of the hypothetical machine (the example machine by Wilkes) includes the following registers:\n\n\n\nA | Multiplicand\nB | Accumulator (least significant half)\nC | Accumulator (most significant half)\nD | Shift register\n\n\nIn addition, there are three registers and two 1-bit flags accessible only to the control unit. The registers are as follows:\n\n\n\nE | Serves as both a memory address register (MAR) and temporary storage\nF | Program counter\nG | Another temporary register; used for counting\n\n\nTable 21.1 lists the machine instruction set for this example. Table 21.2 is the complete set of microinstructions, expressed in symbolic form, that implements the control unit. Thus, a total of 38 microinstructions is all that is required to define the system completely.\n\n\nThe first full column gives the address (row number) of each microinstruction. Those addresses corresponding to opcodes are labeled. Thus, when the opcode for the add instruction (A) is encountered, the microinstruction at location 5 is executed. Columns 2 and 3 express the actions to be taken by the ALU and control unit, respectively. Each symbolic expression must be translated into a set of control signals (microinstruction bits). Columns 4 and 5 have to do with the setting and use of the two flags (flip-flops). Column 4 specifies the signal that sets the flag. For example, (1)C\n   \n    s\n   \n   means that flag number 1 is set by the sign bit of the number in register C. If column 5 contains a flag identifier, then columns 6 and 7 contain the two alternative microinstruction addresses to be used. Otherwise, column 6 specifies the address of the next microinstruction to be fetched.\n\n\nInstructions 0 through 4 constitute the fetch cycle. Microinstruction 4 presents the opcode to a decoder, which generates the address of a microinstruction corresponding to the machine instruction to be fetched. The reader should be able to deduce the complete functioning of the control unit from a careful study of Table 21.2.\n\n\n\n\n**Advantages and Disadvantages**\n\n\nThe principal advantage of the use of microprogramming to implement a control unit is that it simplifies the design of the control unit. Thus, it is both cheaper and less error prone to implement. A\n   *hardwired*\n   control unit must contain complex logic for sequencing through the many micro-operations of the instruction cycle. On the other hand, the decoders and sequencing logic unit of a microprogrammed control unit are very simple pieces of logic.\n\n\nThe principal disadvantage of a microprogrammed unit is that it will be somewhat slower than a hardwired unit of comparable technology. Despite this, microprogramming is the dominant technique for implementing control units in pure CISC architectures, due to its ease of implementation. RISC processors, with their simpler instruction format, typically use hardwired control units. We now examine the microprogrammed approach in greater detail.\n\n\n**Table 21.2**\nNotations:\n   \n    A, B, C, \\dots\n   \n   stand for the various registers in the arithmetical and control register units.\n   \n    C\n   \n   to\n   \n    D\n   \n   indicates that the switching circuits connect the output of register\n   \n    C\n   \n   to the input register\n   \n    D\n   \n   ;\n   \n    (D + A)\n   \n   to\n   \n    C\n   \n   indicates that the output register of\n   \n    A\n   \n   is connected to the one input of the adding unit (the output of\n   \n    D\n   \n   is permanently connected to the other input), and the output of the adder to register\n   \n    C\n   \n   . A numerical symbol\n   \n    n\n   \n   in quotes (e.g., “\n   \n    n\n   \n   ”) stands for the source whose output is the number\n   \n    n\n   \n   in units of the least significant digit.\n\n\n\n | Arithmetical Unit | Control Register Unit | Conditional Flip-Flop | Next Microinstruction\n |  |  |  | Set | Use | 0 | 1\n | 0 |  | F\n      \n      to\n      \n       G\n      \n      and\n      \n       E |  |  | 1 | \n | 1 |  | (G\n      \n      to “1”) to\n      \n       F |  |  | 2 | \n | 2 |  | Store to\n      \n       G |  |  | 3 | \n | 3 |  | G\n      \n      to\n      \n       E |  |  | 4 | \n | 4 |  | E\n      \n      to decoder |  |  | — | \nA | 5 | C\n      \n      to\n      \n       D |  |  |  | 16 | \nS | 6 | C\n      \n      to\n      \n       D |  |  |  | 17 | \nH | 7 | Store to\n      \n       B |  |  |  | 0 | \nV | 8 | Store to\n      \n       A |  |  |  | 27 | \nT | 9 | C\n      \n      to Store |  |  |  | 25 | \nU | 10 | C\n      \n      to Store |  |  |  | 0 | \nR | 11 | B\n      \n      to\n      \n       D | E\n      \n      to\n      \n       G |  |  | 19 | \nL | 12 | C\n      \n      to\n      \n       D | E\n      \n      to\n      \n       G |  |  | 22 | \nG | 13 |  | E\n      \n      to\n      \n       G | (1)\n      \n       C_5 |  | 18 | \nI | 14 | Input to Store |  |  |  | 0 | \nO | 15 | Store to Output |  |  |  | 0 | \n | 16 | (D + \\text{Store})\n      \n      to\n      \n       C |  |  |  | 0 | \n | 17 | (D - \\text{Store})\n      \n      to\n      \n       C |  |  |  | 0 | \n | 18 |  |  |  | 1 | 0 | 1\n | 19 | D\n      \n      to\n      \n       B\n      \n      (\n      \n       R\n      \n      )* | (G - 1)\n      \n      to\n      \n       E |  |  | 20 | \n | 20 | C\n      \n      to\n      \n       D |  | (1)\n      \n       E_5 |  | 21 | \n | 21 | D\n      \n      to\n      \n       C\n      \n      (\n      \n       R\n      \n      ) |  |  | 1 | 11 | 0\n | 22 | D\n      \n      to\n      \n       C\n      \n      (\n      \n       L\n      \n      )† | (G - 1)\n      \n      to\n      \n       E |  |  | 23 | \n | 23 | B\n      \n      to\n      \n       D |  | (1)\n      \n       E_5 |  | 24 | \n | 24 | D\n      \n      to\n      \n       B\n      \n      (\n      \n       L\n      \n      ) |  |  | 1 | 12 | 0\n | 25 | “0” to\n      \n       B |  |  |  | 26 | \n | 26 | B\n      \n      to\n      \n       C |  |  |  | 0 | \n | 27 | “0” to\n      \n       C | “18” to\n      \n       E |  |  | 28 | \n | 28 | B\n      \n      to\n      \n       D | E\n      \n      to\n      \n       G | (1)\n      \n       B_1 |  | 29 | \n | 29 | D\n      \n      to\n      \n       B\n      \n      (\n      \n       R\n      \n      ) | (G - 1)\n      \n      to\n      \n       E |  |  | 30 | \n | 30 | C\n      \n      to\n      \n       D\n      \n      (\n      \n       R\n      \n      ) |  | (2)\n      \n       E_5 | 1 | 31 | 32\n\n\n\n | Arithmetical Unit | Control Register Unit | Conditional Flip-Flop | Next Microinstruction\nSet | Use | 0 | 1\n31 |  | D\n      \n      to\n      \n       C |  |  | 2 | 28 | 33\n32 |  | (\n      \n       D\n      \n      +\n      \n       A\n      \n      ) to\n      \n       C |  |  | 2 | 28 | 33\n33 |  | B\n      \n      to\n      \n       D | (1)\n      \n       B\n      \n\n       1 |  |  | 34 | \n34 |  | D\n      \n      to\n      \n       B\n      \n      (\n      \n       R\n      \n      ) |  |  |  | 35 | \n35 |  | C\n      \n      to\n      \n       D\n      \n      (\n      \n       R\n      \n      ) |  |  | 1 | 36 | 37\n36 |  | D\n      \n      to\n      \n       C |  |  |  | 0 | \n37 |  | (\n      \n       D\n      \n      -\n      \n       A\n      \n      ) to\n      \n       C |  |  |  | 0 | \n\n\n* Right shift. The switching circuits in the arithmetic unit are arranged so that the least significant digit of the register\n   *C*\n   is placed in the most significant place of register\n   *B*\n   during right shift micro-operations, and the most significant digit of register\n   *C*\n   (sign digit) is repeated (thus making the correction for negative numbers).\n\n\n† Left shift. The switching circuits are similarly arranged to pass the most significant digit of register\n   *B*\n   to the least significant place of register\n   *C*\n   during left shift micro-operations."
        },
        {
          "name": "Microinstruction Sequencing",
          "content": "The two basic tasks performed by a microprogrammed control unit are as follows:\n\n\n  * ■\n    **Microinstruction sequencing:**\n    Get the next microinstruction from the control memory.\n  * ■\n    **Microinstruction execution:**\n    Generate the control signals needed to execute the microinstruction.\n\n\nIn designing a control unit, these tasks must be considered together, because both affect the format of the microinstruction and the timing of the control unit. In this section, we will focus on sequencing and say as little as possible about format and timing issues. These issues are examined in more detail in the next section.\n\n\n\n\n**Design Considerations**\n\n\nTwo concerns are involved in the design of a microinstruction sequencing technique: the size of the microinstruction and the address-generation time. The first concern is obvious; minimizing the size of the control memory reduces the cost of that component. The second concern is simply a desire to execute microinstructions as fast as possible.\n\n\nIn executing a microprogram, the address of the next microinstruction to be executed is in one of these categories:\n\n\n  * ■ Determined by instruction register\n  * ■ Next sequential address\n  * ■ Branch\n\n\nThe first category occurs only once per instruction cycle, just after an instruction is fetched. The second category is the most common in most designs. However, the design cannot be optimized just for sequential access. Branches, both conditional and unconditional, are a necessary part of a microprogram. Furthermore, microinstruction sequences tend to be short; one out of every three or four microinstructions could be a branch [SIEW82]. Thus, it is important to design compact, time-efficient techniques for microinstruction branching.\n\n\n\n\n**Sequencing Techniques**\n\n\nBased on the current microinstruction, condition flags, and the contents of the instruction register, a control memory address must be generated for the next microinstruction. A wide variety of techniques have been used. We can group them into three general categories, as illustrated in Figures 21.6 to 21.8. These categories are based on the format of the address information in the microinstruction:\n\n\n\n\n![Block diagram of Branch Control Logic: Two Address Fields. The diagram shows a flow from a Control address register to an Address decoder, then to Control memory. The output of Control memory is split into three fields: Control, Address 1, and Address 2. The Control field feeds into Branch logic along with external Flags. The Address 1 and Address 2 fields feed into a Multiplexer. The output of the Multiplexer feeds back into the Control address register. An Instruction register also feeds into the Multiplexer.](images/image_0351.jpeg)\n\n\ngraph TD\n    CAR[Control address register] --> AD[Address decoder]\n    AD --> CM[Control memory]\n    CM --> C[Control]\n    CM --> A1[Address 1]\n    CM --> A2[Address 2]\n    C --> BL[Branch logic]\n    Flags[Flags] --> BL\n    A1 --> M[Multiplexer]\n    A2 --> M\n    IR[Instruction register] --> M\n    M --> CAR\n  \nBlock diagram of Branch Control Logic: Two Address Fields. The diagram shows a flow from a Control address register to an Address decoder, then to Control memory. The output of Control memory is split into three fields: Control, Address 1, and Address 2. The Control field feeds into Branch logic along with external Flags. The Address 1 and Address 2 fields feed into a Multiplexer. The output of the Multiplexer feeds back into the Control address register. An Instruction register also feeds into the Multiplexer.\n\n\n**Figure 21.6**\n   Branch Control Logic: Two Address Fields\n\n\n\n\n![Block diagram of Branch Control Logic: Single Address Field. The diagram shows the flow of control signals for microinstruction sequencing. An Address decoder provides an address to Control memory. Control memory outputs to a Control buffer register, which is split into Control and Address fields. The Control field goes to Branch logic, and the Address field goes to a Multiplexer. Branch logic also receives Flags and outputs Address selection signals to the Multiplexer. The Multiplexer has three inputs: the Address field, the Instruction register, and the output of a +1 incrementer. The output of the Multiplexer goes to a Control address register, which then feeds back into the Address decoder. The Control address register also has a +1 incrementer feeding into it.](images/image_0352.jpeg)\n\n\nBlock diagram of Branch Control Logic: Single Address Field. The diagram shows the flow of control signals for microinstruction sequencing. An Address decoder provides an address to Control memory. Control memory outputs to a Control buffer register, which is split into Control and Address fields. The Control field goes to Branch logic, and the Address field goes to a Multiplexer. Branch logic also receives Flags and outputs Address selection signals to the Multiplexer. The Multiplexer has three inputs: the Address field, the Instruction register, and the output of a +1 incrementer. The output of the Multiplexer goes to a Control address register, which then feeds back into the Address decoder. The Control address register also has a +1 incrementer feeding into it.\n\n\n**Figure 21.7**\n   Branch Control Logic: Single Address Field\n\n\n  * ■ Two address fields\n  * ■ Single address field\n  * ■ Variable format\n\n\nThe simplest approach is to provide two address fields in each microinstruction. Figure 21.6 suggests how this information is to be used. A multiplexer is provided that serves as a destination for both address fields plus the instruction register. Based on an address-selection input, the multiplexer transmits either the opcode or one of the two addresses to the control address register (CAR). The CAR is subsequently decoded to produce the next microinstruction address. The address-selection signals are provided by a branch logic module whose input consists of control unit flags plus bits from the control portion of the microinstruction.\n\n\nAlthough the two-address approach is simple, it requires more bits in the microinstruction than other approaches. With some additional logic, savings can be achieved. A common approach is to have a single address field (Figure 21.7). With this approach, the options for next address are as follows:\n\n\n  * ■ Address field\n  * ■ Instruction register code\n  * ■ Next sequential address\n\n\n\n\n![Block diagram of Branch Control Logic: Variable Format. The diagram shows a flow from an Address decoder to Control memory, then to a Control buffer register. The Control buffer register outputs a Branch control field to Gate and function logic and an Entire field to a Branch logic module. The Branch logic module also receives Flags and outputs an Address selection signal to a Multiplexer. The Control address register outputs an Address field to the Multiplexer and also has an input from a +1 incrementer. The Instruction register also feeds into the Multiplexer. The Multiplexer's output feeds back into the Control address register.](images/image_0353.jpeg)\n\n\ngraph TD\n    AD[Address decoder] --> CM[Control memory]\n    CM --> CBR[Control buffer register]\n    CBR -- \"Branch control field\" --> GFL[Gate and function logic]\n    CBR -- \"Entire field\" --> BL[Branch logic]\n    BL -- \"Flags\" --> BL\n    BL -- \"Address selection\" --> M[Multiplexer]\n    CAR[Control address register] -- \"Address field\" --> M\n    CAR --> INC[+1]\n    INC --> CAR\n    IR[Instruction register] --> M\n    M --> CAR\n  \nBlock diagram of Branch Control Logic: Variable Format. The diagram shows a flow from an Address decoder to Control memory, then to a Control buffer register. The Control buffer register outputs a Branch control field to Gate and function logic and an Entire field to a Branch logic module. The Branch logic module also receives Flags and outputs an Address selection signal to a Multiplexer. The Control address register outputs an Address field to the Multiplexer and also has an input from a +1 incrementer. The Instruction register also feeds into the Multiplexer. The Multiplexer's output feeds back into the Control address register.\n\n\n**Figure 21.8**\n   Branch Control Logic: Variable Format\n\n\nThe address-selection signals determine which option is selected. This approach reduces the number of address fields to one. Note, however, that the address field often will not be used. Thus, there is some inefficiency in the microinstruction coding scheme.\n\n\nAnother approach is to provide for two entirely different microinstruction formats (Figure 21.8). One bit designates which format is being used. In one format, the remaining bits are used to activate control signals. In the other format, some bits drive the branch logic module, and the remaining bits provide the address. With the first format, the next address is either the next sequential address or an address derived from the instruction register. With the second format, either a conditional or unconditional branch is being specified. One disadvantage of this approach is that one entire cycle is consumed with each branch microinstruction. With the other approaches, address generation occurs as part of the same cycle as control signal generation, minimizing control memory accesses.\n\n\nThe approaches just described are general. Specific implementations will often involve a variation or combination of these techniques.\n\n\n\n\n**Address Generation**\n\n\nWe have looked at the sequencing problem from the point of view of format considerations and general logic requirements. Another viewpoint is to consider the various ways in which the next address can be derived or computed.\n\n\nTable 21.3 lists the various address generation techniques. These can be divided into explicit techniques, in which the address is explicitly available in the microinstruction, and implicit techniques, which require additional logic to generate the address.\n\n\nWe have essentially dealt with the explicit techniques. With a two-field approach, two alternative addresses are available with each microinstruction. Using either a single address field or a variable format, various branch instructions can be implemented. A conditional branch instruction depends on the following types of information:\n\n\n  * ■ ALU flags\n  * ■ Part of the opcode or address mode fields of the machine instruction\n  * ■ Parts of a selected register, such as the sign bit\n  * ■ Status bits within the control unit\n\n\nSeveral implicit techniques are also commonly used. One of these, mapping, is required with virtually all designs. The opcode portion of a machine instruction must be mapped into a microinstruction address. This occurs only once per instruction cycle.\n\n\nA common implicit technique is one that involves combining or adding two portions of an address to form the complete address. This approach was taken for the IBM S/360 family [TUCK67] and used on many of the S/370 models. We will use the IBM 3033 as an example.\n\n\nThe control address register on the IBM 3033 is 13 bits long and is illustrated in Figure 21.9. Two parts of the address can be distinguished. The highest-order 8 bits (00–07) normally do not change from one microinstruction cycle to the next. During the execution of a microinstruction, these 8 bits are copied directly from an 8-bit field of the microinstruction (the BA field) into the highest-order 8 bits of the control address register. This defines a block of 32 microinstructions in control memory. The remaining 5 bits of the control address register are set to specify the specific address of the microinstruction to be fetched next. Each of these bits is determined by a 4-bit field (except one is a 7-bit field) in the current microinstruction; the field specifies the condition for setting the corresponding bit. For example, a bit in the control address register might be set to 1 or 0 depending on whether a carry occurred on the last ALU operation.\n\n\n**Table 21.3**\n   Microinstruction Address Generation Techniques\n\n\n\nExplicit | Implicit\nTwo-field | Mapping\nUnconditional branch | Addition\nConditional branch | Residual control\n\n\n\n\n![Diagram of the IBM 3033 Control Address Register, showing a 22-bit register divided into fields: BA(8) (bits 00-07), BB(4) (bit 08), BC(4) (bit 09), BD(4) (bit 10), BE(4) (bit 11), and BF(7) (bit 12).](images/image_0354.jpeg)\n\n\nThe diagram illustrates the IBM 3033 Control Address Register, a 22-bit register. The bits are numbered from 00 to 12 at the top. The register is divided into fields: BA(8) covers bits 00-07; BB(4) covers bit 08; BC(4) covers bit 09; BD(4) covers bit 10; BE(4) covers bit 11; and BF(7) covers bit 12. Arrows point from the field names to their respective bit positions.\n\n\nDiagram of the IBM 3033 Control Address Register, showing a 22-bit register divided into fields: BA(8) (bits 00-07), BB(4) (bit 08), BC(4) (bit 09), BD(4) (bit 10), BE(4) (bit 11), and BF(7) (bit 12).\n\n\n**Figure 21.9**\n   IBM 3033 Control Address Register\n\n\nThe final approach listed in Table 21.3 is termed\n   *residual control*\n   . This approach involves the use of a microinstruction address that has previously been saved in temporary storage within the control unit. For example, some microinstruction sets come equipped with a subroutine facility. An internal register or stack of registers is used to hold return addresses. An example of this approach is taken on the LSI-11, which we now examine.\n\n\n\n\n**LSI-11 Microinstruction Sequencing**\n\n\nThe LSI-11 is a microcomputer version of a PDP-11, with the main components of the system residing on a single board. The LSI-11 is implemented using a microprogrammed control unit [SEBE76].\n\n\nThe LSI-11 makes use of a 22-bit microinstruction and a control memory of 2K 22-bit words. The next microinstruction address is determined in one of five ways:\n\n\n  * ■\n    **Next sequential address:**\n    In the absence of other instructions, the control unit's control address register is incremented by 1.\n  * ■\n    **Opcode mapping:**\n    At the beginning of each instruction cycle, the next microinstruction address is determined by the opcode.\n  * ■\n    **Subroutine facility:**\n    Explained presently.\n  * ■\n    **Interrupt testing:**\n    Certain microinstructions specify a test for interrupts. If an interrupt has occurred, this determines the next microinstruction address.\n  * ■\n    **Branch:**\n    Conditional and unconditional branch microinstructions are used.\n\n\nA one-level subroutine facility is provided. One bit in every microinstruction is dedicated to this task. When the bit is set, an 11-bit return register is loaded with the updated contents of the control address register. A subsequent microinstruction that specifies a return will cause the control address register to be loaded from the return register.\n\n\nThe return is one form of unconditional branch instruction. Another form of unconditional branch causes the bits of the control address register to be loaded from 11 bits of the microinstruction. The conditional branch instruction makes use of a 4-bit test code within the microinstruction. This code specifies testing of various ALU condition codes to determine the branch decision. If the condition is not true, the next sequential address is selected. If it is true, the 8 lowest-order bits of the control address register are loaded from 8 bits of the microinstruction. This allows branching within a 256-word page of memory.\n\n\nAs can be seen, the LSI-11 includes a powerful address sequencing facility within the control unit. This allows the microprogrammer considerable flexibility and can ease the microprogramming task. On the other hand, this approach requires more control unit logic than do simpler capabilities."
        },
        {
          "name": "Microinstruction Execution",
          "content": "The microinstruction cycle is the basic event on a microprogrammed processor. Each cycle is made up of two parts: fetch and execute. The fetch portion is determined by the generation of a microinstruction address, and this was dealt with in the preceding section. This section deals with the execution of a microinstruction.\n\n\nRecall that the effect of the execution of a microinstruction is to generate control signals. Some of these signals control points internal to the processor. The remaining signals go to the external control bus or other external interface. As an incidental function, the address of the next microinstruction is determined.\n\n\nThe preceding description suggests the organization of a control unit shown in Figure 21.10. This slightly revised version of Figure 21.4 emphasizes the focus of this section. The major modules in this diagram should by now be clear. The sequencing logic module contains the logic to perform the functions discussed in the preceding section. It generates the address of the next microinstruction, using as inputs the instruction register, ALU flags, the control address register (for incrementing), and the control buffer register. The last may provide an actual address, control bits, or both. The module is driven by a clock that determines the timing of the microinstruction cycle.\n\n\nThe control logic module generates control signals as a function of some of the bits in the microinstruction. It should be clear that the format and content of the microinstruction determines the complexity of the control logic module.\n\n\n\n\n**A Taxonomy of Microinstructions**\n\n\nMicroinstructions can be classified in a variety of ways. Distinctions that are commonly made in the literature include the following:\n\n\n  * ■ Vertical/horizontal\n  * ■ Packed/unpacked\n  * ■ Hard/soft microprogramming\n  * ■ Direct/indirect encoding\n\n\nAll of these bear on the format of the microinstruction. None of these terms has been used in a consistent, precise way in the literature. However, an examination of these pairs of qualities serves to illuminate microinstruction design alternatives. In the following paragraphs, we first look at the key design issue underlying all of these pairs of characteristics, and then we look at the concepts suggested by each pair.\n\n\nIn the original proposal by Wilkes [WILK51], each bit of a microinstruction either directly produced a control signal or directly produced one bit of the next address. We have seen, in the preceding section, that more complex address\n\n\n\n\n![Figure 21.10: Control Unit Organization. This block diagram shows the internal components of a control unit. At the top, an 'Instruction register' feeds into a 'Sequencing logic' block. 'Sequencing logic' also receives inputs from 'ALU Flags' and a 'Clock'. It outputs to a 'Control address register'. The 'Control address register' feeds into a large 'Control memory' block. The 'Control memory' feeds into a 'Control buffer register'. The 'Control buffer register' feeds into 'Control logic'. 'Control logic' outputs 'Internal control signals' and 'External control signals'. There is a feedback loop from the 'Control buffer register' back to the 'Sequencing logic' block.](images/image_0355.jpeg)\n\n\nFigure 21.10: Control Unit Organization. This block diagram shows the internal components of a control unit. At the top, an 'Instruction register' feeds into a 'Sequencing logic' block. 'Sequencing logic' also receives inputs from 'ALU Flags' and a 'Clock'. It outputs to a 'Control address register'. The 'Control address register' feeds into a large 'Control memory' block. The 'Control memory' feeds into a 'Control buffer register'. The 'Control buffer register' feeds into 'Control logic'. 'Control logic' outputs 'Internal control signals' and 'External control signals'. There is a feedback loop from the 'Control buffer register' back to the 'Sequencing logic' block.\n\n\n**Figure 21.10**\n   Control Unit Organization\n\n\nsequencing schemes, using fewer microinstruction bits, are possible. These schemes require a more complex sequencing logic module. A similar sort of trade-off exists for the portion of the microinstruction concerned with control signals. By encoding control information, and subsequently decoding it to produce control signals, control word bits can be saved.\n\n\nHow can this encoding be done? To answer that, consider that there are a total of\n   \n    K\n   \n   different internal and external control signals to be driven by the control unit. In Wilkes's scheme,\n   \n    K\n   \n   bits of the microinstruction would be dedicated to this purpose. This allows all of the\n   \n    2^K\n   \n   possible combinations of control signals to be generated during any instruction cycle. But we can do better than this if we observe that not all of the possible combinations will be used. Examples include the following:\n\n\n  * ■ Two sources cannot be gated to the same destination (e.g.,\n    \n     C_2\n    \n    and\n    \n     C_8\n    \n    in Figure 21.5).\n  * ■ A register cannot be both source and destination (e.g.,\n    \n     C_5\n    \n    and\n    \n     C_{12}\n    \n    in Figure 21.5).\n  * ■ Only one pattern of control signals can be presented to the ALU at a time.\n  * ■ Only one pattern of control signals can be presented to the external control bus at a time.\n\n\nSo, for a given processor, all possible allowable combinations of control signals could be listed, giving some number\n   \n    Q < 2^K\n   \n   possibilities. These could be encoded with a minimum\n   \n    \\log_2 Q\n   \n   bits, with\n   \n    (\\log_2 Q) < K\n   \n   . This would be the tightest possible form of encoding that preserves all allowable combinations of control signals. In practice, this form of encoding is not used, for two reasons:\n\n\n  * ■ It is as difficult to program as a pure decoded (Wilkes) scheme. This point is discussed further presently.\n  * ■ It requires a complex and therefore slow control logic module.\n\n\nInstead, some compromises are made. These are of two kinds:\n\n\n  * ■ More bits than are strictly necessary are used to encode the possible combinations.\n  * ■ Some combinations that are physically allowable are not possible to encode.\n\n\nThe latter kind of compromise has the effect of reducing the number of bits. The net result, however, is to use more than\n   \n    \\log_2 Q\n   \n   bits.\n\n\nIn the next subsection, we will discuss specific encoding techniques. The remainder of this subsection deals with the effects of encoding and the various terms used to describe it.\n\n\nBased on the preceding, we can see that the control signal portion of the microinstruction format falls on a spectrum. At one extreme, there is one bit for each control signal; at the other extreme, a highly encoded format is used. Table 21.4 shows that other characteristics of a microprogrammed control unit also fall along a spectrum and that these spectra are, by and large, determined by the degree-of-encoding spectrum.\n\n\nThe second pair of items in the table is rather obvious. The pure Wilkes scheme will require the most bits. It should also be apparent that this extreme presents the most detailed view of the hardware. Every control signal is individually controllable\n\n\n**Table 21.4**\n   The Microinstruction Spectrum\n\n\n\nCharacteristics\nUnencoded | Highly encoded\nMany bits | Few bits\nDetailed view of hardware | Aggregated view of hardware\nDifficult to program | Easy to program\nConcurrency fully exploited | Concurrency not fully exploited\nLittle or no control logic | Complex control logic\nFast execution | Slow execution\nOptimize performance | Optimize programming\nTerminology\nUnpacked | Packed\nHorizontal | Vertical\nHard | Soft\n\n\nby the microprogrammer. Encoding is done in such a way as to aggregate functions or resources, so that the microprogrammer is viewing the processor at a higher, less detailed level. Furthermore, the encoding is designed to ease the microprogramming burden. Again, it should be clear that the task of understanding and orchestrating the use of all the control signals is a difficult one. As was mentioned, one of the consequences of encoding, typically, is to prevent the use of certain otherwise allowable combinations.\n\n\nThe preceding paragraph discusses microinstruction design from the microprogrammer's point of view. But the degree of encoding also can be viewed from its hardware effects. With a pure unencoded format, little or no decode logic is needed; each bit generates a particular control signal. As more compact and more aggregated encoding schemes are used, more complex decode logic is needed. This, in turn, may affect performance. More time is needed to propagate signals through the gates of the more complex control logic module. Thus, the execution of encoded microinstructions takes longer than the execution of unencoded ones.\n\n\nTherefore, all of the characteristics listed in Table 21.4 fall along a spectrum of design strategies. In general, a design that falls toward the left end of the spectrum is intended to optimize the performance of the control unit. Designs toward the right end are more concerned with optimizing the process of microprogramming. Indeed, microinstruction sets near the right end of the spectrum look very much like machine instruction sets. A good example of this is the LSI-11 design, described later in this section. Typically, when the objective is simply to implement a control unit, the design will be near the left end of the spectrum. The IBM 3033 design, discussed presently, is in this category. As we shall discuss later, some systems permit a variety of users to construct different microprograms using the same microinstruction facility. In the latter cases, the design is likely to fall near the right end of the spectrum.\n\n\nWe can now deal with some of the terminology introduced earlier. Table 21.4 indicates how three of these pairs of terms relate to the microinstruction spectrum. In essence, all of these pairs describe the same thing but emphasize different design characteristics.\n\n\nThe degree of packing relates to the degree of identification between a given control task and specific microinstruction bits. As the bits become more\n   *packed*\n   , a given number of bits contains more information. Thus, packing connotes encoding. The terms\n   *horizontal*\n   and\n   *vertical*\n   relate to the relative width of microinstructions. [SIEW82] suggests as a rule of thumb that vertical microinstructions have lengths in the range of 16 to 40 bits and that horizontal microinstructions have lengths in the range of 40 to 100 bits. The terms\n   *hard*\n   and\n   *soft*\n   microprogramming are used to suggest the degree of closeness to the underlying control signals and hardware layout. Hard microprograms are generally fixed and committed to read-only memory. Soft microprograms are more changeable and are suggestive of user microprogramming.\n\n\nThe other pair of terms mentioned at the beginning of this subsection refers to direct versus indirect encoding, a subject to which we now turn.\n\n\n\n\n**Microinstruction Encoding**\n\n\nIn practice, microprogrammed control units are not designed using a pure unencoded or horizontal microinstruction format. At least some degree of encoding is used to reduce control memory width and to simplify the task of microprogramming.\n\n\nThe basic technique for encoding is illustrated in Figure 21.11a. The microinstruction is organized as a set of fields. Each field contains a code, which, upon decoding, activates one or more control signals.\n\n\nLet us consider the implications of this layout. When the microinstruction is executed, every field is decoded and generates control signals. Thus, with\n   \n    N\n   \n   fields,\n   \n    N\n   \n   simultaneous actions are specified. Each action results in the activation of one or more control signals. Generally, but not always, we will want to design the format so that each control signal is activated by no more than one field. Clearly, however, it must be possible for each control signal to be activated by at least one field.\n\n\nNow consider the individual field. A field consisting of\n   \n    L\n   \n   bits can contain one of\n   \n    2^L\n   \n   codes, each of which can be encoded to a different control signal pattern. Because only one code can appear in a field at a time, the codes are mutually exclusive, and, therefore, the actions they cause are mutually exclusive.\n\n\n\n\n![Diagram (a) Direct encoding: A microinstruction is divided into fields. Each field has its own 'Decode logic' block. The outputs of these blocks are individual control signals.](images/image_0356.jpeg)\n\n\nDiagram (a) illustrates direct encoding. A horizontal bar represents a microinstruction, divided into segments labeled 'Field' with '...' at the ends. Below each 'Field' segment is a box labeled 'Decode logic'. Arrows from each 'Decode logic' box point down to a set of dots representing individual control signals. A bracket below these dots is labeled 'Control signals'.\n\n\nDiagram (a) Direct encoding: A microinstruction is divided into fields. Each field has its own 'Decode logic' block. The outputs of these blocks are individual control signals.\n\n\n(a) Direct encoding\n\n\n\n\n![Diagram (b) Indirect encoding: A microinstruction is divided into fields. Each field has its own 'Decode logic' block. The outputs of these blocks are fed into a central 'Decode logic' block, which then generates the control signals.](images/image_0357.jpeg)\n\n\nDiagram (b) illustrates indirect encoding. A horizontal bar represents a microinstruction, divided into segments labeled 'Field' with '...' at the ends. Below each 'Field' segment is a box labeled 'Decode logic'. Arrows from each of these three boxes point to a central, larger box also labeled 'Decode logic'. Arrows from the central box point down to a set of dots representing control signals. A bracket below these dots is labeled 'Control signals'.\n\n\nDiagram (b) Indirect encoding: A microinstruction is divided into fields. Each field has its own 'Decode logic' block. The outputs of these blocks are fed into a central 'Decode logic' block, which then generates the control signals.\n\n\n(b) Indirect encoding\n\n\n**Figure 21.11**\n   Microinstruction Encoding\n\n\nThe design of an encoded microinstruction format can now be stated in simple terms:\n\n\n  * ■ Organize the format into independent fields. That is, each field depicts a set of actions (pattern of control signals) such that actions from different fields can occur simultaneously.\n  * ■ Define each field such that the alternative actions that can be specified by the field are mutually exclusive. That is, only one of the actions specified for a given field could occur at a time.\n\n\nTwo approaches can be taken for organizing the encoded microinstruction into fields: functional and resource. The\n   *functional encoding*\n   method identifies functions within the machine and designates fields by function type. For example, if various sources can be used for transferring data to the accumulator, one field can be designated for this purpose, with each code specifying a different source.\n   *Resource encoding*\n   views the machine as consisting of a set of independent resources and devotes one field to each (e.g., I/O, memory, ALU).\n\n\nAnother aspect of encoding is whether it is direct or indirect (Figure 21.11b). With indirect encoding, one field is used to determine the interpretation of another\n\n\n\n\n![](images/image_0358.jpeg)\n\n\nSimple register transfers | Special sequencing operations\n0, 0, 0, 0, 0, 0\n       \n       | MDR ← Register | 0, 1, 0, 0, 0, 0\n       \n       | CSAR ← Decoded MDR\n0, 0, 0, 0, 0, 1\n       \n       | Register ← MDR | 0, 1, 0, 0, 0, 1\n       \n       | CSAR ← Constant (in next byte)\n0, 0, 0, 0, 1, 0\n       \n       | MAR ← Register | 0, 1, 0, 0, 1, 0\n       \n       | Skip\nRegister select | ALU operations\nMemory operations | 0, 1, 1, 0, 0, 0\n       \n       | ACC ← ACC + Register\n0, 0, 1, 0, 0, 0\n       \n       | Read | 0, 1, 1, 0, 0, 1\n       \n       | ACC ← ACC - Register\n0, 0, 1, 0, 0, 1\n       \n       | Write | 0, 1, 1, 0, 1, 0\n       \n       | ACC ← Register\n | 0, 1, 1, 0, 1, 1\n       \n       | Register ← ACC\n | 0, 1, 1, 1, 0, 0\n       \n       | ACC ← Register + 1\n | Register select\n\n\n(a) Vertical microinstruction format\n\n\n\n\n![](images/image_0359.jpeg)\n\n\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\n\n\n\nField | 1 | 2 | 3 | 4 | 5 | 6\nField definition | 1 - register transfer | 4 - ALU operation | 7 - register selection\n | 2 - memory operation | 5 - register selection | 8 - constant\n | 3 - sequencing operation | 6 - constant |\n\n\n(b) Horizontal microinstruction format\n\n\n**Figure 21.12**\n   Alternative Microinstruction Formats for a Simple Machine\n\n\nfield. For example, consider an ALU that is capable of performing eight different arithmetic operations and eight different shift operations. A 1-bit field could be used to indicate whether a shift or arithmetic operation is to be used; a 3-bit field would indicate the operation. This technique generally implies two levels of decoding, increasing propagation delays.\n\n\nFigure 21.12 is a simple example of these concepts. Assume a processor with a single accumulator and several internal registers, such as a program counter and a temporary register for ALU input. Figure 21.12a shows a highly vertical format. The first 3 bits indicate the type of operation, the next 3 encode the operation, and the final 2 select an internal register. Figure 21.12b is a more horizontal approach, although encoding is still used. In this case, different functions appear in different fields.\n\n\n\n\n**LSI-11 Microinstruction Execution**\n\n\nThe LSI-11 [SEBE76] is a good example of a vertical microinstruction approach. We look first at the organization of the control unit, then at the microinstruction format.\n\n\n**LSI-11 CONTROL UNIT ORGANIZATION**\n   The LSI-11 is the first member of the PDP-11 family that was offered as a single-board processor. The board contains three LSI chips, an internal bus known as the\n   *microinstruction bus*\n   (MIB), and some additional interfacing logic.\n\n\nFigure 21.13 depicts, in simplified form, the organization of the LSI-11 processor. The three chips are the data, control, and control store chips. The data chip contains an 8-bit ALU, twenty-six 8-bit registers, and storage for several condition\n\n\n\n\n![Simplified Block Diagram of the LSI-11 Processor showing the interconnections between the Control store, Control chip, Data chip, Bus control and other processor board logic, and Bus logic, all connected to a Microinstruction bus and an LSI-11 system bus.](images/image_0360.jpeg)\n\n\nThe diagram illustrates the internal architecture of the LSI-11 processor. It features five main components: a Control store (top, light blue), a Control chip (middle left, grey), a Data chip (middle right, grey), Bus control and other processor board logic (bottom left, grey), and Bus logic (bottom right, grey). These components are interconnected via a central horizontal line labeled \"Microinstruction bus\".\n\n\n  * **Control store**\n     connects to the\n     **Microinstruction bus**\n     with a line labeled \"11\".\n  * The\n     **Microinstruction bus**\n     connects to the\n     **Control chip**\n     with a line labeled \"18\".\n  * The\n     **Microinstruction bus**\n     connects to the\n     **Data chip**\n     with a line labeled \"16\".\n  * The\n     **Control chip**\n     connects to the\n     **Bus control and other processor board logic**\n     with a line labeled \"4\".\n  * The\n     **Data chip**\n     connects to the\n     **Bus logic**\n     with a line labeled \"16\".\n  * The\n     **Bus control and other processor board logic**\n     connects to the\n     **LSI-11 system bus**\n     with a line labeled \"22\".\n  * The\n     **Bus logic**\n     connects to the\n     **LSI-11 system bus**\n     with a line labeled \"22\".\n\n\nA legend on the right indicates that paths without a number are \"a path with multiple signals\".\n\n\nSimplified Block Diagram of the LSI-11 Processor showing the interconnections between the Control store, Control chip, Data chip, Bus control and other processor board logic, and Bus logic, all connected to a Microinstruction bus and an LSI-11 system bus.\n\n\n**Figure 21.13**\n   Simplified Block Diagram of the LSI-11 Processor\n\n\ncodes. Sixteen of the registers are used to implement the eight 16-bit general-purpose registers of the PDP-11. Others include a program status word, memory address register (MAR), and memory buffer register. Because the ALU deals with only 8 bits at a time, two passes through the ALU are required to implement a 16-bit PDP-11 arithmetic operation. This is controlled by the microprogram.\n\n\nThe control store chip or chips contain the 22-bit-wide control memory. The control chip contains the logic for sequencing and executing microinstructions. It contains the control address register, the control data register, and a copy of the machine instruction register.\n\n\nThe MIB ties all the components together. During microinstruction fetch, the control chip generates an 11-bit address onto the MIB. Control store is accessed, producing a 22-bit microinstruction, which is placed on the MIB. The low-order 16 bits go to the data chip, while the low-order 18 bits go to the control chip. The high-order 4 bits control special processor board functions.\n\n\nFigure 21.14 provides a still simplified but more detailed look at the LSI-11 control unit: the figure ignores individual chip boundaries. The address sequencing scheme described in Section 21.2 is implemented in two modules. Overall sequence control is provided by the microprogram sequence control module, which is capable\n\n\n\n\n![Block diagram of the LSI-11 Control Unit organization. The diagram shows a vertical stack of components: Control data register (top), Control store, Control address register, Microprogram sequence control, Return register, and Translation array (bottom). The Control data register and Control store are connected by a bidirectional arrow. The Control address register and Control store are connected by a bidirectional arrow. The Control address register and Microprogram sequence control are connected by a bidirectional arrow. The Microprogram sequence control and Return register are connected by a bidirectional arrow. The Return register and Translation array are connected by a bidirectional arrow. The Instruction register (left) and Translation array (bottom) are connected by a bidirectional arrow. An INT signal (right) is connected to the Translation array.](images/image_0361.jpeg)\n\n\ngraph TD\n    CR[Control data register] <--> CS[Control store]\n    CAR[Control address register] <--> CS\n    CAR <--> MSC[Microprogram sequence control]\n    MSC <--> RR[Return register]\n    RR <--> TA[Translation array]\n    IR[Instruction register] <--> TA\n    INT[INT] --> TA\n  \nBlock diagram of the LSI-11 Control Unit organization. The diagram shows a vertical stack of components: Control data register (top), Control store, Control address register, Microprogram sequence control, Return register, and Translation array (bottom). The Control data register and Control store are connected by a bidirectional arrow. The Control address register and Control store are connected by a bidirectional arrow. The Control address register and Microprogram sequence control are connected by a bidirectional arrow. The Microprogram sequence control and Return register are connected by a bidirectional arrow. The Return register and Translation array are connected by a bidirectional arrow. The Instruction register (left) and Translation array (bottom) are connected by a bidirectional arrow. An INT signal (right) is connected to the Translation array.\n\n\n**Figure 21.14**\n   Organization of the LSI-11 Control Unit\n\n\nof incrementing the microinstruction address register and performing unconditional branches. The other forms of address calculation are carried out by a separate translation array. This is a combinatorial circuit that generates an address based on the microinstruction, the machine instruction, the microinstruction program counter, and an interrupt register.\n\n\nThe translation array comes into play on the following occasions:\n\n\n  * ■ The opcode is used to determine the start of a microroutine.\n  * ■ At appropriate times, address mode bits of the microinstruction are tested to perform appropriate addressing.\n  * ■ Interrupt conditions are periodically tested.\n  * ■ Conditional branch microinstructions are evaluated.\n\n\n**LSI-11 MICROINSTRUCTION FORMAT**\n   The LSI-11 uses an extremely vertical microinstruction format, which is only 22 bits wide. The microinstruction set strongly resembles the PDP-11 machine instruction set that it implements. This design was intended to optimize the performance of the control unit within the constraint of a vertical, easily programmed design. Table 21.5 lists some of the LSI-11 microinstructions.\n\n\nFigure 21.15 shows the 22-bit LSI-11 microinstruction format. The high-order 4 bits control special functions on the processor board. The translate bit enables the translation array to check for pending interrupts. The load return register bit is used at the end of a microroutine to cause the next microinstruction address to be loaded from the return register.\n\n\nThe remaining 16 bits are used for highly encoded micro-operations. The format is much like a machine instruction, with a variable-length opcode and one or more operands.\n\n\n**Table 21.5**\n   Some LSI-11 Microinstructions\n\n\n\nArithmetic Operations | General Operations\nAdd word (byte, literal) | MOV word (byte)\nTest word (byte, literal) | Jump\nIncrement word (byte) by 1 | Return\nIncrement word (byte) by 2 | Conditional jump\nNegate word (byte) | Set (reset) flags\nConditionally increment (decrement) byte | Load G low\nConditionally add word (byte) | Conditionally MOV word (byte)\nAdd word (byte) with carry | \nConditionally add digits | \nSubtract word (byte) | Input/Output Operations\nCompare word (byte, literal) | Input word (byte)\nSubtract word (byte) with carry | Input status word (byte)\nDecrement word (byte) by 1 | Read\n | Write\n | Read (write) and increment word (byte) by 1\n | Read (write) and increment word (byte) by 2\n | Read (write) acknowledge\n | Output word (byte, status)\nLogical Operations | \nAND word (byte, literal) | \nTest word (byte) | \nOR word (byte) | \nExclusive-OR word (byte) | \nBit clear word (byte) | \nShift word (byte) right (left) with (without) carry | \nComplement word (byte) | \n\n\n\n\n![](images/image_0362.jpeg)\n\n\n4      1   1\n    \n\n     16\n\n\n\nSpecial functions |  |  | Encoded micro-operations\n\n\n(a) Format of the full LSI-11 microinstruction\n\n\n\n\n\n5 | 11 | 4 | 8 | 4\nOpcode | Jump address | Opcode | Literal value | A register\n\n\nUnconditional jump microinstruction format                                      Literal microinstruction format\n\n\n\n\n\n4 | 4 | 8 | 8 | 4 | 4\nOpcode | Test code | Jump address | Opcode | B register | A register\n\n\nConditional jump microinstruction format                                      Register jump microinstruction format\n\n\n(b) Format of the encoded part of the LSI-11 microinstruction\n\n\n**Figure 21.15**\n   LSI-11 Microinstruction Format\n\n\n\n\n**IBM 3033 Microinstruction Execution**\n\n\nThe standard IBM 3033 control memory consists of 4K words. The first half of these (0000–07FF) contain 108-bit microinstructions, while the remainder (0800–0FFF) are used to store 126-bit microinstructions. The format is depicted in Figure 21.16.\n\n\n\n\n![](images/image_0363.jpeg)\n\n\n0 |  | 35\nP | AA | AB | AC | AD | AE | AF | AG | AH | AJ | AK | AL\nA, B, C, D registers | Arithmetic | Shift\n36 |  | 71\nP | BA | BB | BC | BD | BE | BF | BH\nNext address | Storage address\n72 |  | 107\nP | BH | CA | CB | CC | CD | CE | CF | CG | CH\nStorage address | Shift control | Local storage | Miscellaneous controls\n108 |  | 125\nP | DA | DB | DC | DD | DE | \nTesting and condition code setting\n\n\n**Figure 21.16**\n   IBM 3033 Microinstruction Format\n\n\n**Table 21.6**\n\nALU Control Fields\nAA(3) | Load A register from one of data registers\nAB(3) | Load B register from one of data registers\nAC(3) | Load C register from one of data registers\nAD(3) | Load D register from one of data registers\nAE(4) | Route specified A bits to ALU\nAF(4) | Route specified B bits to ALU\nAG(5) | Specifies ALU arithmetic operation on A input\nAH(4) | Specifies ALU arithmetic operation on B input\nAJ(1) | Specifies D or B input to ALU on B side\nAK(4) | Route arithmetic output to shifter\nCA(3) | Load F register\nCB(1) | Activate shifter\nCC(5) | Specifies logical and carry functions\nCE(7) | Specifies shift amount\nSequencing and Branching Fields\nAL(1) | End operation and perform branch\nBA(8) | Set high-order bits (00–07) of control address register\nBB(4) | Specifies condition for setting bit 8 of control address register\nBC(4) | Specifies condition for setting bit 9 of control address register\nBD(4) | Specifies condition for setting bit 10 of control address register\nBE(4) | Specifies condition for setting bit 11 of control address register\nBF(7) | Specifies condition for setting bit 12 of control address register\n\n\nAlthough this is a rather horizontal format, encoding is still extensively used. The key fields of that format are summarized in Table 21.6.\n\n\nThe ALU operates on inputs from four dedicated, non-user-visible registers, A, B, C, and D. The microinstruction format contains fields for loading these registers from user-visible registers, performing an ALU function, and specifying a user-visible register for storing the result. There are also fields for loading and storing data between registers and memory.\n\n\nThe sequencing mechanism for the IBM 3033 was discussed in Section 21.2."
        },
        {
          "name": "TI 8800 755",
          "content": "The Texas Instruments 8800 Software Development Board (SDB) is a microprogrammable 32-bit computer card. The system has a writable control store, implemented in RAM rather than ROM. Such a system does not achieve the speed or\n\n\ndensity of a microprogrammed system with a ROM control store. However, it is useful for developing prototypes and for educational purposes.\n\n\nThe 8800 SDB consists of the following components (Figure 21.17):\n\n\n  * ■ Microcode memory\n  * ■ Microsequencer\n  * ■ 32-bit ALU\n  * ■ Floating-point and integer processor\n  * ■ Local data memory\n\n\nTwo buses link the internal components of the system. The DA bus provides data from the microinstruction data field to the ALU, the floating-point processor, or the microsequencer. In the latter case, the data consists of an address to be used for a branch instruction. The bus can also be used for the ALU or microsequencer to\n\n\n\n\n![TI 8800 Block Diagram showing internal components and data flow.](images/image_0364.jpeg)\n\n\nThe TI 8800 Block Diagram illustrates the internal architecture of the system. At the top, a\n    **Microcode memory 32K × 128 bits**\n    provides a\n    **Next microcode address**\n    (15 bits) to the\n    **Microinstruction pipeline register**\n    . The memory also outputs a\n    **Microinstruction**\n    (128 bits) to the register. The\n    **Microinstruction pipeline register**\n    outputs a\n    **Control and microinstruction**\n    (96 bits) to the\n    **ACT8832 registered ALU**\n    and a\n    **DA31-DA00**\n    (32 bits) bus to the\n    **ACT8847 floating-point and integer processor**\n    and the\n    **ACT8818 microsequencer**\n    . The\n    **ACT8832**\n    and\n    **ACT8847**\n    are connected to the\n    **System Y bus**\n    (32 bits). The\n    **ACT8832**\n    is also connected to the\n    **Local data memory 32K × 32 bits**\n    . The\n    **Local data memory**\n    and the\n    **PC/AT interface**\n    are connected to the\n    **System Y bus**\n    . The\n    **PC/AT interface**\n    has a 16-bit connection to the\n    **System Y bus**\n    . The\n    **ACT8818 microsequencer**\n    has a bidirectional connection to the\n    **DA31-DA00**\n    bus and a unidirectional connection to the\n    **System Y bus**\n    .\n\n\nTI 8800 Block Diagram showing internal components and data flow.\n\n\n**Figure 21.17**\n   TI 8800 Block Diagram\n\n\nprovide data to other components. The system Y bus connects the ALU and floating-point processor to local memory and to external modules via the PC interface.\n\n\nThe board fits into an IBM PC-compatible host computer. The host computer provides a suitable platform for microcode assembly and debug.\n\n\n\n\n**Microinstruction Format**\n\n\nThe microinstruction format for the 8800 consists of 128 bits broken down into 30 functional fields, as indicated in Table 21.7. Each field consists of one or more bits, and the fields are grouped into five major categories:\n\n\n  * ■ Control of board\n  * ■ 8847 floating-point and integer processor chip\n  * ■ 8832 registered ALU\n  * ■ 8818 microsequencer\n  * ■ WCS data field\n\n\nAs indicated in Figure 21.17, the 32 bits of the WCS data field are fed into the DA bus to be provided as data to the ALU, floating-point processor, or microsequencer. The other 96 bits (fields 1–27) of the microinstruction are control signals that are fed directly to the appropriate module. For simplicity, these other connections are not shown in Figure 21.17.\n\n\nThe first six fields deal with operations that pertain to the control of the board, rather than controlling an individual component. Control operations include the following:\n\n\n  * ■ Selecting condition codes for sequencer control. The first bit of field 1 indicates whether the condition flag is to be set to 1 or 0, and the remaining 4 bits indicate which flag is to be set.\n  * ■ Sending an I/O request to the PC/AT.\n  * ■ Enabling local data memory read/write operations.\n  * ■ Determining the unit driving the system Y bus. One of the four devices attached to the bus (Figure 21.17) is selected.\n\n\nThe last 32 bits are the data field, which contain information specific to a particular microinstruction.\n\n\nThe remaining fields of the microinstruction are best discussed in the context of the device that they control. In the remainder of this section, we discuss the microsequencer and the registered ALU. The floating-point unit introduces no new concepts and is skipped.\n\n\n\n\n**Microsequencer**\n\n\nThe principal function of the 8818 microsequencer is to generate the next microinstruction address for the microprogram. This 15-bit address is provided to the microcode memory (Figure 21.17).\n\n\nThe next address can be selected from one of five sources:\n\n\n  * 1. The microprogram counter (MPC) register, used for repeat (reuse same address) and continue (increment address by 1) instructions.\n\n\n**Table 21.7**\n\nField Number | Number of Bits | Description\nControl of Board\n1 | 5 | Select condition code input\n2 | 1 | Enable/disable external I/O request signal\n3 | 2 | Enable/disable local data memory read/write operations\n4 | 1 | Load status/do no load status\n5 | 2 | Determine unit driving Y bus\n6 | 2 | Determine unit driving DA bus\n8847 Floating-Point and Integer Processing Chip\n7 | 1 | C register control: clock, do not clock\n8 | 1 | Select most significant or least significant bits for Y bus\n9 | 1 | C register data source: ALU, multiplexer\n10 | 4 | Select IEEE or FAST mode for ALU and MUL\n11 | 8 | Select sources for data operands: RA registers, RB registers, P register, 5 register, C register\n12 | 1 | RB register control: clock, do not clock\n13 | 1 | RA register control: clock, do not clock\n14 | 2 | Data source confirmation\n15 | 2 | Enable/disable pipeline registers\n16 | 11 | 8847 ALU function\n8832 Registered ALU\n17 | 2 | Write enable/disable data output to selected register: most significant half, least significant half\n18 | 2 | Select register file data source: DA bus, DB bus, ALU Y MUX output, system Y bus\n19 | 3 | Shift instruction modifier\n20 | 1 | Carry in: force, do not force\n21 | 2 | Set ALU configuration mode: 32, 16, or 8 bits\n22 | 2 | Select input to 5 multiplexer: register file, DB bus, MQ register\n23 | 1 | Select input to R multiplexer: register file, DA bus\n24 | 6 | Select register in file C for write\n25 | 6 | Select register in file B for read\n26 | 6 | Select register in file A for write\n27 | 8 | ALU function\n8818 Microsequencer\n28 | 12 | Control input signals to the 8818\nWCS Data Field\n29 | 16 | Most significant bits of writable control store data field\n30 | 16 | Least significant bits of writable control store data field\n\n\n  * 2. The stack, which supports microprogram subroutine calls as well as iterative loops and returns from interrupts.\n  * 3. The DRA and DRB ports, which provide two additional paths from external hardware by which microprogram addresses can be generated. These two ports are connected to the most significant and least significant 16 bits of the DA bus, respectively. This allows the microsequencer to obtain the next instruction address from the WCS data field of the current microinstruction or from a result calculated by the ALU.\n  * 4. Register counters RCA and RCB, which can be used for additional address storage.\n  * 5. An external input onto the bidirectional Y port to support external interrupts.\n\n\nFigure 21.18 is a logical block diagram of the 8818. The device consists of the following principal functional groups:\n\n\n  * ■ A 16-bit microprogram counter (MPC) consisting of a register and an incrementer.\n  * ■ Two register counters, RCA and RCB, for counting loops and iterations, storing branch addresses, or driving external devices.\n  * ■ A 65-word by 16-bit stack, which allows microprogram subroutine calls and interrupts.\n  * ■ An interrupt return register and Y output enable for interrupt processing at the microinstruction level.\n  * ■ A Y output multiplexer by which the next address can be selected from MPC, RCA, RCB, external buses DRA and DRB, or the stack.\n\n\n**REGISTERS/COUNTERS**\n   The registers RCA and RCB may be loaded from the DA bus, either from the current microinstruction or from the output of the ALU. The values may be used as counters to control the flow of execution and may be automatically decremented when accessed. The values may also be used as microinstruction addresses to be supplied to the Y output multiplexer. Independent control of both registers during a single microinstruction cycle is supported with the exception of simultaneous decrement of both registers.\n\n\n**STACK**\n   The stack allows multiple levels of nested calls or interrupts, and it can be used to support branching and looping. Keep in mind that these operations refer to the control unit, not the overall processor, and that the addresses involved are those of microinstructions in the control memory.\n\n\nSix stack operations are possible:\n\n\n  * 1. Clear, which sets the stack pointer to zero, emptying the stack;\n  * 2. Pop, which decrements the stack pointer;\n  * 3. Push, which puts the contents of the MPC, interrupt return register, or DRA bus onto the stack and increments the stack pointer;\n  * 4. Read, which makes the address indicated by the read pointer available at the Y output multiplexer;\n\n\n\n\n![Block diagram of the TI 8818 Microsequencer. The diagram shows several interconnected components: a Microprogram counter/incrementer, an Interrupt return register, a Stack, a MUX, Dual registers/counters, and a Y output multiplexer. The Microprogram counter/incrementer and Interrupt return register feed into the Stack. The Stack feeds into a MUX. The MUX feeds into the Dual registers/counters. The Dual registers/counters feed into the Y output multiplexer. The Y output multiplexer feeds into the DRA bus (DA31-DA16 and DA15-DA00) and also provides a 'Next microcode address' output. The Y output multiplexer also has an input labeled B3-B0. The Dual registers/counters also feed into the Y output multiplexer.](images/image_0365.jpeg)\n\n\nBlock diagram of the TI 8818 Microsequencer. The diagram shows several interconnected components: a Microprogram counter/incrementer, an Interrupt return register, a Stack, a MUX, Dual registers/counters, and a Y output multiplexer. The Microprogram counter/incrementer and Interrupt return register feed into the Stack. The Stack feeds into a MUX. The MUX feeds into the Dual registers/counters. The Dual registers/counters feed into the Y output multiplexer. The Y output multiplexer feeds into the DRA bus (DA31-DA16 and DA15-DA00) and also provides a 'Next microcode address' output. The Y output multiplexer also has an input labeled B3-B0. The Dual registers/counters also feed into the Y output multiplexer.\n\n\nFigure 21.18 TI 8818 Microsequencer\n\n\n  * 5. Hold, which causes the address of the stack pointer to remain unchanged;\n  * 6. Load stack pointer, which inputs the seven least significant bits of DRA to the stack pointer.\n\n\n**CONTROL OF MICROSEQUENCER**\n   The microsequencer is controlled primarily by the 12-bit field of the current microinstruction, field 28 (Table 21.7). This field consists of the following subfields:\n\n\n  * ■\n    **OS EL (1 bit):**\n    Output select. Determines which value will be placed on the output of the multiplexer that feeds into the DRA bus (upper-left-hand corner of\n\n\nFigure 21.18). The output is selected to come from either the stack or from register RCA. DRA then serves as input to either the Y output multiplexer or to register RCA.\n\n\n  * ■\n    **SELDR (1 bit):**\n    Select DR bus. If set to 1, this bit selects the external DA bus as input to the DRA/DRB buses. If set to 0, selects the output of the DRA multiplexer to the DRA bus (controlled by OSEL) and the contents of RCB to the DRB bus.\n  * ■\n    **ZEROIN (1 bit):**\n    Used to indicate a conditional branch. The behavior of the microsequencer will then depend on the condition code selected in field 1 (Table 21.7).\n  * ■\n    **RC2–RC0 (3 bits):**\n    Register controls. These bits determine the change in the contents of registers RCA and RCB. Each register can either remain the same, decrement, or load from the DRA/DRB buses.\n  * ■\n    **S2–S0 (3 bits):**\n    Stack controls. These bits determine which stack operation is to be performed.\n  * ■\n    **MUX2–MUX0:**\n    Output controls. These bits, together with the condition code if used, control the Y output multiplexer and therefore the next microinstruction address. The multiplexer can select its output from the stack, DRA, DRB, or MPC.\n\n\nThese bits can be set individually by the programmer. However, this is typically not done. Rather, the programmer uses mnemonics that equate to the bit patterns that would normally be required. Table 21.8 lists the 15 mnemonics for field 28. A microcode assembler converts these into the appropriate bit patterns.\n\n\n**Table 21.8**\n   TI 8818 Microsequencer Microinstruction Bits (Field 28)\n\n\n\nMnemonic | Value | Description\nRST8818 | 000000000110 | Reset Instruction\nBRA88181 | 011000111000 | Branch to DRA Instruction\nBRA88180 | 010000111110 | Branch to DRA Instruction\nINC88181 | 000000111110 | Continue Instruction\nINC88180 | 001000001000 | Continue Instruction\nCAL88181 | 010000110000 | Jump to Subroutine at Address Specified by DRA\nCAL88180 | 010000101110 | Jump to Subroutine at Address Specified by DRA\nRET8818 | 000000011010 | Return from Subroutine\nPUSH8818 | 000000110111 | Push Interrupt Return Address onto Stack\nPOP8818 | 100000010000 | Return from Interrupt\nLOADRA | 000010111110 | Load DRA Counter from DA Bus\nLOADRB | 000110111110 | Load DRB Counter from DA Bus\nLOADDRAB | 000110111100 | Load DRA/DRB\nDECRDRA | 010001111100 | Decrement DRA Counter and Branch If Not Zero\nDECRDRB | 010101111100 | Decrement DRB Counter and Branch If Not Zero\n\n\nAs an example, the instruction INC88181 is used to cause the next microinstruction in sequence to be selected, if the currently selected condition code is 1. From Table 21.8, we have\n\n\n\n   \\text{INC88181} = 000000111110\n  \nwhich decodes directly into\n\n\n  * ■\n    **OSEL = 0:**\n    Selects RCA as output from DRA output MUX; in this case the selection is irrelevant.\n  * ■\n    **SELDR = 0:**\n    As defined previously; again, this is irrelevant for this instruction.\n  * ■\n    **ZEROIN = 0:**\n    Combined with the value for MUX, indicates no branch should be taken.\n  * ■\n    **R = 000:**\n    Retain current value of RA and RC.\n  * ■\n    **S = 111:**\n    Retain current state of stack.\n  * ■\n    **MUX = 110:**\n    Choose MPC when condition code = 1, DRA when condition code = 0.\n\n\n\n\n**Registered ALU**\n\n\nThe 8832 is a 32-bit ALU with 64 registers that can be configured to operate as four 8-bit ALUs, two 16-bit ALUs, or a single 32-bit ALU.\n\n\nThe 8832 is controlled by the 39 bits that make up fields 17 through 27 of the microinstruction (Table 21.7); these are supplied to the ALU as control signals. In addition, as indicated in Figure 21.17, the 8832 has external connections to the 32-bit DA bus and the 32-bit system Y bus. Inputs from the DA can be provided simultaneously as input data to the 64-word register file and to the ALU logic module. Input from the system Y bus is provided to the ALU logic module. Results of the ALU and shift operations are output to the DA bus or the system Y bus. Results can also be fed back to the internal register file.\n\n\nThree 6-bit address ports allow a two-operand fetch and an operand write to be performed within the register file simultaneously. An MQ shifter and MQ register can also be configured to function independently to implement double-precision 8-bit, 16-bit, and 32-bit shift operations.\n\n\nFields 17 through 26 of each microinstruction control the way in which data flows within the 8832 and between the 8832 and the external environment. The fields are as follows:\n\n\n  * 17.\n    **Write Enable.**\n    These two bits specify write 32 bits, 16 most significant bits, 16 least significant bits, or do not write into register file. The destination register is defined by field 24.\n  * 18.\n    **Select Register File Data Source.**\n    If a write is to occur to the register file, these two bits specify the source: DA bus, DB bus, ALU output, or system Y bus.\n  * 19.\n    **Shift Instruction Modifier.**\n    Specifies options concerning supplying end fill bits and reading bits that are shifted during shift instructions.\n  * 20.\n    **Carry In.**\n    This bit indicates whether a bit is carried into the ALU for this operation.\n\n\n  * 21.\n    **ALU Configuration Mode.**\n    The 8832 can be configured to operate as a single 32-bit ALU, two 16-bit ALUs, or four 8-bit ALUs.\n  * 22.\n    **S Input.**\n    The ALU logic module inputs are provided by two internal multiplexers referred to as the S and R multiplexers. This field selects the input to be provided by the S multiplexer: register file, DB bus, or MQ register. The source register is defined by field 25.\n  * 23.\n    **R Input.**\n    Selects input to be provided by the R multiplexer: register file or DA bus.\n  * 24.\n    **Destination Register.**\n    Address of register in register file to be used for the destination operand.\n  * 25.\n    **Source Register.**\n    Address of register in register file to be used for the source operand, provided by the S multiplexer.\n  * 26.\n    **Source Register.**\n    Address of register in register file to be used for the source operand, provided by the R multiplexer.\n\n\nFinally, field 27 is an 8-bit opcode that specifies the arithmetic or logical function to be performed by the ALU. Table 21.9 lists the different operations that can be performed.\n\n\nAs an example of the coding used to specify fields 17 through 27, consider the instruction to add the contents of register 1 to register 2 and place the result in register 3. The symbolic instruction is\n\n\nCONT11 [17], WELH, SELRFYMX, [24], R3, R2, R1, PASS + ADD\nThe assembler will translate this into the appropriate bit pattern. The individual components of the instruction can be described as follows:\n\n\n  * ■ CONT11 is the basic NOP instruction.\n  * ■ Field [17] is changed to WELH (write enable, low and high), so that a 32-bit register is written into.\n  * ■ Field [18] is changed to SELRFYMX to select the feedback from the ALU Y MUX output.\n  * ■ Field [24] is changed to designate register R3 for the destination register.\n  * ■ Field [25] is changed to designate register R2 for one of the source registers.\n  * ■ Field [26] is changed to designate register R1 for one of the source registers.\n  * ■ Field [27] is changed to specify an ALU operation of ADD. The ALU shifter instruction is PASS; therefore, the ALU output is not shifted by the shifter.\n\n\nSeveral points can be made about the symbolic notation. It is not necessary to specify the field number for consecutive fields. That is,\n\n\nCONT11 [17],WELH, [18], SELRFYMX\ncan be written as\n\n\nCONT11 [17],WELH, SELRFYMX\nbecause SELRFYMX is in field 18.\n\n\nALU instructions from Group 1 of Table 21.9 must always be used in conjunction with Group 2. ALU instructions from Groups 3 to 5 must not be used with Group 2.\n\n\n**Table 21.9**\n\nGroup 1 | Function\nADD | H#01 | R + S + C_n\nSUBR | H#02 | (\\text{NOT } R) + S + C_n\nSUBS | H#03 | R = (\\text{NOT } S) + C_n\nINSC | H#04 | S + C_n\nINCNS | H#05 | (\\text{NOT } S) + C_n\nINCR | H#06 | R + C_n\nINCNR | H#07 | (\\text{NOT } R) + C_n\nXOR | H#09 | R \\text{ XOR } S\nAND | H#0A | R \\text{ AND } S\nOR | H#0B | R \\text{ OR } S\nNAND | H#0C | R \\text{ NAND } S\nNOR | H#0D | R \\text{ NOR } S\nANDNR | H#0E | (\\text{NOT } R) \\text{ AND } S\nGroup 2 | Function\nSRA | H#00 | Arithmetic right single precision shift\nSRAD | H#10 | Arithmetic right double precision shift\nSRL | H#20 | Logical right single precision shift\nSRLD | H#30 | Logical right double precision shift\nSLA | H#40 | Arithmetic left single precision shift\nSLAD | H#50 | Arithmetic left double precision shift\nSLC | H#60 | Circular left single precision shift\nSLCD | H#70 | Circular left double precision shift\nSRC | H#80 | Circular right single precision shift\nSRCD | H#90 | Circular right double precision shift\nMQSRA | H#A0 | Arithmetic right shift MQ register\nMQSRL | H#B0 | Logical right shift MQ register\nMQSLL | H#C0 | Logical left shift MQ register\nMQSLC | H#D0 | Circular left shift MQ register\nLOADMQ | H#E0 | Load MQ register\nPASS | H#F0 | Pass ALU to Y (no shift operation)\nGroup 3 | Function\nSET1 | H#08 | Set bit 1\nSet0 | H#18 | Set bit 0\nTB1 | H#28 | Test bit 1\nTB0 | H#38 | Test bit 0\nABS | H#48 | Absolute value\nSMTC | H#58 | Sign magnitude/twos-complement\n\n\n\nGroup 3 | Function\nADDI | H#68 | Add immediate\nSUBI | H#78 | Subtract immediate\nBADD | H#88 | Byte add R to S\nBSUBS | H#98 | Byte subtract S from R\nBSUBR | H#A8 | Byte subtract R from S\nBINCS | H#B8 | Byte increment S\nBINCNS | H#C8 | Byte increment negative S\nBXOR | H#D8 | Byte XOR R and S\nBAND | H#E8 | Byte AND R and S\nBOR | H#F8 | Byte OR R and S\nGroup 4 | Function\nCRC | H#00 | Cyclic redundancy character accum.\nSEL | H#10 | Select S or R\nSNORM | H#20 | Single length normalize\nDNORM | H#30 | Double length normalize\nDIVRF | H#40 | Divide remainder fix\nSDIVQF | H#50 | Signed divide quotient fix\nSMULI | H#60 | Signed multiply iterate\nSMULT | H#70 | Signed multiply terminate\nSDIVIN | H#80 | Signed divide initialize\nSDIVIS | H#90 | Signed divide start\nSDIVI | H#A0 | Signed divide iterate\nUDIVIS | H#B0 | Unsigned divide start\nUDIVI | H#C0 | Unsigned divide iterate\nUMULI | H#D0 | Unsigned multiply iterate\nSDIVIT | H#E0 | Signed divide terminate\nUDIVIT | H#F0 | Unsigned divide terminate\nGroup 5 | Function\nLOADFF | H#0F | Load divide/BCD flip-flops\nCLR | H#1F | Clear\nDUMPPF | H#5F | Output divide/BCD flip-flops\nBCDBIN | H#7F | BCD to binary\nEX3BC | H#8F | Excess (3 byte) correction\nEX3C | H#9F | Excess (3 word) correction\nSDIVO | H#AF | Signed divide overflow test\nBINEX3 | H#DF | Binary to excess - 3\nNOP32 | H#FF | No operation"
        }
      ]
    }
  ]
}