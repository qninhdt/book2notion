# Formats for Encoding Data

Encoding translates in-memory data into byte sequences, evolving from fragile language-specific methods to schema-driven binary formats that optimize for size, performance, and cross-version compatibility.

## The Fragility of Language-Specific Formats

Built-in serialization (e.g., `java.io.Serializable`, `pickle`) is generally unsuitable for distributed systems due to:
- **Interoperability:** Tight coupling to a single language.
- **Security:** Decoding can instantiate arbitrary classes, leading to Remote Code Execution (RCE) vulnerabilities.
- **Versioning:** Compatibility is often an afterthought, leading to brittle systems during rolling upgrades.
- **Efficiency:** Often produces bloated byte sequences with poor CPU performance.

## Textual Formats: JSON, XML, and CSV

While human-readable and widely supported, textual formats suffer from technical ambiguities:
- **Numerical Limits:** JSON cannot distinguish integers from floats and lacks precision specifications. Integers $> 2^{53}$ (e.g., 64-bit IDs) lose precision in JavaScript.
- **Binary Data:** Lack of native support for binary strings necessitates Base64 encoding, increasing data size by $\approx 33\%$.
- **Schema Complexity:** XML Schema is overly verbose; JSON Schema is powerful but lacks universal adoption, leading to hardcoded parsing logic.

## Binary Encoding with Field Tags

Apache Thrift and Protocol Buffers (Protobuf) use Interface Definition Languages (IDLs) to define schemas. 
- **Field Tags:** Instead of field names, binary records use numeric tags (e.g., 1, 2, 3). This significantly reduces payload size.
- **Evolution:** 
  - **Forward Compatibility:** Old code ignores new tags using type annotations to skip bytes.
  - **Backward Compatibility:** New code can read old data as long as new fields are optional or have defaults.
- **Constraint:** Tag numbers must never be changed or reused once assigned.

![Example record encoded using Thrift's BinaryProtocol.](images/image_0029.jpeg)

![Example record encoded using Thrift’s CompactProtocol.](images/image_0030.jpeg)

![Example record encoded using Protocol Buffers.](images/image_0031.jpeg)

## Avro: Decoupled Schema Resolution

Avro differs by omitting field tags and types from the binary data entirely, resulting in the most compact encoding.
- **Writer’s vs. Reader’s Schema:** The reader uses its own schema and the writer's schema (provided via metadata or a registry) to resolve differences.
- **Resolution Logic:** Fields are matched by name. If a reader encounters a field missing in the writer's schema, it uses a default value. If the writer has an extra field, the reader ignores it.
- **Dynamic Schemas:** Ideal for database dumps where the schema is generated programmatically from relational tables.

![Example record encoded using Avro.](images/image_0032.jpeg)

![An Avro reader resolves differences between the writer's schema and the reader's schema.](images/image_0033.jpeg)

```go
// Conceptual implementation of a Variable-Length Integer (Varint) decoder
// Used in Thrift CompactProtocol and Protobuf to save space on small numbers.
func DecodeVarint(buf []byte) (int64, int) {
	var x int64
	var s uint
	for i, b := range buf {
		if b < 0x80 {
			return x | int64(b)<<s, i + 1
		}
		x |= int64(b&0x7f) << s
		s += 7
	}
	return 0, 0
}
```

__*Interview:*__

> **Question:** How does Protocol Buffers handle a situation where a new field is added to a schema but the client is running an old version of the code? (level: mid-level)
> **Answer:** Protobuf provides forward compatibility. The old client will see a field tag it doesn't recognize. Because the binary format includes the wire type (datatype) for each tag, the parser knows how many bytes to skip, allowing it to ignore the new field and process the rest of the record.

> **Question:** Why is Avro considered better for 'schema-on-read' or dynamic data environments compared to Thrift or Protobuf? (level: senior)
> **Answer:** Avro does not use hardcoded field tags. Instead, it relies on schema resolution between the writer's and reader's schemas. This makes it easier to generate schemas dynamically (e.g., from a SQL DB) because you don't need to manage a manual mapping of column names to unique tag IDs; the system matches fields by name during the resolution phase.

__*More:*__

### Real-World System Implementations

- **Apache Kafka:** Frequently uses **Avro** in conjunction with the **Confluent Schema Registry**. The registry stores the writer's schema, and only a 5-byte schema ID is prepended to each message, keeping the payload minimal.
- **gRPC:** Uses **Protocol Buffers** as its default serialization and IDL, leveraging the code generation for strict type safety across microservices.
- **Hadoop/Hive:** Uses **Avro** for large-scale data storage in 'Object Container Files,' where the schema is stored once in the file metadata, making millions of records self-describing and efficient to process.

---

Editorial Logic:

Retained:
- **Language-Specific Serialization Risks**: Critical security and interoperability warnings regarding built-in libraries like Java's Serializable or Python's pickle.
- **Numerical Precision in JSON**: Explains the technical limitation of IEEE 754 double-precision in JavaScript affecting large integers (e.g., Twitter IDs).
- **Field Tags in Thrift/Protobuf**: The fundamental mechanism for achieving forward/backward compatibility in binary protocols.
- **Avro Schema Resolution**: Unique approach to evolution that decouples reader and writer schemas, enabling dynamic data handling.

Omitted:
- **Basic Definitions of Encoding/Decoding**: Standard industry terminology (marshalling/unmarshalling) is assumed knowledge for the target audience.
- **MessagePack Byte Breakdown**: Too granular; the conceptual takeaway (binary JSON still includes field names) is more valuable than specific hex offsets.
- **ASN.1 History**: Historical context that does not impact modern system design or interview performance.


---

# Modes of Dataflow

Data flows between processes via three primary patterns—databases, service calls (REST/RPC), and asynchronous message passing—each requiring specific strategies for forward and backward compatibility to ensure system evolvability.

## Dataflow Through Databases

In databases, the process writing data encodes it, and the process reading it decodes it. This is effectively **sending a message to your future self**. 

*   **Compatibility Requirements:** Backward compatibility is mandatory for reading historical data. Forward compatibility is required during rolling upgrades where an older instance might read and update a record recently written by a newer instance.
*   **The Data Loss Risk:** If an older version of the code decodes a record, ignores a new field it doesn't recognize, and writes the record back, that new field is lost. 
*   **Schema Evolution:** Databases like LinkedIn's Espresso use Avro to allow the database to appear as a single schema despite underlying records being stored in multiple historical versions. 
*   **Archival Storage:** Backups or data warehouse dumps are typically encoded using the latest schema in a consistent, immutable format like Parquet or Avro object container files.

![Diagram illustrating data loss when an older application version updates data written by a newer version.](images/image_0034.jpeg)

## Synchronous Dataflow: REST and RPC

Service-oriented and microservices architectures rely on clients making requests to servers via APIs. 

*   **REST vs. SOAP:** REST is a design philosophy leveraging HTTP features (URLs, cache control, JSON). SOAP is an XML-based protocol relying on WSDL for code generation, often leading to interoperability issues between different vendors.
*   **RPC Limitations:** The goal of 'location transparency' (making a remote call look like a local one) is fundamentally flawed due to:
    1.  **Unpredictability:** Network requests can fail or timeout due to external factors.
    2.  **Idempotence:** Retrying a failed request may cause the action to be performed multiple times unless handled explicitly.
    3.  **Latency:** Network RTT is variable and orders of magnitude slower than local stack execution.
    4.  **Memory:** You cannot pass pointers to remote processes; all data must be serialized.
*   **Evolution:** RPC typically assumes servers are updated before clients. This requires backward compatibility on requests and forward compatibility on responses.

## Asynchronous Message-Passing Systems

Message brokers (e.g., RabbitMQ, Apache Kafka) act as intermediaries, providing a pattern between RPC and databases.

*   **Reliability:** Brokers buffer messages if the recipient is down and handle redelivery after crashes.
*   **Decoupling:** Senders do not need to know the IP/port of consumers; they simply publish to a topic.
*   **Dataflow:** Usually one-way and asynchronous. If a consumer republishes to another topic, it must preserve unknown fields to avoid the data loss issues seen in databases.

## Distributed Actor Frameworks

The actor model encapsulates logic and state into 'actors' that communicate via asynchronous messages. 

*   **Location Transparency:** Fits better here than in RPC because the model already assumes message loss and asynchrony even within a single process.
*   **Frameworks:** 
    *   **Akka:** Uses Java serialization by default (non-evolvable) but supports Protobuf for rolling upgrades.
    *   **Orleans:** Uses custom encoding; often requires cluster-wide restarts for version changes unless custom plugins are used.
    *   **Erlang OTP:** High availability but schema changes to records are historically difficult.

```go
package main

import (
	"encoding/json"
	"fmt"
)

// Person represents a model that preserves unknown fields using json.RawMessage.
// This prevents data loss during the Read-Modify-Write cycle in rolling upgrades.
type Person struct {
	UserName       string          `json:"userName"`
	FavoriteNumber int             `json:"favoriteNumber"`
	// ExtraFields captures any fields not explicitly defined in this version.
	ExtraFields    map[string]json.RawMessage `json:"-"` 
}

func (p *Person) UnmarshalJSON(data []byte) error {
	type Alias Person
	aux := &struct {
		*Alias
	}{
		Alias: (*Alias)(p),
	}
	if err := json.Unmarshal(data, &aux); err != nil {
		return err
	}
	
	// Logic to capture unknown fields into p.ExtraFields would go here
	return nil
}

func main() {
	// Example of data with a 'photoURL' field unknown to this version of the code
	input := []byte(`{"userName": "Martin", "favoriteNumber": 1337, "photoURL": "http://example.com/img.jpg"}`)
	var p Person
	json.Unmarshal(input, &p)
	
	fmt.Printf("Decoded: %+v\n", p)
}
```

__*Interview:*__

> **Question:** Why is 'location transparency' considered a dangerous abstraction in distributed systems? (level: senior)
> **Answer:** It hides the fundamental differences between local and remote calls: network latency is variable, partial failures (timeouts) create ambiguity about state, and the lack of shared memory forces expensive serialization. Treating them as identical leads to fragile systems that don't handle network partitions or congestion gracefully.

> **Question:** Explain the 'data outlives code' principle and its impact on schema design. (level: mid-level)
> **Answer:** While application code is replaced entirely during deployment, data in a database may persist for years. This means the database must support multiple versions of the schema simultaneously, requiring the application to handle both forward and backward compatibility to avoid data loss or crashes.

> **Question:** What are the primary architectural advantages of using a message broker over direct RPC calls? (level: mid-level)
> **Answer:** Brokers provide: 1) Buffering to handle load spikes or consumer downtime. 2) Decoupling of service discovery (sender doesn't need consumer's IP). 3) Fan-out capabilities to multiple subscribers. 4) Improved reliability through automatic retries and message persistence.

__*More:*__

### Real-world Schema Evolution: LinkedIn Espresso

LinkedIn's Espresso database uses Avro for its storage layer. When a schema is updated, Espresso doesn't rewrite existing records. Instead, it stores the schema ID with each record. When a record is read, the system uses the writer's schema and the current reader's schema to resolve differences on the fly, implementing the 'Data Outlives Code' principle efficiently.

### The $2^{53}$ Problem in RPC

When using JSON-based RPC, numeric precision is a common pitfall. JavaScript uses IEEE 754 double-precision floats, which cannot exactly represent integers larger than $2^{53} - 1$. If a backend service (e.g., in Go or Java) sends a 64-bit UID, the JavaScript client will truncate it, leading to silent data corruption. This is a primary reason why binary formats like Protobuf or Thrift, which define explicit integer types, are preferred for internal service communication.

---

Editorial Logic:

Retained:
- **Data Outlives Code**: Crucial architectural observation that database records may persist for years across many application versions.
- **RPC Location Transparency Flaws**: Fundamental technical critique of why remote calls cannot be treated like local function calls.
- **Preservation of Unknown Fields**: Critical for preventing data loss during rolling upgrades where old and new code versions coexist.
- **Asynchronous Message-Passing Advantages**: Highlights the reliability and decoupling benefits of using brokers over direct RPC.

Omitted:
- **History of SOAP vs. REST**: The philosophical debate is less technically relevant than the underlying encoding and compatibility mechanisms.
- **Basic Web Browser Definitions**: Introductory material on how browsers use GET/POST is common knowledge for the target audience.
- **Specific Commercial Broker Lists**: Lists of legacy enterprise software (TIBCO, etc.) add little value to the core architectural concepts.

