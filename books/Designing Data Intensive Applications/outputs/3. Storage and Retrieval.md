# Data Structures That Power Your Database

## Data Structures That Power Your Database

{
  "name": "Data Structures for Storage and Retrieval",
  "summary": "Database storage engines are primarily divided into log-structured (LSM-trees) and page-oriented (B-trees) architectures, each optimizing for different trade-offs between write throughput, read latency, and storage efficiency.",
  "retained": [
    {
      "name": "Log-Structured Merge-Trees (LSM-Trees)",
      "reason": "Core architectural pattern for modern high-write throughput databases."
    },
    {
      "name": "Sorted String Tables (SSTables)",
      "reason": "The fundamental on-disk format for LSM-trees that enables efficient merging and range queries."
    },
    {
      "name": "B-Trees",
      "reason": "The industry-standard indexing structure for relational databases and page-oriented storage."
    },
    {
      "name": "Write Amplification",
      "reason": "Critical performance metric describing the ratio of internal writes to user-triggered writes."
    },
    {
      "name": "Secondary Indexing Strategies",
      "reason": "Essential for understanding clustered vs. non-clustered index performance."
    }
  ],
  "omitted": [
    {
      "name": "Bash Script Database",
      "reason": "Introductory pedagogical tool; lacks technical depth for a principal-level reference."
    },
    {
      "name": "Crow Silhouette Figure",
      "reason": "Non-technical decorative element."
    },
    {
      "name": "Basic Hash Map Definitions",
      "reason": "Assumed prerequisite knowledge for the target audience."
    }
  ],
  "subsections": [
    {
      "name": "Hash Indexes and Append-Only Logs",
      "content": "The simplest storage engine appends records to a log. To optimize $O(n)$ lookups, an in-memory hash map stores keys mapped to byte offsets in the data file. \n\n*   **Bitcask Approach:** All keys must fit in RAM. Values are retrieved via a single disk seek.\n*   **Compaction:** To prevent infinite log growth, the engine segments the log and periodically discards duplicate keys, retaining only the most recent update.\n*   **Crash Recovery:** In-memory maps are rebuilt from the log or loaded from on-disk snapshots.",
      "figures": [
        {
          "caption": "Diagram illustrating the storage of key-value pairs in a CSV-like format, indexed with an in-memory hash map.",
          "id": 15
        },
        {
          "caption": "Diagram illustrating the compaction process of a key-value update log. A 'Data file segment' contains two tables of key-value pairs. The first table has rows: mew: 1078, purr: 2103, purr: 2104, mew: 1079, mew: 1080, mew: 1081. The second table has rows: purr: 2105, purr: 2106, purr: 2107, yawn: 511, purr: 2108, mew: 1082. An arrow labeled 'Compaction process' points from these tables to a 'Compacted segment' table, which contains the unique, most recent key-value pairs: yawn: 511, mew: 1082, and purr: 2108.",
          "id": 16
        }
      ]
    },
    {
      "name": "SSTables and LSM-Trees",
      "content": "Sorted String Tables (SSTables) require key-value pairs to be sorted by key within each segment. This enables:\n\n1.  **Efficient Merging:** Merging segments uses a mergesort-like algorithm, even if files exceed RAM.\n2.  **Sparse Indexes:** Only a fraction of keys need to be indexed in memory. A search for a key involves finding the range in the sparse index and scanning the corresponding segment block.\n3.  **Block Compression:** Contiguous blocks of data can be compressed to reduce I/O and disk usage.\n\n**LSM-Tree Workflow:**\n*   Writes are added to an in-memory balanced tree (**Memtable**).\n*   When the Memtable exceeds a size threshold, it is flushed to disk as an SSTable.\n*   Reads check the Memtable, then segments from newest to oldest.\n*   A **Write-Ahead Log (WAL)** ensures durability of the Memtable.",
      "figures": [
        {
          "caption": "Merging several SSTable segments, retaining only the most recent value for each key.",
          "id": 18
        },
        {
          "caption": "An SSTable with an in-memory index.",
          "id": 19
        }
      ]
    },
    {
      "name": "B-Trees: Page-Oriented Storage",
      "content": "B-trees break the database into fixed-size pages (typically 4 KB) and update them in place. \n\n*   **Structure:** A root page branches into internal pages, eventually reaching leaf pages containing actual data or references.\n*   **Branching Factor:** The number of child references per page (typically hundreds), keeping tree depth to $O(\log n)$.\n*   **Reliability:** Since B-trees overwrite pages, they use a **Write-Ahead Log (WAL)** to recover from crashes during page splits.\n*   **Concurrency:** Managed via 'latches' (lightweight locks) to prevent threads from seeing inconsistent tree states during updates.",
      "figures": [
        {
          "caption": "Looking up a key using a B-tree index.",
          "id": 20
        },
        {
          "caption": "Growing a B-tree by splitting a page.",
          "id": 21
        }
      ]
    },
    {
      "name": "Comparative Performance Analysis",
      "content": "| Feature | LSM-Trees | B-Trees |\n| :--- | :--- | :--- |\n| **Write Throughput** | Higher (Sequential writes) | Lower (Random writes/Overwrites) |\n| **Read Latency** | Higher (May check multiple segments) | Lower (Predictable $O(\log n)$) |\n| **Write Amplification** | Variable (Compaction overhead) | High (WAL + Page overwrites) |\n| **Storage Overhead** | Lower (Better compression/No fragmentation) | Higher (Page fragmentation) |\n| **Transaction Isolation** | Complex (Multiple versions of keys) | Simpler (Key-range locks on pages) |"
    }
  ],
  "code": {
    "content": "type Memtable struct {\n\ttree *redblacktree.Tree\n\tsize int\n}\n\nfunc (db *LSMStore) Set(key string, value []byte) error {\n\t// 1. Append to WAL for durability\n\tif err := db.wal.Append(key, value); err != nil {\n\t\treturn err\n\t}\n\n\t// 2. Update in-memory memtable\n\tdb.memtable.Put(key, value)\n\n\t// 3. Flush to SSTable if threshold reached\n\tif db.memtable.Size() > Threshold {\n\t\tgo db.flushToSSTable(db.memtable)\n\t\tdb.memtable = NewMemtable()\n\t}\n\treturn nil\n}",
    "lang": "go"
  },
  "interview": [
    {
      "question": "Explain 'Write Amplification' and why it matters for SSD-based storage engines.",
      "level": "senior",
      "answer": "Write amplification is the ratio of data written to the storage media versus data written to the database. It matters because SSDs have limited erase cycles; high amplification reduces device lifespan and consumes I/O bandwidth, potentially throttling application throughput."
    },
    {
      "question": "How do Bloom Filters optimize read performance in LSM-tree based storage engines?",
      "level": "mid-level",
      "answer": "LSM-trees may require checking multiple SSTable segments for a key. A Bloom Filter is a space-efficient probabilistic structure that can quickly confirm if a key is *definitely not* in a segment, allowing the engine to skip unnecessary disk I/O for non-existent keys."
    },
    {
      "question": "Compare Clustered vs. Non-Clustered indexes regarding read performance.",
      "level": "mid-level",
      "answer": "A clustered index stores the actual row data within the index structure (e.g., InnoDB's primary key), eliminating the 'extra hop' to a heap file. A non-clustered index only stores a reference to the data, requiring an additional disk seek unless the index 'covers' the query."
    }
  ],
  "more": [
    {
      "name": "Real-World Implementations",
      "content": "*   **RocksDB:** A high-performance LSM-tree engine used as the storage layer for TiDB, CockroachDB, and MyRocks.\n*   **Apache Lucene:** Uses SSTable-like sorted files for its term dictionary, employing Finite State Transducers (FST) for the in-memory sparse index to support fuzzy searching.\n*   **PostGIS:** Extends PostgreSQL with R-trees to handle multi-dimensional geospatial queries that standard B-trees cannot efficiently process."
    },
    {
      "name": "Anti-Caching and In-Memory Evolution",
      "content": "Modern in-memory databases like **VoltDB** or **Redis** achieve high performance by eliminating the overhead of mapping in-memory structures to disk-optimized pages. 'Anti-caching' allows these systems to exceed RAM capacity by evicting cold records to disk while keeping all indexes in memory, maintaining the performance characteristics of an in-memory architecture."
    }
  ]
}

---

Editorial Logic:


---

# Transaction Processing vs. Analytics

OLTP systems focus on low-latency, record-level transactions for end-users, whereas OLAP systems utilize data warehouses and dimensional modeling to perform large-scale analytical aggregations without degrading operational performance.

## Operational vs. Analytical Workloads

Applications generally fall into two categories: **Online Transaction Processing (OLTP)** and **Online Analytic Processing (OLAP)**. OLTP systems are interactive, handling a high volume of small, random-access reads and writes, typically indexed by a key. OLAP systems are designed for business intelligence, scanning millions of records to compute aggregates (sums, counts, averages).

| Property | Transaction Processing (OLTP) | Analytic Systems (OLAP) |
| :--- | :--- | :--- |
| **Read Pattern** | Small number of records per query, fetched by key | Aggregate over large number of records |
| **Write Pattern** | Random-access, low-latency writes from user input | Bulk import (ETL) or event stream |
| **Primary User** | End user/customer via web application | Internal analyst for decision support |
| **Data State** | Latest state (current point in time) | History of events over time |
| **Dataset Size** | Gigabytes to Terabytes | Terabytes to Petabytes |

![A stylized illustration of a crow or raven standing on its feet, facing left. The bird is dark blue or black with a lighter blue or white patch on its neck and chest. It is standing on a small, light-colored base.](images/image_0022.jpeg)

## The Data Warehouse and ETL Pipeline

To prevent expensive analytical queries from impacting the availability and latency of production OLTP systems, enterprises use a **Data Warehouse**. This is a separate database containing a read-only copy of data from various OLTP systems. Data is moved via **ETL (Extract-Transform-Load)**:
1. **Extract**: Pulling data from source OLTP databases.
2. **Transform**: Cleaning and converting data into an analysis-friendly schema.
3. **Load**: Inserting the processed data into the warehouse.

![Diagram illustrating the ETL process into a data warehouse. It shows three OLTP systems (Ecommerce site, Stock-keeping app, Vehicle route planner) feeding data into a central Data warehouse via extraction, transformation, and loading steps. A Business analyst then queries the Data warehouse.](images/image_0023.jpeg)

## Dimensional Modeling: Stars and Snowflakes

Analytical schemas are often structured as **Star Schemas** (Dimensional Modeling):
- **Fact Tables**: Central tables where each row represents an event (e.g., a sale). They are often very 'wide' (100+ columns) and contain quantitative data and foreign keys.
- **Dimension Tables**: Surrounding tables representing the 'who, what, where, when, and why' of an event (e.g., product details, store location, date metadata).

In a **Snowflake Schema**, dimensions are further normalized into sub-dimensions (e.g., splitting 'Brand' out of the 'Product' dimension). While Snowflake reduces redundancy, Star schemas are often preferred for their simplicity and query performance.

![Figure 3-9: Example of a star schema for use in a data warehouse. The diagram shows a central fact table, fact_sales, surrounded by five dimension tables: dim_product, dim_store, dim_date, dim_customer, and dim_promotion. Arrows indicate foreign key relationships from the fact table to each dimension table.](images/image_0024.jpeg)

__*Interview:*__

> **Question:** Why shouldn't you run complex analytical queries directly on your production OLTP database? (level: junior)
> **Answer:** Analytical queries often require full table scans or large joins that consume significant CPU and I/O resources. Running these on a production database can cause resource contention, increasing latency for end-user transactions and potentially impacting system availability.

> **Question:** Compare and contrast Star and Snowflake schemas in a data warehouse. (level: mid-level)
> **Answer:** A Star schema consists of a central fact table connected to denormalized dimension tables, making it simpler to query and often faster due to fewer joins. A Snowflake schema normalizes dimension tables into multiple related tables; while it saves storage space and maintains data integrity better, it increases query complexity and join overhead.

> **Question:** In the context of ETL, why is the 'Transform' step often the most complex part of the pipeline? (level: senior)
> **Answer:** Transformation involves reconciling disparate data models from multiple autonomous OLTP systems. This includes data cleaning, handling schema mismatches, resolving duplicate entities (e.g., the same customer in two different systems), and converting data into a format optimized for columnar storage or dimensional modeling.

__*More:*__

### HTAP: Bridging the Gap

While the text describes the divergence of OLTP and OLAP, modern systems are moving toward **HTAP (Hybrid Transactional/Analytical Processing)**. Technologies like TiDB, SingleStore, and specialized engines in SQL Server allow a single database to handle both workloads by maintaining data in both row-based (for OLTP) and column-based (for OLAP) formats simultaneously, using internal replication to keep them in sync.

---

Editorial Logic:

Retained:
- **OLTP vs. OLAP Characteristics**: Fundamental distinction in database usage patterns and performance requirements.
- **Data Warehousing Rationale**: Explains the architectural necessity of isolating analytical workloads from transactional ones.
- **ETL (Extract-Transform-Load)**: The standard process for data movement between operational and analytical systems.
- **Star and Snowflake Schemas**: Core data modeling patterns for analytical environments.

Omitted:
- **Etymology of 'Transaction'**: Historical context that does not impact technical implementation or system design.
- **Specific Database Vendor Lists**: Vendor names are transient and less critical than the underlying architectural concepts.
- **Small vs. Large Company Comparison**: Business-level observation rather than a technical constraint.


---

# Column-Oriented Storage

Column-oriented storage optimizes analytical workloads by storing each column's values contiguously, enabling aggressive compression and efficient vectorized processing of large datasets.

## Architectural Rationale and Layout

In analytical workloads (OLAP), queries typically access millions of rows but only a few columns. Row-oriented storage (common in OLTP) is inefficient here as it requires loading entire rows into memory. Column-oriented storage stores all values of a single column together on disk. This allows the engine to read only the specific attributes required by a query, significantly reducing I/O requirements.

To maintain row integrity, column-oriented stores rely on a consistent ordering: the $k$-th entry in one column file corresponds to the $k$-th entry in all other column files for that table.

![Illustration of a crow or raven, symbolizing the concept of column storage.](images/image_0025.jpeg)

## Compression and Bitmap Indexing

Contiguous column data is highly repetitive, making it ideal for compression. 

1. **Bitmap Encoding**: For a column with $n$ distinct values, the engine creates $n$ separate bitmaps. Each bit represents a row; it is set to 1 if the row contains that value.
2. **Run-Length Encoding (RLE)**: If bitmaps are sparse, they are compressed using RLE. This is particularly effective for the primary sort key of a table.
3. **Bitwise Operations**: Queries with `WHERE` clauses (e.g., `IN`, `AND`, `OR`) can be executed directly on these bitmaps using highly efficient bitwise operators.

## Vectorized Processing and CPU Efficiency

Beyond I/O reduction, columnar formats optimize CPU cycles:
- **L1 Cache Utilization**: Chunks of compressed column data are loaded into the L1 cache. The engine iterates through these chunks in tight loops, minimizing function calls and branch mispredictions.
- **SIMD Instructions**: Modern CPUs can use Single-Instruction-Multi-Data (SIMD) to process multiple data points in a single clock cycle, a technique known as **vectorized processing**.

![Illustration of a crow or raven, symbolizing Cassandra.](images/image_0026.jpeg)

## Sorting and Write Strategies

While rows are stored by column, they must be sorted as a single unit to maintain the $k$-th index relationship. 
- **Multiple Projections**: Systems like Vertica store the same data in multiple different sort orders to optimize for different query patterns.
- **LSM-Trees for Writes**: Because compressed/sorted columns are difficult to update in place, writes are typically handled via LSM-trees. New data is written to a sorted in-memory buffer (row or column-oriented) and periodically merged with columnar files on disk.

## Materialized Aggregates and Data Cubes

For repetitive aggregate queries (SUM, COUNT, AVG), data warehouses use **Materialized Views**. A specific form is the **Data Cube** (or OLAP Cube), a multi-dimensional grid of precomputed aggregates. 
- **Pros**: Provides near-instant results for pre-defined dimensions.
- **Cons**: Reduced flexibility for ad-hoc queries not covered by the cube's dimensions and increased write overhead due to denormalization.

![Figure 3-12: A data cube table showing sales aggregation by date and product. The table has rows for date_key (140101 to 140104 and total) and columns for product_sk (32 to 35 and total). Each cell contains a sum of net_price for the specific date and product combination. Arrows point from SQL queries to specific cells in the table.](images/image_0027.jpeg)

```go
// Example of a vectorized bitwise AND operation on two column bitmaps.
// This approach leverages CPU caches by processing data in contiguous blocks.
package main

func VectorizedAnd(bitmapA, bitmapB []uint64, result []uint64) {
	if len(bitmapA) != len(bitmapB) || len(bitmapA) != len(result) {
		return
	}
	// A tight loop that the compiler can optimize with SIMD instructions
	for i := 0; i < len(bitmapA); i++ {
		result[i] = bitmapA[i] & bitmapB[i]
	}
}
```

__*Interview:*__

> **Question:** Why is column-oriented storage preferred for OLAP workloads but generally avoided for OLTP? (level: mid-level)
> **Answer:** OLAP queries typically scan millions of rows across few columns; columnar storage minimizes I/O by reading only required columns and enables high compression. OLTP workloads involve frequent updates and single-row lookups; in a column store, updating one row requires modifying multiple files, and reading one row requires multiple seeks, making it inefficient for high-concurrency point writes/reads.

> **Question:** How do LSM-trees solve the 'write problem' in columnar databases? (level: senior)
> **Answer:** Columnar files are often compressed and sorted, making in-place updates computationally expensive. LSM-trees buffer writes in a sorted in-memory structure (memtable). When the buffer reaches a threshold, it is flushed to disk as a new columnar file. Background compaction then merges these files, allowing the system to maintain read performance and sorting without requiring expensive random-access updates to existing compressed blocks.

__*More:*__

### Real-World Implementations

- **Apache Parquet**: A popular columnar storage format for Hadoop and Spark, based on Google's Dremel paper. It supports nested data structures while maintaining columnar benefits.
- **ClickHouse & DuckDB**: Modern analytical databases that use vectorized execution engines to process columnar data at speeds approaching hardware limits.
- **Vertica**: A commercial implementation of the C-Store research project, notable for its use of multiple 'projections' (different sort orders for the same data) to optimize query performance.

---

Editorial Logic:

Retained:
- **Columnar Layout**: Fundamental architectural shift from row-oriented storage necessary for OLAP efficiency.
- **Bitmap Encoding and RLE**: Explains how high-cardinality and low-cardinality data are compressed in columnar formats.
- **Vectorized Processing**: Critical performance optimization involving CPU cache utilization and SIMD instructions.
- **LSM-tree Write Path**: Provides the technical solution for the inherent difficulty of updating compressed, sorted columnar data.
- **Materialized Aggregates (Data Cubes)**: Describes common pre-computation techniques used in data warehousing to speed up repetitive queries.

Omitted:
- **Fruit and Candy Query Example**: Narrative example used for illustration; the underlying logic of column selection is captured elsewhere.
- **Cassandra Column Family Clarification**: While useful for terminology, it is a negative definition (what it is not) rather than a core principle of column storage.
- **Introductory Fluff**: Removed rhetorical questions and transitions to maintain high information density.

