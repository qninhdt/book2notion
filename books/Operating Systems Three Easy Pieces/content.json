{
  "chapters": [
    {
      "name": "Introduction to Operating Systems",
      "sections": [
        {
          "name": "Virtualizing The CPU",
          "content": "Figure 2.1 depicts our first program. It doesn't do much. In fact, all it does is call\n   \n    Spin()\n   \n   , a function that repeatedly checks the time and returns once it has run for a second. Then, it prints out the string that the user passed in on the command line, and repeats, forever.\n\n\nLet's say we save this file as\n   \n    cpu.c\n   \n   and decide to compile and run it on a system with a single processor (or\n   **CPU**\n   as we will sometimes call it). Here is what we will see:\n\n\nprompt> gcc -o cpu cpu.c -Wall\nprompt> ./cpu \"A\"\nA\nA\nA\nA\n^C\nprompt>\nNot too interesting of a run — the system begins running the program, which repeatedly checks the time until a second has elapsed. Once a second has passed, the code prints the input string passed in by the user (in this example, the letter \"A\"), and continues. Note the program will run forever; by pressing \"Control-c\" (which on UNIX-based systems will terminate the program running in the foreground) we can halt the program.\n\n\nprompt> ./cpu A & ./cpu B & ./cpu C & ./cpu D &\n[1] 7353\n[2] 7354\n[3] 7355\n[4] 7356\nA\nB\nD\nC\nA\nB\nD\nC\nA\n...\nFigure 2.2:\n   **Running Many Programs At Once**\n\n\nNow, let's do the same thing, but this time, let's run many different instances of this same program. Figure 2.2 shows the results of this slightly more complicated example.\n\n\nWell, now things are getting a little more interesting. Even though we have only one processor, somehow all four of these programs seem to be running at the same time! How does this magic happen?\n   \n    4\n\n\nIt turns out that the operating system, with some help from the hardware, is in charge of this\n   **illusion**\n   , i.e., the illusion that the system has a very large number of virtual CPUs. Turning a single CPU (or a small set of them) into a seemingly infinite number of CPUs and thus allowing many programs to seemingly run at once is what we call\n   **virtualizing the CPU**\n   , the focus of the first major part of this book.\n\n\nOf course, to run programs, and stop them, and otherwise tell the OS which programs to run, there need to be some interfaces (APIs) that you can use to communicate your desires to the OS. We'll talk about these APIs throughout this book; indeed, they are the major way in which most users interact with operating systems.\n\n\nYou might also notice that the ability to run multiple programs at once raises all sorts of new questions. For example, if two programs want to run at a particular time, which\n   *should*\n   run? This question is answered by a\n   **policy**\n   of the OS; policies are used in many different places within an OS to answer these types of questions, and thus we will study them as we learn about the basic\n   **mechanisms**\n   that operating systems implement (such as the ability to run multiple programs at once). Hence the role of the OS as a\n   **resource manager**\n   .\n\n\n4\n   \n   Note how we ran four processes at the same time, by using the\n   \n    &\n   \n   symbol. Doing so runs a job in the background in the\n   \n    zsh\n   \n   shell, which means that the user is able to immediately issue their next command, which in this case is another program to run. If you're using a different shell (e.g.,\n   \n    tcsh\n   \n   ), it works slightly differently; read documentation online for details.\n\n\n1 #include <unistd.h>\n2 #include <stdio.h>\n3 #include <stdlib.h>\n4 #include \"common.h\"\n5\n6 int\n7 main(int argc, char *argv[])\n8 {\n9     int *p = malloc(sizeof(int));           // a1\n10    assert(p != NULL);\n11    printf(\"( %d) address pointed to by p: %p\\n\",\n12            getpid(), p);                  // a2\n13    *p = 0;                                // a3\n14    while (1) {\n15        Spin(1);\n16        *p = *p + 1;\n17        printf(\"( %d) p: %d\\n\", getpid(), *p); // a4\n18    }\n19    return 0;\n20 }\nFigure 2.3:\n   **A Program That Accesses Memory (mem.c)**"
        },
        {
          "name": "Virtualizing Memory",
          "content": "Now let's consider memory. The model of\n   **physical memory**\n   presented by modern machines is very simple. Memory is just an array of bytes; to\n   **read**\n   memory, one must specify an\n   **address**\n   to be able to access the data stored there; to\n   **write**\n   (or\n   **update**\n   ) memory, one must also specify the data to be written to the given address.\n\n\nMemory is accessed all the time when a program is running. A program keeps all of its data structures in memory, and accesses them through various instructions, like loads and stores or other explicit instructions that access memory in doing their work. Don't forget that each instruction of the program is in memory too; thus memory is accessed on each instruction fetch.\n\n\nLet's take a look at a program (in Figure 2.3) that allocates some memory by calling\n   \n    malloc()\n   \n   . The output of this program can be found here:\n\n\nprompt> ./mem\n(2134) address pointed to by p: 0x200000\n(2134) p: 1\n(2134) p: 2\n(2134) p: 3\n(2134) p: 4\n(2134) p: 5\n^C\nprompt> ./mem & ./mem &\n[1] 24113\n[2] 24114\n(24113) address pointed to by p: 0x200000\n(24114) address pointed to by p: 0x200000\n(24113) p: 1\n(24114) p: 1\n(24114) p: 2\n(24113) p: 2\n(24113) p: 3\n(24114) p: 3\n(24113) p: 4\n(24114) p: 4\n...\nFigure 2.4:\n   **Running The Memory Program Multiple Times**\n\n\nThe program does a couple of things. First, it allocates some memory (line a1). Then, it prints out the address of the memory (a2), and then puts the number zero into the first slot of the newly allocated memory (a3). Finally, it loops, delaying for a second and incrementing the value stored at the address held in\n   \n    p\n   \n   . With every print statement, it also prints out what is called the process identifier (the PID) of the running program. This PID is unique per running process.\n\n\nAgain, this first result is not too interesting. The newly allocated memory is at address\n   \n    0x200000\n   \n   . As the program runs, it slowly updates the value and prints out the result.\n\n\nNow, we again run multiple instances of this same program to see what happens (Figure 2.4). We see from the example that each running program has allocated memory at the same address (\n   \n    0x200000\n   \n   ), and yet each seems to be updating the value at\n   \n    0x200000\n   \n   independently! It is as if each running program has its own private memory, instead of sharing the same physical memory with other running programs\n   \n    5\n   \n   .\n\n\nIndeed, that is exactly what is happening here as the OS is\n   **virtualizing memory**\n   . Each process accesses its own private\n   **virtual address space**\n   (sometimes just called its\n   **address space**\n   ), which the OS somehow maps onto the physical memory of the machine. A memory reference within one running program does not affect the address space of other processes (or the OS itself); as far as the running program is concerned, it has physical memory all to itself. The reality, however, is that physical memory is a shared resource, managed by the operating system. Exactly how all of this is accomplished is also the subject of the first part of this book, on the topic of\n   **virtualization**\n   .\n\n\n5\n   \n   For this example to work, you need to make sure address-space randomization is disabled; randomization, as it turns out, can be a good defense against certain kinds of security flaws. Read more about it on your own, especially if you want to learn how to break into computer systems via stack-smashing attacks. Not that we would recommend such a thing..."
        },
        {
          "name": "Concurrency",
          "content": "1 #include <stdio.h>\n 2 #include <stdlib.h>\n 3 #include \"common.h\"\n 4 #include \"common_threads.h\"\n 5\n 6 volatile int counter = 0;\n 7 int loops;\n 8\n 9 void *worker(void *arg) {\n10     int i;\n11     for (i = 0; i < loops; i++) {\n12         counter++;\n13     }\n14     return NULL;\n15 }\n16\n17 int main(int argc, char *argv[]) {\n18     if (argc != 2) {\n19         fprintf(stderr, \"usage: threads <value>\\n\");\n20         exit(1);\n21     }\n22     loops = atoi(argv[1]);\n23     pthread_t p1, p2;\n24     printf(\"Initial value : %d\\n\", counter);\n25\n26     Pthread_create(&p1, NULL, worker, NULL);\n27     Pthread_create(&p2, NULL, worker, NULL);\n28     Pthread_join(p1, NULL);\n29     Pthread_join(p2, NULL);\n30     printf(\"Final value   : %d\\n\", counter);\n31     return 0;\n32 }\nFigure 2.5:\n   **A Multi-threaded Program (threads.c)**\n\n\nAnother main theme of this book is\n   **concurrency**\n   . We use this conceptual term to refer to a host of problems that arise, and must be addressed, when working on many things at once (i.e., concurrently) in the same program. The problems of concurrency arose first within the operating system itself; as you can see in the examples above on virtualization, the OS is juggling many things at once, first running one process, then another, and so forth. As it turns out, doing so leads to some deep and interesting problems.\n\n\nUnfortunately, the problems of concurrency are no longer limited just to the OS itself. Indeed, modern\n   **multi-threaded**\n   programs exhibit the same problems. Let us demonstrate with an example of a\n   **multi-threaded**\n   program (Figure 2.5).\n\n\nAlthough you might not understand this example fully at the moment (and we'll learn a lot more about it in later chapters, in the section of the book on concurrency), the basic idea is simple. The main program creates two\n   **threads**\n   using\n   \n    Pthread_create()\n   \n\n    6\n   \n   . You can think of a thread as a function running within the same memory space as other functions, with more than one of them active at a time. In this example, each thread starts running in a routine called\n   \n    worker()\n   \n   , in which it simply increments a counter in a loop for\n   \n    loops\n   \n   number of times.\n\n\nBelow is a transcript of what happens when we run this program with the input value for the variable\n   \n    loops\n   \n   set to 1000. The value of\n   \n    loops\n   \n   determines how many times each of the two workers will increment the shared counter in a loop. When the program is run with the value of\n   \n    loops\n   \n   set to 1000, what do you expect the final value of\n   \n    counter\n   \n   to be?\n\n\nprompt> gcc -o threads threads.c -Wall -pthread\nprompt> ./threads 1000\nInitial value : 0\nFinal value   : 2000\nAs you probably guessed, when the two threads are finished, the final value of the counter is 2000, as each thread incremented the counter 1000 times. Indeed, when the input value of\n   \n    loops\n   \n   is set to\n   \n    N\n   \n   , we would expect the final output of the program to be\n   \n    2N\n   \n   . But life is not so simple, as it turns out. Let's run the same program, but with higher values for\n   \n    loops\n   \n   , and see what happens:\n\n\nprompt> ./threads 100000\nInitial value : 0\nFinal value   : 143012    // huh??\nprompt> ./threads 100000\nInitial value : 0\nFinal value   : 137298    // what the??\nIn this run, when we gave an input value of 100,000, instead of getting a final value of 200,000, we instead first get 143,012. Then, when we run the program a second time, we not only again get the\n   *wrong*\n   value, but also a\n   *different*\n   value than the last time. In fact, if you run the program over and over with high values of\n   \n    loops\n   \n   , you may find that sometimes you even get the right answer! So why is this happening?\n\n\nAs it turns out, the reason for these odd and unusual outcomes relate to how instructions are executed, which is one at a time. Unfortunately, a key part of the program above, where the shared counter is incremented,\n\n\n6\n   \n   The actual call should be to lower-case\n   \n    pthread_create()\n   \n   ; the upper-case version is our own wrapper that calls\n   \n    pthread_create()\n   \n   and makes sure that the return code indicates that the call succeeded. See the code for details.\n\n\n\n\n**THE CRUX OF THE PROBLEM:**\n\n\n\n\n**HOW TO BUILD CORRECT CONCURRENT PROGRAMS**\n\n\nWhen there are many concurrently executing threads within the same memory space, how can we build a correctly working program? What primitives are needed from the OS? What mechanisms should be provided by the hardware? How can we use them to solve the problems of concurrency?\n\n\ntakes three instructions: one to load the value of the counter from memory into a register, one to increment it, and one to store it back into memory. Because these three instructions do not execute\n   **atomically**\n   (all at once), strange things can happen. It is this problem of\n   **concurrency**\n   that we will address in great detail in the second part of this book."
        },
        {
          "name": "Persistence",
          "content": "The third major theme of the course is\n   **persistence**\n   . In system memory, data can be easily lost, as devices such as DRAM store values in a\n   **volatile**\n   manner; when power goes away or the system crashes, any data in memory is lost. Thus, we need hardware and software to be able to store data\n   **persistently**\n   ; such storage is thus critical to any system as users care a great deal about their data.\n\n\nThe hardware comes in the form of some kind of\n   **input/output**\n   or\n   **I/O**\n   device; in modern systems, a\n   **hard drive**\n   is a common repository for long-lived information, although\n   **solid-state drives (SSDs)**\n   are making headway in this arena as well.\n\n\nThe software in the operating system that usually manages the disk is called the\n   **file system**\n   ; it is thus responsible for storing any\n   **files**\n   the user creates in a reliable and efficient manner on the disks of the system.\n\n\nUnlike the abstractions provided by the OS for the CPU and memory, the OS does not create a private, virtualized disk for each application. Rather, it is assumed that often times, users will want to\n   **share**\n   information that is in files. For example, when writing a C program, you might first use an editor (e.g., Emacs\n   \n    7\n   \n   ) to create and edit the C file (\n   \n    emacs -nw main.c\n   \n   ). Once done, you might use the compiler to turn the source code into an executable (e.g.,\n   \n    gcc -o main main.c\n   \n   ). When you're finished, you might run the new executable (e.g.,\n   \n    ./main\n   \n   ). Thus, you can see how files are shared across different processes. First, Emacs creates a file that serves as input to the compiler; the compiler uses that input file to create a new executable file (in many steps — take a compiler course for details); finally, the new executable is then run. And thus a new program is born!\n\n\n7\n   \n   You should be using Emacs. If you are using vi, there is probably something wrong with you. If you are using something that is not a real code editor, that is even worse.\n\n\n1 #include <stdio.h>\n2 #include <unistd.h>\n3 #include <assert.h>\n4 #include <fcntl.h>\n5 #include <sys/types.h>\n6\n7 int main(int argc, char *argv[]) {\n8     int fd = open(\"/tmp/file\",\n9                 O_WRONLY|O_CREAT|O_TRUNC,\n10                S_IRWXU);\n11     assert(fd > -1);\n12     int rc = write(fd, \"hello world\\n\", 13);\n13     assert(rc == 13);\n14     close(fd);\n15     return 0;\n16 }\nFigure 2.6: A Program That Does I/O (i.o.c)\n\n\nTo understand this better, let's look at some code. Figure 2.6 presents code to create a file (/tmp/file) that contains the string \"hello world\".\n\n\nTo accomplish this task, the program makes three calls into the operating system. The first, a call to\n   \n    open()\n   \n   , opens the file and creates it; the second,\n   \n    write()\n   \n   , writes some data to the file; the third,\n   \n    close()\n   \n   , simply closes the file thus indicating the program won't be writing any more data to it. These\n   **system calls**\n   are routed to the part of the operating system called the\n   **file system**\n   , which then handles the requests and returns some kind of error code to the user.\n\n\nYou might be wondering what the OS does in order to actually write to disk. We would show you but you'd have to promise to close your eyes first; it is that unpleasant. The file system has to do a fair bit of work: first figuring out where on disk this new data will reside, and then keeping track of it in various structures the file system maintains. Doing so requires issuing I/O requests to the underlying storage device, to either read existing structures or update (write) them. As anyone who has written a\n   **device driver**\n\n    8\n   \n   knows, getting a device to do something on your behalf is an intricate and detailed process. It requires a deep knowledge of the low-level device interface and its exact semantics. Fortunately, the OS provides a standard and simple way to access devices through its system calls. Thus, the OS is sometimes seen as a\n   **standard library**\n   .\n\n\nOf course, there are many more details in how devices are accessed, and how file systems manage data persistently atop said devices. For performance reasons, most file systems first delay such writes for a while, hoping to batch them into larger groups. To handle the problems of system crashes during writes, most file systems incorporate some kind of\n\n\n8\n   \n   A device driver is some code in the operating system that knows how to deal with a specific device. We will talk more about devices and device drivers later.\n\n\n**THE CRUX OF THE PROBLEM:\n    \n\n    HOW TO STORE DATA PERSISTENTLY**\n\n\nThe file system is the part of the OS in charge of managing persistent data. What techniques are needed to do so correctly? What mechanisms and policies are required to do so with high performance? How is reliability achieved, in the face of failures in hardware and software?\n\n\nintricate write protocol, such as\n   **journaling**\n   or\n   **copy-on-write**\n   , carefully ordering writes to disk to ensure that if a failure occurs during the write sequence, the system can recover to reasonable state afterwards. To make different common operations efficient, file systems employ many different data structures and access methods, from simple lists to complex b-trees. If all of this doesn't make sense yet, good! We'll be talking about all of this quite a bit more in the third part of this book on\n   **persistence**\n   , where we'll discuss devices and I/O in general, and then disks, RAIDs, and file systems in great detail."
        },
        {
          "name": "Design Goals",
          "content": "So now you have some idea of what an OS actually does: it takes physical\n   **resources**\n   , such as a CPU, memory, or disk, and\n   **virtualizes**\n   them. It handles tough and tricky issues related to\n   **concurrency**\n   . And it stores files\n   **persistently**\n   , thus making them safe over the long-term. Given that we want to build such a system, we want to have some goals in mind to help focus our design and implementation and make trade-offs as necessary; finding the right set of trade-offs is a key to building systems.\n\n\nOne of the most basic goals is to build up some\n   **abstractions**\n   in order to make the system convenient and easy to use. Abstractions are fundamental to everything we do in computer science. Abstraction makes it possible to write a large program by dividing it into small and understandable pieces, to write such a program in a high-level language like C\n   \n    9\n   \n   without thinking about assembly, to write code in assembly without thinking about logic gates, and to build a processor out of gates without thinking too much about transistors. Abstraction is so fundamental that sometimes we forget its importance, but we won't here; thus, in each section, we'll discuss some of the major abstractions that have developed over time, giving you a way to think about pieces of the OS.\n\n\nOne goal in designing and implementing an operating system is to provide high\n   **performance**\n   ; another way to say this is our goal is to\n   **minimize the overheads**\n   of the OS. Virtualization and making the system easy to use are well worth it, but not at any cost; thus, we must strive to pro-\n\n\n9\n   \n   Some of you might object to calling C a high-level language. Remember this is an OS course, though, where we're simply happy not to have to code in assembly all the time!\n\n\nvide virtualization and other OS features without excessive overheads. These overheads arise in a number of forms: extra time (more instructions) and extra space (in memory or on disk). We'll seek solutions that minimize one or the other or both, if possible. Perfection, however, is not always attainable, something we will learn to notice and (where appropriate) tolerate.\n\n\nAnother goal will be to provide\n   **protection**\n   between applications, as well as between the OS and applications. Because we wish to allow many programs to run at the same time, we want to make sure that the malicious or accidental bad behavior of one does not harm others; we certainly don't want an application to be able to harm the OS itself (as that would affect\n   *all*\n   programs running on the system). Protection is at the heart of one of the main principles underlying an operating system, which is that of\n   **isolation**\n   ; isolating processes from one another is the key to protection and thus underlies much of what an OS must do.\n\n\nThe operating system must also run non-stop; when it fails,\n   *all*\n   applications running on the system fail as well. Because of this dependence, operating systems often strive to provide a high degree of\n   **reliability**\n   . As operating systems grow evermore complex (sometimes containing millions of lines of code), building a reliable operating system is quite a challenge — and indeed, much of the on-going research in the field (including some of our own work [BS+09, SS+10]) focuses on this exact problem.\n\n\nOther goals make sense:\n   **energy-efficiency**\n   is important in our increasingly green world;\n   **security**\n   (an extension of protection, really) against malicious applications is critical, especially in these highly-networked times;\n   **mobility**\n   is increasingly important as OSes are run on smaller and smaller devices. Depending on how the system is used, the OS will have different goals and thus likely be implemented in at least slightly different ways. However, as we will see, many of the principles we will present on how to build an OS are useful on a range of different devices."
        },
        {
          "name": "Some History",
          "content": "Before closing this introduction, let us present a brief history of how operating systems developed. Like any system built by humans, good ideas accumulated in operating systems over time, as engineers learned what was important in their design. Here, we discuss a few major developments. For a richer treatment, see Brinch Hansen's excellent history of operating systems [BH00].\n\n\n\n\n**Early Operating Systems: Just Libraries**\n\n\nIn the beginning, the operating system didn't do too much. Basically, it was just a set of libraries of commonly-used functions; for example, instead of having each programmer of the system write low-level I/O\n\n\nhandling code, the “OS” would provide such APIs, and thus make life easier for the developer.\n\n\nUsually, on these old mainframe systems, one program ran at a time, as controlled by a human operator. Much of what you think a modern OS would do (e.g., deciding what order to run jobs in) was performed by this operator. If you were a smart developer, you would be nice to this operator, so that they might move your job to the front of the queue.\n\n\nThis mode of computing was known as\n   **batch**\n   processing, as a number of jobs were set up and then run in a “batch” by the operator. Computers, as of that point, were not used in an interactive manner, because of cost: it was simply too expensive to let a user sit in front of the computer and use it, as most of the time it would just sit idle then, costing the facility hundreds of thousands of dollars per hour [BH00].\n\n\n\n\n**Beyond Libraries: Protection**\n\n\nIn moving beyond being a simple library of commonly-used services, operating systems took on a more central role in managing machines. One important aspect of this was the realization that code run on behalf of the OS was special; it had control of devices and thus should be treated differently than normal application code. Why is this? Well, imagine if you allowed any application to read from anywhere on the disk; the notion of privacy goes out the window, as any program could read any file. Thus, implementing a\n   **file system**\n   (to manage your files) as a library makes little sense. Instead, something else was needed.\n\n\nThus, the idea of a\n   **system call**\n   was invented, pioneered by the Atlas computing system [K+61,L78]. Instead of providing OS routines as a library (where you just make a\n   **procedure call**\n   to access them), the idea here was to add a special pair of hardware instructions and hardware state to make the transition into the OS a more formal, controlled process.\n\n\nThe key difference between a system call and a procedure call is that a system call transfers control (i.e., jumps) into the OS while simultaneously raising the\n   **hardware privilege level**\n   . User applications run in what is referred to as\n   **user mode**\n   which means the hardware restricts what applications can do; for example, an application running in user mode can’t typically initiate an I/O request to the disk, access any physical memory page, or send a packet on the network. When a system call is initiated (usually through a special hardware instruction called a\n   **trap**\n   ), the hardware transfers control to a pre-specified\n   **trap handler**\n   (that the OS set up previously) and simultaneously raises the privilege level to\n   **kernel mode**\n   . In kernel mode, the OS has full access to the hardware of the system and thus can do things like initiate an I/O request or make more memory available to a program. When the OS is done servicing the request, it passes control back to the user via a special\n   **return-from-trap**\n   instruction, which reverts to user mode while simultaneously passing control back to where the application left off.\n\n\n\n\n**The Era of Multiprogramming**\n\n\nWhere operating systems really took off was in the era of computing beyond the mainframe, that of the\n   **minicomputer**\n   . Classic machines like the PDP family from Digital Equipment made computers hugely more affordable; thus, instead of having one mainframe per large organization, now a smaller collection of people within an organization could likely have their own computer. Not surprisingly, one of the major impacts of this drop in cost was an increase in developer activity; more smart people got their hands on computers and thus made computer systems do more interesting and beautiful things.\n\n\nIn particular,\n   **multiprogramming**\n   became commonplace due to the desire to make better use of machine resources. Instead of just running one job at a time, the OS would load a number of jobs into memory and switch rapidly between them, thus improving CPU utilization. This switching was particularly important because I/O devices were slow; having a program wait on the CPU while its I/O was being serviced was a waste of CPU time. Instead, why not switch to another job and run it for a while?\n\n\nThe desire to support multiprogramming and overlap in the presence of I/O and interrupts forced innovation in the conceptual development of operating systems along a number of directions. Issues such as\n   **memory protection**\n   became important; we wouldn't want one program to be able to access the memory of another program. Understanding how to deal with the\n   **concurrency**\n   issues introduced by multiprogramming was also critical; making sure the OS was behaving correctly despite the presence of interrupts is a great challenge. We will study these issues and related topics later in the book.\n\n\nOne of the major practical advances of the time was the introduction of the UNIX operating system, primarily thanks to Ken Thompson (and Dennis Ritchie) at Bell Labs (yes, the phone company). UNIX took many good ideas from different operating systems (particularly from Multics [O72], and some from systems like TENEX [B+72] and the Berkeley Time-Sharing System [S68]), but made them simpler and easier to use. Soon this team was shipping tapes containing UNIX source code to people around the world, many of whom then got involved and added to the system themselves; see the\n   **Aside**\n   (next page) for more detail\n   \n    10\n   \n   .\n\n\n\n\n**The Modern Era**\n\n\nBeyond the minicomputer came a new type of machine, cheaper, faster, and for the masses: the\n   **personal computer**\n   , or\n   **PC**\n   as we call it today. Led by Apple's early machines (e.g., the Apple II) and the IBM PC, this new breed of machine would soon become the dominant force in computing,\n\n\n10\n   \n   We'll use asides and other related text boxes to call attention to various items that don't quite fit the main flow of the text. Sometimes, we'll even use them just to make a joke, because why not have a little fun along the way? Yes, many of the jokes are bad.\n\n\n\n\n**ASIDE: THE IMPORTANCE OF UNIX**\n\n\nIt is difficult to overstate the importance of UNIX in the history of operating systems. Influenced by earlier systems (in particular, the famous\n   **Multics**\n   system from MIT), UNIX brought together many great ideas and made a system that was both simple and powerful.\n\n\nUnderlying the original “Bell Labs” UNIX was the unifying principle of building small powerful programs that could be connected together to form larger workflows. The\n   **shell**\n   , where you type commands, provided primitives such as\n   **pipes**\n   to enable such meta-level programming, and thus it became easy to string together programs to accomplish a bigger task. For example, to find lines of a text file that have the word “foo” in them, and then to count how many such lines exist, you would type:\n   \n    grep foo file.txt|wc -l\n   \n   , thus using the\n   \n    grep\n   \n   and\n   \n    wc\n   \n   (word count) programs to achieve your task.\n\n\nThe UNIX environment was friendly for programmers and developers alike, also providing a compiler for the new\n   **C programming language**\n   . Making it easy for programmers to write their own programs, as well as share them, made UNIX enormously popular. And it probably helped a lot that the authors gave out copies for free to anyone who asked, an early form of\n   **open-source software**\n   .\n\n\nAlso of critical importance was the accessibility and readability of the code. Having a beautiful, small kernel written in C invited others to play with the kernel, adding new and cool features. For example, an enterprising group at Berkeley, led by\n   **Bill Joy**\n   , made a wonderful distribution (the\n   **Berkeley Systems Distribution**\n   , or\n   **BSD**\n   ) which had some advanced virtual memory, file system, and networking subsystems. Joy later co-founded\n   **Sun Microsystems**\n   .\n\n\nUnfortunately, the spread of UNIX was slowed a bit as companies tried to assert ownership and profit from it, an unfortunate (but common) result of lawyers getting involved. Many companies had their own variants:\n   **SunOS**\n   from Sun Microsystems,\n   **AIX**\n   from IBM,\n   **HPUX**\n   (a.k.a. “H-Pucks”) from HP, and\n   **IRIX**\n   from SGI. The legal wrangling among AT&T/Bell Labs and these other players cast a dark cloud over UNIX, and many wondered if it would survive, especially as Windows was introduced and took over much of the PC market...\n\n\nas their low-cost enabled one machine per desktop instead of a shared minicomputer per workgroup.\n\n\nUnfortunately, for operating systems, the PC at first represented a great leap backwards, as early systems forgot (or never knew of) the lessons learned in the era of minicomputers. For example, early operating systems such as\n   **DOS**\n   (the\n   **Disk Operating System**\n   , from\n   **Microsoft**\n   ) didn’t think memory protection was important; thus, a malicious (or perhaps just a poorly-programmed) application could scribble all over mem-\n\n\n\n\n**ASIDE: AND THEN CAME LINUX**\n\n\nFortunately for UNIX, a young Finnish hacker named\n   **Linus Torvalds**\n   decided to write his own version of UNIX which borrowed heavily on the principles and ideas behind the original system, but not from the code base, thus avoiding issues of legality. He enlisted help from many others around the world, took advantage of the sophisticated GNU tools that already existed [G85], and soon\n   **Linux**\n   was born (as well as the modern open-source software movement).\n\n\nAs the internet era came into place, most companies (such as Google, Amazon, Facebook, and others) chose to run Linux, as it was free and could be readily modified to suit their needs; indeed, it is hard to imagine the success of these new companies had such a system not existed. As smart phones became a dominant user-facing platform, Linux found a stronghold there too (via Android), for many of the same reasons. And Steve Jobs took his UNIX-based\n   **NeXTStep**\n   operating environment with him to Apple, thus making UNIX popular on desktops (though many users of Apple technology are probably not even aware of this fact). Thus UNIX lives on, more important today than ever before. The computing gods, if you believe in them, should be thanked for this wonderful outcome.\n\n\nory. The first generations of the\n   **Mac OS**\n   (v9 and earlier) took a cooperative approach to job scheduling; thus, a thread that accidentally got stuck in an infinite loop could take over the entire system, forcing a reboot. The painful list of OS features missing in this generation of systems is long, too long for a full discussion here.\n\n\nFortunately, after some years of suffering, the old features of mini-computer operating systems started to find their way onto the desktop. For example, Mac OS X/macOS has UNIX at its core, including all of the features one would expect from such a mature system. Windows has similarly adopted many of the great ideas in computing history, starting in particular with Windows NT, a great leap forward in Microsoft OS technology. Even today's cell phones run operating systems (such as Linux) that are much more like what a minicomputer ran in the 1970s than what a PC ran in the 1980s (thank goodness); it is good to see that the good ideas developed in the heyday of OS development have found their way into the modern world. Even better is that these ideas continue to develop, providing more features and making modern systems even better for users and applications."
        }
      ]
    },
    {
      "name": "The Abstraction: The Process",
      "sections": [
        {
          "name": "The Abstraction: A Process",
          "content": "The abstraction provided by the OS of a running program is something we will call a\n   **process**\n   . As we said above, a process is simply a running program; at any instant in time, we can summarize a process by taking an inventory of the different pieces of the system it accesses or affects during the course of its execution.\n\n\nTo understand what constitutes a process, we thus have to understand its\n   **machine state**\n   : what a program can read or update when it is running. At any given time, what parts of the machine are important to the execution of this program?\n\n\nOne obvious component of machine state that comprises a process is its\n   *memory*\n   . Instructions lie in memory; the data that the running program reads and writes sits in memory as well. Thus the memory that the process can address (called its\n   **address space**\n   ) is part of the process.\n\n\nAlso part of the process's machine state are\n   *registers*\n   ; many instructions explicitly read or update registers and thus clearly they are important to the execution of the process.\n\n\nNote that there are some particularly special registers that form part of this machine state. For example, the\n   **program counter (PC)**\n   (sometimes called the\n   **instruction pointer**\n   or\n   **IP**\n   ) tells us which instruction of the program will execute next; similarly a\n   **stack pointer**\n   and associated\n   **frame**\n\n\n**TIP: SEPARATE POLICY AND MECHANISM**\nIn many operating systems, a common design paradigm is to separate high-level policies from their low-level mechanisms [L+75]. You can think of the mechanism as providing the answer to a\n   *how*\n   question about a system; for example,\n   *how*\n   does an operating system perform a context switch? The policy provides the answer to a\n   *which*\n   question; for example,\n   *which*\n   process should the operating system run right now? Separating the two allows one easily to change policies without having to rethink the mechanism and is thus a form of\n   **modularity**\n   , a general software design principle.\n\n\n**pointer**\n   are used to manage the stack for function parameters, local variables, and return addresses.\n\n\nFinally, programs often access persistent storage devices too. Such\n   *I/O information*\n   might include a list of the files the process currently has open."
        },
        {
          "name": "Process API",
          "content": "Though we defer discussion of a real process API until a subsequent chapter, here we first give some idea of what must be included in any interface of an operating system. These APIs, in some form, are available on any modern operating system.\n\n\n  * •\n    **Create:**\n    An operating system must include some method to create new processes. When you type a command into the shell, or double-click on an application icon, the OS is invoked to create a new process to run the program you have indicated.\n  * •\n    **Destroy:**\n    As there is an interface for process creation, systems also provide an interface to destroy processes forcefully. Of course, many processes will run and just exit by themselves when complete; when they don't, however, the user may wish to kill them, and thus an interface to halt a runaway process is quite useful.\n  * •\n    **Wait:**\n    Sometimes it is useful to wait for a process to stop running; thus some kind of waiting interface is often provided.\n  * •\n    **Miscellaneous Control:**\n    Other than killing or waiting for a process, there are sometimes other controls that are possible. For example, most operating systems provide some kind of method to suspend a process (stop it from running for a while) and then resume it (continue it running).\n  * •\n    **Status:**\n    There are usually interfaces to get some status information about a process as well, such as how long it has run for, or what state it is in.\n\n\n\n\n![Diagram illustrating the loading of a program from disk into memory to create a process.](images/image_0001.jpeg)\n\n\nThe diagram illustrates the loading process from disk to memory. At the bottom is a cylinder labeled 'Disk' containing a 'Program' with 'code' and 'static data' sections. A horizontal arrow points from the disk to the 'Memory' box above. The 'Memory' box contains a 'Process' with 'code', 'static data', 'heap', and 'stack' sections. A dashed arrow labeled 'Loading: Takes on-disk program and reads it into the address space of process' points from the disk's 'code' and 'static data' to the memory's 'code' and 'static data' sections. A vertical line connects the 'CPU' box (at the top left) to the horizontal arrow, indicating its involvement in the process.\n\n\nDiagram illustrating the loading of a program from disk into memory to create a process.\n\n\nFigure 4.1:\n   **Loading: From Program To Process**"
        },
        {
          "name": "Process Creation: A Little More Detail",
          "content": "One mystery that we should unmask a bit is how programs are transformed into processes. Specifically, how does the OS get a program up and running? How does process creation actually work?\n\n\nThe first thing that the OS must do to run a program is to\n   **load**\n   its code and any static data (e.g., initialized variables) into memory, into the address space of the process. Programs initially reside on\n   **disk**\n   (or, in some modern systems,\n   **flash-based SSDs**\n   ) in some kind of\n   **executable format**\n   ; thus, the process of loading a program and static data into memory requires the OS to read those bytes from disk and place them in memory somewhere (as shown in Figure 4.1).\n\n\nIn early (or simple) operating systems, the loading process is done\n   **early**\n   , i.e., all at once before running the program; modern OSes perform the process\n   **lazily**\n   , i.e., by loading pieces of code or data only as they are needed during program execution. To truly understand how lazy loading of pieces of code and data works, you'll have to understand more about\n\n\nthe machinery of\n   **paging**\n   and\n   **swapping**\n   , topics we'll cover in the future when we discuss the virtualization of memory. For now, just remember that before running anything, the OS clearly must do some work to get the important program bits from disk into memory.\n\n\nOnce the code and static data are loaded into memory, there are a few other things the OS needs to do before running the process. Some memory must be allocated for the program's\n   **run-time stack**\n   (or just\n   **stack**\n   ). As you should likely already know, C programs use the stack for local variables, function parameters, and return addresses; the OS allocates this memory and gives it to the process. The OS will also likely initialize the stack with arguments; specifically, it will fill in the parameters to the\n   \n    main()\n   \n   function, i.e.,\n   \n    argc\n   \n   and the\n   \n    argv\n   \n   array.\n\n\nThe OS may also allocate some memory for the program's\n   **heap**\n   . In C programs, the heap is used for explicitly requested dynamically-allocated data; programs request such space by calling\n   \n    malloc()\n   \n   and free it explicitly by calling\n   \n    free()\n   \n   . The heap is needed for data structures such as linked lists, hash tables, trees, and other interesting data structures. The heap will be small at first; as the program runs, and requests more memory via the\n   \n    malloc()\n   \n   library API, the OS may get involved and allocate more memory to the process to help satisfy such calls.\n\n\nThe OS will also do some other initialization tasks, particularly as related to input/output (I/O). For example, in UNIX systems, each process by default has three open\n   **file descriptors**\n   , for standard input, output, and error; these descriptors let programs easily read input from the terminal and print output to the screen. We'll learn more about I/O, file descriptors, and the like in the third part of the book on\n   **persistence**\n   .\n\n\nBy loading the code and static data into memory, by creating and initializing a stack, and by doing other work as related to I/O setup, the OS has now (finally) set the stage for program execution. It thus has one last task: to start the program running at the entry point, namely\n   \n    main()\n   \n   . By jumping to the\n   \n    main()\n   \n   routine (through a specialized mechanism that we will discuss next chapter), the OS transfers control of the CPU to the newly-created process, and thus the program begins its execution."
        },
        {
          "name": "Process States",
          "content": "Now that we have some idea of what a process is (though we will continue to refine this notion), and (roughly) how it is created, let us talk about the different\n   **states**\n   a process can be in at a given time. The notion that a process can be in one of these states arose in early computer systems [DV66,V+65]. In a simplified view, a process can be in one of three states:\n\n\n  * •\n    **Running:**\n    In the running state, a process is running on a processor. This means it is executing instructions.\n  * •\n    **Ready:**\n    In the ready state, a process is ready to run but for some reason the OS has chosen not to run it at this given moment.\n\n\n\n\n![Figure 4.2: Process State Transitions. A state transition diagram with three states: Running, Ready, and Blocked. Running and Ready are connected by a double-headed arrow labeled 'Descheduled' and 'Scheduled'. Running has a downward arrow to Blocked labeled 'I/O: initiate'. Blocked has an upward arrow to Ready labeled 'I/O: done'.](images/image_0002.jpeg)\n\n\ngraph TD\n    Running((Running)) <-->|Descheduled / Scheduled| Ready((Ready))\n    Running -- \"I/O: initiate\" --> Blocked((Blocked))\n    Blocked -- \"I/O: done\" --> Ready\n  \nFigure 4.2: Process State Transitions. A state transition diagram with three states: Running, Ready, and Blocked. Running and Ready are connected by a double-headed arrow labeled 'Descheduled' and 'Scheduled'. Running has a downward arrow to Blocked labeled 'I/O: initiate'. Blocked has an upward arrow to Ready labeled 'I/O: done'.\n\n\n**Process: State Transitions**\n  * •\n    **Blocked:**\n    In the blocked state, a process has performed some kind of operation that makes it not ready to run until some other event takes place. A common example: when a process initiates an I/O request to a disk, it becomes blocked and thus some other process can use the processor.\n\n\nIf we were to map these states to a graph, we would arrive at the diagram in Figure 4.2. As you can see in the diagram, a process can be moved between the ready and running states at the discretion of the OS. Being moved from ready to running means the process has been\n   **scheduled**\n   ; being moved from running to ready means the process has been\n   **descheduled**\n   . Once a process has become blocked (e.g., by initiating an I/O operation), the OS will keep it as such until some event occurs (e.g., I/O completion); at that point, the process moves to the ready state again (and potentially immediately to running again, if the OS so decides).\n\n\nLet's look at an example of how two processes might transition through some of these states. First, imagine two processes running, each of which only use the CPU (they do no I/O). In this case, a trace of the state of each process might look like this (Figure 4.3).\n\n\n\nTime | Process\n      \n       0 | Process\n      \n       1 | Notes\n1 | Running | Ready | \n2 | Running | Ready | \n3 | Running | Ready | \n4 | Running | Ready | Process\n      \n       0\n      \n      now done\n5 | — | Running | \n6 | — | Running | \n7 | — | Running | \n8 | — | Running | Process\n      \n       1\n      \n      now done\n\n\n**Tracing Process State: CPU Only**\n\nTime | Process\n      \n       0 | Process\n      \n       1 | Notes\n1 | Running | Ready | \n2 | Running | Ready | \n3 | Running | Ready | Process\n      \n       0\n      \n      initiates I/O\n4 | Blocked | Running | Process\n      \n       0\n      \n      is blocked,\n      \n      so Process\n      \n       1\n      \n      runs\n5 | Blocked | Running | \n6 | Blocked | Running | \n7 | Ready | Running | I/O done\n8 | Ready | Running | Process\n      \n       1\n      \n      now done\n9 | Running | — | \n10 | Running | — | Process\n      \n       0\n      \n      now done\n\n\nFigure 4.4: Tracing Process State: CPU and I/O\n\n\nIn this next example, the first process issues an I/O after running for some time. At that point, the process is blocked, giving the other process a chance to run. Figure 4.4 shows a trace of this scenario.\n\n\nMore specifically, Process\n   \n    0\n   \n   initiates an I/O and becomes blocked waiting for it to complete; processes become blocked, for example, when reading from a disk or waiting for a packet from a network. The OS recognizes Process\n   \n    0\n   \n   is not using the CPU and starts running Process\n   \n    1\n   \n   . While Process\n   \n    1\n   \n   is running, the I/O completes, moving Process\n   \n    0\n   \n   back to ready. Finally, Process\n   \n    1\n   \n   finishes, and Process\n   \n    0\n   \n   runs and then is done.\n\n\nNote that there are many decisions the OS must make, even in this simple example. First, the system had to decide to run Process\n   \n    1\n   \n   while Process\n   \n    0\n   \n   issued an I/O; doing so improves resource utilization by keeping the CPU busy. Second, the system decided not to switch back to Process\n   \n    0\n   \n   when its I/O completed; it is not clear if this is a good decision or not. What do you think? These types of decisions are made by the OS\n   **scheduler**\n   , a topic we will discuss a few chapters in the future."
        },
        {
          "name": "Data Structures",
          "content": "The OS is a program, and like any program, it has some key data structures that track various relevant pieces of information. To track the state of each process, for example, the OS likely will keep some kind of\n   **process list**\n   for all processes that are ready and some additional information to track which process is currently running. The OS must also track, in some way, blocked processes; when an I/O event completes, the OS should make sure to wake the correct process and ready it to run again.\n\n\nFigure 4.5 shows what type of information an OS needs to track about each process in the xv6 kernel [CK+08]. Similar process structures exist in “real” operating systems such as Linux, Mac OS X, or Windows; look them up and see how much more complex they are.\n\n\nFrom the figure, you can see a couple of important pieces of information the OS tracks about a process. The\n   **register context**\n   will hold, for a\n\n\n// the registers xv6 will save and restore\n// to stop and subsequently restart a process\nstruct context {\n    int eip;\n    int esp;\n    int ebx;\n    int ecx;\n    int edx;\n    int esi;\n    int edi;\n    int ebp;\n};\n\n// the different states a process can be in\nenum proc_state { UNUSED, EMBRYO, SLEEPING,\n                  RUNNABLE, RUNNING, ZOMBIE };\n\n// the information xv6 tracks about each process\n// including its register context and state\nstruct proc {\n    char *mem;           // Start of process memory\n    uint sz;             // Size of process memory\n    char *kstack;        // Bottom of kernel stack\n                        // for this process\n\n    enum proc_state state; // Process state\n    int pid;             // Process ID\n    struct proc *parent; // Parent process\n    void *chan;          // If !zero, sleeping on chan\n    int killed;          // If !zero, has been killed\n    struct file *ofile[NOFILE]; // Open files\n    struct inode *cwd;   // Current directory\n    struct context context; // Switch here to run process\n    struct trapframe *tf; // Trap frame for the\n                        // current interrupt\n};\n\nFigure 4.5: The xv6 Proc Structure\n\n\nstopped process, the contents of its registers. When a process is stopped, its registers will be saved to this memory location; by restoring these registers (i.e., placing their values back into the actual physical registers), the OS can resume running the process. We'll learn more about this technique known as a\n   **context switch**\n   in future chapters.\n\n\nYou can also see from the figure that there are some other states a process can be in, beyond running, ready, and blocked. Sometimes a system will have an\n   **initial**\n   state that the process is in when it is being created. Also, a process could be placed in a\n   **final**\n   state where it has exited but\n\n\n\n\n**ASIDE: DATA STRUCTURE — THE PROCESS LIST**\n\n\nOperating systems are replete with various important\n   **data structures**\n   that we will discuss in these notes. The\n   **process list**\n   (also called the\n   **task list**\n   ) is the first such structure. It is one of the simpler ones, but certainly any OS that has the ability to run multiple programs at once will have something akin to this structure in order to keep track of all the running programs in the system. Sometimes people refer to the individual structure that stores information about a process as a\n   **Process Control Block (PCB)**\n   , a fancy way of talking about a C structure that contains information about each process (also sometimes called a\n   **process descriptor**\n   ).\n\n\nhas not yet been cleaned up (in UNIX-based systems, this is called the\n   **zombie**\n   state\n   \n    1\n   \n   ). This final state can be useful as it allows other processes (usually the\n   **parent**\n   that created the process) to examine the return code of the process and see if the just-finished process executed successfully (usually, programs return zero in UNIX-based systems when they have accomplished a task successfully, and non-zero otherwise). When finished, the parent will make one final call (e.g.,\n   \n    wait()\n   \n   ) to wait for the completion of the child, and to also indicate to the OS that it can clean up any relevant data structures that referred to the now-extinct process."
        }
      ]
    },
    {
      "name": "Interlude: Process API",
      "sections": [
        {
          "name": "The fork() System Call",
          "content": "The\n   \n    fork()\n   \n   system call is used to create a new process [C63]. However, be forewarned: it is certainly the strangest routine you will ever call\n   \n    1\n   \n   . More specifically, you have a running program whose code looks like what you see in Figure 5.1; examine the code, or better yet, type it in and run it yourself!\n\n\n1\n   \n   Well, OK, we admit that we don't know that for sure; who knows what routines you call when no one is looking? But\n   \n    fork()\n   \n   is pretty odd, no matter how unusual your routine-calling patterns are.\n\n\n1 #include <stdio.h>\n2 #include <stdlib.h>\n3 #include <unistd.h>\n4\n5 int main(int argc, char *argv[]) {\n6     printf(\"hello (pid:%d)\\n\", (int) getpid());\n7     int rc = fork();\n8     if (rc < 0) {\n9         // fork failed\n10        fprintf(stderr, \"fork failed\\n\");\n11        exit(1);\n12    } else if (rc == 0) {\n13        // child (new process)\n14        printf(\"child (pid:%d)\\n\", (int) getpid());\n15    } else {\n16        // parent goes down this path (main)\n17        printf(\"parent of %d (pid:%d)\\n\",\n18               rc, (int) getpid());\n19    }\n20    return 0;\n21 }\n22\nFigure 5.1: Calling\n   \n    fork()\n   \n   (\n   \n    p1.c\n   \n   )\n\n\nWhen you run this program (called\n   \n    p1.c\n   \n   ), you'll see the following:\n\n\nprompt> ./p1\nhello (pid:29146)\nparent of 29147 (pid:29146)\nchild (pid:29147)\nprompt>\nLet us understand what happened in more detail in\n   \n    p1.c\n   \n   . When it first started running, the process prints out a hello message; included in that message is its\n   **process identifier**\n   , also known as a\n   **PID**\n   . The process has a PID of 29146; in UNIX systems, the PID is used to name the process if one wants to do something with the process, such as (for example) stop it from running. So far, so good.\n\n\nNow the interesting part begins. The process calls the\n   \n    fork()\n   \n   system call, which the OS provides as a way to create a new process. The odd part: the process that is created is an (almost)\n   *exact copy of the calling process*\n   . That means that to the OS, it now looks like there are two copies of the program\n   \n    p1\n   \n   running, and both are about to return from the\n   \n    fork()\n   \n   system call. The newly-created process (called the\n   **child**\n   , in contrast to the creating\n   **parent**\n   ) doesn't start running at\n   \n    main()\n   \n   , like you might expect (note, the \"hello\" message only got printed out once); rather, it just comes into life as if it had called\n   \n    fork()\n   \n   itself.\n\n\n1 #include <stdio.h>\n2 #include <stdlib.h>\n3 #include <unistd.h>\n4 #include <sys/wait.h>\n5\n6 int main(int argc, char *argv[]) {\n7     printf(\"hello (pid:%d)\\n\", (int) getpid());\n8     int rc = fork();\n9     if (rc < 0) {           // fork failed; exit\n10         fprintf(stderr, \"fork failed\\n\");\n11         exit(1);\n12     } else if (rc == 0) { // child (new process)\n13         printf(\"child (pid:%d)\\n\", (int) getpid());\n14     } else {              // parent goes down this path\n15         int rc_wait = wait(NULL);\n16         printf(\"parent of %d (%d:%d) (pid:%d)\\n\",\n17             rc, rc_wait, (int) getpid());\n18     }\n19     return 0;\n20 }\n21\nFigure 5.2:\n   **Calling\n    \n     fork()\n    \n    And\n    \n     wait()\n    \n    (p2.c)**\n\n\nYou might have noticed: the child isn't an\n   *exact*\n   copy. Specifically, although it now has its own copy of the address space (i.e., its own private memory), its own registers, its own PC, and so forth, the value it returns to the caller of\n   \n    fork()\n   \n   is different. Specifically, while the parent receives the PID of the newly-created child, the child receives a return code of zero. This differentiation is useful, because it is simple then to write the code that handles the two different cases (as above).\n\n\nYou might also have noticed: the output (of p1.c) is not\n   **deterministic**\n   . When the child process is created, there are now two active processes in the system that we care about: the parent and the child. Assuming we are running on a system with a single CPU (for simplicity), then either the child or the parent might run at that point. In our example (above), the parent did and thus printed out its message first. In other cases, the opposite might happen, as we show in this output trace:\n\n\nprompt> ./p1\nhello (pid:29146)\nchild (pid:29147)\nparent of 29147 (pid:29146)\nprompt>\nThe CPU\n   **scheduler**\n   , a topic we'll discuss in great detail soon, determines which process runs at a given moment in time; because the scheduler is complex, we cannot usually make strong assumptions about what\n\n\nit will choose to do, and hence which process will run first. This\n   **non-determinism**\n   , as it turns out, leads to some interesting problems, particularly in\n   **multi-threaded programs**\n   ; hence, we'll see a lot more non-determinism when we study\n   **concurrency**\n   in the second part of the book."
        },
        {
          "name": "The wait() System Call",
          "content": "So far, we haven't done much: just created a child that prints out a message and exits. Sometimes, as it turns out, it is quite useful for a parent to wait for a child process to finish what it has been doing. This task is accomplished with the\n   \n    wait()\n   \n   system call (or its more complete sibling\n   \n    waitpid()\n   \n   ); see Figure 5.2 for details.\n\n\nIn this example (p2.c), the parent process calls\n   \n    wait()\n   \n   to delay its execution until the child finishes executing. When the child is done,\n   \n    wait()\n   \n   returns to the parent.\n\n\nAdding a\n   \n    wait()\n   \n   call to the code above makes the output deterministic. Can you see why? Go ahead, think about it.\n\n\n*(waiting for you to think ... and done)*\n\n\nNow that you have thought a bit, here is the output:\n\n\nprompt> ./p2\nhello (pid:29266)\nchild (pid:29267)\nparent of 29267 (rc_wait:29267) (pid:29266)\nprompt>\nWith this code, we now know that the child will always print first. Why do we know that? Well, it might simply run first, as before, and thus print before the parent. However, if the parent does happen to run first, it will immediately call\n   \n    wait()\n   \n   ; this system call won't return until the child has run and exited\n   \n    2\n   \n   . Thus, even when the parent runs first, it politely waits for the child to finish running, then\n   \n    wait()\n   \n   returns, and then the parent prints its message."
        },
        {
          "name": "Finally, The exec() System Call",
          "content": "A final and important piece of the process creation API is the\n   \n    exec()\n   \n   system call\n   \n    3\n   \n   . This system call is useful when you want to run a program that is different from the calling program. For example, calling\n   \n    fork()\n\n\n2\n   \n   There are a few cases where\n   \n    wait()\n   \n   returns before the child exits; read the man page for more details, as always. And beware of any absolute and unqualified statements this book makes, such as \"the child will always print first\" or \"UNIX is the best thing in the world, even better than ice cream.\"\n\n\n3\n   \n   On Linux, there are six variants of\n   \n    exec()\n   \n   :\n   \n    execl()\n   \n   ,\n   \n    execle()\n   \n   ,\n   \n    execv()\n   \n   ,\n   \n    execvp()\n   \n   ,\n   \n    execve()\n   \n   . Read the man pages to learn more.\n\n\n1 #include <stdio.h>\n2 #include <stdlib.h>\n3 #include <unistd.h>\n4 #include <string.h>\n5 #include <sys/wait.h>\n6\n7 int main(int argc, char *argv[]) {\n8     printf(\"hello (pid:%d)\\n\", (int) getpid());\n9     int rc = fork();\n10    if (rc < 0) {           // fork failed; exit\n11        fprintf(stderr, \"fork failed\\n\");\n12        exit(1);\n13    } else if (rc == 0) {  // child (new process)\n14        printf(\"child (pid:%d)\\n\", (int) getpid());\n15        char *myargs[3];\n16        myargs[0] = strdup(\"wc\");    // program: \"wc\"\n17        myargs[1] = strdup(\"p3.c\");  // arg: input file\n18        myargs[2] = NULL;           // mark end of array\n19        execvp(myargs[0], myargs);  // runs word count\n20        printf(\"this shouldn't print out\");\n21    } else {                   // parent goes down this path\n22        int rc_wait = wait(NULL);\n23        printf(\"parent of %d (rc_wait:%d) (pid:%d)\\n\",\n24                rc, rc_wait, (int) getpid());\n25    }\n26    return 0;\n27 }\n28\n\nFigure 5.3:\n   **Calling\n    \n     fork()\n    \n    ,\n    \n     wait()\n    \n    , And\n    \n     exec()\n    \n    (\n    \n     p3.c\n    \n    )**\n\n\nin\n   \n    p2.c\n   \n   is only useful if you want to keep running copies of the same program. However, often you want to run a\n   *different*\n   program;\n   \n    exec()\n   \n   does just that (Figure 5.3).\n\n\nIn this example, the child process calls\n   \n    execvp()\n   \n   in order to run the program\n   \n    wc\n   \n   , which is the word counting program. In fact, it runs\n   \n    wc\n   \n   on the source file\n   \n    p3.c\n   \n   , thus telling us how many lines, words, and bytes are found in the file:\n\n\nprompt> ./p3\nhello (pid:29383)\nchild (pid:29384)\n    29    107    1030 p3.c\nparent of 29384 (rc_wait:29384) (pid:29383)\nprompt>\n\nThe\n   \n    fork()\n   \n   system call is strange; its partner in crime,\n   \n    exec()\n   \n   , is not so normal either. What it does: given the name of an executable (e.g.,\n   \n    wc\n   \n   ), and some arguments (e.g.,\n   \n    p3.c\n   \n   ), it\n   **loads**\n   code (and static data) from that\n\n\n**TIP: GETTING IT RIGHT (LAMPSON'S LAW)**\nAs Lampson states in his well-regarded “Hints for Computer Systems Design” [L83], “\n   **Get it right**\n   . Neither abstraction nor simplicity is a substitute for getting it right.” Sometimes, you just have to do the right thing, and when you do, it is way better than the alternatives. There are lots of ways to design APIs for process creation; however, the combination of\n   \n    fork()\n   \n   and\n   \n    exec()\n   \n   are simple and immensely powerful. Here, the UNIX designers simply got it right. And because Lampson so often “got it right”, we name the law in his honor.\n\n\nexecutable and overwrites its current code segment (and current static data) with it; the heap and stack and other parts of the memory space of the program are re-initialized. Then the OS simply runs that program, passing in any arguments as the\n   \n    argv\n   \n   of that process. Thus, it does\n   *not*\n   create a new process; rather, it transforms the currently running program (formerly\n   \n    p3\n   \n   ) into a different running program (\n   \n    wc\n   \n   ). After the\n   \n    exec()\n   \n   in the child, it is almost as if\n   \n    p3.c\n   \n   never ran; a successful call to\n   \n    exec()\n   \n   never returns."
        },
        {
          "name": "Why? Motivating The API",
          "content": "Of course, one big question you might have: why would we build such an odd interface to what should be the simple act of creating a new process? Well, as it turns out, the separation of\n   \n    fork()\n   \n   and\n   \n    exec()\n   \n   is essential in building a UNIX shell, because it lets the shell run code\n   *after*\n   the call to\n   \n    fork()\n   \n   but\n   *before*\n   the call to\n   \n    exec()\n   \n   ; this code can alter the environment of the about-to-be-run program, and thus enables a variety of interesting features to be readily built.\n\n\nThe shell is just a user program\n   \n    4\n   \n   . It shows you a\n   **prompt**\n   and then waits for you to type something into it. You then type a command (i.e., the name of an executable program, plus any arguments) into it; in most cases, the shell then figures out where in the file system the executable resides, calls\n   \n    fork()\n   \n   to create a new child process to run the command, calls some variant of\n   \n    exec()\n   \n   to run the command, and then waits for the command to complete by calling\n   \n    wait()\n   \n   . When the child completes, the shell returns from\n   \n    wait()\n   \n   and prints out a prompt again, ready for your next command.\n\n\nThe separation of\n   \n    fork()\n   \n   and\n   \n    exec()\n   \n   allows the shell to do a whole bunch of useful things rather easily. For example:\n\n\nprompt> wc p3.c > newfile.txt\n4\n   \n   And there are lots of shells;\n   \n    tcsh\n   \n   ,\n   \n    bash\n   \n   , and\n   \n    zsh\n   \n   to name a few. You should pick one, read its man pages, and learn more about it; all UNIX experts do.\n\n\nIn the example above, the output of the program\n   \n    wc\n   \n   is\n   **redirected**\n   into the output file\n   \n    newfile.txt\n   \n   (the greater-than sign is how said redirection is indicated). The way the shell accomplishes this task is quite simple: when the child is created, before calling\n   \n    exec()\n   \n   , the shell (specifically, the code executed in the child process) closes\n   **standard output**\n   and opens the file\n   \n    newfile.txt\n   \n   . By doing so, any output from the soon-to-be-running program\n   \n    wc\n   \n   is sent to the file instead of the screen (open file descriptors are kept open across the\n   \n    exec()\n   \n   call, thus enabling this behavior [SR05]).\n\n\nFigure 5.4 (page 8) shows a program that does exactly this. The reason this redirection works is due to an assumption about how the operating system manages file descriptors. Specifically, UNIX systems start looking for free file descriptors at zero. In this case,\n   \n    STDOUT_FILENO\n   \n   will be the first available one and thus get assigned when\n   \n    open()\n   \n   is called. Subsequent writes by the child process to the standard output file descriptor, for example by routines such as\n   \n    printf()\n   \n   , will then be routed transparently to the newly-opened file instead of the screen.\n\n\nHere is the output of running the\n   \n    p4.c\n   \n   program:\n\n\nprompt> ./p4\nprompt> cat p4.output\n32      109      846 p4.c\nprompt>\nYou'll notice (at least) two interesting tidbits about this output. First, when\n   \n    p4\n   \n   is run, it looks as if nothing has happened; the shell just prints the command prompt and is immediately ready for your next command. However, that is not the case; the program\n   \n    p4\n   \n   did indeed call\n   \n    fork()\n   \n   to create a new child, and then run the\n   \n    wc\n   \n   program via a call to\n   \n    execvp()\n   \n   . You don't see any output printed to the screen because it has been redirected to the file\n   \n    p4.output\n   \n   . Second, you can see that when we\n   \n    cat\n   \n   the output file, all the expected output from running\n   \n    wc\n   \n   is found. Cool, right?\n\n\nUNIX pipes are implemented in a similar way, but with the\n   \n    pipe()\n   \n   system call. In this case, the output of one process is connected to an in-kernel\n   **pipe**\n   (i.e., queue), and the input of another process is connected to that same pipe; thus, the output of one process seamlessly is used as input to the next, and long and useful chains of commands can be strung together. As a simple example, consider looking for a word in a file, and then counting how many times said word occurs; with pipes and the utilities\n   \n    grep\n   \n   and\n   \n    wc\n   \n   , it is easy; just type\n   \n    grep -o foo file | wc -l\n   \n   into the command prompt and marvel at the result.\n\n\nFinally, while we just have sketched out the process API at a high level, there is a lot more detail about these calls out there to be learned and digested; we'll learn more, for example, about file descriptors when we talk about file systems in the third part of the book. For now, suffice it to say that the\n   \n    fork()/exec()\n   \n   combination is a powerful way to create and manipulate processes.\n\n\n1 #include <stdio.h>\n2 #include <stdlib.h>\n3 #include <unistd.h>\n4 #include <string.h>\n5 #include <font1.h>\n6 #include <sys/wait.h>\n7\n8 int main(int argc, char *argv[]) {\n9     int rc = fork();\n10    if (rc < 0) {\n11        // fork failed\n12        fprintf(stderr, \"fork failed\\n\");\n13        exit(1);\n14    } else if (rc == 0) {\n15        // child: redirect standard output to a file\n16        close(STDOUT_FILENO);\n17        open(\"./p4_output\", O_CREAT|O_WRONLY|O_TRUNC,\n18             S_IRWXU);\n19        // now exec \"wc\"...\n20        char *myargs[3];\n21        myargs[0] = strdup(\"wc\");    // program: wc\n22        myargs[1] = strdup(\"p4.c\");  // arg: file to count\n23        myargs[2] = NULL;           // mark end of array\n24        execvp(myargs[0], myargs);  // runs word count\n25    } else {\n26        // parent goes down this path (main)\n27        int rc_wait = wait(NULL);\n28    }\n29    return 0;\n30 }\n\nFigure 5.4: All Of The Above With Redirection (p4.c)"
        },
        {
          "name": "Process Control And Users",
          "content": "Beyond\n   \n    fork()\n   \n   ,\n   \n    exec()\n   \n   , and\n   \n    wait()\n   \n   , there are a lot of other interfaces for interacting with processes in UNIX systems. For example, the\n   \n    kill()\n   \n   system call is used to send\n   **signals**\n   to a process, including directives to pause, die, and other useful imperatives. For convenience, in most UNIX shells, certain keystroke combinations are configured to deliver a specific signal to the currently running process; for example, control-c sends a\n   \n    SIGINT\n   \n   (interrupt) to the process (normally terminating it) and control-z sends a\n   \n    SIGTSTP\n   \n   (stop) signal thus pausing the process in mid-execution (you can resume it later with a command, e.g., the\n   \n    fg\n   \n   built-in command found in many shells).\n\n\nThe entire signals subsystem provides a rich infrastructure to deliver external events to processes, including ways to receive and process those signals within individual processes, and ways to send signals to individual processes as well as entire\n   **process groups**\n   . To use this form of com-\n\n\n\n\n**ASIDE: RTFM — READ THE MAN PAGES**\n\n\nMany times in this book, when referring to a particular system call or library call, we'll tell you to read the\n   **manual pages**\n   , or\n   **man pages**\n   for short. Man pages are the original form of documentation that exist on UNIX systems; realize that they were created before the thing called\n   **the web**\n   existed.\n\n\nSpending some time reading man pages is a key step in the growth of a systems programmer; there are tons of useful tidbits hidden in those pages. Some particularly useful pages to read are the man pages for whichever shell you are using (e.g.,\n   **tcsh**\n   , or\n   **bash**\n   ), and certainly for any system calls your program makes (in order to see what return values and error conditions exist).\n\n\nFinally, reading the man pages can save you some embarrassment. When you ask colleagues about some intricacy of\n   \n    fork()\n   \n   , they may simply reply: “RTFM.” This is your colleagues’ way of gently urging you to Read The Man pages. The F in RTFM just adds a little color to the phrase...\n\n\ncommunication, a process should use the\n   \n    signal()\n   \n   system call to “catch” various signals; doing so ensures that when a particular signal is delivered to a process, it will suspend its normal execution and run a particular piece of code in response to the signal. Read elsewhere [SR05] to learn more about signals and their many intricacies.\n\n\nThis naturally raises the question: who can send a signal to a process, and who cannot? Generally, the systems we use can have multiple people using them at the same time; if one of these people can arbitrarily send signals such as\n   \n    SIGINT\n   \n   (to interrupt a process, likely terminating it), the usability and security of the system will be compromised. As a result, modern systems include a strong conception of the notion of a\n   **user**\n   . The user, after entering a password to establish credentials, logs in to gain access to system resources. The user may then launch one or many processes, and exercise full control over them (pause them, kill them, etc.). Users generally can only control their own processes; it is the job of the operating system to parcel out resources (such as CPU, memory, and disk) to each user (and their processes) to meet overall system goals."
        },
        {
          "name": "Useful Tools",
          "content": "There are many command-line tools that are useful as well. For example, using the\n   \n    ps\n   \n   command allows you to see which processes are running; read the\n   **man pages**\n   for some useful flags to pass to\n   \n    ps\n   \n   . The tool\n   \n    top\n   \n   is also quite helpful, as it displays the processes of the system and how much CPU and other resources they are eating up. Humerously, many times when you run it,\n   \n    top\n   \n   claims it is the top resource hog; perhaps it is a bit of an egomaniac. The command\n   \n    kill\n   \n   can be used to send arbitrary\n\n\n\n\n**ASIDE: THE SUPERUSER (ROOT)**\n\n\nA system generally needs a user who can\n   **administer**\n   the system, and is not limited in the way most users are. Such a user should be able to kill an arbitrary process (e.g., if it is abusing the system in some way), even though that process was not started by this user. Such a user should also be able to run powerful commands such as\n   \n    shutdown\n   \n   (which, unsurprisingly, shuts down the system). In UNIX-based systems, these special abilities are given to the\n   **superuser**\n   (sometimes called\n   **root**\n   ). While most users can't kill other users processes, the superuser can. Being root is much like being Spider-Man: with great power comes great responsibility [QI15]. Thus, to increase\n   **security**\n   (and avoid costly mistakes), it's usually better to be a regular user; if you do need to be root, tread carefully, as all of the destructive powers of the computing world are now at your fingertips.\n\n\nsignals to processes, as can the slightly more user friendly\n   \n    killall\n   \n   . Be sure to use these carefully; if you accidentally kill your window manager, the computer you are sitting in front of may become quite difficult to use.\n\n\nFinally, there are many different kinds of CPU meters you can use to get a quick glance understanding of the load on your system; for example, we always keep\n   **MenuMeters**\n   (from Raging Menace software) running on our Macintosh toolbars, so we can see how much CPU is being utilized at any moment in time. In general, the more information about what is going on, the better."
        }
      ]
    },
    {
      "name": "Mechanism: Limited Direct Execution",
      "sections": [
        {
          "name": "Basic Technique: Limited Direct Execution",
          "content": "To make a program run as fast as one might expect, not surprisingly OS developers came up with a technique, which we call\n   **limited direct execution**\n   . The “direct execution” part of the idea is simple: just run the program directly on the CPU. Thus, when the OS wishes to start a program running, it creates a process entry for it in a process list, allocates some memory for it, loads the program code into memory (from disk), locates its entry point (i.e., the\n   \n    main()\n   \n   routine or something similar), jumps\n\n\n\n\n![](images/image_0003.jpeg)\n\n\nOS | Program\nCreate entry for process list | \nAllocate memory for program | \nLoad program into memory | \nSet up stack with\n       \n        argc/argv | \nClear registers | \nExecute\n       \n        call\n       \n\n        main() | Run\n       \n        main()\n | Execute\n       \n        return\n       \n       from\n       \n        main\nFree memory of process | \nRemove from process list |\n\n\nFigure 6.1:\n   **Direct Execution Protocol (Without Limits)**\n\n\nto it, and starts running the user’s code. Figure 6.1 shows this basic direct execution protocol (without any limits, yet), using a normal call and return to jump to the program’s\n   \n    main()\n   \n   and later back into the kernel.\n\n\nSounds simple, no? But this approach gives rise to a few problems in our quest to virtualize the CPU. The first is simple: if we just run a program, how can the OS make sure the program doesn’t do anything that we don’t want it to do, while still running it efficiently? The second: when we are running a process, how does the operating system stop it from running and switch to another process, thus implementing the\n   **time sharing**\n   we require to virtualize the CPU?\n\n\nIn answering these questions below, we’ll get a much better sense of what is needed to virtualize the CPU. In developing these techniques, we’ll also see where the “limited” part of the name arises from; without limits on running programs, the OS wouldn’t be in control of anything and thus would be “just a library” — a very sad state of affairs for an aspiring operating system!"
        },
        {
          "name": "Problem #1: Restricted Operations",
          "content": "Direct execution has the obvious advantage of being fast; the program runs natively on the hardware CPU and thus executes as quickly as one would expect. But running on the CPU introduces a problem: what if the process wishes to perform some kind of restricted operation, such as issuing an I/O request to a disk, or gaining access to more system resources such as CPU or memory?\n\n\n\n\n**THE CRUX: HOW TO PERFORM RESTRICTED OPERATIONS**\n\n\nA process must be able to perform I/O and some other restricted operations, but without giving the process complete control over the system. How can the OS and hardware work together to do so?\n\n\n\n\n**ASIDE: WHY SYSTEM CALLS LOOK LIKE PROCEDURE CALLS**\n\n\nYou may wonder why a call to a system call, such as\n   \n    open()\n   \n   or\n   \n    read()\n   \n   , looks exactly like a typical procedure call in C; that is, if it looks just like a procedure call, how does the system know it's a system call, and do all the right stuff? The simple reason: it\n   *is*\n   a procedure call, but hidden inside that procedure call is the famous trap instruction. More specifically, when you call\n   \n    open()\n   \n   (for example), you are executing a procedure call into the C library. Therein, whether for\n   \n    open()\n   \n   or any of the other system calls provided, the library uses an agreed-upon calling convention with the kernel to put the arguments to\n   \n    open()\n   \n   in well-known locations (e.g., on the stack, or in specific registers), puts the system-call number into a well-known location as well (again, onto the stack or a register), and then executes the aforementioned trap instruction. The code in the library after the trap unpacks return values and returns control to the program that issued the system call. Thus, the parts of the C library that make system calls are hand-coded in assembly, as they need to carefully follow convention in order to process arguments and return values correctly, as well as execute the hardware-specific trap instruction. And now you know why you personally don't have to write assembly code to trap into an OS; somebody has already written that assembly for you.\n\n\nOne approach would simply be to let any process do whatever it wants in terms of I/O and other related operations. However, doing so would prevent the construction of many kinds of systems that are desirable. For example, if we wish to build a file system that checks permissions before granting access to a file, we can't simply let any user process issue I/Os to the disk; if we did, a process could simply read or write the entire disk and thus all protections would be lost.\n\n\nThus, the approach we take is to introduce a new processor mode, known as\n   **user mode**\n   ; code that runs in user mode is restricted in what it can do. For example, when running in user mode, a process can't issue I/O requests; doing so would result in the processor raising an exception; the OS would then likely kill the process.\n\n\nIn contrast to user mode is\n   **kernel mode**\n   , which the operating system (or kernel) runs in. In this mode, code that runs can do what it likes, including privileged operations such as issuing I/O requests and executing all types of restricted instructions.\n\n\nWe are still left with a challenge, however: what should a user process do when it wishes to perform some kind of privileged operation, such as reading from disk? To enable this, virtually all modern hardware provides the ability for user programs to perform a\n   **system call**\n   . Pioneered on ancient machines such as the Atlas [K+61,L78], system calls allow the kernel to carefully expose certain key pieces of functionality to user programs, such as accessing the file system, creating and destroying processes, communicating with other processes, and allocating more\n\n\n**TIP: USE PROTECTED CONTROL TRANSFER**\nThe hardware assists the OS by providing different modes of execution. In\n   **user mode**\n   , applications do not have full access to hardware resources. In\n   **kernel mode**\n   , the OS has access to the full resources of the machine. Special instructions to\n   **trap**\n   into the kernel and\n   **return-from-trap**\n   back to user-mode programs are also provided, as well as instructions that allow the OS to tell the hardware where the\n   **trap table**\n   resides in memory.\n\n\nmemory. Most operating systems provide a few hundred calls (see the POSIX standard for details [P10]); early Unix systems exposed a more concise subset of around twenty calls.\n\n\nTo execute a system call, a program must execute a special\n   **trap**\n   instruction. This instruction simultaneously jumps into the kernel and raises the privilege level to kernel mode; once in the kernel, the system can now perform whatever privileged operations are needed (if allowed), and thus do the required work for the calling process. When finished, the OS calls a special\n   **return-from-trap**\n   instruction, which, as you might expect, returns into the calling user program while simultaneously reducing the privilege level back to user mode.\n\n\nThe hardware needs to be a bit careful when executing a trap, in that it must make sure to save enough of the caller’s registers in order to be able to return correctly when the OS issues the return-from-trap instruction. On x86, for example, the processor will push the program counter, flags, and a few other registers onto a per-process\n   **kernel stack**\n   ; the return-from-trap will pop these values off the stack and resume execution of the user-mode program (see the Intel systems manuals [I11] for details). Other hardware systems use different conventions, but the basic concepts are similar across platforms.\n\n\nThere is one important detail left out of this discussion: how does the trap know which code to run inside the OS? Clearly, the calling process can’t specify an address to jump to (as you would when making a procedure call); doing so would allow programs to jump anywhere into the kernel which clearly is a\n   **Very Bad Idea**\n\n    1\n   \n   . Thus the kernel must carefully control what code executes upon a trap.\n\n\nThe kernel does so by setting up a\n   **trap table**\n   at boot time. When the machine boots up, it does so in privileged (kernel) mode, and thus is free to configure machine hardware as need be. One of the first things the OS thus does is to tell the hardware what code to run when certain exceptional events occur. For example, what code should run when a hard-disk interrupt takes place, when a keyboard interrupt occurs, or when a program makes a system call? The OS informs the hardware of the\n\n\n1\n   \n   Imagine jumping into code to access a file, but just after a permission check; in fact, it is likely such an ability would enable a wily programmer to get the kernel to run arbitrary code sequences [S07]. In general, try to avoid Very Bad Ideas like this one.\n\n\n\n\n![](images/image_0004.jpeg)\n\n\nOS @ boot\n       \n       (kernel mode) | Hardware | \ninitialize trap table | remember address of...\n       \n       syscall handler | \nOS @ run\n       \n       (kernel mode) | Hardware | Program\n       \n       (user mode)\nCreate entry for process list\n       \n       Allocate memory for program\n       \n       Load program into memory\n       \n       Setup user stack with argv\n       \n       Fill kernel stack with reg/PC\n       \n\n        return-from-trap | restore regs\n       \n       (from kernel stack)\n       \n       move to user mode\n       \n       jump to main | Run main()\n       \n\n       ...\n       \n       Call system call\n       \n\n        trap\n       \n       into OS\nHandle trap\n       \n       Do work of syscall\n       \n\n        return-from-trap | save regs\n       \n       (to kernel stack)\n       \n       move to kernel mode\n       \n       jump to trap handler | \nFree memory of process\n       \n       Remove from process list | restore regs\n       \n       (from kernel stack)\n       \n       move to user mode\n       \n       jump to PC after trap | ...\n       \n\n        return from main\n       \n\n\n        trap\n       \n       (via\n       \n        exit()\n       \n       )\n\n\n**Limited Direct Execution Protocol**\nlocations of these\n   **trap handlers**\n   , usually with some kind of special instruction. Once the hardware is informed, it remembers the location of these handlers until the machine is next rebooted, and thus the hardware knows what to do (i.e., what code to jump to) when system calls and other exceptional events take place.\n\n\n**TIP: BE WARY OF USER INPUTS IN SECURE SYSTEMS**\nEven though we have taken great pains to protect the OS during system calls (by adding a hardware trapping mechanism, and ensuring all calls to the OS are routed through it), there are still many other aspects to implementing a\n   **secure**\n   operating system that we must consider. One of these is the handling of arguments at the system call boundary; the OS must check what the user passes in and ensure that arguments are properly specified, or otherwise reject the call.\n\n\nFor example, with a\n   \n    write()\n   \n   system call, the user specifies an address of a buffer as a source of the write call. If the user (either accidentally or maliciously) passes in a “bad” address (e.g., one inside the kernel’s portion of the address space), the OS must detect this and reject the call. Otherwise, it would be possible for a user to read all of kernel memory; given that kernel (virtual) memory also usually includes all of the physical memory of the system, this small slip would enable a program to read the memory of any other process in the system.\n\n\nIn general, a secure system must treat user inputs with great suspicion. Not doing so will undoubtedly lead to easily hacked software, a despairing sense that the world is an unsafe and scary place, and the loss of job security for the all-too-trusting OS developer.\n\n\nTo specify the exact system call, a\n   **system-call number**\n   is usually assigned to each system call. The user code is thus responsible for placing the desired system-call number in a register or at a specified location on the stack; the OS, when handling the system call inside the trap handler, examines this number, ensures it is valid, and, if it is, executes the corresponding code. This level of indirection serves as a form of\n   **protection**\n   ; user code cannot specify an exact address to jump to, but rather must request a particular service via number.\n\n\nOne last aside: being able to execute the instruction to tell the hardware where the trap tables are is a very powerful capability. Thus, as you might have guessed, it is also a\n   **privileged**\n   operation. If you try to execute this instruction in user mode, the hardware won’t let you, and you can probably guess what will happen (hint: adios, offending program). Point to ponder: what horrible things could you do to a system if you could install your own trap table? Could you take over the machine?\n\n\nThe timeline (with time increasing downward, in Figure 6.2) summarizes the protocol. We assume each process has a kernel stack where registers (including general purpose registers and the program counter) are saved to and restored from (by the hardware) when transitioning into and out of the kernel.\n\n\nThere are two phases in the limited direct execution (LDE) protocol. In the first (at boot time), the kernel initializes the trap table, and the CPU remembers its location for subsequent use. The kernel does so via a\n\n\nprivileged instruction (all privileged instructions are highlighted in bold).\n\n\nIn the second (when running a process), the kernel sets up a few things (e.g., allocating a node on the process list, allocating memory) before using a return-from-trap instruction to start the execution of the process; this switches the CPU to user mode and begins running the process. When the process wishes to issue a system call, it traps back into the OS, which handles it and once again returns control via a return-from-trap to the process. The process then completes its work, and returns from\n   \n    main()\n   \n   ; this usually will return into some stub code which will properly exit the program (say, by calling the\n   \n    exit()\n   \n   system call, which traps into the OS). At this point, the OS cleans up and we are done."
        },
        {
          "name": "Problem #2: Switching Between Processes",
          "content": "The next problem with direct execution is achieving a switch between processes. Switching between processes should be simple, right? The OS should just decide to stop one process and start another. What's the big deal? But it actually is a little bit tricky: specifically, if a process is running on the CPU, this by definition means the OS is\n   *not*\n   running. If the OS is not running, how can it do anything at all? (hint: it can't) While this sounds almost philosophical, it is a real problem: there is clearly no way for the OS to take an action if it is not running on the CPU. Thus we arrive at the crux of the problem.\n\n\n\n\n**THE CRUX: HOW TO REGAIN CONTROL OF THE CPU**\n\n\nHow can the operating system\n   **regain control**\n   of the CPU so that it can switch between processes?\n\n\n\n\n**A Cooperative Approach: Wait For System Calls**\n\n\nOne approach that some systems have taken in the past (for example, early versions of the Macintosh operating system [M11], or the old Xerox Alto system [A79]) is known as the\n   **cooperative**\n   approach. In this style, the OS\n   *trusts*\n   the processes of the system to behave reasonably. Processes that run for too long are assumed to periodically give up the CPU so that the OS can decide to run some other task.\n\n\nThus, you might ask, how does a friendly process give up the CPU in this utopian world? Most processes, as it turns out, transfer control of the CPU to the OS quite frequently by making\n   **system calls**\n   , for example, to open a file and subsequently read it, or to send a message to another machine, or to create a new process. Systems like this often include an explicit\n   **yield**\n   system call, which does nothing except to transfer control to the OS so it can run other processes.\n\n\nApplications also transfer control to the OS when they do something illegal. For example, if an application divides by zero, or tries to access\n\n\nmemory that it shouldn't be able to access, it will generate a\n   **trap**\n   to the OS. The OS will then have control of the CPU again (and likely terminate the offending process).\n\n\nThus, in a cooperative scheduling system, the OS regains control of the CPU by waiting for a system call or an illegal operation of some kind to take place. You might also be thinking: isn't this passive approach less than ideal? What happens, for example, if a process (whether malicious, or just full of bugs) ends up in an infinite loop, and never makes a system call? What can the OS do then?\n\n\n\n\n**A Non-Cooperative Approach: The OS Takes Control**\n\n\nWithout some additional help from the hardware, it turns out the OS can't do much at all when a process refuses to make system calls (or mistakes) and thus return control to the OS. In fact, in the cooperative approach, your only recourse when a process gets stuck in an infinite loop is to resort to the age-old solution to all problems in computer systems:\n   **reboot the machine**\n   . Thus, we again arrive at a subproblem of our general quest to gain control of the CPU.\n\n\n\n\n**THE CRUX: HOW TO GAIN CONTROL WITHOUT COOPERATION**\n\n\nHow can the OS gain control of the CPU even if processes are not being cooperative? What can the OS do to ensure a rogue process does not take over the machine?\n\n\nThe answer turns out to be simple and was discovered by a number of people building computer systems many years ago: a\n   **timer interrupt**\n   [M+63]. A timer device can be programmed to raise an interrupt every so many milliseconds; when the interrupt is raised, the currently running process is halted, and a pre-configured\n   **interrupt handler**\n   in the OS runs. At this point, the OS has regained control of the CPU, and thus can do what it pleases: stop the current process, and start a different one.\n\n\nAs we discussed before with system calls, the OS must inform the hardware of which code to run when the timer interrupt occurs; thus, at boot time, the OS does exactly that. Second, also during the boot\n\n\n\n\n**TIP: DEALING WITH APPLICATION MISBEHAVIOR**\n\n\nOperating systems often have to deal with misbehaving processes, those that either through design (maliciousness) or accident (bugs) attempt to do something that they shouldn't. In modern systems, the way the OS tries to handle such malefeasance is to simply terminate the offender. One strike and you're out! Perhaps brutal, but what else should the OS do when you try to access memory illegally or execute an illegal instruction?\n\n\nsequence, the OS must start the timer, which is of course a privileged operation. Once the timer has begun, the OS can thus feel safe in that control will eventually be returned to it, and thus the OS is free to run user programs. The timer can also be turned off (also a privileged operation), something we will discuss later when we understand concurrency in more detail.\n\n\nNote that the hardware has some responsibility when an interrupt occurs, in particular to save enough of the state of the program that was running when the interrupt occurred such that a subsequent return-from-trap instruction will be able to resume the running program correctly. This set of actions is quite similar to the behavior of the hardware during an explicit system-call trap into the kernel, with various registers thus getting saved (e.g., onto a kernel stack) and thus easily restored by the return-from-trap instruction.\n\n\n\n\n**Saving and Restoring Context**\n\n\nNow that the OS has regained control, whether cooperatively via a system call, or more forcefully via a timer interrupt, a decision has to be made: whether to continue running the currently-running process, or switch to a different one. This decision is made by a part of the operating system known as the\n   **scheduler**\n   ; we will discuss scheduling policies in great detail in the next few chapters.\n\n\nIf the decision is made to switch, the OS then executes a low-level piece of code which we refer to as a\n   **context switch**\n   . A context switch is conceptually simple: all the OS has to do is save a few register values for the currently-executing process (onto its kernel stack, for example) and restore a few for the soon-to-be-executing process (from its kernel stack). By doing so, the OS thus ensures that when the return-from-trap instruction is finally executed, instead of returning to the process that was running, the system resumes execution of another process.\n\n\nTo save the context of the currently-running process, the OS will execute some low-level assembly code to save the general purpose registers, PC, and the kernel stack pointer of the currently-running process, and then restore said registers, PC, and switch to the kernel stack for the soon-to-be-executing process. By switching stacks, the kernel enters the call to the switch code in the context of one process (the one that was interrupted) and returns in the context of another (the soon-to-be-executing\n\n\n\n\n**TIP: USE THE TIMER INTERRUPT TO REGAIN CONTROL**\n\n\nThe addition of a\n   **timer interrupt**\n   gives the OS the ability to run again on a CPU even if processes act in a non-cooperative fashion. Thus, this hardware feature is essential in helping the OS maintain control of the machine.\n\n\n\n\n**TIP: REBOOT IS USEFUL**\n\n\nEarlier on, we noted that the only solution to infinite loops (and similar behaviors) under cooperative preemption is to\n   **reboot**\n   the machine. While you may scoff at this hack, researchers have shown that reboot (or in general, starting over some piece of software) can be a hugely useful tool in building robust systems [C+04].\n\n\nSpecifically, reboot is useful because it moves software back to a known and likely more tested state. Reboots also reclaim stale or leaked resources (e.g., memory) which may otherwise be hard to handle. Finally, reboots are easy to automate. For all of these reasons, it is not uncommon in large-scale cluster Internet services for system management software to periodically reboot sets of machines in order to reset them and thus obtain the advantages listed above.\n\n\nThus, next time you reboot, you are not just enacting some ugly hack. Rather, you are using a time-tested approach to improving the behavior of a computer system. Well done!\n\n\none). When the OS then finally executes a\n   \n    return-from-trap\n   \n   instruction, the soon-to-be-executing process becomes the currently-running process. And thus the context switch is complete.\n\n\nA timeline of the entire process is shown in Figure 6.3. In this example, Process A is running and then is interrupted by the timer interrupt. The hardware saves its registers (onto its kernel stack) and enters the kernel (switching to kernel mode). In the timer interrupt handler, the OS decides to switch from running Process A to Process B. At that point, it calls the\n   \n    switch()\n   \n   routine, which carefully saves current register values (into the process structure of A), restores the registers of Process B (from its process structure entry), and then\n   **switches contexts**\n   , specifically by changing the stack pointer to use B's kernel stack (and not A's). Finally, the OS\n   \n    returns-from-trap\n   \n   , which restores B's registers and starts running it.\n\n\nNote that there are two types of register saves/restores that happen during this protocol. The first is when the timer interrupt occurs; in this case, the\n   *user registers*\n   of the running process are implicitly saved by the\n   *hardware*\n   , using the kernel stack of that process. The second is when the OS decides to switch from A to B; in this case, the\n   *kernel registers*\n   are explicitly saved by the\n   *software*\n   (i.e., the OS), but this time into memory in the process structure of the process. The latter action moves the system from running as if it just trapped into the kernel from A to as if it just trapped into the kernel from B.\n\n\nTo give you a better sense of how such a switch is enacted, Figure 6.4 shows the context switch code for xv6. See if you can make sense of it (you'll have to know a bit of x86, as well as some xv6, to do so). The\n   \n    context structures old\n   \n   and\n   \n    new\n   \n   are found in the old and new process's process structures, respectively.\n\n\n\n\n![](images/image_0005.jpeg)\n\n\nOS @ boot\n       \n       (kernel mode) | Hardware | \ninitialize trap table | remember addresses of...\n       \n       syscall handler\n       \n       timer handler | \nstart interrupt timer | start timer\n       \n       interrupt CPU in X ms | \nOS @ run\n       \n       (kernel mode) | Hardware | Program\n       \n       (user mode)\n |  | Process A\n |  | ...\n | timer interrupt\n       \n\n       save regs(A) → k-stack(A)\n       \n       move to kernel mode\n       \n       jump to trap handler | \nHandle the trap\n       \n       Call\n       \n        switch()\n       \n       routine\n       \n       save regs(A) → proc.t(A)\n       \n       restore regs(B) ← proc.t(B)\n       \n       switch to k-stack(B)\n       \n\n        return-from-trap (into B) | restore regs(B) ← k-stack(B)\n       \n       move to user mode\n       \n       jump to B's PC | Process B\n |  | ...\n\n\n**Limited Direct Execution Protocol (Timer Interrupt)**"
        },
        {
          "name": "Worried About Concurrency?",
          "content": "Some of you, as attentive and thoughtful readers, may be now thinking: “Hmmm... what happens when, during a system call, a timer interrupt occurs?” or “What happens when you’re handling one interrupt and another one happens? Doesn’t that get hard to handle in the kernel?” Good questions — we really have some hope for you yet!\n\n\nThe answer is yes, the OS does indeed need to be concerned as to what happens if, during interrupt or trap handling, another interrupt occurs. This, in fact, is the exact topic of the entire second piece of this book, on\n   **concurrency**\n   ; we’ll defer a detailed discussion until then.\n\n\nTo whet your appetite, we’ll just sketch some basics of how the OS handles these tricky situations. One simple thing an OS might do is\n   **disable interrupts**\n   during interrupt processing; doing so ensures that when one interrupt is being handled, no other one will be delivered to the CPU.\n\n\n1 # void swtch(struct context *old, struct context *new);\n2 #\n3 # Save current register context in old\n4 # and then load register context from new.\n5 .globl swtch\n6 swtch:\n7     # Save old registers\n8     movl 4(%esp), %eax    # put old ptr into eax\n9     popl 0(%eax)          # save the old IP\n10    movl %esp, 4(%eax)    # and stack\n11    movl %ebx, 8(%eax)    # and other registers\n12    movl %ecx, 12(%eax)\n13    movl %edx, 16(%eax)\n14    movl %esi, 20(%eax)\n15    movl %edi, 24(%eax)\n16    movl %ebp, 28(%eax)\n17\n18    # Load new registers\n19    movl 4(%esp), %eax    # put new ptr into eax\n20    movl 28(%eax), %ebp   # restore other registers\n21    movl 24(%eax), %edi\n22    movl 20(%eax), %esi\n23    movl 16(%eax), %edx\n24    movl 12(%eax), %ecx\n25    movl 8(%eax), %ebx\n26    movl 4(%eax), %esp    # stack is switched here\n27    pushl 0(%eax)         # return addr put in place\n28    ret                   # finally return into new ctxt\nFigure 6.4:\n   **The xv6 Context Switch Code**\n\n\nOf course, the OS has to be careful in doing so; disabling interrupts for too long could lead to lost interrupts, which is (in technical terms) bad.\n\n\nOperating systems also have developed a number of sophisticated\n   **locking**\n   schemes to protect concurrent access to internal data structures. This enables multiple activities to be on-going within the kernel at the same time, particularly useful on multiprocessors. As we'll see in the next piece of this book on concurrency, though, such locking can be complicated and lead to a variety of interesting and hard-to-find bugs."
        }
      ]
    },
    {
      "name": "Scheduling: Introduction",
      "sections": [
        {
          "name": "Workload Assumptions",
          "content": "Before getting into the range of possible policies, let us first make a number of simplifying assumptions about the processes running in the system, sometimes collectively called the\n   **workload**\n   . Determining the workload is a critical part of building policies, and the more you know about workload, the more fine-tuned your policy can be.\n\n\nThe workload assumptions we make here are mostly unrealistic, but that is alright (for now), because we will relax them as we go, and eventually develop what we will refer to as ... (\n   *dramatic pause*\n   ) ...\n\n\na\n   **fully-operational scheduling discipline**\n\n    1\n   \n   .\n\n\nWe will make the following assumptions about the processes, sometimes called\n   **jobs**\n   , that are running in the system:\n\n\n  * 1. Each job runs for the same amount of time.\n  * 2. All jobs arrive at the same time.\n  * 3. Once started, each job runs to completion.\n  * 4. All jobs only use the CPU (i.e., they perform no I/O)\n  * 5. The run-time of each job is known.\n\n\nWe said many of these assumptions were unrealistic, but just as some animals are more equal than others in Orwell's\n   *Animal Farm*\n   [O45], some assumptions are more unrealistic than others in this chapter. In particular, it might bother you that the run-time of each job is known: this would make the scheduler omniscient, which, although it would be great (probably), is not likely to happen anytime soon."
        },
        {
          "name": "Scheduling Metrics",
          "content": "Beyond making workload assumptions, we also need one more thing to enable us to compare different scheduling policies: a\n   **scheduling metric**\n   . A metric is just something that we use to\n   *measure*\n   something, and there are a number of different metrics that make sense in scheduling.\n\n\nFor now, however, let us also simplify our life by simply having a single metric:\n   **turnaround time**\n   . The turnaround time of a job is defined as the time at which the job completes minus the time at which the job arrived in the system. More formally, the turnaround time\n   \n    T_{\\text{turnaround}}\n   \n   is:\n\n\nT_{\\text{turnaround}} = T_{\\text{completion}} - T_{\\text{arrival}} \\quad (7.1)\n\n\nBecause we have assumed that all jobs arrive at the same time, for now\n   \n    T_{\\text{arrival}} = 0\n   \n   and hence\n   \n    T_{\\text{turnaround}} = T_{\\text{completion}}\n   \n   . This fact will change as we relax the aforementioned assumptions.\n\n\nYou should note that turnaround time is a\n   **performance**\n   metric, which will be our primary focus this chapter. Another metric of interest is\n   **fairness**\n   , as measured (for example) by\n   **Jain's Fairness Index**\n   [J91]. Performance and fairness are often at odds in scheduling; a scheduler, for example, may optimize performance but at the cost of preventing a few jobs from running, thus decreasing fairness. This conundrum shows us that life isn't always perfect."
        },
        {
          "name": "First In, First Out (FIFO)",
          "content": "The most basic algorithm we can implement is known as\n   **First In, First Out (FIFO)**\n   scheduling or sometimes\n   **First Come, First Served (FCFS)**\n   .\n\n\n1\n   \n   Said in the same way you would say \"A fully-operational Death Star.\"\n\n\nFIFO has a number of positive properties: it is clearly simple and thus easy to implement. And, given our assumptions, it works pretty well.\n\n\nLet's do a quick example together. Imagine three jobs arrive in the system, A, B, and C, at roughly the same time (\n   \n    T_{arrival} = 0\n   \n   ). Because FIFO has to put some job first, let's assume that while they all arrived simultaneously, A arrived just a hair before B which arrived just a hair before C. Assume also that each job runs for 10 seconds. What will the\n   **average turnaround time**\n   be for these jobs?\n\n\n\n\n![Figure 7.1: FIFO Simple Example. A Gantt chart showing three jobs A, B, and C arriving at time 0. Job A runs from 0 to 10, job B from 10 to 20, and job C from 20 to 30. The x-axis is labeled 'Time' and ranges from 0 to 120.](images/image_0006.jpeg)\n\n\nJob | Start Time | End Time\nA | 0 | 10\nB | 10 | 20\nC | 20 | 30\n\n\nFigure 7.1: FIFO Simple Example. A Gantt chart showing three jobs A, B, and C arriving at time 0. Job A runs from 0 to 10, job B from 10 to 20, and job C from 20 to 30. The x-axis is labeled 'Time' and ranges from 0 to 120.\n\n\nFigure 7.1:\n   **FIFO Simple Example**\n\n\nFrom Figure 7.1, you can see that A finished at 10, B at 20, and C at 30. Thus, the average turnaround time for the three jobs is simply\n   \n    \\frac{10+20+30}{3} = 20\n   \n   . Computing turnaround time is as easy as that.\n\n\nNow let's relax one of our assumptions. In particular, let's relax assumption 1, and thus no longer assume that each job runs for the same amount of time. How does FIFO perform now? What kind of workload could you construct to make FIFO perform poorly?\n\n\n*(think about this before reading on ... keep thinking ... got it?!)*\n\n\nPresumably you've figured this out by now, but just in case, let's do an example to show how jobs of different lengths can lead to trouble for FIFO scheduling. In particular, let's again assume three jobs (A, B, and C), but this time A runs for 100 seconds while B and C run for 10 each.\n\n\n\n\n![Figure 7.2: Why FIFO Is Not That Great. A Gantt chart showing job A running from 0 to 100, followed by job B from 100 to 110, and job C from 110 to 120. The x-axis is labeled 'Time' and ranges from 0 to 120.](images/image_0007.jpeg)\n\n\nJob | Start Time | End Time\nA | 0 | 100\nB | 100 | 110\nC | 110 | 120\n\n\nFigure 7.2: Why FIFO Is Not That Great. A Gantt chart showing job A running from 0 to 100, followed by job B from 100 to 110, and job C from 110 to 120. The x-axis is labeled 'Time' and ranges from 0 to 120.\n\n\nFigure 7.2:\n   **Why FIFO Is Not That Great**\n\n\nAs you can see in Figure 7.2, Job A runs first for the full 100 seconds before B or C even get a chance to run. Thus, the average turnaround time for the system is high: a painful 110 seconds (\n   \n    \\frac{100+110+120}{3} = 110\n   \n   ).\n\n\nThis problem is generally referred to as the\n   **convoy effect**\n   [B+79], where a number of relatively-short potential consumers of a resource get queued\n\n\n\n\n**TIP: THE PRINCIPLE OF SJF**\n\n\nShortest Job First represents a general scheduling principle that can be applied to any system where the perceived turnaround time per customer (or, in our case, a job) matters. Think of any line you have waited in: if the establishment in question cares about customer satisfaction, it is likely they have taken SJF into account. For example, grocery stores commonly have a “ten-items-or-less” line to ensure that shoppers with only a few things to purchase don’t get stuck behind the family preparing for some upcoming nuclear winter.\n\n\nbehind a heavyweight resource consumer. This scheduling scenario might remind you of a single line at a grocery store and what you feel like when you see the person in front of you with three carts full of provisions and a checkbook out; it’s going to be a while\n   \n    2\n   \n   .\n\n\nSo what should we do? How can we develop a better algorithm to deal with our new reality of jobs that run for different amounts of time? Think about it first; then read on."
        },
        {
          "name": "Shortest Job First (SJF)",
          "content": "It turns out that a very simple approach solves this problem; in fact it is an idea stolen from operations research [C54,PV56] and applied to scheduling of jobs in computer systems. This new scheduling discipline is known as\n   **Shortest Job First (SJF)**\n   , and the name should be easy to remember because it describes the policy quite completely: it runs the shortest job first, then the next shortest, and so on.\n\n\n\n\n![Gantt chart showing SJF scheduling of jobs A, B, and C. Job B runs from time 0 to 10, job C runs from time 10 to 20, and job A runs from time 20 to 120. The x-axis is labeled 'Time' and ranges from 0 to 120.](images/image_0008.jpeg)\n\n\nA Gantt chart illustrating the Shortest Job First (SJF) scheduling policy. The horizontal axis represents time, ranging from 0 to 120 in increments of 20. Three jobs are shown: B, C, and A. Job B is represented by a light gray bar from time 0 to 10. Job C is represented by a medium gray bar from time 10 to 20. Job A is represented by a black bar from time 20 to 120. The labels 'B', 'C', and 'A' are positioned above their respective bars.\n\n\nGantt chart showing SJF scheduling of jobs A, B, and C. Job B runs from time 0 to 10, job C runs from time 10 to 20, and job A runs from time 20 to 120. The x-axis is labeled 'Time' and ranges from 0 to 120.\n\n\nFigure 7.3: SJF Simple Example\n\n\nLet’s take our example above but with SJF as our scheduling policy. Figure 7.3 shows the results of running A, B, and C. Hopefully the diagram makes it clear why SJF performs much better with regards to average turnaround time. Simply by running B and C before A, SJF reduces average turnaround from 110 seconds to 50 (\n   \n    \\frac{10+20+120}{3} = 50\n   \n   ), more than a factor of two improvement.\n\n\n2\n   \n   Recommended action in this case: either quickly switch to a different line, or take a long, deep, and relaxing breath. That’s right, breathe in, breathe out. It will be OK, don’t worry.\n\n\n\n\n**ASIDE: PREEMPTIVE SCHEDULERS**\n\n\nIn the old days of batch computing, a number of\n   **non-preemptive**\n   schedulers were developed; such systems would run each job to completion before considering whether to run a new job. Virtually all modern schedulers are\n   **preemptive**\n   , and quite willing to stop one process from running in order to run another. This implies that the scheduler employs the mechanisms we learned about previously; in particular, the scheduler can perform a\n   **context switch**\n   , stopping one running process temporarily and resuming (or starting) another.\n\n\nIn fact, given our assumptions about jobs all arriving at the same time, we could prove that SJF is indeed an\n   **optimal**\n   scheduling algorithm. However, you are in a systems class, not theory or operations research; no proofs are allowed.\n\n\nThus we arrive upon a good approach to scheduling with SJF, but our assumptions are still fairly unrealistic. Let's relax another. In particular, we can target assumption 2, and now assume that jobs can arrive at any time instead of all at once. What problems does this lead to?\n\n\n*(Another pause to think ... are you thinking? Come on, you can do it)*\n\n\nHere we can illustrate the problem again with an example. This time, assume A arrives at\n   \n    t = 0\n   \n   and needs to run for 100 seconds, whereas B and C arrive at\n   \n    t = 10\n   \n   and each need to run for 10 seconds. With pure SJF, we'd get the schedule seen in Figure 7.4.\n\n\n\n\n![Gantt chart showing SJF scheduling with late arrivals. Job A runs from time 0 to 100. Jobs B and C arrive at time 10 and run from time 100 to 110 and 110 to 120 respectively. The chart shows a long wait for B and C due to A's completion time.](images/image_0009.jpeg)\n\n\nThe figure is a Gantt chart illustrating SJF scheduling with late arrivals. The horizontal axis represents time from 0 to 120. Job A is represented by a large black bar starting at time 0 and ending at time 100. A vertical arrow labeled \"[B,C arrive]\" points to the start of job A at time 0. Jobs B and C are represented by two adjacent gray bars starting at time 100 and ending at time 110 and 120 respectively. The chart shows that jobs B and C suffer a long wait time of 100 seconds each because they must wait for job A to complete.\n\n\nGantt chart showing SJF scheduling with late arrivals. Job A runs from time 0 to 100. Jobs B and C arrive at time 10 and run from time 100 to 110 and 110 to 120 respectively. The chart shows a long wait for B and C due to A's completion time.\n\n\nFigure 7.4: SJF With Late Arrivals From B and C\n\n\nAs you can see from the figure, even though B and C arrived shortly after A, they still are forced to wait until A has completed, and thus suffer the same convoy problem. Average turnaround time for these three jobs is 103.33 seconds (\n   \n    \\frac{100+(110-10)+(120-10)}{3}\n   \n   ). What can a scheduler do?"
        },
        {
          "name": "Shortest Time-to-Completion First (STCF)",
          "content": "To address this concern, we need to relax assumption 3 (that jobs must run to completion), so let's do that. We also need some machinery within the scheduler itself. As you might have guessed, given our previous discussion about timer interrupts and context switching, the scheduler can\n\n\n\n\n![Figure 7.5: STCF Simple Example. A Gantt chart showing job execution over time (0 to 120). Job A starts at time 0 and runs for 10 units. At time 10, jobs B and C arrive. Job B runs for 10 units (total time 20). Job C runs for 10 units (total time 30). Job A resumes and runs for 90 units (total time 120). The x-axis is labeled 'Time' with ticks at 0, 20, 40, 60, 80, 100, and 120. The y-axis shows job names A, B, and C. A bracket at the top left indicates '[B,C arrive]' at time 10.](images/image_0010.jpeg)\n\n\nFigure 7.5: STCF Simple Example. A Gantt chart showing job execution over time (0 to 120). Job A starts at time 0 and runs for 10 units. At time 10, jobs B and C arrive. Job B runs for 10 units (total time 20). Job C runs for 10 units (total time 30). Job A resumes and runs for 90 units (total time 120). The x-axis is labeled 'Time' with ticks at 0, 20, 40, 60, 80, 100, and 120. The y-axis shows job names A, B, and C. A bracket at the top left indicates '[B,C arrive]' at time 10.\n\n\nFigure 7.5: STCF Simple Example\n\n\ncertainly do something else when B and C arrive: it can\n   **preempt**\n   job A and decide to run another job, perhaps continuing A later. SJF by our definition is a\n   **non-preemptive**\n   scheduler, and thus suffers from the problems described above.\n\n\nFortunately, there is a scheduler which does exactly that: add preemption to SJF, known as the\n   **Shortest Time-to-Completion First (STCF)**\n   or\n   **Preemptive Shortest Job First (PSJF)**\n   scheduler [CK68]. Any time a new job enters the system, the STCF scheduler determines which of the remaining jobs (including the new job) has the least time left, and schedules that one. Thus, in our example, STCF would preempt A and run B and C to completion; only when they are finished would A's remaining time be scheduled. Figure 7.5 shows an example.\n\n\nThe result is a much-improved average turnaround time: 50 seconds\n   \n    \\left(\\frac{(120-0)+(20-10)+(30-10)}{3}\\right)\n   \n   . And as before, given our new assumptions, STCF is provably optimal; given that SJF is optimal if all jobs arrive at the same time, you should probably be able to see the intuition behind the optimality of STCF."
        },
        {
          "name": "A New Metric: Response Time",
          "content": "Thus, if we knew job lengths, and that jobs only used the CPU, and our only metric was turnaround time, STCF would be a great policy. In fact, for a number of early batch computing systems, these types of scheduling algorithms made some sense. However, the introduction of time-shared machines changed all that. Now users would sit at a terminal and demand interactive performance from the system as well. And thus, a new metric was born:\n   **response time**\n   .\n\n\nWe define response time as the time from when the job arrives in a system to the first time it is scheduled\n   \n    3\n   \n   . More formally:\n\n\nT_{\\text{response}} = T_{\\text{firstrun}} - T_{\\text{arrival}} \\quad (7.2)\n\n\n3\n   \n   Some define it slightly differently, e.g., to also include the time until the job produces some kind of “response”; our definition is the best-case version of this, essentially assuming that the job produces a response instantaneously.\n\n\n\n\n![Figure 7.6: SJF Again (Bad for Response Time). A Gantt chart showing three jobs: A, B, and C. Job A starts at time 0 and ends at time 5. Job B starts at time 5 and ends at time 10. Job C starts at time 10 and ends at time 15. The x-axis is labeled 'Time' and ranges from 0 to 30.](images/image_0011.jpeg)\n\n\nFigure 7.6: SJF Again (Bad for Response Time). A Gantt chart showing three jobs: A, B, and C. Job A starts at time 0 and ends at time 5. Job B starts at time 5 and ends at time 10. Job C starts at time 10 and ends at time 15. The x-axis is labeled 'Time' and ranges from 0 to 30.\n\n\nFigure 7.6: SJF Again (Bad for Response Time)\n\n\n\n\n![Figure 7.7: Round Robin (Good For Response Time). A Gantt chart showing the execution of three jobs (A, B, C) using Round Robin scheduling with a time slice of 2 units. The sequence is A, B, C, A, B, C, A, B, C, A, B, C. The x-axis is labeled 'Time' and ranges from 0 to 30.](images/image_0012.jpeg)\n\n\nFigure 7.7: Round Robin (Good For Response Time). A Gantt chart showing the execution of three jobs (A, B, C) using Round Robin scheduling with a time slice of 2 units. The sequence is A, B, C, A, B, C, A, B, C, A, B, C. The x-axis is labeled 'Time' and ranges from 0 to 30.\n\n\nFigure 7.7: Round Robin (Good For Response Time)\n\n\nFor example, if we had the schedule from Figure 7.5 (with A arriving at time 0, and B and C at time 10), the response time of each job is as follows: 0 for job A, 0 for B, and 10 for C (average: 3.33).\n\n\nAs you might be thinking, STCF and related disciplines are not particularly good for response time. If three jobs arrive at the same time, for example, the third job has to wait for the previous two jobs to run\n   *in their entirety*\n   before being scheduled just once. While great for turnaround time, this approach is quite bad for response time and interactivity. Indeed, imagine sitting at a terminal, typing, and having to wait 10 seconds to see a response from the system just because some other job got scheduled in front of yours: not too pleasant.\n\n\nThus, we are left with another problem: how can we build a scheduler that is sensitive to response time?"
        },
        {
          "name": "Round Robin",
          "content": "To solve this problem, we will introduce a new scheduling algorithm, classically referred to as\n   **Round-Robin (RR)**\n   scheduling [K64]. The basic idea is simple: instead of running jobs to completion, RR runs a job for a\n   **time slice**\n   (sometimes called a\n   **scheduling quantum**\n   ) and then switches to the next job in the run queue. It repeatedly does so until the jobs are finished. For this reason, RR is sometimes called\n   **time-slicing**\n   . Note that the length of a time slice must be a multiple of the timer-interrupt period; thus if the timer interrupts every 10 milliseconds, the time slice could be 10, 20, or any other multiple of 10 ms.\n\n\nTo understand RR in more detail, let's look at an example. Assume three jobs A, B, and C arrive at the same time in the system, and that\n\n\n\n\n**TIP: AMORTIZATION CAN REDUCE COSTS**\n\n\nThe general technique of\n   **amortization**\n   is commonly used in systems when there is a fixed cost to some operation. By incurring that cost less often (i.e., by performing the operation fewer times), the total cost to the system is reduced. For example, if the time slice is set to 10 ms, and the context-switch cost is 1 ms, roughly 10% of time is spent context switching and is thus wasted. If we want to\n   *amortize*\n   this cost, we can increase the time slice, e.g., to 100 ms. In this case, less than 1% of time is spent context switching, and thus the cost of time-slicing has been amortized.\n\n\nthey each wish to run for 5 seconds. An SJF scheduler runs each job to completion before running another (Figure 7.6). In contrast, RR with a time-slice of 1 second would cycle through the jobs quickly (Figure 7.7).\n\n\nThe average response time of RR is:\n   \n    \\frac{0+1+2}{3} = 1\n   \n   ; for SJF, average response time is:\n   \n    \\frac{0+5+10}{3} = 5\n   \n   .\n\n\nAs you can see, the length of the time slice is critical for RR. The shorter it is, the better the performance of RR under the response-time metric. However, making the time slice too short is problematic: suddenly the cost of context switching will dominate overall performance. Thus, deciding on the length of the time slice presents a trade-off to a system designer, making it long enough to\n   **amortize**\n   the cost of switching without making it so long that the system is no longer responsive.\n\n\nNote that the cost of context switching does not arise solely from the OS actions of saving and restoring a few registers. When programs run, they build up a great deal of state in CPU caches, TLBs, branch predictors, and other on-chip hardware. Switching to another job causes this state to be flushed and new state relevant to the currently-running job to be brought in, which may exact a noticeable performance cost [MB91].\n\n\nRR, with a reasonable time slice, is thus an excellent scheduler if response time is our only metric. But what about our old friend turnaround time? Let's look at our example above again. A, B, and C, each with running times of 5 seconds, arrive at the same time, and RR is the scheduler with a (long) 1-second time slice. We can see from the picture above that A finishes at 13, B at 14, and C at 15, for an average of 14. Pretty awful!\n\n\nIt is not surprising, then, that RR is indeed one of the\n   *worst*\n   policies if turnaround time is our metric. Intuitively, this should make sense: what RR is doing is stretching out each job as long as it can, by only running each job for a short bit before moving to the next. Because turnaround time only cares about when jobs finish, RR is nearly pessimistic, even worse than simple FIFO in many cases.\n\n\nMore generally, any policy (such as RR) that is\n   **fair**\n   , i.e., that evenly divides the CPU among active processes on a small time scale, will perform poorly on metrics such as turnaround time. Indeed, this is an inherent trade-off: if you are willing to be unfair, you can run shorter jobs to completion, but at the cost of response time; if you instead value fairness,\n\n\n\n\n**TIP: OVERLAP ENABLES HIGHER UTILIZATION**\n\n\nWhen possible,\n   **overlap**\n   operations to maximize the utilization of systems. Overlap is useful in many different domains, including when performing disk I/O or sending messages to remote machines; in either case, starting the operation and then switching to other work is a good idea, and improves the overall utilization and efficiency of the system.\n\n\nresponse time is lowered, but at the cost of turnaround time. This type of\n   **trade-off**\n   is common in systems; you can't have your cake and eat it too\n   \n    4\n   \n   .\n\n\nWe have developed two types of schedulers. The first type (SJF, STCF) optimizes turnaround time, but is bad for response time. The second type (RR) optimizes response time but is bad for turnaround. And we still have two assumptions which need to be relaxed: assumption 4 (that jobs do no I/O), and assumption 5 (that the run-time of each job is known). Let's tackle those assumptions next."
        },
        {
          "name": "Incorporating I/O",
          "content": "First we will relax assumption 4 — of course all programs perform I/O. Imagine a program that didn't take any input: it would produce the same output each time. Imagine one without output: it is the proverbial tree falling in the forest, with no one to see it; it doesn't matter that it ran.\n\n\nA scheduler clearly has a decision to make when a job initiates an I/O request, because the currently-running job won't be using the CPU during the I/O; it is\n   **blocked**\n   waiting for I/O completion. If the I/O is sent to a hard disk drive, the process might be blocked for a few milliseconds or longer, depending on the current I/O load of the drive. Thus, the scheduler should probably schedule another job on the CPU at that time.\n\n\nThe scheduler also has to make a decision when the I/O completes. When that occurs, an interrupt is raised, and the OS runs and moves the process that issued the I/O from blocked back to the ready state. Of course, it could even decide to run the job at that point. How should the OS treat each job?\n\n\nTo understand this issue better, let us assume we have two jobs, A and B, which each need 50 ms of CPU time. However, there is one obvious difference: A runs for 10 ms and then issues an I/O request (assume here that I/Os each take 10 ms), whereas B simply uses the CPU for 50 ms and performs no I/O. The scheduler runs A first, then B after (Figure 7.8).\n\n\nAssume we are trying to build a STCF scheduler. How should such a scheduler account for the fact that A is broken up into 5 10-ms sub-jobs,\n\n\n4\n   \n   A saying that confuses people, because it should be \"You can't\n   *keep*\n   your cake and eat it too\" (which is kind of obvious, no?). Amazingly, there is a wikipedia page about this saying; even more amazingly, it is kind of fun to read [W15]. As they say in Italian, you can't\n   *Avere la botte piena e la moglie ubriaca*\n   .\n\n\n\n\n![Figure 7.8: Poor Use Of Resources. A Gantt chart showing CPU and Disk usage over time (0 to 140 ms). Process A (CPU) runs in 10ms bursts at 0, 20, 40, 60, and 80 ms. Process B (CPU) runs in a single 50ms burst from 80 to 130 ms. Disk I/O occurs at 10ms intervals: 10-20, 30-40, 50-60, and 70-80 ms. The chart shows significant idle time for the CPU between 130 ms and 140 ms.](images/image_0013.jpeg)\n\n\nFigure 7.8: Poor Use Of Resources. A Gantt chart showing CPU and Disk usage over time (0 to 140 ms). Process A (CPU) runs in 10ms bursts at 0, 20, 40, 60, and 80 ms. Process B (CPU) runs in a single 50ms burst from 80 to 130 ms. Disk I/O occurs at 10ms intervals: 10-20, 30-40, 50-60, and 70-80 ms. The chart shows significant idle time for the CPU between 130 ms and 140 ms.\n\n\nFigure 7.8: Poor Use Of Resources\n\n\n\n\n![Figure 7.9: Overlap Allows Better Use Of Resources. A Gantt chart showing CPU and Disk usage over time (0 to 140 ms). Process A (CPU) runs in 10ms bursts at 0, 20, 40, 60, 80, 100, and 120 ms. Process B (CPU) runs in 10ms bursts at 10, 30, 50, 70, 90, 110, and 130 ms. Disk I/O occurs at 10ms intervals: 10-20, 30-40, 50-60, and 70-80 ms. The chart shows that CPU usage is maximized by overlapping CPU bursts with Disk I/O periods.](images/image_0014.jpeg)\n\n\nFigure 7.9: Overlap Allows Better Use Of Resources. A Gantt chart showing CPU and Disk usage over time (0 to 140 ms). Process A (CPU) runs in 10ms bursts at 0, 20, 40, 60, 80, 100, and 120 ms. Process B (CPU) runs in 10ms bursts at 10, 30, 50, 70, 90, 110, and 130 ms. Disk I/O occurs at 10ms intervals: 10-20, 30-40, 50-60, and 70-80 ms. The chart shows that CPU usage is maximized by overlapping CPU bursts with Disk I/O periods.\n\n\nFigure 7.9: Overlap Allows Better Use Of Resources\n\n\nwhereas B is just a single 50-ms CPU demand? Clearly, just running one job and then the other without considering how to take I/O into account makes little sense.\n\n\nA common approach is to treat each 10-ms sub-job of A as an independent job. Thus, when the system starts, its choice is whether to schedule a 10-ms A or a 50-ms B. With STCF, the choice is clear: choose the shorter one, in this case A. Then, when the first sub-job of A has completed, only B is left, and it begins running. Then a new sub-job of A is submitted, and it preempts B and runs for 10 ms. Doing so allows for\n   **overlap**\n   , with the CPU being used by one process while waiting for the I/O of another process to complete; the system is thus better utilized (see Figure 7.9).\n\n\nAnd thus we see how a scheduler might incorporate I/O. By treating each CPU burst as a job, the scheduler makes sure processes that are “interactive” get run frequently. While those interactive jobs are performing I/O, other CPU-intensive jobs run, thus better utilizing the processor."
        },
        {
          "name": "No More Oracle",
          "content": "With a basic approach to I/O in place, we come to our final assumption: that the scheduler knows the length of each job. As we said before, this is likely the worst assumption we could make. In fact, in a general-purpose OS (like the ones we care about), the OS usually knows very little about the length of each job. Thus, how can we build an approach that behaves like SJF/STCF without such\n   *a priori*\n   knowledge? Further, how can we incorporate some of the ideas we have seen with the RR scheduler so that response time is also quite good?"
        }
      ]
    },
    {
      "name": "Scheduling:The Multi-Level Feedback Queue",
      "sections": [
        {
          "name": "MLFQ: Basic Rules",
          "content": "To build such a scheduler, in this chapter we will describe the basic algorithms behind a multi-level feedback queue; although the specifics of many implemented MLFQs differ [E95], most approaches are similar.\n\n\nIn our treatment, the MLFQ has a number of distinct\n   **queues**\n   , each assigned a different\n   **priority level**\n   . At any given time, a job that is ready to run is on a single queue. MLFQ uses priorities to decide which job should run at a given time: a job with higher priority (i.e., a job on a higher queue) is chosen to run.\n\n\nOf course, more than one job may be on a given queue, and thus have the\n   *same*\n   priority. In this case, we will just use round-robin scheduling among those jobs.\n\n\nThus, we arrive at the first two basic rules for MLFQ:\n\n\n  * •\n    **Rule 1:**\n    If\n    \n     \\text{Priority}(A) > \\text{Priority}(B)\n    \n    , A runs (B doesn't).\n  * •\n    **Rule 2:**\n    If\n    \n     \\text{Priority}(A) = \\text{Priority}(B)\n    \n    , A & B run in RR.\n\n\nThe key to MLFQ scheduling therefore lies in how the scheduler sets priorities. Rather than giving a fixed priority to each job, MLFQ\n   *varies*\n   the priority of a job based on its\n   *observed behavior*\n   . If, for example, a job repeatedly relinquishes the CPU while waiting for input from the keyboard, MLFQ will keep its priority high, as this is how an interactive process might behave. If, instead, a job uses the CPU intensively for long periods of time, MLFQ will reduce its priority. In this way, MLFQ will try to\n   *learn*\n   about processes as they run, and thus use the\n   *history*\n   of the job to predict its\n   *future*\n   behavior.\n\n\nIf we were to put forth a picture of what the queues might look like at a given instant, we might see something like the following (Figure 8.1, page 3). In the figure, two jobs (A and B) are at the highest priority level, while job C is in the middle and Job D is at the lowest priority. Given our current knowledge of how MLFQ works, the scheduler would just alternate time slices between A and B because they are the highest priority jobs in the system; poor jobs C and D would never even get to run — an outrage!\n\n\nOf course, just showing a static snapshot of some queues does not really give you an idea of how MLFQ works. What we need is to under-\n\n\n\n\n![Diagram of a Multi-Level Feedback Queue (MLFQ) system with four queues (Q8 to Q1) and four jobs (A, B, C, D).](images/image_0015.jpeg)\n\n\nThe diagram illustrates a Multi-Level Feedback Queue (MLFQ) system with four priority levels, Q8 through Q1. The queues are arranged vertically from top to bottom: Q8, Q7, Q6, Q5, Q4, Q3, Q2, and Q1. The top level, Q8, is labeled \"[High Priority]\" and the bottom level, Q1, is labeled \"[Low Priority]\".\n\n\n  * Queue Q8 contains job A, which is currently running and will be followed by job B.\n  * Queue Q7 is empty.\n  * Queue Q6 is empty.\n  * Queue Q5 is empty.\n  * Queue Q4 contains job C.\n  * Queue Q3 is empty.\n  * Queue Q2 is empty.\n  * Queue Q1 contains job D.\n\n\nDiagram of a Multi-Level Feedback Queue (MLFQ) system with four queues (Q8 to Q1) and four jobs (A, B, C, D).\n\n\nFigure 8.1: MLFQ Example\n\n\nstand how job priority\n   *changes*\n   over time. And that, in a surprise only to those who are reading a chapter from this book for the first time, is exactly what we will do next."
        },
        {
          "name": "Attempt #1: How To Change Priority",
          "content": "We now must decide how MLFQ is going to change the priority level of a job (and thus which queue it is on) over the lifetime of a job. To do this, we must keep in mind our workload: a mix of interactive jobs that are short-running (and may frequently relinquish the CPU), and some longer-running “CPU-bound” jobs that need a lot of CPU time but where response time isn’t important.\n\n\nFor this, we need a new concept, which we will call the job’s\n   **allotment**\n   . The allotment is the amount of time a job can spend at a given priority level before the scheduler reduces its priority. For simplicity, at first, we will assume the allotment is equal to a single time slice.\n\n\nHere is our first attempt at a priority-adjustment algorithm:\n\n\n  * •\n    **Rule 3:**\n    When a job enters the system, it is placed at the highest priority (the topmost queue).\n  * •\n    **Rule 4a:**\n    If a job uses up its allotment while running, its priority is\n    *reduced*\n    (i.e., it moves down one queue).\n  * •\n    **Rule 4b:**\n    If a job gives up the CPU (for example, by performing an I/O operation) before the allotment is up, it stays at the\n    *same*\n    priority level (i.e., its allotment is reset).\n\n\n\n\n![Figure 8.2: Long-running Job Over Time. A Gantt chart showing a job's progression through three priority queues (Q2, Q1, Q0) over time (0 to 200 ms). The job starts in Q2 for 10 ms, moves to Q1 for 10 ms, and then to Q0 for 180 ms.](images/image_0016.jpeg)\n\n\nA Gantt chart illustrating the execution of a long-running job across three priority queues (Q2, Q1, Q0) over a 200 ms time period. The x-axis represents time from 0 to 200 ms in 50 ms increments. The y-axis lists the queues Q2, Q1, and Q0. The job's execution is represented by black bars: a 10 ms bar in Q2 (0-10 ms), a 10 ms bar in Q1 (10-20 ms), and a 180 ms bar in Q0 (20-200 ms). Dashed horizontal lines separate the queues, and a dashed vertical line marks the 20 ms boundary between Q1 and Q0.\n\n\nFigure 8.2: Long-running Job Over Time. A Gantt chart showing a job's progression through three priority queues (Q2, Q1, Q0) over time (0 to 200 ms). The job starts in Q2 for 10 ms, moves to Q1 for 10 ms, and then to Q0 for 180 ms.\n\n\nFigure 8.2:\n   **Long-running Job Over Time**\n\n\n\n\n**Example 1: A Single Long-Running Job**\n\n\nLet's look at some examples. First, we'll look at what happens when there has been a long running job in the system, with a time slice of 10 ms (and with the allotment set equal to the time slice). Figure 8.2 shows what happens to this job over time in a three-queue scheduler.\n\n\nAs you can see in the example, the job enters at the highest priority (Q2). After a single time slice of 10 ms, the scheduler reduces the job's priority by one, and thus the job is on Q1. After running at Q1 for a time slice, the job is finally lowered to the lowest priority in the system (Q0), where it remains. Pretty simple, no?\n\n\n\n\n**Example 2: Along Came A Short Job**\n\n\nNow let's look at a more complicated example, and hopefully see how MLFQ tries to approximate SJF. In this example, there are two jobs: A, which is a long-running CPU-intensive job, and B, which is a short-running interactive job. Assume A has been running for some time, and then B arrives. What will happen? Will MLFQ approximate SJF for B?\n\n\nFigure 8.3 on page 5 (left) plots the results of this scenario. Job A (shown in black) is running along in the lowest-priority queue (as would any long-running CPU-intensive jobs); B (shown in gray) arrives at time\n   \n    T = 100\n   \n   , and thus is inserted into the highest queue; as its run-time is short (only 20 ms), B completes before reaching the bottom queue, in two time slices; then A resumes running (at low priority).\n\n\nFrom this example, you can hopefully understand one of the major goals of the algorithm: because it doesn't\n   *know*\n   whether a job will be a short job or a long-running job, it first\n   *assumes*\n   it might be a short job, thus giving the job high priority. If it actually is a short job, it will run quickly and complete; if it is not a short job, it will slowly move down the queues, and thus soon prove itself to be a long-running more batch-like process. In this manner, MLFQ approximates SJF.\n\n\n\n\n![Figure 8.3: Along Came An Interactive Job: Two Examples. The figure shows two Gantt charts illustrating the execution of jobs in a Multi-Level Feedback Queue (MLFQ) with three priority levels: Q0 (highest), Q1, and Q2 (lowest). The left chart shows a long-running batch job A (black) and an interactive job B (gray) that quickly relinquishes the CPU. The right chart shows job A (black) and job B (gray) competing for CPU time, with job B being preempted frequently by job A.](images/image_0017.jpeg)\n\n\nThe figure consists of two Gantt charts side-by-side, each showing three priority levels: Q0 (highest), Q1, and Q2 (lowest). The x-axis for both charts represents time from 0 to 200.\n\n\n**Left Chart:**\n\n\n  * Q0: A long black bar from time 0 to approximately 150.\n  * Q1: A short black bar from time 0 to 10, and a short gray bar from time 100 to 110.\n  * Q2: A short black bar from time 0 to 10, and a short gray bar from time 100 to 110.\n\n\n**Right Chart:**\n\n\n  * Q0: A long black bar from time 0 to approximately 150, with many short vertical gray bars (job B) interspersed throughout its execution.\n  * Q1: A short black bar from time 0 to 10.\n  * Q2: A short black bar from time 0 to 10.\n\n\nFigure 8.3: Along Came An Interactive Job: Two Examples. The figure shows two Gantt charts illustrating the execution of jobs in a Multi-Level Feedback Queue (MLFQ) with three priority levels: Q0 (highest), Q1, and Q2 (lowest). The left chart shows a long-running batch job A (black) and an interactive job B (gray) that quickly relinquishes the CPU. The right chart shows job A (black) and job B (gray) competing for CPU time, with job B being preempted frequently by job A.\n\n\nFigure 8.3: Along Came An Interactive Job: Two Examples\n\n\n\n\n**Example 3: What About I/O?**\n\n\nLet's now look at an example with some I/O. As Rule 4b states above, if a process gives up the processor before using up its allotment, we keep it at the same priority level. The intent of this rule is simple: if an interactive job, for example, is doing a lot of I/O (say by waiting for user input from the keyboard or mouse), it will relinquish the CPU before its allotment is complete; in such case, we don't wish to penalize the job and thus simply keep it at the same level.\n\n\nFigure 8.3 (right) shows an example of how this works, with an interactive job B (shown in gray) that needs the CPU only for 1 ms before performing an I/O competing for the CPU with a long-running batch job A (shown in black). The MLFQ approach keeps B at the highest priority because B keeps releasing the CPU; if B is an interactive job, MLFQ further achieves its goal of running interactive jobs quickly.\n\n\n\n\n**Problems With Our Current MLFQ**\n\n\nWe thus have a basic MLFQ. It seems to do a fairly good job, sharing the CPU fairly between long-running jobs, and letting short or I/O-intensive interactive jobs run quickly. Unfortunately, the approach we have developed thus far contains serious flaws. Can you think of any?\n\n\n*(This is where you pause and think as deviously as you can)*\n\n\nFirst, there is the problem of\n   **starvation**\n   : if there are “too many” interactive jobs in the system, they will combine to consume\n   *all*\n   CPU time, and thus long-running jobs will\n   *never*\n   receive any CPU time (they\n   **starve**\n   ). We'd like to make some progress on these jobs even in this scenario.\n\n\nSecond, a smart user could rewrite their program to\n   **game the scheduler**\n   . Gaming the scheduler generally refers to the idea of doing something sneaky to trick the scheduler into giving you more than your fair share of the resource. The algorithm we have described is susceptible to\n\n\n\n\n**TIP: SCHEDULING MUST BE SECURE FROM ATTACK**\n\n\nYou might think that a scheduling policy, whether inside the OS itself (as discussed herein), or in a broader context (e.g., in a distributed storage system's I/O request handling [Y+18]), is not a\n   **security**\n   concern, but in increasingly many cases, it is exactly that. Consider the modern datacenter, in which users from around the world share CPUs, memories, networks, and storage systems; without care in policy design and enforcement, a single user may be able to adversely harm others and gain advantage for itself. Thus, scheduling policy forms an important part of the security of a system, and should be carefully constructed.\n\n\nthe following attack: before the allotment is used, issue an I/O operation (e.g., to a file) and thus relinquish the CPU; doing so allows you to remain in the same queue, and thus gain a higher percentage of CPU time. When done right (e.g., by running for 99% of the allotment before relinquishing the CPU), a job could nearly monopolize the CPU.\n\n\nFinally, a program may\n   *change its behavior*\n   over time; what was CPU-bound may transition to a phase of interactivity. With our current approach, such a job would be out of luck and not be treated like the other interactive jobs in the system."
        },
        {
          "name": "Attempt #2: The Priority Boost",
          "content": "Let's try to change the rules and see if we can avoid the problem of starvation. What could we do in order to guarantee that CPU-bound jobs will make some progress (even if it is not much?).\n\n\nThe simple idea here is to periodically\n   **boost**\n   the priority of all the jobs in the system. There are many ways to achieve this, but let's just do something simple: throw them all in the topmost queue; hence, a new rule:\n\n\n  * •\n    **Rule 5:**\n    After some time period\n    \n     S\n    \n    , move all the jobs in the system to the topmost queue.\n\n\nOur new rule solves two problems at once. First, processes are guaranteed not to starve: by sitting in the top queue, a job will share the CPU with other high-priority jobs in a round-robin fashion, and thus eventually receive service. Second, if a CPU-bound job has become interactive, the scheduler treats it properly once it has received the priority boost.\n\n\nLet's see an example. In this scenario, we just show the behavior of a long-running job when competing for the CPU with two short-running interactive jobs. Two graphs are shown in Figure 8.4 (page 7). On the left, there is no priority boost, and thus the long-running job gets starved once the two short jobs arrive; on the right, there is a priority boost every 100 ms (which is likely too small of a value, but used here for the example),\n\n\n\n\n![Figure 8.4: Two Gantt charts comparing job execution without and with priority boost. The left chart shows a long-running job in Q0 (0-100ms) and a short job in Q2 (100-200ms). The right chart shows the long-running job in Q0 (0-50ms), then boosted to Q2 (50-100ms), then back to Q0 (100-150ms), and finally boosted to Q2 (150-200ms).](images/image_0018.jpeg)\n\n\nThe figure consists of two Gantt charts side-by-side, each showing three priority levels: Q0 (bottom), Q1 (middle), and Q2 (top). The x-axis for both charts represents time from 0 to 200 ms.\n\n\n**Left Chart (Without Priority Boost):**\n\n\n  * Q0: A long black bar from time 0 to 100 ms.\n  * Q2: A short black bar from time 100 to 105 ms.\n  * Q1: No visible activity.\n\n\n**Right Chart (With Priority Boost):**\n\n\n  * Q0: A black bar from time 0 to 50 ms.\n  * Q2: A black bar from time 50 to 100 ms.\n  * Q0: A black bar from time 100 to 150 ms.\n  * Q2: A black bar from time 150 to 200 ms.\n  * Q1: No visible activity.\n\n\nFigure 8.4: Two Gantt charts comparing job execution without and with priority boost. The left chart shows a long-running job in Q0 (0-100ms) and a short job in Q2 (100-200ms). The right chart shows the long-running job in Q0 (0-50ms), then boosted to Q2 (50-100ms), then back to Q0 (100-150ms), and finally boosted to Q2 (150-200ms).\n\n\n**Without (Left) and With (Right) Priority Boost**\nand thus we at least guarantee that the long-running job will make some progress, getting boosted to the highest priority every 100 ms and thus getting to run periodically.\n\n\nOf course, the addition of the time period\n   \n    S\n   \n   leads to the obvious question: what should\n   \n    S\n   \n   be set to? John Ousterhout, a well-regarded systems researcher [O11], used to call such values in systems\n   **voo-doo constants**\n   , because they seemed to require some form of black magic to set them correctly. Unfortunately,\n   \n    S\n   \n   has that flavor. If it is set too high, long-running jobs could starve; too low, and interactive jobs may not get a proper share of the CPU. As such, it is often left to the system administrator to find the right value – or in the modern world, increasingly, to automatic methods based on machine learning [A+17]."
        },
        {
          "name": "Attempt #3: Better Accounting",
          "content": "We now have one more problem to solve: how to prevent gaming of our scheduler? The real culprit here, as you might have guessed, are Rules 4a and 4b, which let a job retain its priority by relinquishing the CPU before its allotment expires. So what should we do?\n\n\n\n\n**TIP: AVOID VOODOO CONSTANTS (OUSTERHOUT'S LAW)**\n\n\nAvoiding voodoo constants is a good idea whenever possible. Unfortunately, as in the example above, it is often difficult. One could try to make the system learn a good value, but that too is not straightforward. The frequent result: a configuration file filled with default parameter values that a seasoned administrator can tweak when something isn't quite working correctly. As you can imagine, these are often left unmodified, and thus we are left to hope that the defaults work well in the field. This tip brought to you by our old OS professor, John Ousterhout, and hence we call it\n   **Ousterhout's Law**\n   .\n\n\n\n\n![Figure 8.5: Two Gantt charts comparing CPU scheduling without and with gaming tolerance. The left chart shows a process staying in Q0, while the right chart shows it moving down to Q1 and Q2 due to I/O.](images/image_0019.jpeg)\n\n\nThe figure consists of two side-by-side Gantt charts. Both charts have a horizontal axis representing time from 0 to 300. They show three priority queues: Q0 (bottom), Q1 (middle), and Q2 (top). In the left chart (Without Gaming Tolerance), a process in Q0 uses its time slice (0-100) and then issues an I/O (100-150). It then immediately returns to Q0 and uses another time slice (150-200) before issuing another I/O (200-250). In the right chart (With Gaming Tolerance), the process in Q0 uses its time slice (0-100) and issues an I/O (100-150). It then moves down to Q1 and uses its time slice (150-160). It then moves down to Q2 and uses its time slice (160-170). It then moves back down to Q1 and uses its time slice (170-180). It then moves back down to Q0 and uses its time slice (180-190). It then issues an I/O (190-200) and moves down to Q1. It uses its time slice (200-210) and moves down to Q2. It uses its time slice (210-220) and moves down to Q1. It uses its time slice (220-230) and moves down to Q2. It uses its time slice (230-240) and moves down to Q1. It uses its time slice (240-250) and issues an I/O (250-260). It then moves down to Q2 and uses its time slice (260-270). It then moves down to Q1 and uses its time slice (270-280). It then moves down to Q2 and uses its time slice (280-290). It then moves down to Q1 and uses its time slice (290-300).\n\n\nFigure 8.5: Two Gantt charts comparing CPU scheduling without and with gaming tolerance. The left chart shows a process staying in Q0, while the right chart shows it moving down to Q1 and Q2 due to I/O.\n\n\nFigure 8.5:\n   **Without (Left) and With (Right) Gaming Tolerance**\n\n\nThe solution here is to perform better\n   **accounting**\n   of CPU time at each level of the MLFQ. Instead of forgetting how much of its allotment a process used at a given level when it performs I/O, the scheduler should keep track; once a process has used its allotment, it is demoted to the next priority queue. Whether it uses its allotment in one long burst or many small ones should not matter. We thus rewrite Rules 4a and 4b to the following single rule:\n\n\n  * •\n    **Rule 4:**\n    Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).\n\n\nLet's look at an example. Figure 8.5 shows what happens when a workload tries to game the scheduler with the old Rules 4a and 4b (on the left) as well the new anti-gaming Rule 4. Without any protection from gaming, a process can issue an I/O before its allotment ends, thus staying at the same priority level, and dominating CPU time. With better accounting in place (right), regardless of the I/O behavior of the process, it slowly moves down the queues, and thus cannot gain an unfair share of the CPU."
        },
        {
          "name": "Tuning MLFQ And Other Issues",
          "content": "A few other issues arise with MLFQ scheduling. One big question is how to\n   **parameterize**\n   such a scheduler. For example, how many queues should there be? How big should the time slice be per queue? The allotment? How often should priority be boosted in order to avoid starvation and account for changes in behavior? There are no easy answers to these questions, and thus only some experience with workloads and subsequent tuning of the scheduler will lead to a satisfactory balance.\n\n\nFor example, most MLFQ variants allow for varying time-slice length across different queues. The high-priority queues are usually given short time slices; they are comprised of interactive jobs, after all, and thus\n\n\n\n\n![Figure 8.6: Lower Priority, Longer Quanta. A Gantt chart showing three queues (Q2, Q1, Q0) over a 300ms period. Q2 (highest priority) has two 20ms jobs. Q1 has two 40ms jobs. Q0 (lowest priority) has two 40ms jobs. Time slices are 20ms, 40ms, and 40ms respectively.](images/image_0020.jpeg)\n\n\nQueue | Job 1 | Job 2\nQ2 | 0-20ms | 20-40ms\nQ1 | 40-80ms | 80-120ms\nQ0 | 120-160ms | 160-200ms\n\n\nFigure 8.6: Lower Priority, Longer Quanta. A Gantt chart showing three queues (Q2, Q1, Q0) over a 300ms period. Q2 (highest priority) has two 20ms jobs. Q1 has two 40ms jobs. Q0 (lowest priority) has two 40ms jobs. Time slices are 20ms, 40ms, and 40ms respectively.\n\n\n**Lower Priority, Longer Quanta**\nquickly alternating between them makes sense (e.g., 10 or fewer milliseconds). The low-priority queues, in contrast, contain long-running jobs that are CPU-bound; hence, longer time slices work well (e.g., 100s of ms). Figure 8.6 shows an example in which two jobs run for 20 ms at the highest queue (with a 10-ms time slice), 40 ms in the middle (20-ms time slice), and with a 40-ms time slice at the lowest.\n\n\nThe Solaris MLFQ implementation — the Time-Sharing scheduling class, or TS — is particularly easy to configure; it provides a set of tables that determine exactly how the priority of a process is altered throughout its lifetime, how long each time slice is, and how often to boost the priority of a job [AD00]; an administrator can muck with this table in order to make the scheduler behave in different ways. Default values for the table are 60 queues, with slowly increasing time-slice lengths from 20 milliseconds (highest priority) to a few hundred milliseconds (lowest), and priorities boosted around every 1 second or so.\n\n\nOther MLFQ schedulers don't use a table or the exact rules described in this chapter; rather they adjust priorities using mathematical formulae. For example, the FreeBSD scheduler (version 4.3) uses a formula to calculate the current priority level of a job, basing it on how much CPU the process has used [LM+89]; in addition, usage is decayed over time, providing the desired priority boost in a different manner than described herein. See Epema's paper for an excellent overview of such\n   **decay-usage**\n   algorithms and their properties [E95].\n\n\nFinally, many schedulers have a few other features that you might encounter. For example, some schedulers reserve the highest priority levels for operating system work; thus typical user jobs can never obtain the highest levels of priority in the system. Some systems also allow some user\n   **advice**\n   to help set priorities; for example, by using the command-line utility\n   \n    nice\n   \n   you can increase or decrease the priority of a job (somewhat) and thus increase or decrease its chances of running at any given time. See the man page for more.\n\n\n\n\n**TIP: USE ADVICE WHERE POSSIBLE**\n\n\nAs the operating system rarely knows what is best for each and every process of the system, it is often useful to provide interfaces to allow users or administrators to provide some\n   **hints**\n   to the OS. We often call such hints\n   **advice**\n   , as the OS need not necessarily pay attention to it, but rather might take the advice into account in order to make a better decision. Such hints are useful in many parts of the OS, including the scheduler (e.g., with\n   \n    nice\n   \n   ), memory manager (e.g.,\n   \n    madvice\n   \n   ), and file system (e.g., informed prefetching and caching [P+95])."
        }
      ]
    },
    {
      "name": "Scheduling: Proportional Share",
      "sections": [
        {
          "name": "Basic Concept: Tickets Represent Your Share",
          "content": "Underlying lottery scheduling is one very basic concept:\n   **tickets**\n   , which are used to represent the share of a resource that a process (or user or whatever) should receive. The percent of tickets that a process has represents its share of the system resource in question.\n\n\nLet's look at an example. Imagine two processes, A and B, and further that A has 75 tickets while B has only 25. Thus, what we would like is for A to receive 75% of the CPU and B the remaining 25%.\n\n\nLottery scheduling achieves this probabilistically (but not deterministically) by holding a lottery every so often (say, every time slice). Holding a lottery is straightforward: the scheduler must know how many total tickets there are (in our example, there are 100). The scheduler then picks\n\n\n\n\n**TIP: USE RANDOMNESS**\n\n\nOne of the most beautiful aspects of lottery scheduling is its use of\n   **randomness**\n   . When you have to make a decision, using such a randomized approach is often a robust and simple way of doing so.\n\n\nRandom approaches have at least three advantages over more traditional decisions. First, random often avoids strange corner-case behaviors that a more traditional algorithm may have trouble handling. For example, consider the LRU replacement policy (studied in more detail in a future chapter on virtual memory); while often a good replacement algorithm, LRU attains worst-case performance for some cyclic-sequential workloads. Random, on the other hand, has no such worst case.\n\n\nSecond, random also is lightweight, requiring little state to track alternatives. In a traditional fair-share scheduling algorithm, tracking how much CPU each process has received requires per-process accounting, which must be updated after running each process. Doing so randomly necessitates only the most minimal of per-process state (e.g., the number of tickets each has).\n\n\nFinally, random can be quite fast. As long as generating a random number is quick, making the decision is also, and thus random can be used in a number of places where speed is required. Of course, the faster the need, the more random tends towards pseudo-random.\n\n\na winning ticket, which is a number from 0 to 99\n   \n    1\n   \n   . Assuming A holds tickets 0 through 74 and B 75 through 99, the winning ticket simply determines whether A or B runs. The scheduler then loads the state of that winning process and runs it.\n\n\nHere is an example output of a lottery scheduler's winning tickets:\n\n\n63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43 0 49 12\nHere is the resulting schedule:\n\n\n  A       A   A       A   A   A   A   A   A   A       A       A   A   A   A   A   A\n  B               B       B               B       B\nAs you can see from the example, the use of randomness in lottery scheduling leads to a probabilistic correctness in meeting the desired proportion, but no guarantee. In our example above, B only gets to run 4 out of 20 time slices (20%), instead of the desired 25% allocation. However, the longer these two jobs compete, the more likely they are to achieve the desired percentages.\n\n\n1\n   \n   Computer Scientists always start counting at 0. It is so odd to non-computer-types that famous people have felt obliged to write about why we do it this way [D82].\n\n\n\n\n**TIP: USE TICKETS TO REPRESENT SHARES**\n\n\nOne of the most powerful (and basic) mechanisms in the design of lottery (and stride) scheduling is that of the\n   **ticket**\n   . The ticket is used to represent a process's share of the CPU in these examples, but can be applied much more broadly. For example, in more recent work on virtual memory management for hypervisors, Waldspurger shows how tickets can be used to represent a guest operating system's share of memory [W02]. Thus, if you are ever in need of a mechanism to represent a proportion of ownership, this concept just might be ... (wait for it) ... the ticket."
        },
        {
          "name": "Ticket Mechanisms",
          "content": "Lottery scheduling also provides a number of mechanisms to manipulate tickets in different and sometimes useful ways. One way is with the concept of\n   **ticket currency**\n   . Currency allows a user with a set of tickets to allocate tickets among their own jobs in whatever currency they would like; the system then automatically converts said currency into the correct global value.\n\n\nFor example, assume users A and B have each been given 100 tickets. User A is running two jobs, A1 and A2, and gives them each 500 tickets (out of 1000 total) in A's currency. User B is running only 1 job and gives it 10 tickets (out of 10 total). The system converts A1's and A2's allocation from 500 each in A's currency to 50 each in the global currency; similarly, B1's 10 tickets is converted to 100 tickets. The lottery is then held over the global ticket currency (200 total) to determine which job runs.\n\n\n\nUser A | → | 500 | (A's currency) | to A1 | → | 50 | (global currency)\n |  | → | 500 | (A's currency) | to A2 | → | 50 | (global currency)\nUser B | → | 10 | (B's currency) | to B1 | → | 100 | (global currency)\n\n\nAnother useful mechanism is\n   **ticket transfer**\n   . With transfers, a process can temporarily hand off its tickets to another process. This ability is especially useful in a client/server setting, where a client process sends a message to a server asking it to do some work on the client's behalf. To speed up the work, the client can pass the tickets to the server and thus try to maximize the performance of the server while the server is handling the client's request. When finished, the server then transfers the tickets back to the client and all is as before.\n\n\nFinally,\n   **ticket inflation**\n   can sometimes be a useful technique. With inflation, a process can temporarily raise or lower the number of tickets it owns. Of course, in a competitive scenario with processes that do not trust one another, this makes little sense; one greedy process could give itself a vast number of tickets and take over the machine. Rather, inflation can be applied in an environment where a group of processes trust one another; in such a case, if any one process knows it needs more CPU time, it can boost its ticket value as a way to reflect that need to the system, all without communicating with any other processes.\n\n\n1 // counter: used to track if we've found the winner yet\n2 int counter = 0;\n3\n4 // winner: call some random number generator to\n5 //          get a value >= 0 and <= (totaltickets - 1)\n6 int winner = getrandom(0, totaltickets);\n7\n8 // current: use this to walk through the list of jobs\n9 node_t *current = head;\n10 while (current) {\n11     counter = counter + current->tickets;\n12     if (counter > winner)\n13         break; // found the winner\n14     current = current->next;\n15 }\n16 // 'current' is the winner: schedule it...\n\nFigure 9.1: Lottery Scheduling Decision Code"
        },
        {
          "name": "Implementation",
          "content": "Probably the most amazing thing about lottery scheduling is the simplicity of its implementation. All you need is a good random number generator to pick the winning ticket, a data structure to track the processes of the system (e.g., a list), and the total number of tickets.\n\n\nLet's assume we keep the processes in a list. Here is an example comprised of three processes, A, B, and C, each with some number of tickets.\n\n\n\n\n![Diagram showing a linked list of three processes: Job:A (Tix:100), Job:B (Tix:50), and Job:C (Tix:250). The list starts at 'head' and ends at 'NULL'.](images/image_0021.jpeg)\n\n\ngraph LR\n    head --> A((Job:A\nTix:100))\n    A --> B((Job:B\nTix:50))\n    B --> C((Job:C\nTix:250))\n    C --> NULL\n\nDiagram showing a linked list of three processes: Job:A (Tix:100), Job:B (Tix:50), and Job:C (Tix:250). The list starts at 'head' and ends at 'NULL'.\n\n\nTo make a scheduling decision, we first have to pick a random number (the winner) from the total number of tickets (\n   \n    400\n   \n   )\n   \n    2\n   \n   . Let's say we pick the number 300. Then, we simply traverse the list, with a simple counter used to help us find the winner (Figure 9.1).\n\n\nThe code walks the process list, adding each ticket value to\n   \n    counter\n   \n   until the value exceeds\n   \n    winner\n   \n   . Once that is the case, the current list element is the winner. With our example of the winning ticket being 300, the following takes place. First,\n   \n    counter\n   \n   is incremented to 100 to account for A's tickets; because 100 is less than 300, the loop continues. Then\n   \n    counter\n   \n   would be updated to 150 (B's tickets), still less than 300 and thus again we continue. Finally,\n   \n    counter\n   \n   is updated to 400 (clearly greater than 300), and thus we break out of the loop with\n   \n    current\n   \n   pointing at C (the winner).\n\n\n2\n   \n   Surprisingly, as pointed out by Björn Lindberg, this can be challenging to do correctly; for more details, see\n   http://stackoverflow.com/questions/2509679/how-to-generate-a-random-number-from-within-a-range\n   .\n\n\n\n\n![Figure 9.2: Lottery Fairness Study. A line graph showing Fairness (Y-axis, 0.0 to 1.0) versus Job Length (X-axis, logarithmic scale from 1 to 1000). The curve starts at approximately (1, 0.5) and increases, leveling off near 1.0 as Job Length increases.](images/image_0022.jpeg)\n\n\nJob Length | Fairness\n1 | 0.50\n2 | 0.65\n5 | 0.75\n10 | 0.80\n20 | 0.85\n50 | 0.90\n100 | 0.95\n200 | 0.98\n500 | 0.99\n1000 | 1.00\n\n\nFigure 9.2: Lottery Fairness Study. A line graph showing Fairness (Y-axis, 0.0 to 1.0) versus Job Length (X-axis, logarithmic scale from 1 to 1000). The curve starts at approximately (1, 0.5) and increases, leveling off near 1.0 as Job Length increases.\n\n\nFigure 9.2:\n   **Lottery Fairness Study**\n\n\nTo make this process most efficient, it might generally be best to organize the list in sorted order, from the highest number of tickets to the lowest. The ordering does not affect the correctness of the algorithm; however, it does ensure in general that the fewest number of list iterations are taken, especially if there are a few processes that possess most of the tickets."
        },
        {
          "name": "An Example",
          "content": "To make the dynamics of lottery scheduling more understandable, we now perform a brief study of the completion time of two jobs competing against one another, each with the same number of tickets (100) and same run time (\n   \n    R\n   \n   , which we will vary).\n\n\nIn this scenario, we'd like for each job to finish at roughly the same time, but due to the randomness of lottery scheduling, sometimes one job finishes before the other. To quantify this difference, we define a simple\n   **fairness metric**\n   ,\n   \n    F\n   \n   which is simply the time the first job completes divided by the time that the second job completes. For example, if\n   \n    R = 10\n   \n   , and the first job finishes at time 10 (and the second job at 20),\n   \n    F = \\frac{10}{20} = 0.5\n   \n   . When both jobs finish at nearly the same time,\n   \n    F\n   \n   will be quite close to 1. In this scenario, that is our goal: a perfectly fair scheduler would achieve\n   \n    F = 1\n   \n   .\n\n\nFigure 9.2 plots the average fairness as the length of the two jobs (\n   \n    R\n   \n   ) is varied from 1 to 1000 over thirty trials (results are generated via the simulator provided at the end of the chapter). As you can see from the graph, when the job length is not very long, average fairness can be quite low. Only as the jobs run for a significant number of time slices does the lottery scheduler approach the desired fair outcome."
        },
        {
          "name": "How To Assign Tickets?",
          "content": "One problem we have not addressed with lottery scheduling is: how to assign tickets to jobs? This problem is a tough one, because of course how the system behaves is strongly dependent on how tickets are allocated. One approach is to assume that the users know best; in such a case, each user is handed some number of tickets, and a user can allocate tickets to any jobs they run as desired. However, this solution is a non-solution: it really doesn't tell you what to do. Thus, given a set of jobs, the \"ticket-assignment problem\" remains open."
        },
        {
          "name": "Stride Scheduling",
          "content": "You might also be wondering: why use randomness at all? As we saw above, while randomness gets us a simple (and approximately correct) scheduler, it occasionally will not deliver the exact right proportions, especially over short time scales. For this reason, Waldspurger invented\n   **stride scheduling**\n   , a deterministic fair-share scheduler [W95].\n\n\nStride scheduling is also straightforward. Each job in the system has a stride, which is inverse in proportion to the number of tickets it has. In our example above, with jobs A, B, and C, with 100, 50, and 250 tickets, respectively, we can compute the stride of each by dividing some large number by the number of tickets each process has been assigned. For example, if we divide 10,000 by each of those ticket values, we obtain the following stride values for A, B, and C: 100, 200, and 40. We call this value the\n   **stride**\n   of each process; every time a process runs, we will increment a counter for it (called its\n   **pass**\n   value) by its stride to track its global progress.\n\n\nThe scheduler then uses the stride and pass to determine which process should run next. The basic idea is simple: at any given time, pick the process to run that has the lowest pass value so far; when you run a process, increment its pass counter by its stride. A pseudocode implementation is provided by Waldspurger [W95]:\n\n\ncurr = remove_min(queue);    // pick client with min pass\nschedule(curr);              // run for quantum\ncurr->pass += curr->stride; // update pass using stride\ninsert(queue, curr);         // return curr to queue\nIn our example, we start with three processes (A, B, and C), with stride values of 100, 200, and 40, and all with pass values initially at 0. Thus, at first, any of the processes might run, as their pass values are equally low. Assume we pick A (arbitrarily; any of the processes with equal low pass values can be chosen). A runs; when finished with the time slice, we update its pass value to 100. Then we run B, whose pass value is then set to 200. Finally, we run C, whose pass value is incremented to 40. At this point, the algorithm will pick the lowest pass value, which is C's, and run it, updating its pass to 80 (C's stride is 40, as you recall). Then C will\n\n\n\nPass(A)\n      \n      (stride=100) | Pass(B)\n      \n      (stride=200) | Pass(C)\n      \n      (stride=40) | Who Runs?\n0 | 0 | 0 | A\n100 | 0 | 0 | B\n100 | 200 | 0 | C\n100 | 200 | 40 | C\n100 | 200 | 80 | C\n100 | 200 | 120 | A\n200 | 200 | 120 | C\n200 | 200 | 160 | C\n200 | 200 | 200 | ...\n\n\n**Stride Scheduling: A Trace**\nrun again (still the lowest pass value), raising its pass to 120. A will run now, updating its pass to 200 (now equal to B's). Then C will run twice more, updating its pass to 160 then 200. At this point, all pass values are equal again, and the process will repeat, ad infinitum. Figure 9.3 traces the behavior of the scheduler over time.\n\n\nAs we can see from the figure, C ran five times, A twice, and B just once, exactly in proportion to their ticket values of 250, 100, and 50. Lottery scheduling achieves the proportions probabilistically over time; stride scheduling gets them exactly right at the end of each scheduling cycle.\n\n\nSo you might be wondering: given the precision of stride scheduling, why use lottery scheduling at all? Well, lottery scheduling has one nice property that stride scheduling does not: no global state. Imagine a new job enters in the middle of our stride scheduling example above; what should its pass value be? Should it be set to 0? If so, it will monopolize the CPU. With lottery scheduling, there is no global state per process; we simply add a new process with whatever tickets it has, update the single global variable to track how many total tickets we have, and go from there. In this way, lottery makes it much easier to incorporate new processes in a sensible manner."
        },
        {
          "name": "The Linux Completely Fair Scheduler (CFS)",
          "content": "Despite these earlier works in fair-share scheduling, the current Linux approach achieves similar goals in an alternate manner. The scheduler, entitled the\n   **Completely Fair Scheduler**\n   (or\n   **CFS**\n   ) [J09], implements fair-share scheduling, but does so in a highly efficient and scalable manner.\n\n\nTo achieve its efficiency goals, CFS aims to spend very little time making scheduling decisions, through both its inherent design and its clever use of data structures well-suited to the task. Recent studies have shown that scheduler efficiency is surprisingly important; specifically, in a study of Google datacenters, Kanav et al. show that even after aggressive optimization, scheduling uses about 5% of overall datacenter CPU time [K+15]. Reducing that overhead as much as possible is thus a key goal in modern scheduler architecture.\n\n\n\n\n![Figure 9.4: CFS Simple Example. A bar chart showing the execution of four processes (A, B, C, D) over time. The x-axis is labeled 'Time' and ranges from 0 to 250. The chart shows a repeating pattern of two time slices: a black bar for process A and a grey bar for process B, followed by a black bar for process C and a grey bar for process D. This pattern repeats, with process A running from 0 to 25, B from 25 to 50, C from 50 to 75, D from 75 to 100, A from 100 to 125, B from 125 to 150, C from 150 to 175, D from 175 to 200, and B from 200 to 225.](images/image_0023.jpeg)\n\n\nFigure 9.4: CFS Simple Example. A bar chart showing the execution of four processes (A, B, C, D) over time. The x-axis is labeled 'Time' and ranges from 0 to 250. The chart shows a repeating pattern of two time slices: a black bar for process A and a grey bar for process B, followed by a black bar for process C and a grey bar for process D. This pattern repeats, with process A running from 0 to 25, B from 25 to 50, C from 50 to 75, D from 75 to 100, A from 100 to 125, B from 125 to 150, C from 150 to 175, D from 175 to 200, and B from 200 to 225.\n\n\nFigure 9.4: CFS Simple Example\n\n\n\n\n**Basic Operation**\n\n\nWhereas most schedulers are based around the concept of a fixed time slice, CFS operates a bit differently. Its goal is simple: to fairly divide a CPU evenly among all competing processes. It does so through a simple counting-based technique known as\n   **virtual runtime**\n   (\n   **vruntime**\n   ).\n\n\nAs each process runs, it accumulates\n   \n    vruntime\n   \n   . In the most basic case, each process's\n   \n    vruntime\n   \n   increases at the same rate, in proportion with physical (real) time. When a scheduling decision occurs, CFS will pick the process with the\n   *lowest*\n\n    vruntime\n   \n   to run next.\n\n\nThis raises a question: how does the scheduler know when to stop the currently running process, and run the next one? The tension here is clear: if CFS switches too often, fairness is increased, as CFS will ensure that each process receives its share of CPU even over miniscule time windows, but at the cost of performance (too much context switching); if CFS switches less often, performance is increased (reduced context switching), but at the cost of near-term fairness.\n\n\nCFS manages this tension through various control parameters. The first is\n   \n    sched_latency\n   \n   . CFS uses this value to determine how long one process should run before considering a switch (effectively determining its time slice but in a dynamic fashion). A typical\n   \n    sched_latency\n   \n   value is 48 (milliseconds); CFS divides this value by the number (\n   \n    n\n   \n   ) of processes running on the CPU to determine the time slice for a process, and thus ensures that over this period of time, CFS will be completely fair.\n\n\nFor example, if there are\n   \n    n = 4\n   \n   processes running, CFS divides the value of\n   \n    sched_latency\n   \n   by\n   \n    n\n   \n   to arrive at a per-process time slice of 12 ms. CFS then schedules the first job and runs it until it has used 12 ms of (virtual) runtime, and then checks to see if there is a job with lower\n   \n    vruntime\n   \n   to run instead. In this case, there is, and CFS would switch to one of the three other jobs, and so forth. Figure 9.4 shows an example where the four jobs (A, B, C, D) each run for two time slices in this fashion; two of them (C, D) then complete, leaving just two remaining, which then each run for 24 ms in round-robin fashion.\n\n\nBut what if there are “too many” processes running? Wouldn’t that lead to too small of a time slice, and thus too many context switches? Good question! And the answer is yes.\n\n\nTo address this issue, CFS adds another parameter,\n   \n    min_granularity\n   \n   , which is usually set to a value like 6 ms. CFS will never set the time slice\n\n\nof a process to less than this value, ensuring that not too much time is spent in scheduling overhead.\n\n\nFor example, if there are ten processes running, our original calculation would divide\n   \n    sched_latency\n   \n   by ten to determine the time slice (result: 4.8 ms). However, because of\n   \n    min_granularity\n   \n   , CFS will set the time slice of each process to 6 ms instead. Although CFS won't (quite) be perfectly fair over the target scheduling latency (\n   \n    sched_latency\n   \n   ) of 48 ms, it will be close, while still achieving high CPU efficiency.\n\n\nNote that CFS utilizes a periodic timer interrupt, which means it can only make decisions at fixed time intervals. This interrupt goes off frequently (e.g., every 1 ms), giving CFS a chance to wake up and determine if the current job has reached the end of its run. If a job has a time slice that is not a perfect multiple of the timer interrupt interval, that is OK; CFS tracks\n   \n    vruntime\n   \n   precisely, which means that over the long haul, it will eventually approximate ideal sharing of the CPU.\n\n\n\n\n**Weighting (Niceness)**\n\n\nCFS also enables controls over process priority, enabling users or administrators to give some processes a higher share of the CPU. It does this not with tickets, but through a classic UNIX mechanism known as the\n   *nice*\n   level of a process. The nice parameter can be set anywhere from -20 to +19 for a process, with a default of 0. Positive nice values imply\n   *lower*\n   priority and negative values imply\n   *higher*\n   priority; when you're too nice, you just don't get as much (scheduling) attention, alas.\n\n\nCFS maps the nice value of each process to a weight, as shown here:\n\n\nstatic const int prio_to_weight[40] = {\n    /* -20 */ 88761, 71755, 56483, 46273, 36291,\n    /* -15 */ 29154, 23254, 18705, 14949, 11916,\n    /* -10 */ 9548, 7620, 6100, 4904, 3906,\n    /*  -5 */ 3121, 2501, 1991, 1586, 1277,\n    /*   0 */ 1024, 820, 655, 526, 423,\n    /*   5 */ 335, 272, 215, 172, 137,\n    /*  10 */ 110, 87, 70, 56, 45,\n    /*  15 */ 36, 29, 23, 18, 15,\n};\nThese weights allow us to compute the effective time slice of each process (as we did before), but now accounting for their priority differences. The formula used to do so is as follows, assuming\n   \n    n\n   \n   processes:\n\n\n\\text{time\\_slice}_k = \\frac{\\text{weight}_k}{\\sum_{i=0}^{n-1} \\text{weight}_i} \\cdot \\text{sched\\_latency} \\quad (9.1)\n\n\nLet's do an example to see how this works. Assume there are two jobs, A and B. A, because it's our most precious job, is given a higher pri-\n\n\nority by assigning it a nice value of -5; B, because we hates it\n   \n    3\n   \n   , just has the default priority (nice value equal to 0). This means\n   \n    \\text{weight}_A\n   \n   (from the table) is 3121, whereas\n   \n    \\text{weight}_B\n   \n   is 1024. If you then compute the time slice of each job, you'll find that A's time slice is about\n   \n    \\frac{3}{4}\n   \n   of\n   \n    \\text{sched\\_latency}\n   \n   (hence, 36 ms), and B's about\n   \n    \\frac{1}{4}\n   \n   (hence, 12 ms).\n\n\nIn addition to generalizing the time slice calculation, the way CFS calculates\n   \n    \\text{vruntime}\n   \n   must also be adapted. Here is the new formula, which takes the actual run time that process\n   \n    i\n   \n   has accrued (\n   \n    \\text{runtime}_i\n   \n   ) and scales it inversely by the weight of the process, by dividing the default weight of 1024 (\n   \n    \\text{weight}_0\n   \n   ) by its weight,\n   \n    \\text{weight}_i\n   \n   . In our running example, A's\n   \n    \\text{vruntime}\n   \n   will accumulate at one-third the rate of B's.\n\n\n\\text{vruntime}_i = \\text{vruntime}_i + \\frac{\\text{weight}_0}{\\text{weight}_i} \\cdot \\text{runtime}_i \\quad (9.2)\n\n\nOne smart aspect of the construction of the table of weights above is that the table preserves CPU proportionality ratios when the difference in nice values is constant. For example, if process A instead had a nice value of 5 (not -5), and process B had a nice value of 10 (not 0), CFS would schedule them in exactly the same manner as before. Run through the math yourself to see why.\n\n\n\n\n**Using Red-Black Trees**\n\n\nOne major focus of CFS is efficiency, as stated above. For a scheduler, there are many facets of efficiency, but one of them is as simple as this: when the scheduler has to find the next job to run, it should do so as quickly as possible. Simple data structures like lists don't scale: modern systems sometimes are comprised of 1000s of processes, and thus searching through a long-list every so many milliseconds is wasteful.\n\n\nCFS addresses this by keeping processes in a\n   **red-black tree**\n   [B72]. A red-black tree is one of many types of balanced trees; in contrast to a simple binary tree (which can degenerate to list-like performance under worst-case insertion patterns), balanced trees do a little extra work to maintain low depths, and thus ensure that operations are logarithmic (and not linear) in time.\n\n\nCFS does not keep\n   *all*\n   processes in this structure; rather, only running (or runnable) processes are kept therein. If a process goes to sleep (say, waiting on an I/O to complete, or for a network packet to arrive), it is removed from the tree and kept track of elsewhere.\n\n\nLet's look at an example to make this more clear. Assume there are ten jobs, and that they have the following values of\n   \n    \\text{vruntime}\n   \n   : 1, 5, 9, 10, 14, 18, 17, 21, 22, and 24. If we kept these jobs in an ordered list, finding the next job to run would be simple: just remove the first element. However,\n\n\n3\n   \n   Yes, yes, we are using bad grammar here on purpose, please don't send in a bug fix. Why? Well, just a most mild of references to the Lord of the Rings, and our favorite anti-hero Gollum, nothing to get too excited about.\n\n\n\n\n![Figure 9.5: CFS Red-Black Tree. A binary search tree with nodes containing values 14, 9, 18, 1, 10, 17, 22, 5, 21, and 24. Node 14 is the root. Node 9 is the left child of 14, and node 18 is the right child. Node 1 is the left child of 9, and node 10 is the right child. Node 17 is the left child of 18, and node 22 is the right child. Node 5 is the left child of 1, node 21 is the left child of 22, and node 24 is the right child of 22. Nodes 1, 5, 10, 17, 21, and 24 are black, while nodes 9, 14, 18, and 22 are red.](images/image_0024.jpeg)\n\n\nFigure 9.5: CFS Red-Black Tree. A binary search tree with nodes containing values 14, 9, 18, 1, 10, 17, 22, 5, 21, and 24. Node 14 is the root. Node 9 is the left child of 14, and node 18 is the right child. Node 1 is the left child of 9, and node 10 is the right child. Node 17 is the left child of 18, and node 22 is the right child. Node 5 is the left child of 1, node 21 is the left child of 22, and node 24 is the right child of 22. Nodes 1, 5, 10, 17, 21, and 24 are black, while nodes 9, 14, 18, and 22 are red.\n\n\nFigure 9.5: CFS Red-Black Tree\n\n\nwhen placing that job back into the list (in order), we would have to scan the list, looking for the right spot to insert it, an\n   \n    O(n)\n   \n   operation. Any search is also quite inefficient, also taking linear time on average.\n\n\nKeeping the same values in a red-black tree makes most operations more efficient, as depicted in Figure 9.5. Processes are ordered in the tree by\n   \n    vruntime\n   \n   , and most operations (such as insertion and deletion) are logarithmic in time, i.e.,\n   \n    O(\\log n)\n   \n   . When\n   \n    n\n   \n   is in the thousands, logarithmic is noticeably more efficient than linear.\n\n\n\n\n**Dealing With I/O And Sleeping Processes**\n\n\nOne problem with picking the lowest\n   \n    vruntime\n   \n   to run next arises with jobs that have gone to sleep for a long period of time. Imagine two processes, A and B, one of which (A) runs continuously, and the other (B) which has gone to sleep for a long period of time (say, 10 seconds). When B wakes up, its\n   \n    vruntime\n   \n   will be 10 seconds behind A's, and thus (if we're not careful), B will now monopolize the CPU for the next 10 seconds while it catches up, effectively starving A.\n\n\nCFS handles this case by altering the\n   \n    vruntime\n   \n   of a job when it wakes up. Specifically, CFS sets the\n   \n    vruntime\n   \n   of that job to the minimum value found in the tree (remember, the tree only contains running jobs) [B+18]. In this way, CFS avoids starvation, but not without a cost: jobs that sleep for short periods of time frequently do not ever get their fair share of the CPU [AC97].\n\n\n\n\n**Other CFS Fun**\n\n\nCFS has many other features, too many to discuss at this point in the book. It includes numerous heuristics to improve cache performance, has strategies for handling multiple CPUs effectively (as discussed later in the book), can schedule across large groups of processes (instead of treating\n\n\n**TIP: USE EFFICIENT DATA STRUCTURES WHEN APPROPRIATE**\nIn many cases, a list will do. In many cases, it will not. Knowing which data structure to use when is a hallmark of good engineering. In the case discussed herein, simple lists found in earlier schedulers simply do not work well on modern systems, particular in the heavily loaded servers found in datacenters. Such systems contain thousands of active processes; searching through a long list to find the next job to run on each core every few milliseconds would waste precious CPU cycles. A better structure was needed, and CFS provided one by adding an excellent implementation of a red-black tree. More generally, when picking a data structure for a system you are building, carefully consider its access patterns and its frequency of usage; by understanding these, you will be able to implement the right structure for the task at hand.\n\n\neach process as an independent entity), and many other interesting features. Read recent research, starting with Bouron [B+18], to learn more."
        }
      ]
    },
    {
      "name": "Multiprocessor Scheduling (Advanced)",
      "sections": [
        {
          "name": "Background: Multiprocessor Architecture",
          "content": "To understand the new issues surrounding multiprocessor scheduling, we have to understand a new and fundamental difference between single-CPU hardware and multi-CPU hardware. This difference centers around the use of hardware\n   **caches**\n   (e.g., Figure 10.1), and exactly how data is shared across multiple processors. We now discuss this issue further, at a high level. Details are available elsewhere [CSG99], in particular in an upper-level or perhaps graduate computer architecture course.\n\n\nIn a system with a single CPU, there are a hierarchy of\n   **hardware caches**\n   that in general help the processor run programs faster. Caches are small, fast memories that (in general) hold copies of\n   *popular*\n   data that is found in the main memory of the system. Main memory, in contrast, holds\n   *all*\n   of the data, but access to this larger memory is slower. By keeping frequently accessed data in a cache, the system can make the large, slow memory appear to be a fast one.\n\n\n\n\n![Diagram showing two CPUs, each with its own cache, connected to a shared memory via a bus.](images/image_0025.jpeg)\n\n\nThe diagram illustrates a multiprocessor system with two CPUs. Each CPU is represented by a box divided into two sections: the top section is labeled 'CPU' and the bottom section is labeled 'Cache'. Both CPU boxes are connected to a central 'Memory' box by a horizontal line labeled 'Bus'.\n\n\nDiagram showing two CPUs, each with its own cache, connected to a shared memory via a bus.\n\n\nFigure 10.2:\n   **Two CPUs With Caches Sharing Memory**\n\n\nAs an example, consider a program that issues an explicit load instruction to fetch a value from memory, and a simple system with only a single CPU; the CPU has a small cache (say 64 KB) and a large main memory. The first time a program issues this load, the data resides in main memory, and thus takes a long time to fetch (perhaps in the tens of nanoseconds, or even hundreds). The processor, anticipating that the data may be reused, puts a copy of the loaded data into the CPU cache. If the program later fetches this same data item again, the CPU first checks for it in the cache; if it finds it there, the data is fetched much more quickly (say, just a few nanoseconds), and thus the program runs faster.\n\n\nCaches are thus based on the notion of\n   **locality**\n   , of which there are two kinds:\n   **temporal locality**\n   and\n   **spatial locality**\n   . The idea behind temporal locality is that when a piece of data is accessed, it is likely to be accessed again in the near future; imagine variables or even instructions themselves being accessed over and over again in a loop. The idea behind spatial locality is that if a program accesses a data item at address\n   \n    x\n   \n   , it is likely to access data items near\n   \n    x\n   \n   as well; here, think of a program streaming through an array, or instructions being executed one after the other. Because locality of these types exist in many programs, hardware systems can make good guesses about which data to put in a cache and thus work well.\n\n\nNow for the tricky part: what happens when you have multiple processors in a single system, with a single shared main memory, as we see in Figure 10.2?\n\n\nAs it turns out, caching with multiple CPUs is much more complicated. Imagine, for example, that a program running on CPU 1 reads a data item (with value\n   \n    D\n   \n   ) at address\n   \n    A\n   \n   ; because the data is not in the cache on CPU 1, the system fetches it from main memory, and gets the\n\n\nvalue\n   \n    D\n   \n   . The program then modifies the value at address\n   \n    A\n   \n   , just updating its cache with the new value\n   \n    D'\n   \n   ; writing the data through all the way to main memory is slow, so the system will (usually) do that later. Then assume the OS decides to stop running the program and move it to CPU 2. The program then re-reads the value at address\n   \n    A\n   \n   ; there is no such data in CPU 2's cache, and thus the system fetches the value from main memory, and gets the old value\n   \n    D\n   \n   instead of the correct value\n   \n    D'\n   \n   . Oops!\n\n\nThis general problem is called the problem of\n   **cache coherence**\n   , and there is a vast research literature that describes many different subtleties involved with solving the problem [SHW11]. Here, we will skip all of the nuance and make some major points; take a computer architecture class (or three) to learn more.\n\n\nThe basic solution is provided by the hardware: by monitoring memory accesses, hardware can ensure that basically the “right thing” happens and that the view of a single shared memory is preserved. One way to do this on a bus-based system (as described above) is to use an old technique known as\n   **bus snooping**\n   [G83]; each cache pays attention to memory updates by observing the bus that connects them to main memory. When a CPU then sees an update for a data item it holds in its cache, it will notice the change and either\n   **invalidate**\n   its copy (i.e., remove it from its own cache) or\n   **update**\n   it (i.e., put the new value into its cache too). Write-back caches, as hinted at above, make this more complicated (because the write to main memory isn't visible until later), but you can imagine how the basic scheme might work."
        },
        {
          "name": "Don’t Forget Synchronization",
          "content": "Given that the caches do all of this work to provide coherence, do programs (or the OS itself) have to worry about anything when they access shared data? The answer, unfortunately, is yes, and is documented in great detail in the second piece of this book on the topic of concurrency. While we won't get into the details here, we'll sketch/review some of the basic ideas here (assuming you're familiar with concurrency).\n\n\nWhen accessing (and in particular, updating) shared data items or structures across CPUs, mutual exclusion primitives (such as locks) should likely be used to guarantee correctness (other approaches, such as building\n   **lock-free**\n   data structures, are complex and only used on occasion; see the chapter on deadlock in the piece on concurrency for details). For example, assume we have a shared queue being accessed on multiple CPUs concurrently. Without locks, adding or removing elements from the queue concurrently will not work as expected, even with the underlying coherence protocols; one needs locks to atomically update the data structure to its new state.\n\n\nTo make this more concrete, imagine this code sequence, which is used to remove an element from a shared linked list, as we see in Figure 10.3. Imagine if threads on two CPUs enter this routine at the same time. If\n\n\n1  typedef struct __Node_t {\n2      int          value;\n3      struct __Node_t *next;\n4  } Node_t;\n5\n6  int List_Pop() {\n7      Node_t *tmp = head;           // remember old head\n8      int value  = head->value;    // ... and its value\n9      head       = head->next;     // advance to next\n10     free(tmp);                    // free old head\n11     return value;                  // return value @head\n12 }\n\nFigure 10.3:\n   **Simple List Delete Code**\n\n\nThread 1 executes the first line, it will have the current value of\n   \n    head\n   \n   stored in its\n   \n    tmp\n   \n   variable; if Thread 2 then executes the first line as well, it also will have the same value of\n   \n    head\n   \n   stored in its own private\n   \n    tmp\n   \n   variable (\n   \n    tmp\n   \n   is allocated on the stack, and thus each thread will have its own private storage for it). Thus, instead of each thread removing an element from the head of the list, each thread will try to remove the same head element, leading to all sorts of problems (such as an attempted double free of the head element at Line 10, as well as potentially returning the same data value twice).\n\n\nThe solution, of course, is to make such routines correct via\n   **locking**\n   . In this case, allocating a simple mutex (e.g.,\n   \n    pthread_mutex_t m;\n   \n   ) and then adding a\n   \n    lock(&m)\n   \n   at the beginning of the routine and an\n   \n    unlock(&m)\n   \n   at the end will solve the problem, ensuring that the code will execute as desired. Unfortunately, as we will see, such an approach is not without problems, in particular with regards to performance. Specifically, as the number of CPUs grows, access to a synchronized shared data structure becomes quite slow."
        },
        {
          "name": "One Final Issue: Cache Affinity",
          "content": "One final issue arises in building a multiprocessor cache scheduler, known as\n   **cache affinity**\n   [TTG95]. This notion is simple: a process, when run on a particular CPU, builds up a fair bit of state in the caches (and TLBs) of the CPU. The next time the process runs, it is often advantageous to run it on the same CPU, as it will run faster if some of its state is already present in the caches on that CPU. If, instead, one runs a process on a different CPU each time, the performance of the process will be worse, as it will have to reload the state each time it runs (note it will run correctly on a different CPU thanks to the cache coherence protocols of the hardware). Thus, a multiprocessor scheduler should consider cache affinity when making its scheduling decisions, perhaps preferring to keep a process on the same CPU if at all possible."
        },
        {
          "name": "Single-Queue Scheduling",
          "content": "With this background in place, we now discuss how to build a scheduler for a multiprocessor system. The most basic approach is to simply reuse the basic framework for single processor scheduling, by putting all jobs that need to be scheduled into a single queue; we call this\n   **single-queue multiprocessor scheduling**\n   or\n   **SQMS**\n   for short. This approach has the advantage of simplicity; it does not require much work to take an existing policy that picks the best job to run next and adapt it to work on more than one CPU (where it might pick the best two jobs to run, if there are two CPUs, for example).\n\n\nHowever, SQMS has obvious shortcomings. The first problem is a lack of\n   **scalability**\n   . To ensure the scheduler works correctly on multiple CPUs, the developers will have inserted some form of\n   **locking**\n   into the code, as described above. Locks ensure that when SQMS code accesses the single queue (say, to find the next job to run), the proper outcome arises.\n\n\nLocks, unfortunately, can greatly reduce performance, particularly as the number of CPUs in the systems grows [A90]. As contention for such a single lock increases, the system spends more and more time in lock overhead and less time doing the work the system should be doing (note: it would be great to include a real measurement of this in here someday).\n\n\nThe second main problem with SQMS is cache affinity. For example, let us assume we have five jobs to run (\n   \n    A, B, C, D, E\n   \n   ) and four processors. Our scheduling queue thus looks like this:\n\n\n\n\n![A linear queue diagram showing nodes A, B, C, D, and E connected by arrows, ending with NULL.](images/image_0026.jpeg)\n\n\nQueue → A → B → C → D → E → NULL\n\n\nA linear queue diagram showing nodes A, B, C, D, and E connected by arrows, ending with NULL.\n\n\nOver time, assuming each job runs for a time slice and then another job is chosen, here is a possible job schedule across CPUs:\n\n\n\nCPU 0 | A | E | D | C | B | ... (repeat) ...\nCPU 1 | B | A | E | D | C | ... (repeat) ...\nCPU 2 | C | B | A | E | D | ... (repeat) ...\nCPU 3 | D | C | B | A | E | ... (repeat) ...\n\n\nBecause each CPU simply picks the next job to run from the globally-shared queue, each job ends up bouncing around from CPU to CPU, thus doing exactly the opposite of what would make sense from the standpoint of cache affinity.\n\n\nTo handle this problem, most SQMS schedulers include some kind of affinity mechanism to try to make it more likely that process will continue\n\n\nto run on the same CPU if possible. Specifically, one might provide affinity for some jobs, but move others around to balance load. For example, imagine the same five jobs scheduled as follows:\n\n\n\nCPU 0 | A | E | A | A | A | ... (repeat) ...\nCPU 1 | B | B | E | B | B | ... (repeat) ...\nCPU 2 | C | C | C | E | C | ... (repeat) ...\nCPU 3 | D | D | D | D | E | ... (repeat) ...\n\n\nIn this arrangement, jobs\n   *A*\n   through\n   *D*\n   are not moved across processors, with only job\n   *E*\n**migrating**\n   from CPU to CPU, thus preserving affinity for most. You could then decide to migrate a different job the next time through, thus achieving some kind of affinity fairness as well. Implementing such a scheme, however, can be complex.\n\n\nThus, we can see the SQMS approach has its strengths and weaknesses. It is straightforward to implement given an existing single-CPU scheduler, which by definition has only a single queue. However, it does not scale well (due to synchronization overheads), and it does not readily preserve cache affinity."
        },
        {
          "name": "Multi-Queue Scheduling",
          "content": "Because of the problems caused in single-queue schedulers, some systems opt for multiple queues, e.g., one per CPU. We call this approach\n   **multi-queue multiprocessor scheduling**\n   (or MQMS).\n\n\nIn MQMS, our basic scheduling framework consists of multiple scheduling queues. Each queue will likely follow a particular scheduling discipline, such as round robin, though of course any algorithm can be used. When a job enters the system, it is placed on exactly one scheduling queue, according to some heuristic (e.g., random, or picking one with fewer jobs than others). Then it is scheduled essentially independently, thus avoiding the problems of information sharing and synchronization found in the single-queue approach.\n\n\nFor example, assume we have a system where there are just two CPUs (labeled CPU 0 and CPU 1), and some number of jobs enter the system:\n   *A*\n   ,\n   *B*\n   ,\n   *C*\n   , and\n   *D*\n   for example. Given that each CPU has a scheduling queue now, the OS has to decide into which queue to place each job. It might do something like this:\n\n\n\n\n![Diagram showing two separate scheduling queues, Q0 and Q1. Q0 contains jobs A and C, and Q1 contains jobs B and D. Each job is represented by a circle, and arrows indicate the sequence within each queue.](images/image_0027.jpeg)\n\n\ngraph LR\n    subgraph Q0\n        A((A)) --> C((C))\n    end\n    subgraph Q1\n        B((B)) --> D((D))\n    end\n    \nDiagram showing two separate scheduling queues, Q0 and Q1. Q0 contains jobs A and C, and Q1 contains jobs B and D. Each job is represented by a circle, and arrows indicate the sequence within each queue.\n\n\nDepending on the queue scheduling policy, each CPU now has two jobs to choose from when deciding what should run. For example, with\n   **round robin**\n   , the system might produce a schedule that looks like this:\n\n\n\nCPU 0 | A | A | C | C | A | A | C | C | A | A | C | C | ...\nCPU 1 | B | B | D | D | B | B | D | D | B | B | D | D | ...\n\n\nMQMS has a distinct advantage of SQMS in that it should be inherently more scalable. As the number of CPUs grows, so too does the number of queues, and thus lock and cache contention should not become a central problem. In addition, MQMS intrinsically provides cache affinity; jobs stay on the same CPU and thus reap the advantage of reusing cached contents therein.\n\n\nBut, if you've been paying attention, you might see that we have a new problem, which is fundamental in the multi-queue based approach:\n   **load imbalance**\n   . Let's assume we have the same set up as above (four jobs, two CPUs), but then one of the jobs (say\n   *C*\n   ) finishes. We now have the following scheduling queues:\n\n\n\n\n![](images/image_0028.jpeg)\n\n\nQ0 → A\n\n\nQ1 → B → D\n\n\nIf we then run our round-robin policy on each queue of the system, we will see this resulting schedule:\n\n\n\nCPU 0 | A | A | A | A | A | A | A | A | A | A | A | A | ...\nCPU 1 | B | B | D | D | B | B | D | D | B | B | D | D | ...\n\n\nAs you can see from this diagram,\n   *A*\n   gets twice as much CPU as\n   *B*\n   and\n   *D*\n   , which is not the desired outcome. Even worse, let's imagine that both\n   *A*\n   and\n   *C*\n   finish, leaving just jobs\n   *B*\n   and\n   *D*\n   in the system. The two scheduling queues, and resulting timeline, will look like this:\n\n\n\n\n![](images/image_0029.jpeg)\n\n\nQ0 →\n\n\nQ1 → B → D\n\n\n\nCPU 0 |  |  |  |  |  |  |  |  |  |  |  |  | ...\nCPU 1 | B | B | D | D | B | B | D | D | B | B | D | D | ...\n\n\nHow terrible – CPU 0 is idle! (\n   *insert dramatic and sinister music here*\n   )\n   \n\n   And thus our CPU usage timeline looks quite sad.\n\n\nSo what should a poor multi-queue multiprocessor scheduler do? How can we overcome the insidious problem of load imbalance and defeat the evil forces of ... the Decepticons\n   \n    1\n   \n   ? How do we stop asking questions that are hardly relevant to this otherwise wonderful book?\n\n\n\n\n**CRUX: HOW TO DEAL WITH LOAD IMBALANCE**\n\n\nHow should a multi-queue multiprocessor scheduler handle load imbalance, so as to better achieve its desired scheduling goals?\n\n\nThe obvious answer to this query is to move jobs around, a technique which we (once again) refer to as\n   **migration**\n   . By migrating a job from one CPU to another, true load balance can be achieved.\n\n\nLet's look at a couple of examples to add some clarity. Once again, we have a situation where one CPU is idle and the other has some jobs.\n\n\n\n\n![Diagram showing initial load imbalance. Q0 has an arrow pointing to the right (idle). Q1 has an arrow pointing to B, which points to D.](images/image_0030.jpeg)\n\n\nQ0 →      Q1 → B → D\n\n\nDiagram showing initial load imbalance. Q0 has an arrow pointing to the right (idle). Q1 has an arrow pointing to B, which points to D.\n\n\nIn this case, the desired migration is easy to understand: the OS should simply move one of\n   *B*\n   or\n   *D*\n   to CPU 0. The result of this single job migration is evenly balanced load and everyone is happy.\n\n\nA more tricky case arises in our earlier example, where\n   *A*\n   was left alone on CPU 0 and\n   *B*\n   and\n   *D*\n   were alternating on CPU 1:\n\n\n\n\n![Diagram showing a more complex load imbalance. Q0 has an arrow pointing to A. Q1 has an arrow pointing to B, which points to D.](images/image_0031.jpeg)\n\n\nQ0 → A      Q1 → B → D\n\n\nDiagram showing a more complex load imbalance. Q0 has an arrow pointing to A. Q1 has an arrow pointing to B, which points to D.\n\n\nIn this case, a single migration does not solve the problem. What would you do in this case? The answer, alas, is continuous migration of one or more jobs. One possible solution is to keep switching jobs, as we see in the following timeline. In the figure, first\n   *A*\n   is alone on CPU 0, and\n   *B*\n   and\n   *D*\n   alternate on CPU 1. After a few time slices,\n   *B*\n   is moved to compete with\n   *A*\n   on CPU 0, while\n   *D*\n   enjoys a few time slices alone on CPU 1. And thus load is balanced:\n\n\n\nCPU 0 | A | A | A | A | B | A | B | A | B | B | B | B | ...\nCPU 1 | B | D | B | D | D | D | D | D | A | D | A | D | ...\n\n\nOf course, many other possible migration patterns exist. But now for the tricky part: how should the system decide to enact such a migration?\n\n\n1\n   \n   Little known fact is that the home planet of Cybertron was destroyed by bad CPU scheduling decisions. And now let that be the first and last reference to Transformers in this book, for which we sincerely apologize.\n\n\nOne basic approach is to use a technique known as\n   **work stealing**\n   [FLR98]. With a work-stealing approach, a (source) queue that is low on jobs will occasionally peek at another (target) queue, to see how full it is. If the target queue is (notably) more full than the source queue, the source will “steal” one or more jobs from the target to help balance load.\n\n\nOf course, there is a natural tension in such an approach. If you look around at other queues too often, you will suffer from high overhead and have trouble scaling, which was the entire purpose of implementing the multiple queue scheduling in the first place! If, on the other hand, you don’t look at other queues very often, you are in danger of suffering from severe load imbalances. Finding the right threshold remains, as is common in system policy design, a black art."
        },
        {
          "name": "Linux Multiprocessor Schedulers",
          "content": "Interestingly, in the Linux community, no common solution has emerged to building a multiprocessor scheduler. Over time, three different schedulers arose: the O(1) scheduler, the Completely Fair Scheduler (CFS), and the BF Scheduler (BFS)\n   \n    2\n   \n   . See Meehan’s dissertation for an excellent overview of the strengths and weaknesses of said schedulers [M11]; here we just summarize a few of the basics.\n\n\nBoth O(1) and CFS use multiple queues, whereas BFS uses a single queue, showing that both approaches can be successful. Of course, there are many other details which separate these schedulers. For example, the O(1) scheduler is a priority-based scheduler (similar to the MLFQ discussed before), changing a process’s priority over time and then scheduling those with highest priority in order to meet various scheduling objectives; interactivity is a particular focus. CFS, in contrast, is a deterministic proportional-share approach (more like Stride scheduling, as discussed earlier). BFS, the only single-queue approach among the three, is also proportional-share, but based on a more complicated scheme known as Earliest Eligible Virtual Deadline First (EEVDF) [SA96]. Read more about these modern algorithms on your own; you should be able to understand how they work now!"
        }
      ]
    },
    {
      "name": "The Abstraction: Address Spaces",
      "sections": [
        {
          "name": "Early Systems",
          "content": "From the perspective of memory, early machines didn't provide much of an abstraction to users. Basically, the physical memory of the machine looked something like what you see in Figure 13.1 (page 2).\n\n\nThe OS was a set of routines (a library, really) that sat in memory (starting at physical address 0 in this example), and there would be one running program (a process) that currently sat in physical memory (starting at physical address 64k in this example) and used the rest of memory. There were few illusions here, and the user didn't expect much from the OS. Life was sure easy for OS developers in those days, wasn't it?"
        },
        {
          "name": "Multiprogramming and Time Sharing",
          "content": "After a time, because machines were expensive, people began to share machines more effectively. Thus the era of\n   **multiprogramming**\n   was born [DV66], in which multiple processes were ready to run at a given time, and the OS would switch between them, for example when one decided to perform an I/O. Doing so increased the effective\n   **utilization**\n   of the CPU. Such increases in\n   **efficiency**\n   were particularly important in those days where each machine cost hundreds of thousands or even millions of dollars (and you thought your Mac was expensive!).\n\n\nSoon enough, however, people began demanding more of machines, and the era of\n   **time sharing**\n   was born [S59, L60, M62, M83]. Specifically, many realized the limitations of batch computing, particularly on programmers themselves [CV65], who were tired of long (and hence ineffec-\n\n\n\n\n![Diagram of memory layout in the early days of operating systems.](images/image_0032.jpeg)\n\n\nThe diagram illustrates a vertical memory layout. At the top, a label '0KB' is positioned to the left of a vertical line. Below this, a shaded rectangular box is labeled 'Operating System (code, data, etc.)'. Below this box, a label '64KB' is positioned to the left of the vertical line. Below the 64KB label, a larger, unshaded rectangular box is labeled 'Current Program (code, data, etc.)'. At the bottom of the vertical line, a label 'max' is positioned to the left.\n\n\nDiagram of memory layout in the early days of operating systems.\n\n\nFigure 13.1:\n   **Operating Systems: The Early Days**\n\n\ntive) program-debug cycles. The notion of\n   **interactivity**\n   became important, as many users might be concurrently using a machine, each waiting for (or hoping for) a timely response from their currently-executing tasks.\n\n\nOne way to implement time sharing would be to run one process for a short while, giving it full access to all memory (Figure 13.1), then stop it, save all of its state to some kind of disk (including all of physical memory), load some other process's state, run it for a while, and thus implement some kind of crude sharing of the machine [M+63].\n\n\nUnfortunately, this approach has a big problem: it is way too slow, particularly as memory grows. While saving and restoring register-level state (the PC, general-purpose registers, etc.) is relatively fast, saving the entire contents of memory to disk is brutally non-performant. Thus, what we'd rather do is leave processes in memory while switching between them, allowing the OS to implement time sharing efficiently (as shown in Figure 13.2, page 3).\n\n\nIn the diagram, there are three processes (A, B, and C) and each of them have a small part of the 512KB physical memory carved out for them. Assuming a single CPU, the OS chooses to run one of the processes (say A), while the others (B and C) sit in the ready queue waiting to run.\n\n\nAs time sharing became more popular, you can probably guess that new demands were placed on the operating system. In particular, allowing multiple programs to reside concurrently in memory makes\n   **protection**\n   an important issue; you don't want a process to be able to read, or worse, write some other process's memory.\n\n\n\n\n![Diagram of memory layout for three processes sharing memory.](images/image_0033.jpeg)\n\n\nThe diagram shows a vertical stack of memory blocks. On the left, address ranges are listed: 0KB, 64KB, 128KB, 192KB, 256KB, 320KB, 384KB, 448KB, and 512KB. On the right, corresponding blocks are shown. The blocks are: Operating System (code, data, etc.) from 0KB to 64KB; a free block from 64KB to 128KB; Process C (code, data, etc.) from 128KB to 192KB; Process B (code, data, etc.) from 192KB to 256KB; a free block from 256KB to 320KB; Process A (code, data, etc.) from 320KB to 384KB; a free block from 384KB to 448KB; and a final free block from 448KB to 512KB.\n\n\n\n0KB | Operating System (code, data, etc.)\n64KB | (free)\n128KB | Process C (code, data, etc.)\n192KB | Process B (code, data, etc.)\n256KB | (free)\n320KB | Process A (code, data, etc.)\n384KB | (free)\n448KB | (free)\n512KB | \n\n\nDiagram of memory layout for three processes sharing memory.\n\n\n**Three Processes: Sharing Memory**"
        },
        {
          "name": "The Address Space",
          "content": "However, we have to keep those pesky users in mind, and doing so requires the OS to create an\n   **easy to use**\n   abstraction of physical memory. We call this abstraction the\n   **address space**\n   , and it is the running program's view of memory in the system. Understanding this fundamental OS abstraction of memory is key to understanding how memory is virtualized.\n\n\nThe address space of a process contains all of the memory state of the running program. For example, the\n   **code**\n   of the program (the instructions) have to live in memory somewhere, and thus they are in the address space. The program, while it is running, uses a\n   **stack**\n   to keep track of where it is in the function call chain as well as to allocate local variables and pass parameters and return values to and from routines. Finally, the\n   **heap**\n   is used for dynamically-allocated, user-managed memory, such as that you might receive from a call to\n   \n    malloc()\n   \n   in C or\n   \n    new\n   \n   in an object-oriented language such as C++ or Java. Of course, there are other things in there too (e.g., statically-initialized variables), but for now let us just assume those three components: code, stack, and heap.\n\n\nIn the example in Figure 13.3 (page 4), we have a tiny address space (only 16KB)\n   \n    1\n   \n   . The program code lives at the top of the address space\n\n\n1\n   \n   We will often use small examples like this because (a) it is a pain to represent a 32-bit address space and (b) the math is harder. We like simple math.\n\n\n\n\n![Diagram of an example address space showing Program Code, Heap, and Stack segments with their respective sizes and growth directions.](images/image_0034.jpeg)\n\n\nThe diagram illustrates an address space from 0KB at the top to 16KB at the bottom. It is divided into three main segments:\n\n\n  * **Program Code**\n     : Located at the top, from 0KB to 1KB. It is labeled \"the code segment: where instructions live\".\n  * **Heap**\n     : Located between 1KB and 15KB. It is labeled \"the heap segment: contains malloc'd data dynamic data structures (it grows positively)\". A downward arrow indicates its growth direction. The area below the heap is shaded with diagonal lines and labeled \"(free)\".\n  * **Stack**\n     : Located at the bottom, from 15KB to 16KB. It is labeled \"the stack segment: contains local variables, arguments to routines, return values, etc.\". An upward arrow indicates its growth direction (labeled \"(it grows negatively)\").\n\n\nDiagram of an example address space showing Program Code, Heap, and Stack segments with their respective sizes and growth directions.\n\n\nFigure 13.3: An Example Address Space\n\n\n(starting at 0 in this example, and is packed into the first 1K of the address space). Code is static (and thus easy to place in memory), so we can place it at the top of the address space and know that it won't need any more space as the program runs.\n\n\nNext, we have the two regions of the address space that may grow (and shrink) while the program runs. Those are the heap (at the top) and the stack (at the bottom). We place them like this because each wishes to be able to grow, and by putting them at opposite ends of the address space, we can allow such growth: they just have to grow in opposite directions. The heap thus starts just after the code (at 1KB) and grows downward (say when a user requests more memory via\n   \n    malloc()\n   \n   ); the stack starts at 16KB and grows upward (say when a user makes a procedure call). However, this placement of stack and heap is just a convention; you could arrange the address space in a different way if you'd like (as we'll see later, when multiple\n   **threads**\n   co-exist in an address space, no nice way to divide the address space like this works anymore, alas).\n\n\nOf course, when we describe the address space, what we are describing is the\n   **abstraction**\n   that the OS is providing to the running program. The program really isn't in memory at physical addresses 0 through 16KB; rather it is loaded at some arbitrary physical address(es). Examine processes A, B, and C in Figure 13.2; there you can see how each process is loaded into memory at a different address. And hence the problem:\n\n\n**THE CRUX: HOW TO VIRTUALIZE MEMORY**\nHow can the OS build this abstraction of a private, potentially large address space for multiple running processes (all sharing memory) on top of a single, physical memory?\n\n\nWhen the OS does this, we say the OS is\n   **virtualizing memory**\n   , because the running program thinks it is loaded into memory at a particular address (say 0) and has a potentially very large address space (say 32-bits or 64-bits); the reality is quite different.\n\n\nWhen, for example, process A in Figure 13.2 tries to perform a load at address 0 (which we will call a\n   **virtual address**\n   ), somehow the OS, in tandem with some hardware support, will have to make sure the load doesn't actually go to physical address 0 but rather to physical address 320KB (where A is loaded into memory). This is the key to virtualization of memory, which underlies every modern computer system in the world."
        },
        {
          "name": "Goals",
          "content": "Thus we arrive at the job of the OS in this set of notes: to virtualize memory. The OS will not only virtualize memory, though; it will do so with style. To make sure the OS does so, we need some goals to guide us. We have seen these goals before (think of the Introduction), and we'll see them again, but they are certainly worth repeating.\n\n\nOne major goal of a virtual memory (VM) system is\n   **transparency**\n\n    2\n   \n   . The OS should implement virtual memory in a way that is invisible to the running program. Thus, the program shouldn't be aware of the fact that memory is virtualized; rather, the program behaves as if it has its own private physical memory. Behind the scenes, the OS (and hardware) does all the work to multiplex memory among many different jobs, and hence implements the illusion.\n\n\nAnother goal of VM is\n   **efficiency**\n   . The OS should strive to make the virtualization as\n   **efficient**\n   as possible, both in terms of time (i.e., not making programs run much more slowly) and space (i.e., not using too much memory for structures needed to support virtualization). In implementing time-efficient virtualization, the OS will have to rely on hardware support, including hardware features such as TLBs (which we will learn about in due course).\n\n\nFinally, a third VM goal is\n   **protection**\n   . The OS should make sure to\n   **protect**\n   processes from one another as well as the OS itself from pro-\n\n\n2\n   \n   This usage of transparency is sometimes confusing; some students think that “being transparent” means keeping everything out in the open, i.e., what government should be like. Here, it means the opposite: that the illusion provided by the OS should not be visible to applications. Thus, in common usage, a transparent system is one that is hard to notice, not one that responds to requests as stipulated by the Freedom of Information Act.\n\n\n**TIP: THE PRINCIPLE OF ISOLATION**\nIsolation is a key principle in building reliable systems. If two entities are properly isolated from one another, this implies that one can fail without affecting the other. Operating systems strive to isolate processes from each other and in this way prevent one from harming the other. By using memory isolation, the OS further ensures that running programs cannot affect the operation of the underlying OS. Some modern OS's take isolation even further, by walling off pieces of the OS from other pieces of the OS. Such\n   **microkernels**\n   [BH70, R+89, S+03] thus may provide greater reliability than typical monolithic kernel designs.\n\n\ncesses. When one process performs a load, a store, or an instruction fetch, it should not be able to access or affect in any way the memory contents of any other process or the OS itself (that is, anything\n   *outside*\n   its address space). Protection thus enables us to deliver the property of\n   **isolation**\n   among processes; each process should be running in its own isolated cocoon, safe from the ravages of other faulty or even malicious processes.\n\n\nIn the next chapters, we'll focus our exploration on the basic\n   **mechanisms**\n   needed to virtualize memory, including hardware and operating systems support. We'll also investigate some of the more relevant\n   **policies**\n   that you'll encounter in operating systems, including how to manage free space and which pages to kick out of memory when you run low on space. In doing so, we'll build up your understanding of how a modern virtual memory system really works\n   \n    3\n   \n   ."
        }
      ]
    },
    {
      "name": "Interlude: Memory API",
      "sections": [
        {
          "name": "Types of Memory",
          "content": "In running a C program, there are two types of memory that are allocated. The first is called\n   **stack**\n   memory, and allocations and deallocations of it are managed\n   *implicitly*\n   by the compiler for you, the programmer; for this reason it is sometimes called\n   **automatic**\n   memory.\n\n\nDeclaring memory on the stack in C is easy. For example, let's say you need some space in a function\n   \n    func()\n   \n   for an integer, called\n   \n    x\n   \n   . To declare such a piece of memory, you just do something like this:\n\n\nvoid func() {\n    int x; // declares an integer on the stack\n    ...\n}\nThe compiler does the rest, making sure to make space on the stack when you call into\n   \n    func()\n   \n   . When you return from the function, the compiler deallocates the memory for you; thus, if you want some information to live beyond the call invocation, you had better not leave that information on the stack.\n\n\n1\n   \n   Indeed, we hope all chapters are! But this one is shorter and pointier, we think.\n\n\nIt is this need for long-lived memory that gets us to the second type of memory, called\n   **heap**\n   memory, where all allocations and deallocations are\n   *explicitly*\n   handled by you, the programmer. A heavy responsibility, no doubt! And certainly the cause of many bugs. But if you are careful and pay attention, you will use such interfaces correctly and without too much trouble. Here is an example of how one might allocate an integer on the heap:\n\n\nvoid func() {\n    int *x = (int *) malloc(sizeof(int));\n    ...\n}\nA couple of notes about this small code snippet. First, you might notice that both stack and heap allocation occur on this line: first the compiler knows to make room for a pointer to an integer when it sees your declaration of said pointer (\n   \n    int *x\n   \n   ); subsequently, when the program calls\n   \n    malloc()\n   \n   , it requests space for an integer on the heap; the routine returns the address of such an integer (upon success, or\n   \n    NULL\n   \n   on failure), which is then stored on the stack for use by the program.\n\n\nBecause of its explicit nature, and because of its more varied usage, heap memory presents more challenges to both users and systems. Thus, it is the focus of the remainder of our discussion."
        },
        {
          "name": "The malloc() Call",
          "content": "The\n   \n    malloc()\n   \n   call is quite simple: you pass it a size asking for some room on the heap, and it either succeeds and gives you back a pointer to the newly-allocated space, or fails and returns\n   \n    NULL\n   \n   .\n   \n    2\n\n\nThe manual page shows what you need to do to use\n   \n    malloc\n   \n   ; type\n   \n    man malloc\n   \n   at the command line and you will see:\n\n\n#include <stdlib.h>\n...\nvoid *malloc(size_t size);\nFrom this information, you can see that all you need to do is include the header file\n   \n    stdlib.h\n   \n   to use\n   \n    malloc\n   \n   . In fact, you don't really need to even do this, as the C library, which all C programs link with by default, has the code for\n   \n    malloc()\n   \n   inside of it; adding the header just lets the compiler check whether you are calling\n   \n    malloc()\n   \n   correctly (e.g., passing the right number of arguments to it, of the right type).\n\n\nThe single parameter\n   \n    malloc()\n   \n   takes is of type\n   \n    size_t\n   \n   which simply describes how many bytes you need. However, most programmers do not type in a number here directly (such as 10); indeed, it would be\n\n\n2\n   \n   Note that\n   \n    NULL\n   \n   in C isn't really anything special, usually just a macro for the value zero, e.g.,\n   \n    #define NULL 0\n   \n   or sometimes\n   \n    #define NULL (void *)0\n   \n   .\n\n\n\n\n**TIP: WHEN IN DOUBT, TRY IT OUT**\n\n\nIf you aren't sure how some routine or operator you are using behaves, there is no substitute for simply trying it out and making sure it behaves as you expect. While reading the manual pages or other documentation is useful, how it works in practice is what matters. Write some code and test it! That is no doubt the best way to make sure your code behaves as you desire. Indeed, that is what we did to double-check the things we were saying about\n   \n    sizeof()\n   \n   were actually true!\n\n\nconsidered poor form to do so. Instead, various routines and macros are utilized. For example, to allocate space for a double-precision floating point value, you simply do this:\n\n\ndouble *d = (double *) malloc(sizeof(double));\nWow, that's lot of double-ing! This invocation of\n   \n    malloc()\n   \n   uses the\n   \n    sizeof()\n   \n   operator to request the right amount of space; in C, this is generally thought of as a\n   *compile-time*\n   operator, meaning that the actual size is known at\n   *compile time*\n   and thus a number (in this case, 8, for a\n   \n    double\n   \n   ) is substituted as the argument to\n   \n    malloc()\n   \n   . For this reason,\n   \n    sizeof()\n   \n   is correctly thought of as an operator and not a function call (a function call would take place at run time).\n\n\nYou can also pass in the name of a variable (and not just a type) to\n   \n    sizeof()\n   \n   , but in some cases you may not get the desired results, so be careful. For example, let's look at the following code snippet:\n\n\nint *x = malloc(10 * sizeof(int));\nprintf(\"%d\\n\", sizeof(x));\nIn the first line, we've declared space for an array of 10 integers, which is fine and dandy. However, when we use\n   \n    sizeof()\n   \n   in the next line, it returns a small value, such as 4 (on 32-bit machines) or 8 (on 64-bit machines). The reason is that in this case,\n   \n    sizeof()\n   \n   thinks we are simply asking how big a\n   *pointer*\n   to an integer is, not how much memory we have dynamically allocated. However, sometimes\n   \n    sizeof()\n   \n   does work as you might expect:\n\n\nint x[10];\nprintf(\"%d\\n\", sizeof(x));\nIn this case, there is enough static information for the compiler to know that 40 bytes have been allocated.\n\n\nAnother place to be careful is with strings. When declaring space for a string, use the following idiom:\n   \n    malloc(strlen(s) + 1)\n   \n   , which gets the length of the string using the function\n   \n    strlen()\n   \n   , and adds 1 to it\n\n\nin order to make room for the end-of-string character. Using\n   \n    sizeof()\n   \n   may lead to trouble here.\n\n\nYou might also notice that\n   \n    malloc()\n   \n   returns a pointer to type\n   \n    void\n   \n   . Doing so is just the way in C to pass back an address and let the programmer decide what to do with it. The programmer further helps out by using what is called a\n   **cast**\n   ; in our example above, the programmer casts the return type of\n   \n    malloc()\n   \n   to a pointer to a double. Casting doesn't really accomplish anything, other than tell the compiler and other programmers who might be reading your code: \"yeah, I know what I'm doing.\" By casting the result of\n   \n    malloc()\n   \n   , the programmer is just giving some reassurance; the cast is not needed for the correctness."
        },
        {
          "name": "The free() Call",
          "content": "As it turns out, allocating memory is the easy part of the equation; knowing when, how, and even if to free memory is the hard part. To free heap memory that is no longer in use, programmers simply call\n   \n    free()\n   \n   :\n\n\nint *x = malloc(10 * sizeof(int));\n...\nfree(x);\nThe routine takes one argument, a pointer returned by\n   \n    malloc()\n   \n   . Thus, you might notice, the size of the allocated region is not passed in by the user, and must be tracked by the memory-allocation library itself."
        },
        {
          "name": "Common Errors",
          "content": "There are a number of common errors that arise in the use of\n   \n    malloc()\n   \n   and\n   \n    free()\n   \n   . Here are some we've seen over and over again in teaching the undergraduate operating systems course. All of these examples compile and run with nary a peep from the compiler; while compiling a C program is necessary to build a correct C program, it is far from sufficient, as you will learn (often in the hard way).\n\n\nCorrect memory management has been such a problem, in fact, that many newer languages have support for\n   **automatic memory management**\n   . In such languages, while you call something akin to\n   \n    malloc()\n   \n   to allocate memory (usually\n   \n    new\n   \n   or something similar to allocate a new object), you never have to call something to free space; rather, a\n   **garbage collector**\n   runs and figures out what memory you no longer have references to and frees it for you.\n\n\n\n\n**Forgetting To Allocate Memory**\n\n\nMany routines expect memory to be allocated before you call them. For example, the routine\n   \n    strcpy(dst, src)\n   \n   copies a string from a source pointer to a destination pointer. However, if you are not careful, you might do this:\n\n\nchar *src = \"hello\";\nchar *dst;           // oops! unallocated\nstrcpy(dst, src);    // segfault and die\n\n\n**TIP: IT COMPILED OR IT RAN\n   \n    \\neq\n   \n   IT IS CORRECT**\n\n\nJust because a program compiled(!) or even ran once or many times correctly does not mean the program is correct. Many events may have conspired to get you to a point where you believe it works, but then something changes and it stops. A common student reaction is to say (or yell) “But it worked before!” and then blame the compiler, operating system, hardware, or even (dare we say it) the professor. But the problem is usually right where you think it would be, in your code. Get to work and debug it before you blame those other components.\n\n\nWhen you run this code, it will likely lead to a\n   **segmentation fault**\n\n    3\n   \n   , which is a fancy term for\n   **YOU DID SOMETHING WRONG WITH MEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY**\n   .\n\n\nIn this case, the proper code might instead look like this:\n\n\nchar *src = \"hello\";\nchar *dst = (char *) malloc(strlen(src) + 1);\nstrcpy(dst, src); // work properly\nAlternately, you could use\n   \n    strdup()\n   \n   and make your life even easier. Read the\n   \n    strdup\n   \n   man page for more information.\n\n\n\n\n**Not Allocating Enough Memory**\n\n\nA related error is not allocating enough memory, sometimes called a\n   **buffer overflow**\n   . In the example above, a common error is to make\n   *almost*\n   enough room for the destination buffer.\n\n\nchar *src = \"hello\";\nchar *dst = (char *) malloc(strlen(src)); // too small!\nstrcpy(dst, src); // work properly\nOddly enough, depending on how\n   \n    malloc\n   \n   is implemented and many other details, this program will often run seemingly correctly. In some cases, when the string copy executes, it writes one byte too far past the end of the allocated space, but in some cases this is harmless, perhaps overwriting a variable that isn’t used anymore. In some cases, these overflows can be incredibly harmful, and in fact are the source of many security vulnerabilities in systems [W06]. In other cases, the\n   \n    malloc\n   \n   library\n\n\n3\n   \n   Although it sounds arcane, you will soon learn why such an illegal memory access is called a segmentation fault; if that isn’t incentive to read on, what is?\n\n\nallocated a little extra space anyhow, and thus your program actually doesn't scribble on some other variable's value and works quite fine. In even other cases, the program will indeed fault and crash. And thus we learn another valuable lesson: even though it ran correctly once, doesn't mean it's correct.\n\n\n\n\n**Forgetting to Initialize Allocated Memory**\n\n\nWith this error, you call\n   \n    malloc()\n   \n   properly, but forget to fill in some values into your newly-allocated data type. Don't do this! If you do forget, your program will eventually encounter an\n   **uninitialized read**\n   , where it reads from the heap some data of unknown value. Who knows what might be in there? If you're lucky, some value such that the program still works (e.g., zero). If you're not lucky, something random and harmful.\n\n\n\n\n**Forgetting To Free Memory**\n\n\nAnother common error is known as a\n   **memory leak**\n   , and it occurs when you forget to free memory. In long-running applications or systems (such as the OS itself), this is a huge problem, as slowly leaking memory eventually leads one to run out of memory, at which point a restart is required. Thus, in general, when you are done with a chunk of memory, you should make sure to free it. Note that using a garbage-collected language doesn't help here: if you still have a reference to some chunk of memory, no garbage collector will ever free it, and thus memory leaks remain a problem even in more modern languages.\n\n\nIn some cases, it may seem like not calling\n   \n    free()\n   \n   is reasonable. For example, your program is short-lived, and will soon exit; in this case, when the process dies, the OS will clean up all of its allocated pages and thus no memory leak will take place per se. While this certainly \"works\" (see the aside on page 7), it is probably a bad habit to develop, so be wary of choosing such a strategy. In the long run, one of your goals as a programmer is to develop good habits; one of those habits is understanding how you are managing memory, and (in languages like C), freeing the blocks you have allocated. Even if you can get away with not doing so, it is probably good to get in the habit of freeing each and every byte you explicitly allocate.\n\n\n\n\n**Freeing Memory Before You Are Done With It**\n\n\nSometimes a program will free memory before it is finished using it; such a mistake is called a\n   **dangling pointer**\n   , and it, as you can guess, is also a bad thing. The subsequent use can crash the program, or overwrite valid memory (e.g., you called\n   \n    free()\n   \n   , but then called\n   \n    malloc()\n   \n   again to allocate something else, which then recycles the errantly-freed memory).\n\n\n**ASIDE: WHY NO MEMORY IS LEAKED ONCE YOUR PROCESS EXITS**\nWhen you write a short-lived program, you might allocate some space using\n   \n    malloc()\n   \n   . The program runs and is about to complete: is there need to call\n   \n    free()\n   \n   a bunch of times just before exiting? While it seems wrong not to, no memory will be “lost” in any real sense. The reason is simple: there are really two levels of memory management in the system. The first level of memory management is performed by the OS, which hands out memory to processes when they run, and takes it back when processes exit (or otherwise die). The second level of management is\n   *within*\n   each process, for example within the heap when you call\n   \n    malloc()\n   \n   and\n   \n    free()\n   \n   . Even if you fail to call\n   \n    free()\n   \n   (and thus leak memory in the heap), the operating system will reclaim\n   *all*\n   the memory of the process (including those pages for code, stack, and, as relevant here, heap) when the program is finished running. No matter what the state of your heap in your address space, the OS takes back all of those pages when the process dies, thus ensuring that no memory is lost despite the fact that you didn’t free it.\n\n\nThus, for short-lived programs, leaking memory often does not cause any operational problems (though it may be considered poor form). When you write a long-running server (such as a web server or database management system, which never exit), leaked memory is a much bigger issue, and will eventually lead to a crash when the application runs out of memory. And of course, leaking memory is an even larger issue inside one particular program: the operating system itself. Showing us once again: those who write the kernel code have the toughest job of all...\n\n\n\n\n**Freeing Memory Repeatedly**\n\n\nPrograms also sometimes free memory more than once; this is known as the\n   **double free**\n   . The result of doing so is undefined. As you can imagine, the memory-allocation library might get confused and do all sorts of weird things; crashes are a common outcome.\n\n\n\n\n**Calling\n   \n    free()\n   \n   Incorrectly**\n\n\nOne last problem we discuss is the call of\n   \n    free()\n   \n   incorrectly. After all,\n   \n    free()\n   \n   expects you only to pass to it one of the pointers you received from\n   \n    malloc()\n   \n   earlier. When you pass in some other value, bad things can (and do) happen. Thus, such\n   **invalid frees**\n   are dangerous and of course should also be avoided.\n\n\n\n\n**Summary**\n\n\nAs you can see, there are lots of ways to abuse memory. Because of frequent errors with memory, a whole ecosystem of tools have developed to help find such problems in your code. Check out both\n   **purify**\n   [HJ92] and\n   **valgrind**\n   [SN05]; both are excellent at helping you locate the source of your memory-related problems. Once you become accustomed to using these powerful tools, you will wonder how you survived without them."
        },
        {
          "name": "Underlying OS Support",
          "content": "You might have noticed that we haven't been talking about system calls when discussing\n   \n    malloc()\n   \n   and\n   \n    free()\n   \n   . The reason for this is simple: they are not system calls, but rather library calls. Thus the\n   \n    malloc\n   \n   library manages space within your virtual address space, but itself is built on top of some system calls which call into the OS to ask for more memory or release some back to the system.\n\n\nOne such system call is called\n   \n    brk\n   \n   , which is used to change the location of the program's\n   **break**\n   : the location of the end of the heap. It takes one argument (the address of the new break), and thus either increases or decreases the size of the heap based on whether the new break is larger or smaller than the current break. An additional call\n   \n    sbrk\n   \n   is passed an increment but otherwise serves a similar purpose.\n\n\nNote that you should never directly call either\n   \n    brk\n   \n   or\n   \n    sbrk\n   \n   . They are used by the memory-allocation library; if you try to use them, you will likely make something go (horribly) wrong. Stick to\n   \n    malloc()\n   \n   and\n   \n    free()\n   \n   instead.\n\n\nFinally, you can also obtain memory from the operating system via the\n   \n    mmap()\n   \n   call. By passing in the correct arguments,\n   \n    mmap()\n   \n   can create an\n   **anonymous**\n   memory region within your program — a region which is not associated with any particular file but rather with\n   **swap space**\n   , something we'll discuss in detail later on in virtual memory. This memory can then also be treated like a heap and managed as such. Read the manual page of\n   \n    mmap()\n   \n   for more details."
        },
        {
          "name": "Other Calls",
          "content": "There are a few other calls that the memory-allocation library supports. For example,\n   \n    calloc()\n   \n   allocates memory and also zeroes it before returning; this prevents some errors where you assume that memory is zeroed and forget to initialize it yourself (see the paragraph on \"uninitialized reads\" above). The routine\n   \n    realloc()\n   \n   can also be useful, when you've allocated space for something (say, an array), and then need to add something to it:\n   \n    realloc()\n   \n   makes a new larger region of memory, copies the old region into it, and returns the pointer to the new region."
        }
      ]
    },
    {
      "name": "Mechanism: Address Translation",
      "sections": [
        {
          "name": "Assumptions",
          "content": "Our first attempts at virtualizing memory will be very simple, almost laughably so. Go ahead, laugh all you want; pretty soon it will be the OS laughing at you, when you try to understand the ins and outs of TLBs, multi-level page tables, and other technical wonders. Don’t like the idea of the OS laughing at you? Well, you may be out of luck then; that’s just how the OS rolls.\n\n\nSpecifically, we will assume for now that the user’s address space must be placed\n   *contiguously*\n   in physical memory. We will also assume, for simplicity, that the size of the address space is not too big; specifically, that it is\n   *less than the size of physical memory*\n   . Finally, we will also assume that each address space is exactly the\n   *same size*\n   . Don’t worry if these assumptions sound unrealistic; we will relax them as we go, thus achieving a realistic virtualization of memory."
        },
        {
          "name": "An Example",
          "content": "To understand better what we need to do to implement address translation, and why we need such a mechanism, let’s look at a simple example. Imagine there is a process whose address space is as indicated in Figure 15.1. What we are going to examine here is a short code sequence that loads a value from memory, increments it by three, and then stores the value back into memory. You can imagine the C-language representation of this code might look like this:\n\n\n\n\n**TIP: INTERPOSITION IS POWERFUL**\n\n\nInterposition is a generic and powerful technique that is often used to great effect in computer systems. In virtualizing memory, the hardware will interpose on each memory access, and translate each virtual address issued by the process to a physical address where the desired information is actually stored. However, the general technique of interposition is much more broadly applicable; indeed, almost any well-defined interface can be interposed upon, to add new functionality or improve some other aspect of the system. One of the usual benefits of such an approach is\n   **transparency**\n   ; the interposition often is done without changing the interface of the client, thus requiring no changes to said client.\n\n\nvoid func() {\n    int x = 3000; // thanks, Perry.\n    x = x + 3;    // line of code we are interested in\n    ...\nThe compiler turns this line of code into assembly, which might look something like this (in x86 assembly). Use\n   \n    objdump\n   \n   on Linux or\n   \n    otool\n   \n   on a Mac to disassemble it:\n\n\n128: movl 0x0(%ebx), %eax      ;load 0+ebx into eax\n132: addl $0x03, %eax          ;add 3 to eax register\n135: movl %eax, 0x0(%ebx)      ;store eax back to mem\nThis code snippet is relatively straightforward; it presumes that the address of\n   \n    x\n   \n   has been placed in the register\n   \n    ebx\n   \n   , and then loads the value at that address into the general-purpose register\n   \n    eax\n   \n   using the\n   \n    movl\n   \n   instruction (for “longword” move). The next instruction adds 3 to\n   \n    eax\n   \n   , and the final instruction stores the value in\n   \n    eax\n   \n   back into memory at that same location.\n\n\nIn Figure 15.1 (page 4), observe how both the code and data are laid out in the process’s address space; the three-instruction code sequence is located at address 128 (in the code section near the top), and the value of the variable\n   \n    x\n   \n   at address 15 KB (in the stack near the bottom). In the figure, the initial value of\n   \n    x\n   \n   is 3000, as shown in its location on the stack.\n\n\nWhen these instructions run, from the perspective of the process, the following memory accesses take place.\n\n\n  * • Fetch instruction at address 128\n  * • Execute this instruction (load from address 15 KB)\n  * • Fetch instruction at address 132\n  * • Execute this instruction (no memory reference)\n  * • Fetch the instruction at address 135\n  * • Execute this instruction (store to address 15 KB)\n\n\n\n\n![Diagram of a process address space showing Program Code, Heap, free memory, and Stack segments.](images/image_0035.jpeg)\n\n\nThe diagram illustrates a process address space from 0KB to 16KB. The segments are defined as follows:\n\n\n  * **Program Code**\n     (0KB to 1KB):\n       * 128:\n       \n        movl 0x0(%ebx),%eax\n  * 132:\n       \n        addl 0x03, %eax\n  * 135:\n       \n        movl %eax,0x0(%ebx)\n  * **Heap**\n     (1KB to 4KB): A white rectangular area.\n  * **(free)**\n     (4KB to 14KB): A large area with diagonal hatching.\n  * **Stack**\n     (14KB to 16KB): A white rectangular area at the bottom.\n\n\nAnnotations include:\n\n  * A downward arrow from the Heap segment to the (free) segment.\n  * An upward arrow from the (free) segment to the Stack segment.\n  * The value\n      \n       3000\n      \n      is written in the Stack segment.\n\n\n\nDiagram of a process address space showing Program Code, Heap, free memory, and Stack segments.\n\n\nFigure 15.1: A Process And Its Address Space\n\n\n\n\n![Diagram of Physical Memory with a Single Relocated Process. The memory is divided into 8 KB slots from 0KB to 64KB. The Operating System occupies the first 16KB (0KB to 16KB). The next 16KB (16KB to 32KB) is marked as '(not in use)'. The third 16KB (32KB to 48KB) contains a 'Code Heap' (32KB to 36KB), an '(allocated but not in use)' section (36KB to 40KB), and a 'Stack' (40KB to 44KB). The final 16KB (48KB to 64KB) is marked as '(not in use)'. A bracket on the right labeled 'Relocated Process' points to the Code Heap, the allocated but not in use section, and the Stack.](images/image_0036.jpeg)\n\n\nDiagram of Physical Memory with a Single Relocated Process. The memory is divided into 8 KB slots from 0KB to 64KB. The Operating System occupies the first 16KB (0KB to 16KB). The next 16KB (16KB to 32KB) is marked as '(not in use)'. The third 16KB (32KB to 48KB) contains a 'Code Heap' (32KB to 36KB), an '(allocated but not in use)' section (36KB to 40KB), and a 'Stack' (40KB to 44KB). The final 16KB (48KB to 64KB) is marked as '(not in use)'. A bracket on the right labeled 'Relocated Process' points to the Code Heap, the allocated but not in use section, and the Stack.\n\n\nFigure 15.2:\n   **Physical Memory with a Single Relocated Process**\n\n\nFrom the program's perspective, its\n   **address space**\n   starts at address 0 and grows to a maximum of 16 KB; all memory references it generates should be within these bounds. However, to virtualize memory, the OS wants to place the process somewhere else in physical memory, not necessarily at address 0. Thus, we have the problem: how can we\n   **relocate**\n   this process in memory in a way that is\n   **transparent**\n   to the process? How can we provide the illusion of a virtual address space starting at 0, when in reality the address space is located at some other physical address?\n\n\nAn example of what physical memory might look like once this process's address space has been placed in memory is found in Figure 15.2. In the figure, you can see the OS using the first slot of physical memory for itself, and that it has relocated the process from the example above into the slot starting at physical memory address 32 KB. The other two slots are free (16 KB-32 KB and 48 KB-64 KB)."
        },
        {
          "name": "Dynamic (Hardware-based) Relocation",
          "content": "To gain some understanding of hardware-based address translation, we'll first discuss its first incarnation. Introduced in the first time-sharing machines of the late 1950's is a simple idea referred to as\n   **base and bounds**\n   ; the technique is also referred to as\n   **dynamic relocation**\n   ; we'll use both terms interchangeably [SS74].\n\n\nSpecifically, we'll need two hardware registers within each CPU: one is called the\n   **base**\n   register, and the other the\n   **bounds**\n   (sometimes called a\n   **limit**\n   register). This base-and-bounds pair is going to allow us to place the\n\n\n\n\n**ASIDE: SOFTWARE-BASED RELOCATION**\n\n\nIn the early days, before hardware support arose, some systems performed a crude form of relocation purely via software methods. The basic technique is referred to as\n   **static relocation**\n   , in which a piece of software known as the\n   **loader**\n   takes an executable that is about to be run and rewrites its addresses to the desired offset in physical memory.\n\n\nFor example, if an instruction was a load from address 1000 into a register (e.g.,\n   \n    movl 1000, %eax\n   \n   ), and the address space of the program was loaded starting at address 3000 (and not 0, as the program thinks), the loader would rewrite the instruction to offset each address by 3000 (e.g.,\n   \n    movl 4000, %eax\n   \n   ). In this way, a simple static relocation of the process's address space is achieved.\n\n\nHowever, static relocation has numerous problems. First and most importantly, it does not provide protection, as processes can generate bad addresses and thus illegally access other process's or even OS memory; in general, hardware support is likely needed for true protection [WL+93]. Another negative is that once placed, it is difficult to later relocate an address space to another location [M65].\n\n\naddress space anywhere we'd like in physical memory, and do so while ensuring that the process can only access its own address space.\n\n\nIn this setup, each program is written and compiled as if it is loaded at address zero. However, when a program starts running, the OS decides where in physical memory it should be loaded and sets the base register to that value. In the example above, the OS decides to load the process at physical address 32 KB and thus sets the base register to this value.\n\n\nInteresting things start to happen when the process is running. Now, when any memory reference is generated by the process, it is\n   **translated**\n   by the processor in the following manner:\n\n\n\n   \\text{physical address} = \\text{virtual address} + \\text{base}\n  \nEach memory reference generated by the process is a\n   **virtual address**\n   ; the hardware in turn adds the contents of the base register to this address and the result is a\n   **physical address**\n   that can be issued to the memory system.\n\n\nTo understand this better, let's trace through what happens when a single instruction is executed. Specifically, let's look at one instruction from our earlier sequence:\n\n\n128: movl 0x0(%ebx), %eax\nThe program counter (PC) is set to 128; when the hardware needs to fetch this instruction, it first adds the value to the base register value of 32 KB (32768) to get a physical address of 32896; the hardware then fetches the instruction from that physical address. Next, the processor begins executing the instruction. At some point, the process then issues\n\n\n**TIP: HARDWARE-BASED DYNAMIC RELOCATION**\nWith dynamic relocation, a little hardware goes a long way. Namely, a\n   **base**\n   register is used to transform virtual addresses (generated by the program) into physical addresses. A\n   **bounds**\n   (or\n   **limit**\n   ) register ensures that such addresses are within the confines of the address space. Together they provide a simple and efficient virtualization of memory.\n\n\nthe load from virtual address 15 KB, which the processor takes and again adds to the base register (32 KB), getting the final physical address of 47 KB and thus the desired contents.\n\n\nTransforming a virtual address into a physical address is exactly the technique we refer to as\n   **address translation**\n   ; that is, the hardware takes a virtual address the process thinks it is referencing and transforms it into a physical address which is where the data actually resides. Because this relocation of the address happens at runtime, and because we can move address spaces even after the process has started running, the technique is often referred to as\n   **dynamic relocation**\n   [M65].\n\n\nNow you might be asking: what happened to that bounds (limit) register? After all, isn't this the base\n   *and*\n   bounds approach? Indeed, it is. As you might have guessed, the bounds register is there to help with protection. Specifically, the processor will first check that the memory reference is\n   *within bounds*\n   to make sure it is legal; in the simple example above, the bounds register would always be set to 16 KB. If a process generates a virtual address that is greater than (or equal to) the bounds, or one that is negative, the CPU will raise an exception, and the process will likely be terminated. The point of the bounds is thus to make sure that all addresses generated by the process are legal and within the “bounds” of the process, as you might have guessed.\n\n\nWe should note that the base and bounds registers are hardware structures kept on the chip (one pair per CPU). Sometimes people call the part of the processor that helps with address translation the\n   **memory management unit (MMU)**\n   ; as we develop more sophisticated memory-management techniques, we will be adding more circuitry to the MMU.\n\n\nA small aside about bound registers, which can be defined in one of two ways. In one way (as above), it holds the\n   *size*\n   of the address space, and thus the hardware checks the virtual address against it first before adding the base. In the second way, it holds the\n   *physical address*\n   of the end of the address space, and thus the hardware first adds the base and then makes sure the address is within bounds. Both methods are logically equivalent; for simplicity, we'll usually assume the former method.\n\n\n**Example Translations**\nTo understand address translation via base-and-bounds in more detail, let's take a look at an example. Imagine a process with an address space of size 4 KB (yes, unrealistically small) has been loaded at physical address 16 KB. Here are the results of a number of address translations:\n\n\n\nVirtual Address |  | Physical Address\n0 | → | 16 KB\n1 KB | → | 17 KB\n3000 | → | 19384\n4400 | → | Fault (out of bounds)\n\n\nAs you can see from the example, it is easy for you to simply add the base address to the virtual address (which can rightly be viewed as an\n   *offset*\n   into the address space) to get the resulting physical address. Only if the virtual address is “too big” or negative will the result be a fault, causing an exception to be raised."
        },
        {
          "name": "Hardware Support: A Summary",
          "content": "Let us now summarize the support we need from the hardware (also see Figure 15.3, page 9). First, as discussed in the chapter on CPU virtualization, we require two different CPU modes. The OS runs in\n   **privileged mode**\n   (or\n   **kernel mode**\n   ), where it has access to the entire machine; applications run in\n   **user mode**\n   , where they are limited in what they can do. A single bit, perhaps stored in some kind of\n   **processor status word**\n   , indicates which mode the CPU is currently running in; upon certain special occasions (e.g., a system call or some other kind of exception or interrupt), the CPU switches modes.\n\n\nThe hardware must also provide the\n   **base and bounds registers**\n   themselves; each CPU thus has an additional pair of registers, part of the\n   **memory management unit (MMU)**\n   of the CPU. When a user program is running, the hardware will translate each address, by adding the base value to the virtual address generated by the user program. The hardware must also be able to check whether the address is valid, which is accomplished by using the bounds register and some circuitry within the CPU.\n\n\nThe hardware should provide special instructions to modify the base and bounds registers, allowing the OS to change them when different processes run. These instructions are\n   **privileged**\n   ; only in kernel (or privileged) mode can the registers be modified. Imagine the havoc a user process could wreak\n   \n    1\n   \n   if it could arbitrarily change the base register while\n\n\n1\n   \n   Is there anything other than “havoc” that can be “wreaked”? [W17]\n\n\n\n\n**ASIDE: DATA STRUCTURE — THE FREE LIST**\n\n\nThe OS must track which parts of free memory are not in use, so as to be able to allocate memory to processes. Many different data structures can of course be used for such a task; the simplest (which we will assume here) is a\n   **free list**\n   , which simply is a list of the ranges of the physical memory which are not currently in use.\n\n\n\nHardware Requirements | Notes\nPrivileged mode | Needed to prevent user-mode processes from executing privileged operations\nBase/bounds registers | Need pair of registers per CPU to support address translation and bounds checks\nAbility to translate virtual addresses and check if within bounds | Circuitry to do translations and check limits; in this case, quite simple\nPrivileged instruction(s) to update base/bounds | OS must be able to set these values before letting a user program run\nPrivileged instruction(s) to register exception handlers | OS must be able to tell hardware what code to run if exception occurs\nAbility to raise exceptions | When processes try to access privileged instructions or out-of-bounds memory\n\n\nFigure 15.3:\n   **Dynamic Relocation: Hardware Requirements**\n\n\nrunning. Imagine it! And then quickly flush such dark thoughts from your mind, as they are the ghastly stuff of which nightmares are made.\n\n\nFinally, the CPU must be able to generate\n   **exceptions**\n   in situations where a user program tries to access memory illegally (with an address that is “out of bounds”); in this case, the CPU should stop executing the user program and arrange for the OS “out-of-bounds”\n   **exception handler**\n   to run. The OS handler can then figure out how to react, in this case likely terminating the process. Similarly, if a user program tries to change the values of the (privileged) base and bounds registers, the CPU should raise an exception and run the “tried to execute a privileged operation while in user mode” handler. The CPU also must provide a method to inform it of the location of these handlers; a few more privileged instructions are thus needed."
        },
        {
          "name": "Operating System Issues",
          "content": "Just as the hardware provides new features to support dynamic relocation, the OS now has new issues it must handle; the combination of hardware support and OS management leads to the implementation of a simple virtual memory. Specifically, there are a few critical junctures where the OS must get involved to implement our base-and-bounds version of virtual memory.\n\n\nFirst, the OS must take action when a process is created, finding space for its address space in memory. Fortunately, given our assumptions that each address space is (a) smaller than the size of physical memory and (b) the same size, this is quite easy for the OS; it can simply view physical memory as an array of slots, and track whether each one is free or in use. When a new process is created, the OS will have to search a data structure (often called a\n   **free list**\n   ) to find room for the new address space and then mark it used. With variable-sized address spaces, life is more complicated, but we will leave that concern for future chapters.\n\n\n\nOS Requirements | Notes\nMemory management | Need to allocate memory for new processes;\n       \n       Reclaim memory from terminated processes;\n       \n       Generally manage memory via\n       \n        free list\nBase/bounds management | Must set base/bounds properly upon context switch\nException handling | Code to run when exceptions arise;\n       \n       likely action is to terminate offending process\n\n\nFigure 15.4:\n   **Dynamic Relocation: Operating System Responsibilities**\n\n\nLet's look at an example. In Figure 15.2 (page 5), you can see the OS using the first slot of physical memory for itself, and that it has relocated the process from the example above into the slot starting at physical memory address 32 KB. The other two slots are free (16 KB-32 KB and 48 KB-64 KB); thus, the\n   **free list**\n   should consist of these two entries.\n\n\nSecond, the OS must do some work when a process is terminated (i.e., when it exits gracefully, or is forcefully killed because it misbehaved), reclaiming all of its memory for use in other processes or the OS. Upon termination of a process, the OS thus puts its memory back on the free list, and cleans up any associated data structures as need be.\n\n\nThird, the OS must also perform a few additional steps when a context switch occurs. There is only one base and bounds register pair on each CPU, after all, and their values differ for each running program, as each program is loaded at a different physical address in memory. Thus, the OS must\n   *save and restore*\n   the base-and-bounds pair when it switches between processes. Specifically, when the OS decides to stop running a process, it must save the values of the base and bounds registers to memory, in some per-process structure such as the\n   **process structure**\n   or\n   **process control block**\n   (PCB). Similarly, when the OS resumes a running process (or runs it the first time), it must set the values of the base and bounds on the CPU to the correct values for this process.\n\n\nWe should note that when a process is stopped (i.e., not running), it is possible for the OS to move an address space from one location in memory to another rather easily. To move a process's address space, the OS first deschedules the process; then, the OS copies the address space from the current location to the new location; finally, the OS updates the saved base register (in the process structure) to point to the new location. When the process is resumed, its (new) base register is restored, and it begins running again, oblivious that its instructions and data are now in a completely new spot in memory.\n\n\nFourth, the OS must provide\n   **exception handlers**\n   , or functions to be called, as discussed above; the OS installs these handlers at boot time (via privileged instructions). For example, if a process tries to access memory outside its bounds, the CPU will raise an exception; the OS must be prepared to take action when such an exception arises. The common reaction of the OS will be one of hostility: it will likely terminate the offending process. The OS should be highly protective of the machine it is running, and thus it does not take kindly to a process trying to access memory or\n\n\n\nOS @ boot\n      \n      (kernel mode) | Hardware | (No Program Yet)\ninitialize trap table | remember addresses of...\n      \n      system call handler\n      \n      timer handler\n      \n      illegal mem-access handler\n      \n      illegal instruction handler | \nstart interrupt timer | start timer; interrupt after X ms | \ninitialize process table\n      \n      initialize free list |  | \n\n\nFigure 15.5:\n   **Limited Direct Execution (Dynamic Relocation) @ Boot**\n\n\nexecute instructions that it shouldn't. Bye bye, misbehaving process; it's been nice knowing you.\n\n\nFigures 15.5 and 15.6 (page 12) illustrate much of the hardware/OS interaction in a timeline. The first figure shows what the OS does at boot time to ready the machine for use, and the second shows what happens when a process (Process A) starts running; note how its memory translations are handled by the hardware with no OS intervention. At some point (middle of second figure), a timer interrupt occurs, and the OS switches to Process B, which executes a \"bad load\" (to an illegal memory address); at that point, the OS must get involved, terminating the process and cleaning up by freeing B's memory and removing its entry from the process table. As you can see from the figures, we are still following the basic approach of\n   **limited direct execution**\n   . In most cases, the OS just sets up the hardware appropriately and lets the process run directly on the CPU; only when the process misbehaves does the OS have to become involved."
        }
      ]
    },
    {
      "name": "Segmentation",
      "sections": [
        {
          "name": "Segmentation: Generalized Base/Bounds",
          "content": "To solve this problem, an idea was born, and it is called\n   **segmentation**\n   . It is quite an old idea, going at least as far back as the very early 1960's [H61, G62]. The idea is simple: instead of having just one base and bounds pair in our MMU, why not have a base and bounds pair per logical\n   **segment**\n   of the address space? A segment is just a contiguous portion of the address space of a particular length, and in our canonical\n\n\n\n\n![Diagram of an address space from 0KB to 16KB. The space is divided into segments: Program Code (0KB to 3KB), a shaded region (3KB to 4KB), Heap (4KB to 7KB), a large shaded region (7KB to 14KB) labeled '(free)', and Stack (14KB to 16KB). Arrows indicate the boundaries between these segments.](images/image_0037.jpeg)\n\n\nThe diagram illustrates a vertical address space from 0KB at the top to 16KB at the bottom. It is divided into several segments:\n\n\n  * **Program Code:**\n     Occupies the top segment from 0KB to 3KB.\n  * **Shaded Region:**\n     A small shaded segment from 3KB to 4KB.\n  * **Heap:**\n     Occupies the segment from 4KB to 7KB.\n  * **Free Space:**\n     A large shaded region from 7KB to 14KB, labeled \"(free)\".\n  * **Stack:**\n     Occupies the bottom segment from 14KB to 16KB.\n\n\nArrows indicate the boundaries: a downward arrow at 7KB and an upward arrow at 14KB.\n\n\nDiagram of an address space from 0KB to 16KB. The space is divided into segments: Program Code (0KB to 3KB), a shaded region (3KB to 4KB), Heap (4KB to 7KB), a large shaded region (7KB to 14KB) labeled '(free)', and Stack (14KB to 16KB). Arrows indicate the boundaries between these segments.\n\n\nFigure 16.1:\n   **An Address Space (Again)**\n\n\naddress space, we have three logically-different segments: code, stack, and heap. What segmentation allows the OS to do is to place each one of those segments in different parts of physical memory, and thus avoid filling physical memory with unused virtual address space.\n\n\nLet's look at an example. Assume we want to place the address space from Figure 16.1 into physical memory. With a base and bounds pair per segment, we can place each segment\n   *independently*\n   in physical memory. For example, see Figure 16.2 (page 3); there you see a 64KB physical memory with those three segments in it (and 16KB reserved for the OS).\n\n\n\n\n![Figure 16.2: Placing Segments In Physical Memory. A vertical diagram showing memory segments from 0KB to 64KB. The Operating System is at the top (0KB to 16KB). Below it is a segment labeled '(not in use)' (16KB to 28KB). The 'Stack' segment is at 28KB (28KB to 32KB). The 'Code' segment is at 32KB (32KB to 34KB). The 'Heap' segment is at 34KB (34KB to 36KB). The bottom segment is labeled '(not in use)' (36KB to 64KB).](images/image_0038.jpeg)\n\n\nFigure 16.2: Placing Segments In Physical Memory. A vertical diagram showing memory segments from 0KB to 64KB. The Operating System is at the top (0KB to 16KB). Below it is a segment labeled '(not in use)' (16KB to 28KB). The 'Stack' segment is at 28KB (28KB to 32KB). The 'Code' segment is at 32KB (32KB to 34KB). The 'Heap' segment is at 34KB (34KB to 36KB). The bottom segment is labeled '(not in use)' (36KB to 64KB).\n\n\nFigure 16.2: Placing Segments In Physical Memory\n\n\nAs you can see in the diagram, only used memory is allocated space in physical memory, and thus large address spaces with large amounts of unused address space (which we sometimes call\n   **sparse address spaces**\n   ) can be accommodated.\n\n\nThe hardware structure in our MMU required to support segmentation is just what you'd expect: in this case, a set of three base and bounds register pairs. Figure 16.3 below shows the register values for the example above; each bounds register holds the size of a segment.\n\n\n\nSegment | Base | Size\nCode | 32K | 2K\nHeap | 34K | 3K\nStack | 28K | 2K\n\n\nFigure 16.3: Segment Register Values\n\n\nYou can see from the figure that the code segment is placed at physical address 32KB and has a size of 2KB and the heap segment is placed at 34KB and has a size of 3KB. The size segment here is exactly the same as the bounds register introduced previously; it tells the hardware exactly how many bytes are valid in this segment (and thus, enables the hardware to determine when a program has made an illegal access outside of those bounds).\n\n\nLet's do an example translation, using the address space in Figure 16.1. Assume a reference is made to virtual address 100 (which is in the code segment, as you can see visually in Figure 16.1, page 2). When the refer-\n\n\n\n\n**ASIDE: THE SEGMENTATION FAULT**\n\n\nThe term\n   **segmentation fault**\n   or violation arises from a memory access on a segmented machine to an illegal address. Humorously, the term persists, even on machines with no support for segmentation at all. Or not so humorously, if you can't figure out why your code keeps faulting,\n\n\nence takes place (say, on an instruction fetch), the hardware will add the base value to the\n   *offset*\n   into this segment (100 in this case) to arrive at the desired physical address:\n   \n    100 + 32\\text{KB}\n   \n   , or 32868. It will then check that the address is within bounds (100 is less than 2KB), find that it is, and issue the reference to physical memory address 32868.\n\n\nNow let's look at an address in the heap, virtual address 4200 (again refer to Figure 16.1). If we just add the virtual address 4200 to the base of the heap (34KB), we get a physical address of 39016, which is\n   *not*\n   the correct physical address. What we need to first do is extract the\n   *offset*\n   into the heap, i.e., which byte(s)\n   *in this segment*\n   the address refers to. Because the heap starts at virtual address 4KB (4096), the offset of 4200 is actually 4200 minus 4096, or 104. We then take this offset (104) and add it to the base register physical address (34K) to get the desired result: 34920.\n\n\nWhat if we tried to refer to an illegal address (i.e., a virtual address of 7KB or greater), which is beyond the end of the heap? You can imagine what will happen: the hardware detects that the address is out of bounds, traps into the OS, likely leading to the termination of the offending process. And now you know the origin of the famous term that all C programmers learn to dread: the\n   **segmentation violation**\n   or\n   **segmentation fault**\n   ."
        },
        {
          "name": "Which Segment Are We Referring To?",
          "content": "The hardware uses segment registers during translation. How does it know the offset into a segment, and to which segment an address refers?\n\n\nOne common approach, sometimes referred to as an\n   **explicit**\n   approach, is to chop up the address space into segments based on the top few bits of the virtual address; this technique was used in the VAX/VMS system [LL82]. In our example above, we have three segments; thus we need two bits to accomplish our task. If we use the top two bits of our 14-bit virtual address to select the segment, our virtual address looks like this:\n\n\n\n\n![Diagram of a 14-bit virtual address structure. The address is shown as a row of 14 bits, numbered 13 down to 0 from left to right. A vertical line is drawn between bit 12 and bit 11. A bracket labeled 'Segment' spans from bit 13 to bit 12. A bracket labeled 'Offset' spans from bit 11 to bit 0.](images/image_0039.jpeg)\n\n\nDiagram of a 14-bit virtual address structure. The address is shown as a row of 14 bits, numbered 13 down to 0 from left to right. A vertical line is drawn between bit 12 and bit 11. A bracket labeled 'Segment' spans from bit 13 to bit 12. A bracket labeled 'Offset' spans from bit 11 to bit 0.\n\n\nIn our example, then, if the top two bits are 00, the hardware knows the virtual address is in the code segment, and thus uses the code base and bounds pair to relocate the address to the correct physical location. If the top two bits are 01, the hardware knows the address is in the heap,\n\n\nand thus uses the heap base and bounds. Let's take our example heap virtual address from above (4200) and translate it, just to make sure this is clear. The virtual address 4200, in binary form, can be seen here:\n\n\n\n\n![](images/image_0040.jpeg)\n\n\n| 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nSegment | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0\nOffset |\n\n\nAs you can see from the picture, the top two bits (01) tell the hardware which\n   *segment*\n   we are referring to. The bottom 12 bits are the\n   *offset*\n   into the segment: 0000 0110 1000, or hex 0x068, or 104 in decimal. Thus, the hardware simply takes the first two bits to determine which segment register to use, and then takes the next 12 bits as the offset into the segment. By adding the base register to the offset, the hardware arrives at the final physical address. Note the offset eases the bounds check too: we can simply check if the offset is less than the bounds; if not, the address is illegal. Thus, if base and bounds were arrays (with one entry per segment), the hardware would be doing something like this to obtain the desired physical address:\n\n\n1 // get top 2 bits of 14-bit VA\n2 Segment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT\n3 // now get offset\n4 Offset = VirtualAddress & OFFSET_MASK\n5 if (Offset >= Bounds[Segment])\n6     RaiseException(PROTECTION_FAULT)\n7 else\n8     PhysAddr = Base[Segment] + Offset\n9     Register = AccessMemory(PhysAddr)\nIn our running example, we can fill in values for the constants above. Specifically, SEG_MASK would be set to 0x3000, SEG_SHIFT to 12, and OFFSET_MASK to 0xFFF.\n\n\nYou may also have noticed that when we use the top two bits, and we only have three segments (code, heap, stack), one segment of the address space goes unused. To fully utilize the virtual address space (and avoid an unused segment), some systems put code in the same segment as the heap and thus use only one bit to select which segment to use [LL82].\n\n\nAnother issue with using the top so many bits to select a segment is that it limits use of the virtual address space. Specifically, each segment is limited to a\n   *maximum size*\n   , which in our example is 4KB (using the top two bits to choose segments implies the 16KB address space gets chopped into four pieces, or 4KB in this example). If a running program wishes to grow a segment (say the heap, or the stack) beyond that maximum, the program is out of luck.\n\n\nThere are other ways for the hardware to determine which segment a particular address is in. In the\n   **implicit**\n   approach, the hardware deter-\n\n\nmynes the segment by noticing how the address was formed. If, for example, the address was generated from the program counter (i.e., it was an instruction fetch), then the address is within the code segment; if the address is based off of the stack or base pointer, it must be in the stack segment; any other address must be in the heap."
        },
        {
          "name": "What About The Stack?",
          "content": "Thus far, we've left out one important component of the address space: the stack. The stack has been relocated to physical address 28KB in the diagram above, but with one critical difference:\n   *it grows backwards*\n   (i.e., towards lower addresses). In physical memory, it \"starts\" at 28KB\n   \n    1\n   \n   and grows back to 26KB, corresponding to virtual addresses 16KB to 14KB; translation must proceed differently.\n\n\nThe first thing we need is a little extra hardware support. Instead of just base and bounds values, the hardware also needs to know which way the segment grows (a bit, for example, that is set to 1 when the segment grows in the positive direction, and 0 for negative). Our updated view of what the hardware tracks is seen in Figure 16.4:\n\n\n\nSegment | Base | Size (max 4K) | Grows Positive?\nCode\n      \n       00 | 32K | 2K | 1\nHeap\n      \n       01 | 34K | 3K | 1\nStack\n      \n       11 | 28K | 2K | 0\n\n\nFigure 16.4:\n   **Segment Registers (With Negative-Growth Support)**\n\n\nWith the hardware understanding that segments can grow in the negative direction, the hardware must now translate such virtual addresses slightly differently. Let's take an example stack virtual address and translate it to understand the process.\n\n\nIn this example, assume we wish to access virtual address 15KB, which should map to physical address 27KB. Our virtual address, in binary form, thus looks like this: 11 1100 0000 0000 (hex 0x3C00). The hardware uses the top two bits (11) to designate the segment, but then we are left with an offset of 3KB. To obtain the correct negative offset, we must subtract the maximum segment size from 3KB: in this example, a segment can be 4KB, and thus the correct negative offset is 3KB minus 4KB which equals -1KB. We simply add the negative offset (-1KB) to the base (28KB) to arrive at the correct physical address: 27KB. The bounds check can be calculated by ensuring the absolute value of the negative offset is less than or equal to the segment's current size (in this case, 2KB).\n\n\n1\n   \n   Although we say, for simplicity, that the stack \"starts\" at 28KB, this value is actually the byte just\n   *below*\n   the location of the backward growing region; the first valid byte is actually 28KB minus 1. In contrast, forward-growing regions start at the address of the first byte of the segment. We take this approach because it makes the math to compute the physical address straightforward: the physical address is just the base plus the negative offset."
        },
        {
          "name": "Support for Sharing",
          "content": "As support for segmentation grew, system designers soon realized that they could realize new types of efficiencies with a little more hardware support. Specifically, to save memory, sometimes it is useful to\n   **share**\n   certain memory segments between address spaces. In particular,\n   **code sharing**\n   is common and still in use in systems today.\n\n\nTo support sharing, we need a little extra support from the hardware, in the form of\n   **protection bits**\n   . Basic support adds a few bits per segment, indicating whether or not a program can read or write a segment, or perhaps execute code that lies within the segment. By setting a code segment to read-only, the same code can be shared across multiple processes, without worry of harming isolation; while each process still thinks that it is accessing its own private memory, the OS is secretly sharing memory which cannot be modified by the process, and thus the illusion is preserved.\n\n\nAn example of the additional information tracked by the hardware (and OS) is shown in Figure 16.5. As you can see, the code segment is set to read and execute, and thus the same physical segment in memory could be mapped into multiple virtual address spaces.\n\n\n\nSegment | Base | Size (max 4K) | Grows Positive? | Protection\nCode\n      \n       00 | 32K | 2K | 1 | Read-Execute\nHeap\n      \n       01 | 34K | 3K | 1 | Read-Write\nStack\n      \n       11 | 28K | 2K | 0 | Read-Write\n\n\nFigure 16.5:\n   **Segment Register Values (with Protection)**\n\n\nWith protection bits, the hardware algorithm described earlier would also have to change. In addition to checking whether a virtual address is within bounds, the hardware also has to check whether a particular access is permissible. If a user process tries to write to a read-only segment, or execute from a non-executable segment, the hardware should raise an exception, and thus let the OS deal with the offending process."
        },
        {
          "name": "Fine-grained vs. Coarse-grained Segmentation",
          "content": "Most of our examples thus far have focused on systems with just a few segments (i.e., code, stack, heap); we can think of this segmentation as\n   **coarse-grained**\n   , as it chops up the address space into relatively large, coarse chunks. However, some early systems (e.g., Multics [CV65, DD68]) were more flexible and allowed for address spaces to consist of a large number of smaller segments, referred to as\n   **fine-grained**\n   segmentation.\n\n\nSupporting many segments requires even further hardware support, with a\n   **segment table**\n   of some kind stored in memory. Such segment tables usually support the creation of a very large number of segments, and thus enable a system to use segments in more flexible ways than we have thus far discussed. For example, early machines like the Burroughs B5000 had support for thousands of segments, and expected a compiler to chop\n\n\n\n\n![Diagram illustrating Non-compacted and Compacted Memory. The diagram shows two vertical memory layouts from 0KB to 64KB. The 'Not Compacted' layout on the left shows the Operating System (8KB) at the top, followed by several segments: a 'not in use' segment (16KB), an 'Allocated' segment (24KB), another 'not in use' segment (32KB), another 'Allocated' segment (40KB), a third 'not in use' segment (48KB), and a final 'Allocated' segment (56KB) at the bottom. The 'Compacted' layout on the right shows the Operating System (8KB) at the top, followed by a large 'Allocated' segment (24KB to 40KB), and a large 'not in use' segment (48KB to 56KB) at the bottom. The compacted layout eliminates the gaps between segments.](images/image_0041.jpeg)\n\n\nDiagram illustrating Non-compacted and Compacted Memory. The diagram shows two vertical memory layouts from 0KB to 64KB. The 'Not Compacted' layout on the left shows the Operating System (8KB) at the top, followed by several segments: a 'not in use' segment (16KB), an 'Allocated' segment (24KB), another 'not in use' segment (32KB), another 'Allocated' segment (40KB), a third 'not in use' segment (48KB), and a final 'Allocated' segment (56KB) at the bottom. The 'Compacted' layout on the right shows the Operating System (8KB) at the top, followed by a large 'Allocated' segment (24KB to 40KB), and a large 'not in use' segment (48KB to 56KB) at the bottom. The compacted layout eliminates the gaps between segments.\n\n\nFigure 16.6:\n   **Non-compacted and Compacted Memory**\n\n\ncode and data into separate segments which the OS and hardware would then support [RK68]. The thinking at the time was that by having fine-grained segments, the OS could better learn about which segments are in use and which are not and thus utilize main memory more effectively."
        },
        {
          "name": "OS Support",
          "content": "You now should have a basic idea as to how segmentation works. Pieces of the address space are relocated into physical memory as the system runs, and thus a huge savings of physical memory is achieved relative to our simpler approach with just a single base/bounds pair for the entire address space. Specifically, all the unused space between the stack and the heap need not be allocated in physical memory, allowing us to fit more address spaces into physical memory and support a large and sparse virtual address space per process.\n\n\nHowever, segmentation raises a number of new issues for the operating system. The first is an old one: what should the OS do on a context switch? You should have a good guess by now: the segment registers must be saved and restored. Clearly, each process has its own virtual address space, and the OS must make sure to set up these registers correctly before letting the process run again.\n\n\nThe second is OS interaction when segments grow (or perhaps shrink). For example, a program may call\n   \n    malloc()\n   \n   to allocate an object. In some cases, the existing heap will be able to service the request, and thus\n\n\n\n\n**TIP: IF 1000 SOLUTIONS EXIST, NO GREAT ONE DOES**\n\n\nThe fact that so many different algorithms exist to try to minimize external fragmentation is indicative of a stronger underlying truth: there is no one “best” way to solve the problem. Thus, we settle for something reasonable and hope it is good enough. The only real solution (as we will see in forthcoming chapters) is to avoid the problem altogether, by never allocating memory in variable-sized chunks.\n\n\nmalloc()\n   \n   will find free space for the object and return a pointer to it to the caller. In others, however, the heap segment itself may need to grow. In this case, the memory-allocation library will perform a system call to grow the heap (e.g., the traditional UNIX\n   \n    sbrk()\n   \n   system call). The OS will then (usually) provide more space, updating the segment size register to the new (bigger) size, and informing the library of success; the library can then allocate space for the new object and return successfully to the calling program. Do note that the OS could reject the request, if no more physical memory is available, or if it decides that the calling process already has too much.\n\n\nThe last, and perhaps most important, issue is managing free space in physical memory. When a new address space is created, the OS has to be able to find space in physical memory for its segments. Previously, we assumed that each address space was the same size, and thus physical memory could be thought of as a bunch of slots where processes would fit in. Now, we have a number of segments per process, and each segment might be a different size.\n\n\nThe general problem that arises is that physical memory quickly becomes full of little holes of free space, making it difficult to allocate new segments, or to grow existing ones. We call this problem\n   **external fragmentation**\n   [R69]; see Figure 16.6 (left).\n\n\nIn the example, a process comes along and wishes to allocate a 20KB segment. In that example, there is 24KB free, but not in one contiguous segment (rather, in three non-contiguous chunks). Thus, the OS cannot satisfy the 20KB request. Similar problems could occur when a request to grow a segment arrives; if the next so many bytes of physical space are not available, the OS will have to reject the request, even though there may be free bytes available elsewhere in physical memory.\n\n\nOne solution to this problem would be to\n   **compact**\n   physical memory by rearranging the existing segments. For example, the OS could stop whichever processes are running, copy their data to one contiguous region of memory, change their segment register values to point to the new physical locations, and thus have a large free extent of memory with which to work. By doing so, the OS enables the new allocation request to succeed. However, compaction is expensive, as copying segments is memory-intensive and generally uses a fair amount of processor time; see\n\n\nFigure 16.6 (right) for a diagram of compacted physical memory. Compaction also (ironically) makes requests to grow existing segments hard to serve, and may thus cause further rearrangement to accommodate such requests.\n\n\nA simpler approach might instead be to use a free-list management algorithm that tries to keep large extents of memory available for allocation. There are literally hundreds of approaches that people have taken, including classic algorithms like\n   **best-fit**\n   (which keeps a list of free spaces and returns the one closest in size that satisfies the desired allocation to the requester),\n   **worst-fit**\n   ,\n   **first-fit**\n   , and more complex schemes like the\n   **buddy algorithm**\n   [K68]. An excellent survey by Wilson et al. is a good place to start if you want to learn more about such algorithms [W+95], or you can wait until we cover some of the basics in a later chapter. Unfortunately, though, no matter how smart the algorithm, external fragmentation will still exist; a good algorithm attempts to minimize it."
        }
      ]
    },
    {
      "name": "Free-Space Management",
      "sections": [
        {
          "name": "Assumptions",
          "content": "Most of this discussion will focus on the great history of allocators found in user-level memory-allocation libraries. We draw on Wilson's excellent survey [W+95] but encourage interested readers to go to the source document itself for more details\n   \n    1\n   \n   .\n\n\nWe assume a basic interface such as that provided by\n   \n    malloc()\n   \n   and\n   \n    free()\n   \n   . Specifically,\n   \n    void *malloc(size_t size)\n   \n   takes a single parameter,\n   \n    size\n   \n   , which is the number of bytes requested by the application; it hands back a pointer (of no particular type, or a\n   **void pointer**\n   in C lingo) to a region of that size (or greater). The complementary routine\n   \n    void free(void *ptr)\n   \n   takes a pointer and frees the corresponding chunk. Note the implication of the interface: the user, when freeing the space, does not inform the library of its size; thus, the library must be able to figure out how big a chunk of memory is when handed just a pointer to it. We'll discuss how to do this a bit later on in the chapter.\n\n\nThe space that this library manages is known historically as the\n   **heap**\n   , and the generic data structure used to manage free space in the heap is some kind of\n   **free list**\n   . This structure contains references to all of the free chunks of space in the managed region of memory. Of course, this data structure need not be a list\n   *per se*\n   , but just some kind of data structure to track free space.\n\n\nWe further assume that primarily we are concerned with\n   **external fragmentation**\n   , as described above. Allocators could of course also have the problem of\n   **internal fragmentation**\n   ; if an allocator hands out chunks of memory bigger than that requested, any unasked for (and thus unused) space in such a chunk is considered\n   *internal*\n   fragmentation (because the waste occurs inside the allocated unit) and is another example of space waste. However, for the sake of simplicity, and because it is the more interesting of the two types of fragmentation, we'll mostly focus on external fragmentation.\n\n\nWe'll also assume that once memory is handed out to a client, it cannot be relocated to another location in memory. For example, if a program calls\n   \n    malloc()\n   \n   and is given a pointer to some space within the heap, that memory region is essentially \"owned\" by the program (and cannot be moved by the library) until the program returns it via a corresponding call to\n   \n    free()\n   \n   . Thus,\n   **no compaction**\n   of free space is possible, which\n\n\n1\n   \n   It is nearly 80 pages long; thus, you really have to be interested!\n\n\nwould be useful to combat fragmentation\n   \n    2\n   \n   . Compaction could, however, be used in the OS to deal with fragmentation when implementing\n   **segmentation**\n   (as discussed in said chapter on segmentation).\n\n\nFinally, we'll assume that the allocator manages a contiguous region of bytes. In some cases, an allocator could ask for that region to grow; for example, a user-level memory-allocation library might call into the kernel to grow the heap (via a system call such as\n   \n    sbrk\n   \n   ) when it runs out of space. However, for simplicity, we'll just assume that the region is a single fixed size throughout its life."
        },
        {
          "name": "Low-level Mechanisms",
          "content": "Before delving into some policy details, we'll first cover some common mechanisms used in most allocators. First, we'll discuss the basics of splitting and coalescing, common techniques in most any allocator. Second, we'll show how one can track the size of allocated regions quickly and with relative ease. Finally, we'll discuss how to build a simple list inside the free space to keep track of what is free and what isn't.\n\n\n\n\n**Splitting and Coalescing**\n\n\nA free list contains a set of elements that describe the free space still remaining in the heap. Thus, assume the following 30-byte heap:\n\n\n\n\n![Diagram of a 30-byte heap divided into segments: free (0-9), used (10-19), and free (20-29).](images/image_0042.jpeg)\n\n\nfree | used | free\n0 | 10 | 20 | 30\n\n\nDiagram of a 30-byte heap divided into segments: free (0-9), used (10-19), and free (20-29).\n\n\nThe free list for this heap would have two elements on it. One entry describes the first 10-byte free segment (bytes 0-9), and one entry describes the other free segment (bytes 20-29):\n\n\n\n\n![Diagram of a free list with two nodes: head points to a node with addr:0, len:10, which points to a node with addr:20, len:10, which points to NULL.](images/image_0043.jpeg)\n\n\ngraph LR; head --> node1((addr:0\nlen:10)); node1 --> node2((addr:20\nlen:10)); node2 --> NULL;\nDiagram of a free list with two nodes: head points to a node with addr:0, len:10, which points to a node with addr:20, len:10, which points to NULL.\n\n\nAs described above, a request for anything greater than 10 bytes will fail (returning\n   \n    NULL\n   \n   ); there just isn't a single contiguous chunk of memory of that size available. A request for exactly that size (10 bytes) could be satisfied easily by either of the free chunks. But what happens if the request is for something\n   *smaller*\n   than 10 bytes?\n\n\nAssume we have a request for just a single byte of memory. In this case, the allocator will perform an action known as\n   **splitting**\n   : it will find\n\n\n2\n   \n   Once you hand a pointer to a chunk of memory to a C program, it is generally difficult to determine all references (pointers) to that region, which may be stored in other variables or even in registers at a given point in execution. This may not be the case in more strongly-typed, garbage-collected languages, which would thus enable compaction as a technique to combat fragmentation.\n\n\na free chunk of memory that can satisfy the request and split it into two. The first chunk it will return to the caller; the second chunk will remain on the list. Thus, in our example above, if a request for 1 byte were made, and the allocator decided to use the second of the two elements on the list to satisfy the request, the call to\n   \n    malloc()\n   \n   would return 20 (the address of the 1-byte allocated region) and the list would end up looking like this:\n\n\n\n\n![Diagram showing a linked list of two free chunks. The first chunk has address 0 and length 10. The second chunk has address 21 and length 9. The list ends with NULL.](images/image_0044.jpeg)\n\n\ngraph LR\n    head --> node1((addr:0\nlen:10))\n    node1 --> node2((addr:21\nlen:9))\n    node2 --> NULL\n  \nDiagram showing a linked list of two free chunks. The first chunk has address 0 and length 10. The second chunk has address 21 and length 9. The list ends with NULL.\n\n\nIn the picture, you can see the list basically stays intact; the only change is that the free region now starts at 21 instead of 20, and the length of that free region is now just 9\n   \n    3\n   \n   . Thus, the split is commonly used in allocators when requests are smaller than the size of any particular free chunk.\n\n\nA corollary mechanism found in many allocators is known as\n   **coalescing**\n   of free space. Take our example from above once more (free 10 bytes, used 10 bytes, and another free 10 bytes).\n\n\nGiven this (tiny) heap, what happens when an application calls\n   \n    free(10)\n   \n   , thus returning the space in the middle of the heap? If we simply add this free space back into our list without too much thinking, we might end up with a list that looks like this:\n\n\n\n\n![Diagram showing a linked list of three free chunks. The first chunk has address 0 and length 10. The second chunk has address 10 and length 10. The third chunk has address 20 and length 10. The list ends with NULL.](images/image_0045.jpeg)\n\n\ngraph LR\n    head --> node1((addr:0\nlen:10))\n    node1 --> node2((addr:10\nlen:10))\n    node2 --> node3((addr:20\nlen:10))\n    node3 --> NULL\n  \nDiagram showing a linked list of three free chunks. The first chunk has address 0 and length 10. The second chunk has address 10 and length 10. The third chunk has address 20 and length 10. The list ends with NULL.\n\n\nNote the problem: while the entire heap is now free, it is seemingly divided into three chunks of 10 bytes each. Thus, if a user requests 20 bytes, a simple list traversal will not find such a free chunk, and return failure.\n\n\nWhat allocators do in order to avoid this problem is coalesce free space when a chunk of memory is freed. The idea is simple: when returning a free chunk in memory, look carefully at the addresses of the chunk you are returning as well as the nearby chunks of free space; if the newly-freed space sits right next to one (or two, as in this example) existing free chunks, merge them into a single larger free chunk. Thus, with coalescing, our final list should look like this:\n\n\n\n\n![Diagram showing a linked list of one free chunk. The chunk has address 0 and length 30. The list ends with NULL.](images/image_0046.jpeg)\n\n\ngraph LR\n    head --> node1((addr:0\nlen:30))\n    node1 --> NULL\n  \nDiagram showing a linked list of one free chunk. The chunk has address 0 and length 30. The list ends with NULL.\n\n\nIndeed, this is what the heap list looked like at first, before any allocations were made. With coalescing, an allocator can better ensure that large free extents are available for the application.\n\n\n3\n   \n   This discussion assumes that there are no headers, an unrealistic but simplifying assumption we make for now.\n\n\n\n\n![Diagram of an allocated region plus header. A pointer 'ptr' points to the start of a 20-byte block. The block is divided into two sections: a header at the top and the allocated region below. A bracket on the right labels the header as 'The header used by malloc library' and the 20 bytes below as 'The 20 bytes returned to caller'.](images/image_0047.jpeg)\n\n\nDiagram of an allocated region plus header. A pointer 'ptr' points to the start of a 20-byte block. The block is divided into two sections: a header at the top and the allocated region below. A bracket on the right labels the header as 'The header used by malloc library' and the 20 bytes below as 'The 20 bytes returned to caller'.\n\n\nFigure 17.1: An Allocated Region Plus Header\n\n\n\n\n![Diagram showing the specific contents of the header. A pointer 'hptr' points to a header block containing 'size: 20' and 'magic: 1234567'. A pointer 'ptr' points to the start of the 20-byte allocated region below. A bracket on the right labels the 20 bytes as 'The 20 bytes returned to caller'.](images/image_0048.jpeg)\n\n\nDiagram showing the specific contents of the header. A pointer 'hptr' points to a header block containing 'size: 20' and 'magic: 1234567'. A pointer 'ptr' points to the start of the 20-byte allocated region below. A bracket on the right labels the 20 bytes as 'The 20 bytes returned to caller'.\n\n\nFigure 17.2: Specific Contents Of The Header\n\n\n\n\n**Tracking The Size Of Allocated Regions**\n\n\nYou might have noticed that the interface to\n   \n    free(void *ptr)\n   \n   does not take a size parameter; thus it is assumed that given a pointer, the malloc library can quickly determine the size of the region of memory being freed and thus incorporate the space back into the free list.\n\n\nTo accomplish this task, most allocators store a little bit of extra information in a\n   **header**\n   block which is kept in memory, usually just before the handed-out chunk of memory. Let's look at an example again (Figure 17.1). In this example, we are examining an allocated block of size 20 bytes, pointed to by\n   \n    ptr\n   \n   ; imagine the user called\n   \n    malloc()\n   \n   and stored the results in\n   \n    ptr\n   \n   , e.g.,\n   \n    ptr = malloc(20);\n   \n   .\n\n\nThe header minimally contains the size of the allocated region (in this case, 20); it may also contain additional pointers to speed up deallocation, a magic number to provide additional integrity checking, and other information. Let's assume a simple header which contains the size of the region and a magic number, like this:\n\n\ntypedef struct {\n    int size;\n    int magic;\n} header_t;\nThe example above would look like what you see in Figure 17.2. When the user calls\n   \n    free(ptr)\n   \n   , the library then uses simple pointer arithmetic to figure out where the header begins:\n\n\nvoid free(void *ptr) {\n    header_t *hptr = (header_t *) ptr - 1;\n    ...\nAfter obtaining such a pointer to the header, the library can easily determine whether the magic number matches the expected value as a sanity check (\n   \n    assert(hptr->magic == 1234567)\n   \n   ) and calculate the total size of the newly-freed region via simple math (i.e., adding the size of the header to size of the region). Note the small but critical detail in the last sentence: the size of the free region is the size of the header plus the size of the space allocated to the user. Thus, when a user requests\n   \n    N\n   \n   bytes of memory, the library does not search for a free chunk of size\n   \n    N\n   \n   ; rather, it searches for a free chunk of size\n   \n    N\n   \n   plus the size of the header.\n\n\n\n\n**Embedding A Free List**\n\n\nThus far we have treated our simple free list as a conceptual entity; it is just a list describing the free chunks of memory in the heap. But how do we build such a list inside the free space itself?\n\n\nIn a more typical list, when allocating a new node, you would just call\n   \n    malloc()\n   \n   when you need space for the node. Unfortunately, within the memory-allocation library, you can't do this! Instead, you need to build the list\n   *inside*\n   the free space itself. Don't worry if this sounds a little weird; it is, but not so weird that you can't do it!\n\n\nAssume we have a 4096-byte chunk of memory to manage (i.e., the heap is 4KB). To manage this as a free list, we first have to initialize said list; initially, the list should have one entry, of size 4096 (minus the header size). Here is the description of a node of the list:\n\n\ntypedef struct __node_t {\n    int size;\n    struct __node_t *next;\n} node_t;\nNow let's look at some code that initializes the heap and puts the first element of the free list inside that space. We are assuming that the heap is built within some free space acquired via a call to the system call\n   \n    mmap()\n   \n   ; this is not the only way to build such a heap but serves us well in this example. Here is the code:\n\n\n// mmap() returns a pointer to a chunk of free space\nnode_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE,\n                    MAP_ANON|MAP_PRIVATE, -1, 0);\nhead->size = 4096 - sizeof(node_t);\nhead->next = NULL;\n\n\n![Diagram of a heap with one free chunk. A pointer 'head' points to a header block. The header contains 'size: 4088' and 'next: 0'. To the right, text explains: '[virtual address: 16KB]', 'header: size field', and 'header: next field (NULL is 0)'. A bracket labeled 'the rest of the 4KB chunk' points to the remaining space in the block.](images/image_0049.jpeg)\n\n\nhead → [size: 4088, next: 0, ...]\n\n\n[virtual address: 16KB]\n    \n\n    header: size field\n    \n\n    header: next field (NULL is 0)\n\n\nthe rest of the 4KB chunk\n\n\nDiagram of a heap with one free chunk. A pointer 'head' points to a header block. The header contains 'size: 4088' and 'next: 0'. To the right, text explains: '[virtual address: 16KB]', 'header: size field', and 'header: next field (NULL is 0)'. A bracket labeled 'the rest of the 4KB chunk' points to the remaining space in the block.\n\n\n**A Heap With One Free Chunk**\n\n\n![Diagram of a heap after one allocation. A pointer 'ptr' points to a header block for an allocated chunk, containing 'size: 100' and 'magic: 1234567'. A bracket labeled 'The 100 bytes now allocated' points to the rest of this block. A pointer 'head' points to a header block for a free chunk, containing 'size: 3980' and 'next: 0'. A bracket labeled 'The free 3980 byte chunk' points to the rest of this block. Text to the right indicates '[virtual address: 16KB]'.](images/image_0050.jpeg)\n\n\nptr → [size: 100, magic: 1234567, ...]\n\n\n[virtual address: 16KB]\n\n\nThe 100 bytes now allocated\n\n\nhead → [size: 3980, next: 0, ...]\n\n\nThe free 3980 byte chunk\n\n\nDiagram of a heap after one allocation. A pointer 'ptr' points to a header block for an allocated chunk, containing 'size: 100' and 'magic: 1234567'. A bracket labeled 'The 100 bytes now allocated' points to the rest of this block. A pointer 'head' points to a header block for a free chunk, containing 'size: 3980' and 'next: 0'. A bracket labeled 'The free 3980 byte chunk' points to the rest of this block. Text to the right indicates '[virtual address: 16KB]'.\n\n\n**A Heap: After One Allocation**\nAfter running this code, the status of the list is that it has a single entry, of size 4088. Yes, this is a tiny heap, but it serves as a fine example for us here. The\n   \n    head\n   \n   pointer contains the beginning address of this range; let's assume it is 16KB (though any virtual address would be fine). Visually, the heap thus looks like what you see in Figure 17.3.\n\n\nNow, let's imagine that a chunk of memory is requested, say of size 100 bytes. To service this request, the library will first find a chunk that is large enough to accommodate the request; because there is only one free chunk (size: 4088), this chunk will be chosen. Then, the chunk will be\n   **split**\n   into two: one chunk big enough to service the request (and header, as described above), and the remaining free chunk. Assuming an 8-byte header (an integer size and an integer magic number), the space in the heap now looks like what you see in Figure 17.4.\n\n\nThus, upon the request for 100 bytes, the library allocated 108 bytes\n\n\n\n\n![Diagram of heap memory layout with three allocated chunks and one free chunk.](images/image_0051.jpeg)\n\n\nThe diagram illustrates the heap memory layout. At the top, a label indicates the virtual address is 16KB. The heap is divided into several regions:\n\n\n  * **Allocated Chunks:**\n     Three chunks are shown, each with a header (size and magic) and a body of 100 bytes. The headers are:\n       * Header 1: size: 100, magic: 1234567. A bracket indicates \"100 bytes still allocated\".\n  * Header 2: size: 100, magic: 1234567. A bracket indicates \"100 bytes still allocated (but about to be freed)\".\n  * Header 3: size: 100, magic: 1234567. A bracket indicates \"100-bytes still allocated\".\n  * **Free List:**\n     A pointer labeled\n     **sptr**\n     points to the first header of the allocated chunks. A pointer labeled\n     **head**\n     points to a free node at the bottom of the heap.\n  * **Free Node:**\n     The node pointed to by\n     **head**\n     has a header with size: 3764 and next: 0. A bracket indicates \"The free 3764-byte chunk\".\n\n\nDiagram of heap memory layout with three allocated chunks and one free chunk.\n\n\n**Figure 17.5: Free Space With Three Chunks Allocated**\n\n\nout of the existing one free chunk, returns a pointer (marked\n   **ptr**\n   in the figure above) to it, stashs the header information immediately before the allocated space for later use upon\n   \n    free()\n   \n   , and shrinks the one free node in the list to 3980 bytes (4088 minus 108).\n\n\nNow let's look at the heap when there are three allocated regions, each of 100 bytes (or 108 including the header). A visualization of this heap is shown in Figure 17.5.\n\n\nAs you can see therein, the first 324 bytes of the heap are now allocated, and thus we see three headers in that space as well as three 100-byte regions being used by the calling program. The free list remains uninteresting: just a single node (pointed to by\n   **head**\n   ), but now only 3764\n\n\n\n\n![Diagram illustrating free space management with two chunks allocated. The memory is shown as a vertical stack of blocks. At the top, a block is labeled '[virtual address: 16KB]'. Below it, a block is highlighted with a bracket and the text '100 bytes still allocated'. A pointer 'head' points to the start of this block. Below that, another block is highlighted with a bracket and the text '(now a free chunk of memory)'. A pointer 'spt' points to the start of this block. Below this free chunk, another block is highlighted with a bracket and the text '100-bytes still allocated'. Below that, a block is highlighted with a bracket and the text 'The free 3764-byte chunk'. The blocks contain fields like 'size' and 'magic'.](images/image_0052.jpeg)\n\n\nThe diagram illustrates a memory layout with two chunks of allocated memory and a free chunk. At the top, a block is labeled with its virtual address: 16KB. Below it, a block is highlighted with a bracket and the text \"100 bytes still allocated\". A pointer labeled \"head\" points to the start of this block. Below that, another block is highlighted with a bracket and the text \"(now a free chunk of memory)\". A pointer labeled \"spt\" points to the start of this block. Below this free chunk, another block is highlighted with a bracket and the text \"100-bytes still allocated\". Below that, a block is highlighted with a bracket and the text \"The free 3764-byte chunk\". The blocks contain fields like \"size\" and \"magic\".\n\n\nDiagram illustrating free space management with two chunks allocated. The memory is shown as a vertical stack of blocks. At the top, a block is labeled '[virtual address: 16KB]'. Below it, a block is highlighted with a bracket and the text '100 bytes still allocated'. A pointer 'head' points to the start of this block. Below that, another block is highlighted with a bracket and the text '(now a free chunk of memory)'. A pointer 'spt' points to the start of this block. Below this free chunk, another block is highlighted with a bracket and the text '100-bytes still allocated'. Below that, a block is highlighted with a bracket and the text 'The free 3764-byte chunk'. The blocks contain fields like 'size' and 'magic'.\n\n\nFigure 17.6: Free Space With Two Chunks Allocated\n\n\nbytes in size after the three splits. But what happens when the calling program returns some memory via\n   \n    free()\n   \n   ?\n\n\nIn this example, the application returns the middle chunk of allocated memory, by calling\n   \n    free(16500)\n   \n   (the value 16500 is arrived upon by adding the start of the memory region, 16384, to the 108 of the previous chunk and the 8 bytes of the header for this chunk). This value is shown in the previous diagram by the pointer\n   \n    spt\n   \n   .\n\n\nThe library immediately figures out the size of the free region, and then adds the free chunk back onto the free list. Assuming we insert at the head of the free list, the space now looks like this (Figure 17.6).\n\n\n\n\n![Diagram of a non-coalesced free list showing fragmentation.](images/image_0053.jpeg)\n\n\nThe diagram illustrates a linked list of free memory chunks. The list is pointed to by a 'head' arrow. The chunks are represented as vertical rectangles, each containing a 'size' and a 'next' field. The first chunk has a size of 100 and a next pointer of 16492. The second chunk has a size of 100 and a next pointer of 16708. The third chunk has a size of 100 and a next pointer of 16384. The fourth chunk has a size of 3764 and a next pointer of 0. To the right of the list, arrows indicate that the first three chunks are now free, and the last chunk is labeled 'The free 3764-byte chunk'. A label at the top right indicates a 'virtual address: 16KB'.\n\n\nDiagram of a non-coalesced free list showing fragmentation.\n\n\nFigure 17.7: A Non-Coalesced Free List\n\n\nNow we have a list that starts with a small free chunk (100 bytes, pointed to by the head of the list) and a large free chunk (3764 bytes). Our list finally has more than one element on it! And yes, the free space is fragmented, an unfortunate but common occurrence.\n\n\nOne last example: let's assume now that the last two in-use chunks are freed. Without coalescing, you end up with fragmentation (Figure 17.7).\n\n\nAs you can see from the figure, we now have a big mess! Why? Simple, we forgot to\n   **coalesce**\n   the list. Although all of the memory is free, it is chopped up into pieces, thus appearing as a fragmented memory despite not being one. The solution is simple: go through the list and\n   **merge**\n   neighboring chunks; when finished, the heap will be whole again.\n\n\n\n\n**Growing The Heap**\n\n\nWe should discuss one last mechanism found within many allocation libraries. Specifically, what should you do if the heap runs out of space? The simplest approach is just to fail. In some cases this is the only option, and thus returning NULL is an honorable approach. Don't feel bad! You tried, and though you failed, you fought the good fight.\n\n\nMost traditional allocators start with a small-sized heap and then request more memory from the OS when they run out. Typically, this means they make some kind of system call (e.g.,\n   \n    sbrk\n   \n   in most UNIX systems) to grow the heap, and then allocate the new chunks from there. To service the\n   \n    sbrk\n   \n   request, the OS finds free physical pages, maps them into the address space of the requesting process, and then returns the value of the end of the new heap; at that point, a larger heap is available, and the request can be successfully serviced."
        },
        {
          "name": "Basic Strategies",
          "content": "Now that we have some machinery under our belt, let's go over some basic strategies for managing free space. These approaches are mostly based on pretty simple policies that you could think up yourself; try it before reading and see if you come up with all of the alternatives (or maybe some new ones!).\n\n\nThe ideal allocator is both fast and minimizes fragmentation. Unfortunately, because the stream of allocation and free requests can be arbitrary (after all, they are determined by the programmer), any particular strategy can do quite badly given the wrong set of inputs. Thus, we will not describe a “best” approach, but rather talk about some basics and discuss their pros and cons.\n\n\n\n\n**Best Fit**\n\n\nThe\n   **best fit**\n   strategy is quite simple: first, search through the free list and find chunks of free memory that are as big or bigger than the requested size. Then, return the one that is the smallest in that group of candidates; this is the so called best-fit chunk (it could be called smallest fit too). One pass through the free list is enough to find the correct block to return.\n\n\nThe intuition behind best fit is simple: by returning a block that is close to what the user asks, best fit tries to reduce wasted space. However, there is a cost; naive implementations pay a heavy performance penalty when performing an exhaustive search for the correct free block.\n\n\n\n\n**Worst Fit**\n\n\nThe\n   **worst fit**\n   approach is the opposite of best fit; find the largest chunk and return the requested amount; keep the remaining (large) chunk on the free list. Worst fit tries to thus leave big chunks free instead of lots of\n\n\nsmall chunks that can arise from a best-fit approach. Once again, however, a full search of free space is required, and thus this approach can be costly. Worse, most studies show that it performs badly, leading to excess fragmentation while still having high overheads.\n\n\n\n\n**First Fit**\n\n\nThe\n   **first fit**\n   method simply finds the first block that is big enough and returns the requested amount to the user. As before, the remaining free space is kept free for subsequent requests.\n\n\nFirst fit has the advantage of speed — no exhaustive search of all the free spaces are necessary — but sometimes pollutes the beginning of the free list with small objects. Thus, how the allocator manages the free list's order becomes an issue. One approach is to use\n   **address-based ordering**\n   ; by keeping the list ordered by the address of the free space, coalescing becomes easier, and fragmentation tends to be reduced.\n\n\n\n\n**Next Fit**\n\n\nInstead of always beginning the first-fit search at the beginning of the list, the\n   **next fit**\n   algorithm keeps an extra pointer to the location within the list where one was looking last. The idea is to spread the searches for free space throughout the list more uniformly, thus avoiding splintering of the beginning of the list. The performance of such an approach is quite similar to first fit, as an exhaustive search is once again avoided.\n\n\n\n\n**Examples**\n\n\nHere are a few examples of the above strategies. Envision a free list with three elements on it, of sizes 10, 30, and 20 (we'll ignore headers and other details here, instead just focusing on how strategies operate):\n\n\n\n\n![Diagram of a free list with three elements: 10, 30, and 20.](images/image_0054.jpeg)\n\n\nDiagram illustrating a free list structure. It starts with the label \"head\" followed by an arrow pointing to a circle containing the number 10. This is followed by an arrow pointing to a circle containing the number 30, then an arrow pointing to a circle containing the number 20, and finally an arrow pointing to the text \"NULL\".\n\n\nDiagram of a free list with three elements: 10, 30, and 20.\n\n\nAssume an allocation request of size 15. A best-fit approach would search the entire list and find that 20 was the best fit, as it is the smallest free space that can accommodate the request. The resulting free list:\n\n\n\n\n![Diagram of a free list after a best-fit allocation of size 15, leaving a chunk of size 5.](images/image_0055.jpeg)\n\n\nDiagram illustrating the resulting free list after a best-fit allocation of size 15. It starts with the label \"head\" followed by an arrow pointing to a circle containing the number 10. This is followed by an arrow pointing to a circle containing the number 30, then an arrow pointing to a circle containing the number 5, and finally an arrow pointing to the text \"NULL\".\n\n\nDiagram of a free list after a best-fit allocation of size 15, leaving a chunk of size 5.\n\n\nAs happens in this example, and often happens with a best-fit approach, a small free chunk is now left over. A worst-fit approach is similar but instead finds the largest chunk, in this example 30. The resulting list:\n\n\n\n\n![Diagram of a free list after a worst-fit allocation of size 15, leaving a chunk of size 15.](images/image_0056.jpeg)\n\n\nDiagram illustrating the resulting free list after a worst-fit allocation of size 15. It starts with the label \"head\" followed by an arrow pointing to a circle containing the number 10. This is followed by an arrow pointing to a circle containing the number 15, then an arrow pointing to a circle containing the number 20, and finally an arrow pointing to the text \"NULL\".\n\n\nDiagram of a free list after a worst-fit allocation of size 15, leaving a chunk of size 15.\n\n\nThe first-fit strategy, in this example, does the same thing as worst-fit, also finding the first free block that can satisfy the request. The difference is in the search cost; both best-fit and worst-fit look through the entire list; first-fit only examines free chunks until it finds one that fits, thus reducing search cost.\n\n\nThese examples just scratch the surface of allocation policies. More detailed analysis with real workloads and more complex allocator behaviors (e.g., coalescing) are required for a deeper understanding. Perhaps something for a homework section, you say?"
        },
        {
          "name": "Other Approaches",
          "content": "Beyond the basic approaches described above, there have been a host of suggested techniques and algorithms to improve memory allocation in some way. We list a few of them here for your consideration (i.e., to make you think about a little more than just best-fit allocation).\n\n\n\n\n**Segregated Lists**\n\n\nOne interesting approach that has been around for some time is the use of\n   **segregated lists**\n   . The basic idea is simple: if a particular application has one (or a few) popular-sized request that it makes, keep a separate list just to manage objects of that size; all other requests are forwarded to a more general memory allocator.\n\n\nThe benefits of such an approach are obvious. By having a chunk of memory dedicated for one particular size of requests, fragmentation is much less of a concern; moreover, allocation and free requests can be served quite quickly when they are of the right size, as no complicated search of a list is required.\n\n\nJust like any good idea, this approach introduces new complications into a system as well. For example, how much memory should one dedicate to the pool of memory that serves specialized requests of a given size, as opposed to the general pool? One particular allocator, the\n   **slab allocator**\n   by uber-engineer Jeff Bonwick (which was designed for use in the Solaris kernel), handles this issue in a rather nice way [B94].\n\n\nSpecifically, when the kernel boots up, it allocates a number of\n   **object caches**\n   for kernel objects that are likely to be requested frequently (such as locks, file-system inodes, etc.); the object caches thus are each segregated free lists of a given size and serve memory allocation and free requests quickly. When a given cache is running low on free space, it requests some\n   **slabs**\n   of memory from a more general memory allocator (the total amount requested being a multiple of the page size and the object in question). Conversely, when the reference counts of the objects within a given slab all go to zero, the general allocator can reclaim them from the specialized allocator, which is often done when the VM system needs more memory.\n\n\n**ASIDE: GREAT ENGINEERS ARE REALLY GREAT**\nEngineers like Jeff Bonwick (who not only wrote the slab allocator mentioned herein but also was the lead of an amazing file system, ZFS) are the heart of Silicon Valley. Behind almost any great product or technology is a human (or small group of humans) who are way above average in their talents, abilities, and dedication. As Mark Zuckerberg (of Facebook) says: “Someone who is exceptional in their role is not just a little better than someone who is pretty good. They are 100 times better.” This is why, still today, one or two people can start a company that changes the face of the world forever (think Google, Apple, or Facebook). Work hard and you might become such a “100x” person as well! Failing that, find a way to work\n   *with*\n   such a person; you’ll learn more in a day than most learn in a month.\n\n\nThe slab allocator also goes beyond most segregated list approaches by keeping free objects on the lists in a pre-initialized state. Bonwick shows that initialization and destruction of data structures is costly [B94]; by keeping freed objects in a particular list in their initialized state, the slab allocator thus avoids frequent initialization and destruction cycles per object and thus lowers overheads noticeably.\n\n\n\n\n**Buddy Allocation**\n\n\nBecause coalescing is critical for an allocator, some approaches have been designed around making coalescing simple. One good example is found in the\n   **binary buddy allocator**\n   [K65].\n\n\nIn such a system, free memory is first conceptually thought of as one big space of size\n   \n    2^N\n   \n   . When a request for memory is made, the search for free space recursively divides free space by two until a block that is big enough to accommodate the request is found (and a further split into two would result in a space that is too small). At this point, the requested block is returned to the user. Here is an example of a 64KB free space getting divided in the search for a 7KB block (Figure 17.8, page 15).\n\n\nIn the example, the leftmost 8KB block is allocated (as indicated by the darker shade of gray) and returned to the user; note that this scheme can suffer from\n   **internal fragmentation**\n   , as you are only allowed to give out power-of-two-sized blocks.\n\n\nThe beauty of buddy allocation is found in what happens when that block is freed. When returning the 8KB block to the free list, the allocator checks whether the “buddy” 8KB is free; if so, it coalesces the two blocks into a 16KB block. The allocator then checks if the buddy of the 16KB block is still free; if so, it coalesces those two blocks. This recursive coalescing process continues up the tree, either restoring the entire free space or stopping when a buddy is found to be in use.\n\n\n\n\n![Diagram illustrating a Buddy-managed Heap structure. A 64 KB block at the top splits into two 32 KB blocks. The left 32 KB block splits into two 16 KB blocks. The left 16 KB block splits into two 8 KB blocks. The right 16 KB block remains as a single 16 KB block. The right 32 KB block remains as a single 32 KB block.](images/image_0057.jpeg)\n\n\ngraph TD\n    A[64 KB] --> B[32 KB]\n    A --> C[32 KB]\n    B --> D[16 KB]\n    B --> E[16 KB]\n    D --> F[8 KB]\n    D --> G[8 KB]\n    E --> H[16 KB]\n    C --> I[32 KB]\nDiagram illustrating a Buddy-managed Heap structure. A 64 KB block at the top splits into two 32 KB blocks. The left 32 KB block splits into two 16 KB blocks. The left 16 KB block splits into two 8 KB blocks. The right 16 KB block remains as a single 16 KB block. The right 32 KB block remains as a single 32 KB block.\n\n\nFigure 17.8: Example Buddy-managed Heap\n\n\nThe reason buddy allocation works so well is that it is simple to determine the buddy of a particular block. How, you ask? Think about the addresses of the blocks in the free space above. If you think carefully enough, you'll see that the address of each buddy pair only differs by a single bit; which bit is determined by the level in the buddy tree. And thus you have a basic idea of how binary buddy allocation schemes work. For more detail, as always, see the Wilson survey [W+95].\n\n\n\n\n**Other Ideas**\n\n\nOne major problem with many of the approaches described above is their lack of\n   **scaling**\n   . Specifically, searching lists can be quite slow. Thus, advanced allocators use more complex data structures to address these costs, trading simplicity for performance. Examples include balanced binary trees, splay trees, or partially-ordered trees [W+95].\n\n\nGiven that modern systems often have multiple processors and run multi-threaded workloads (something you'll learn about in great detail in the section of the book on Concurrency), it is not surprising that a lot of effort has been spent making allocators work well on multiprocessor-based systems. Two wonderful examples are found in Berger et al. [B+00] and Evans [E06]; check them out for the details.\n\n\nThese are but two of the thousands of ideas people have had over time about memory allocators; read on your own if you are curious. Failing that, read about how the glibc allocator works [S15], to give you a sense of what the real world is like."
        }
      ]
    },
    {
      "name": "Paging: Introduction",
      "sections": [
        {
          "name": "A Simple Example And Overview",
          "content": "To help make this approach more clear, let's illustrate it with a simple example. Figure 18.1 (page 2) presents an example of a tiny address space, only 64 bytes total in size, with four 16-byte pages (virtual pages 0, 1, 2, and 3). Real address spaces are much bigger, of course, commonly 32 bits and thus 4-GB of address space, or even 64 bits\n   \n    1\n   \n   ; in the book, we'll often use tiny examples to make them easier to digest.\n\n\n1\n   \n   A 64-bit address space is hard to imagine, it is so amazingly large. An analogy might help: if you think of a 32-bit address space as the size of a tennis court, a 64-bit address space is about the size of Europe(!).\n\n\n\n\n![Figure 18.1: A Simple 64-byte Address Space. The diagram shows a vertical stack of four rectangular boxes representing pages. The left side has numerical labels: 0, 16, 32, 48, and 64. The right side has labels: (page 0 of the address space), (page 1), (page 2), and (page 3).](images/image_0058.jpeg)\n\n\n0 | □ | (page 0 of the address space)\n16 | □ | (page 1)\n32 | □ | (page 2)\n48 | □ | (page 3)\n64 |  | \n\n\nFigure 18.1: A Simple 64-byte Address Space. The diagram shows a vertical stack of four rectangular boxes representing pages. The left side has numerical labels: 0, 16, 32, 48, and 64. The right side has labels: (page 0 of the address space), (page 1), (page 2), and (page 3).\n\n\nFigure 18.1: A Simple 64-byte Address Space\n\n\nPhysical memory, as shown in Figure 18.2, also consists of a number of fixed-sized slots, in this case eight page frames (making for a 128-byte physical memory, also ridiculously small). As you can see in the diagram, the pages of the virtual address space have been placed at different locations throughout physical memory; the diagram also shows the OS using some of physical memory for itself.\n\n\nPaging, as we will see, has a number of advantages over our previous approaches. Probably the most important improvement will be\n   *flexibility*\n   : with a fully-developed paging approach, the system will be able to support the abstraction of an address space effectively, regardless of how a process uses the address space; we won't, for example, make assumptions about the direction the heap and stack grow and how they are used.\n\n\n\n\n![Figure 18.2: A 64-Byte Address Space In A 128-Byte Physical Memory. The diagram shows a vertical stack of eight rectangular boxes representing page frames. The left side has numerical labels: 0, 16, 32, 48, 64, 80, 96, 112, and 128. The right side has labels: page frame 0 of physical memory, page frame 1, page frame 2, page frame 3, page frame 4, page frame 5, page frame 6, and page frame 7. Some boxes are shaded gray and labeled (unused).](images/image_0059.jpeg)\n\n\n0 | reserved for OS | page frame 0 of physical memory\n16 | (unused) | page frame 1\n32 | page 3 of AS | page frame 2\n48 | page 0 of AS | page frame 3\n64 | (unused) | page frame 4\n80 | page 2 of AS | page frame 5\n96 | (unused) | page frame 6\n112 | page 1 of AS | page frame 7\n128 |  | \n\n\nFigure 18.2: A 64-Byte Address Space In A 128-Byte Physical Memory. The diagram shows a vertical stack of eight rectangular boxes representing page frames. The left side has numerical labels: 0, 16, 32, 48, 64, 80, 96, 112, and 128. The right side has labels: page frame 0 of physical memory, page frame 1, page frame 2, page frame 3, page frame 4, page frame 5, page frame 6, and page frame 7. Some boxes are shaded gray and labeled (unused).\n\n\nFigure 18.2: A 64-Byte Address Space In A 128-Byte Physical Memory\n\n\nAnother advantage is the\n   *simplicity*\n   of free-space management that paging affords. For example, when the OS wishes to place our tiny 64-byte address space into our eight-page physical memory, it simply finds four free pages; perhaps the OS keeps a\n   **free list**\n   of all free pages for this, and just grabs the first four free pages off of this list. In the example, the OS has placed virtual page 0 of the address space (AS) in physical frame 3, virtual page 1 of the AS in physical frame 7, page 2 in frame 5, and page 3 in frame 2. Page frames 1, 4, and 6 are currently free.\n\n\nTo record where each virtual page of the address space is placed in physical memory, the operating system usually keeps a\n   *per-process*\n   data structure known as a\n   **page table**\n   . The major role of the page table is to store\n   **address translations**\n   for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides. For our simple example (Figure 18.2, page 2), the page table would thus have the following four entries: (Virtual Page 0 → Physical Frame 3), (VP 1 → PF 7), (VP 2 → PF 5), and (VP 3 → PF 2).\n\n\nIt is important to remember that this page table is a\n   *per-process*\n   data structure (most page table structures we discuss are\n   *per-process*\n   structures; an exception we'll touch on is the\n   **inverted page table**\n   ). If another process were to run in our example above, the OS would have to manage a different page table for it, as its virtual pages obviously map to\n   *different*\n   physical pages (modulo any sharing going on).\n\n\nNow, we know enough to perform an address-translation example. Let's imagine the process with that tiny address space (64 bytes) is performing a memory access:\n\n\nmovl <virtual address>, %eax\nSpecifically, let's pay attention to the explicit load of the data from address\n   \n    <virtual address>\n   \n   into the register\n   \n    %eax\n   \n   (and thus ignore the instruction fetch that must have happened prior).\n\n\nTo\n   **translate**\n   this virtual address that the process generated, we have to first split it into two components: the\n   **virtual page number (VPN)**\n   , and the\n   **offset**\n   within the page. For this example, because the virtual address space of the process is 64 bytes, we need 6 bits total for our virtual address (\n   \n    2^6 = 64\n   \n   ). Thus, our virtual address can be conceptualized as follows:\n\n\n\nVa5 | Va4 | Va3 | Va2 | Va1 | Va0\n\n\nIn this diagram, Va5 is the highest-order bit of the virtual address, and Va0 the lowest-order bit. Because we know the page size (16 bytes), we can further divide the virtual address as follows:\n\n\n\nVPN | offset\nVa5 | Va4 | Va3 | Va2 | Va1 | Va0\n\n\nThe page size is 16 bytes in a 64-byte address space; thus we need to be able to select 4 pages, and the top 2 bits of the address do just that. Thus, we have a 2-bit virtual page number (VPN). The remaining bits tell us which byte of the page we are interested in, 4 bits in this case; we call this the offset.\n\n\nWhen a process generates a virtual address, the OS and hardware must combine to translate it into a meaningful physical address. For example, let us assume the load above was to virtual address 21:\n\n\nmovl 21, %eax\nTurning “21” into binary form, we get “010101”, and thus we can examine this virtual address and see how it breaks down into a virtual page number (VPN) and offset:\n\n\n\n\n![](images/image_0060.jpeg)\n\n\nVPN | offset\n0 | 1 | 0 | 1 | 0 | 1\n\n\nThus, the virtual address “21” is on the 5th (“0101”th) byte of virtual page “01” (or 1). With our virtual page number, we can now index our page table and find which physical frame virtual page 1 resides within. In the page table above the\n   **physical frame number (PFN)**\n   (also sometimes called the\n   **physical page number or PPN**\n   ) is 7 (binary 111). Thus, we can translate this virtual address by replacing the VPN with the PFN and then issue the load to physical memory (Figure 18.3).\n\n\n\n\n![](images/image_0061.jpeg)\n\n\nVirtual Address\n\n\n\nVPN | offset\n0 | 1 | 0 | 1 | 0 | 1\n\n\n↓ ↓ ↓ ↓ ↓ ↓\n\n\nAddress Translation\n\n\n↓ ↓ ↓ ↓ ↓ ↓\n\n\nPhysical Address\n\n\n\nPFN | offset\n1 | 1 | 1 | 0 | 1 | 0\n\n\nFigure 18.3: The Address Translation Process\n\n\n\n0 | page table:\n      \n      3 7 5 2 | page frame 0 of physical memory\n16 | (unused) | page frame 1\n32 | page 3 of AS | page frame 2\n48 | page 0 of AS | page frame 3\n64 | (unused) | page frame 4\n80 | page 2 of AS | page frame 5\n96 | (unused) | page frame 6\n112 | page 1 of AS | page frame 7\n128 |  | \n\n\nFigure 18.4:\n   **Example: Page Table in Kernel Physical Memory**\n\n\nNote the offset stays the same (i.e., it is not translated), because the offset just tells us which byte\n   *within*\n   the page we want. Our final physical address is 1110101 (117 in decimal), and is exactly where we want our load to fetch data from (Figure 18.2, page 2).\n\n\nWith this basic overview in mind, we can now ask (and hopefully, answer) a few basic questions you may have about paging. For example, where are these page tables stored? What are the typical contents of the page table, and how big are the tables? Does paging make the system (too) slow? These and other beguiling questions are answered, at least in part, in the text below. Read on!"
        },
        {
          "name": "Where Are Page Tables Stored?",
          "content": "Page tables can get terribly large, much bigger than the small segment table or base/bounds pair we have discussed previously. For example, imagine a typical 32-bit address space, with 4KB pages. This virtual address splits into a 20-bit VPN and 12-bit offset (recall that 10 bits would be needed for a 1KB page size, and just add two more to get to 4KB).\n\n\nA 20-bit VPN implies that there are\n   \n    2^{20}\n   \n   translations that the OS would have to manage for each process (that's roughly a million); assuming we need 4 bytes per\n   **page table entry (PTE)**\n   to hold the physical translation plus any other useful stuff, we get an immense 4MB of memory needed for each page table! That is pretty large. Now imagine there are 100 processes running: this means the OS would need 400MB of memory just for all those address translations! Even in the modern era, where\n\n\n\n\n**ASIDE: DATA STRUCTURE — THE PAGE TABLE**\n\n\nOne of the most important data structures in the memory management subsystem of a modern OS is the\n   **page table**\n   . In general, a page table stores\n   **virtual-to-physical address translations**\n   , thus letting the system know where each page of an address space actually resides in physical memory. Because each address space requires such translations, in general there is one page table per process in the system. The exact structure of the page table is either determined by the hardware (older systems) or can be more flexibly managed by the OS (modern systems).\n\n\nmachines have gigabytes of memory, it seems a little crazy to use a large chunk of it just for translations, no? And we won't even think about how big such a page table would be for a 64-bit address space; that would be too gruesome and perhaps scare you off entirely.\n\n\nBecause page tables are so big, we don't keep any special on-chip hardware in the MMU to store the page table of the currently-running process. Instead, we store the page table for each process in\n   *memory*\n   somewhere. Let's assume for now that the page tables live in physical memory that the OS manages; later we'll see that much of OS memory itself can be virtualized, and thus page tables can be stored in OS virtual memory (and even swapped to disk), but that is too confusing right now, so we'll ignore it. In Figure 18.4 (page 5) is a picture of a page table in OS memory; see the tiny set of translations in there?"
        },
        {
          "name": "What’s Actually In The Page Table?",
          "content": "Let's talk a little about page table organization. The page table is just a data structure that is used to map virtual addresses (or really, virtual page numbers) to physical addresses (physical frame numbers). Thus, any data structure could work. The simplest form is called a\n   **linear page table**\n   , which is just an array. The OS\n   *indexes*\n   the array by the virtual page number (VPN), and looks up the page-table entry (PTE) at that index in order to find the desired physical frame number (PFN). For now, we will assume this simple linear structure; in later chapters, we will make use of more advanced data structures to help solve some problems with paging.\n\n\nAs for the contents of each PTE, we have a number of different bits in there worth understanding at some level. A\n   **valid bit**\n   is common to indicate whether the particular translation is valid; for example, when a program starts running, it will have code and heap at one end of its address space, and the stack at the other. All the unused space in-between will be marked\n   **invalid**\n   , and if the process tries to access such memory, it will generate a trap to the OS which will likely terminate the process. Thus, the valid bit is crucial for supporting a sparse address space; by simply marking all the unused pages in the address space invalid, we remove the need to allocate physical frames for those pages and thus save a great deal of memory.\n\n\n\n31 |  | 12 | 11 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\n | PFN |  |  |  |  | G | PAT | D | A | PCD | PWT | U/S | R/W | P\n\n\nFigure 18.5: An x86 Page Table Entry (PTE)\n\n\nWe also might have\n   **protection bits**\n   , indicating whether the page could be read from, written to, or executed from. Again, accessing a page in a way not allowed by these bits will generate a trap to the OS.\n\n\nThere are a couple of other bits that are important but we won't talk about much for now. A\n   **present bit**\n   indicates whether this page is in physical memory or on disk (i.e., it has been\n   **swapped out**\n   ). We will understand this machinery further when we study how to\n   **swap**\n   parts of the address space to disk to support address spaces that are larger than physical memory; swapping allows the OS to free up physical memory by moving rarely-used pages to disk. A\n   **dirty bit**\n   is also common, indicating whether the page has been modified since it was brought into memory.\n\n\nA\n   **reference bit**\n   (a.k.a.\n   **accessed bit**\n   ) is sometimes used to track whether a page has been accessed, and is useful in determining which pages are popular and thus should be kept in memory; such knowledge is critical during\n   **page replacement**\n   , a topic we will study in great detail in subsequent chapters.\n\n\nFigure 18.5 shows an example page table entry from the x86 architecture [I09]. It contains a present bit (P); a read/write bit (R/W) which determines if writes are allowed to this page; a user/supervisor bit (U/S) which determines if user-mode processes can access the page; a few bits (PWT, PCD, PAT, and G) that determine how hardware caching works for these pages; an accessed bit (A) and a dirty bit (D); and finally, the page frame number (PFN) itself.\n\n\nRead the Intel Architecture Manuals [I09] for more details on x86 paging support. Be forewarned, however; reading manuals such as these, while quite informative (and certainly necessary for those who write code to use such page tables in the OS), can be challenging at first. A little patience, and a lot of desire, is required.\n\n\n\n\n**ASIDE: WHY NO VALID BIT?**\n\n\nYou may notice that in the Intel example, there are no separate valid and present bits, but rather just a present bit (P). If that bit is set (P=1), it means the page is both present and valid. If not (P=0), it means that the page may not be present in memory (but is valid), or may not be valid. An access to a page with P=0 will trigger a trap to the OS; the OS must then use additional structures it keeps to determine whether the page is valid (and thus perhaps should be swapped back in) or not (and thus the program is attempting to access memory illegally). This sort of judiciousness is common in hardware, which often just provide the minimal set of features upon which the OS can build a full service."
        },
        {
          "name": "Paging: Also Too Slow",
          "content": "With page tables in memory, we already know that they might be too big. As it turns out, they can slow things down too. For example, take our simple instruction:\n\n\nmovl 21, %eax\nAgain, let's just examine the explicit reference to address 21 and not worry about the instruction fetch. In this example, we'll assume the hardware performs the translation for us. To fetch the desired data, the system must first\n   **translate**\n   the virtual address (21) into the correct physical address (117). Thus, before fetching the data from address 117, the system must first fetch the proper page table entry from the process's page table, perform the translation, and then load the data from physical memory.\n\n\nTo do so, the hardware must know where the page table is for the currently-running process. Let's assume for now that a single\n   **page-table base register**\n   contains the physical address of the starting location of the page table. To find the location of the desired PTE, the hardware will thus perform the following functions:\n\n\nVPN      = (VirtualAddress & VPN_MASK) >> SHIFT\nPTEAddr  = PageTableBaseRegister + (VPN * sizeof(PTE))\nIn our example,\n   \n    VPN_MASK\n   \n   would be set to 0x30 (hex 30, or binary 110000) which picks out the VPN bits from the full virtual address;\n   \n    SHIFT\n   \n   is set to 4 (the number of bits in the offset), such that we move the VPN bits down to form the correct integer virtual page number. For example, with virtual address 21 (010101), and masking turns this value into 010000; the shift turns it into 01, or virtual page 1, as desired. We then use this value as an index into the array of PTEs pointed to by the page table base register.\n\n\nOnce this physical address is known, the hardware can fetch the PTE from memory, extract the PFN, and concatenate it with the offset from the virtual address to form the desired physical address. Specifically, you can think of the PFN being left-shifted by\n   \n    SHIFT\n   \n   , and then bitwise OR'd with the offset to form the final address as follows:\n\n\noffset    = VirtualAddress & OFFSET_MASK\nPhysAddr  = (PFN << SHIFT) | offset\nFinally, the hardware can fetch the desired data from memory and put it into register\n   \n    %eax\n   \n   . The program has now succeeded at loading a value from memory!\n\n\nTo summarize, we now describe the initial protocol for what happens on each memory reference. Figure 18.6 (page 9) shows the approach. For every memory reference (whether an instruction fetch or an explicit load or store), paging requires us to perform one extra memory reference in order to first fetch the translation from the page table. That is a lot of\n\n\n1 // Extract the VPN from the virtual address\n2 VPN = (VirtualAddress & VPN_MASK) >> SHIFT\n3\n4 // Form the address of the page-table entry (PTE)\n5 PTEAddr = PIBR + (VPN * sizeof(PTE))\n6\n7 // Fetch the PTE\n8 PTE = AccessMemory(PTEAddr)\n9\n10 // Check if process can access the page\n11 if (PTE.Valid == False)\n12     RaiseException(SEGMENTATION_FAULT)\n13 else if (CanAccess(PTE.ProtectBits) == False)\n14     RaiseException(PROTECTION_FAULT)\n15 else\n16     // Access OK: form physical address and fetch it\n17     offset = VirtualAddress & OFFSET_MASK\n18     PhysAddr = (PTE.PFN << PFN_SHIFT) | offset\n19     Register = AccessMemory(PhysAddr)\nFigure 18.6:\n   **Accessing Memory With Paging**\n\n\nwork! Extra memory references are costly, and in this case will likely slow down the process by a factor of two or more.\n\n\nAnd now you can hopefully see that there are\n   *two*\n   real problems that we must solve. Without careful design of both hardware and software, page tables will cause the system to run too slowly, as well as take up too much memory. While seemingly a great solution for our memory virtualization needs, these two crucial problems must first be overcome."
        },
        {
          "name": "A Memory Trace",
          "content": "Before closing, we now trace through a simple memory access example to demonstrate all of the resulting memory accesses that occur when using paging. The code snippet (in C, in a file called\n   \n    array.c\n   \n   ) that we are interested in is as follows:\n\n\nint array[1000];\n...\nfor (i = 0; i < 1000; i++)\n    array[i] = 0;\nWe compile\n   \n    array.c\n   \n   and run it with the following commands:\n\n\nprompt> gcc -o array array.c -Wall -O\nprompt> ./array\nOf course, to truly understand what memory accesses this code snippet (which simply initializes an array) will make, we'll have to know (or assume) a few more things. First, we'll have to\n   **disassemble**\n   the resulting binary (using\n   \n    objdump\n   \n   on Linux, or\n   \n    otool\n   \n   on a Mac) to see what assembly instructions are used to initialize the array in a loop. Here is the resulting assembly code:\n\n\n1024 movl $0x0, (%edi, %eax, 4)\n1028 incl %eax\n1032 cmpl $0x03e8, %eax\n1036 jne 1024\nThe code, if you know a little\n   \n    x86\n   \n   , is actually quite easy to understand\n   \n    2\n   \n   . The first instruction moves the value zero (shown as\n   \n    $0x0\n   \n   ) into the virtual memory address of the location of the array; this address is computed by taking the contents of\n   \n    %edi\n   \n   and adding\n   \n    %eax\n   \n   multiplied by four to it. Thus,\n   \n    %edi\n   \n   holds the base address of the array, whereas\n   \n    %eax\n   \n   holds the array index (\n   \n    i\n   \n   ); we multiply by four because the array is an array of integers, each of size four bytes.\n\n\nThe second instruction increments the array index held in\n   \n    %eax\n   \n   , and the third instruction compares the contents of that register to the hex value\n   \n    0x03e8\n   \n   , or decimal 1000. If the comparison shows that two values are not yet equal (which is what the\n   \n    jne\n   \n   instruction tests), the fourth instruction jumps back to the top of the loop.\n\n\nTo understand which memory accesses this instruction sequence makes (at both the virtual and physical levels), we'll have to assume something about where in virtual memory the code snippet and array are found, as well as the contents and location of the page table.\n\n\nFor this example, we assume a virtual address space of size 64KB (unrealistically small). We also assume a page size of 1KB.\n\n\nAll we need to know now are the contents of the page table, and its location in physical memory. Let's assume we have a linear (array-based) page table and that it is located at physical address 1KB (1024).\n\n\nAs for its contents, there are just a few virtual pages we need to worry about having mapped for this example. First, there is the virtual page the code lives on. Because the page size is 1KB, virtual address 1024 resides on the second page of the virtual address space (VPN=1, as VPN=0 is the first page). Let's assume this virtual page maps to physical frame 4 (VPN 1 → PFN 4).\n\n\nNext, there is the array itself. Its size is 4000 bytes (1000 integers), and we assume that it resides at virtual addresses 40000 through 44000 (not including the last byte). The virtual pages for this decimal range are VPN=39 ... VPN=42. Thus, we need mappings for these pages. Let's assume these virtual-to-physical mappings for the example: (VPN 39 → PFN 7), (VPN 40 → PFN 8), (VPN 41 → PFN 9), (VPN 42 → PFN 10).\n\n\n2\n   \n   We are cheating a little bit here, assuming each instruction is four bytes in size for simplicity; in actuality,\n   \n    x86\n   \n   instructions are variable-sized.\n\n\n\n\n![Figure 18.7: A Virtual (And Physical) Memory Trace. The figure consists of four vertically stacked line graphs sharing a common x-axis labeled 'Memory Access' from 0 to 50. 1. Top graph: 'Page Table (PA)'. It shows light gray squares at virtual addresses 1024, 1074, 1124, 1174, and 1224. Arrows point to 'PageTable[1]' at 1024 and 'PageTable[39]' at 1174. 2. Second graph: 'Array (VA)'. It shows dark gray squares at virtual addresses 40000, 40050, and 40100. The label 'mov' is placed next to the 40100 point. 3. Third graph: 'Array (PA)'. It shows dark gray squares at physical addresses 7232, 7282, and 7332. 4. Bottom graph: 'Code (VA)'. It shows black squares at virtual addresses 1024, 1074, and 1124. The labels 'mov', 'inc', 'cmp', and 'jne' are placed next to the 1074, 1124, and 1024 points respectively. The physical addresses for the code are 4096, 4146, and 4196.](images/image_0062.jpeg)\n\n\nFigure 18.7: A Virtual (And Physical) Memory Trace. The figure consists of four vertically stacked line graphs sharing a common x-axis labeled 'Memory Access' from 0 to 50. 1. Top graph: 'Page Table (PA)'. It shows light gray squares at virtual addresses 1024, 1074, 1124, 1174, and 1224. Arrows point to 'PageTable[1]' at 1024 and 'PageTable[39]' at 1174. 2. Second graph: 'Array (VA)'. It shows dark gray squares at virtual addresses 40000, 40050, and 40100. The label 'mov' is placed next to the 40100 point. 3. Third graph: 'Array (PA)'. It shows dark gray squares at physical addresses 7232, 7282, and 7332. 4. Bottom graph: 'Code (VA)'. It shows black squares at virtual addresses 1024, 1074, and 1124. The labels 'mov', 'inc', 'cmp', and 'jne' are placed next to the 1074, 1124, and 1024 points respectively. The physical addresses for the code are 4096, 4146, and 4196.\n\n\nFigure 18.7: A Virtual (And Physical) Memory Trace\n\n\nWe are now ready to trace the memory references of the program. When it runs, each instruction fetch will generate two memory references: one to the page table to find the physical frame that the instruction resides within, and one to the instruction itself to fetch it to the CPU for processing. In addition, there is one explicit memory reference in the form of the\n   \n    mov\n   \n   instruction; this adds another page table access first (to translate the array virtual address to the correct physical one) and then the array access itself.\n\n\nThe entire process, for the first five loop iterations, is depicted in Figure 18.7 (page 11). The bottom most graph shows the instruction memory references on the y-axis in black (with virtual addresses on the left, and the actual physical addresses on the right); the middle graph shows array accesses in dark gray (again with virtual on left and physical on right); finally, the topmost graph shows page table memory accesses in light gray (just physical, as the page table in this example resides in physical memory). The x-axis, for the entire trace, shows memory accesses across the first five iterations of the loop; there are 10 memory accesses per loop, which includes four instruction fetches, one explicit update of memory, and five page table accesses to translate those four fetches and one explicit update.\n\n\nSee if you can make sense of the patterns that show up in this visualization. In particular, what will change as the loop continues to run beyond these first five iterations? Which new memory locations will be accessed? Can you figure it out?\n\n\nThis has just been the simplest of examples (only a few lines of C code), and yet you might already be able to sense the complexity of understanding the actual memory behavior of real applications. Don't worry: it definitely gets worse, because the mechanisms we are about to introduce only complicate this already complex machinery. Sorry\n   \n    3\n   \n   !"
        }
      ]
    },
    {
      "name": "Paging: Faster Translations (TLBs)",
      "sections": [
        {
          "name": "TLB Basic Algorithm",
          "content": "Figure 19.1 shows a rough sketch of how hardware might handle a virtual address translation, assuming a simple\n   **linear page table**\n   (i.e., the page table is an array) and a\n   **hardware-managed TLB**\n   (i.e., the hardware handles much of the responsibility of page table accesses; we'll explain more about this below).\n\n\nThe algorithm the hardware follows works like this: first, extract the virtual page number (VPN) from the virtual address (Line 1 in Figure 19.1), and check if the TLB holds the translation for this VPN (Line 2). If it does, we have a\n   **TLB hit**\n   , which means the TLB holds the translation. Success! We can now extract the page frame number (PFN) from the relevant TLB entry, concatenate that onto the offset from the original virtual address, and form the desired physical address (PA), and access memory (Lines 5–7), assuming protection checks do not fail (Line 4).\n\n\nIf the CPU does not find the translation in the TLB (a\n   **TLB miss**\n   ), we have some more work to do. In this example, the hardware accesses the page table to find the translation (Lines 11–12), and, assuming that the virtual memory reference generated by the process is valid and accessible (Lines 13, 15), updates the TLB with the translation (Line 18). These set of actions are costly, primarily because of the extra memory reference needed to access the page table (Line 12). Finally, once the TLB is updated, the hardware retries the instruction; this time, the translation is found in the TLB, and the memory reference is processed quickly.\n\n\nThe TLB, like all caches, is built on the premise that in the common case, translations are found in the cache (i.e., are hits). If so, little overhead is added, as the TLB is found near the processing core and is designed to be quite fast. When a miss occurs, the high cost of paging is incurred; the page table must be accessed to find the translation, and an extra memory reference (or more, with more complex page tables) results. If this happens often, the program will likely run noticeably more slowly; memory accesses, relative to most CPU instructions, are quite costly, and TLB misses lead to more memory accesses. Thus, it is our hope to avoid TLB misses as much as we can."
        },
        {
          "name": "Example: Accessing An Array",
          "content": "To make clear the operation of a TLB, let's examine a simple virtual address trace and see how a TLB can improve its performance. In this example, let's assume we have an array of 10 4-byte integers in memory, starting at virtual address 100. Assume further that we have a small 8-bit virtual address space, with 16-byte pages; thus, a virtual address breaks down into a 4-bit VPN (there are 16 virtual pages) and a 4-bit offset (there are 16 bytes on each of those pages).\n\n\nFigure 19.2 (page 4) shows the array laid out on the 16 16-byte pages of the system. As you can see, the array's first entry (\n   \n    a[0]\n   \n   ) begins on (VPN=06, offset=04); only three 4-byte integers fit onto that page. The array continues onto the next page (VPN=07), where the next four entries (\n   \n    a[3] \\dots a[6]\n   \n   ) are found. Finally, the last three entries of the 10-entry array (\n   \n    a[7] \\dots a[9]\n   \n   ) are located on the next page of the address space (VPN=08).\n\n\nNow let's consider a simple loop that accesses each array element, something that would look like this in C:\n\n\nint i, sum = 0;\nfor (i = 0; i < 10; i++) {\n    sum += a[i];\n}\nFor the sake of simplicity, we will pretend that the only memory accesses the loop generates are to the array (ignoring the variables\n   \n    i\n   \n   and\n   \n    sum\n   \n   , as well as the instructions themselves). When the first array element (\n   \n    a[0]\n   \n   ) is accessed, the CPU will see a load to virtual address 100. The hardware extracts the VPN from this (VPN=06), and uses that to check the TLB for a valid translation. Assuming this is the first time the program accesses the array, the result will be a TLB miss.\n\n\nThe next access is to\n   \n    a[1]\n   \n   , and there is some good news here: a TLB hit! Because the second element of the array is packed next to the first, it lives on the same page; because we've already accessed this page when accessing the first element of the array, the translation is already loaded\n\n\n\n | Offset\n | 00 | 04 | 08 | 12 | 16\nVPN = 00 |  |  |  |  | \nVPN = 01 |  |  |  |  | \nVPN = 02 |  |  |  |  | \nVPN = 03 |  |  |  |  | \nVPN = 04 |  |  |  |  | \nVPN = 05 |  |  |  |  | \nVPN = 06 |  | a[0] | a[1] | a[2] | \nVPN = 07 | a[3] | a[4] | a[5] | a[6] | \nVPN = 08 | a[7] | a[8] | a[9] |  | \nVPN = 09 |  |  |  |  | \nVPN = 10 |  |  |  |  | \nVPN = 11 |  |  |  |  | \nVPN = 12 |  |  |  |  | \nVPN = 13 |  |  |  |  | \nVPN = 14 |  |  |  |  | \nVPN = 15 |  |  |  |  | \n\n\nFigure 19.2:\n   **Example: An Array In A Tiny Address Space**\n\n\ninto the TLB. And hence the reason for our success. Access to\n   \n    a[2]\n   \n   encounters similar success (another hit), because it too lives on the same page as\n   \n    a[0]\n   \n   and\n   \n    a[1]\n   \n   .\n\n\nUnfortunately, when the program accesses\n   \n    a[3]\n   \n   , we encounter another TLB miss. However, once again, the next entries (\n   \n    a[4]\n   \n   ...\n   \n    a[6]\n   \n   ) will hit in the TLB, as they all reside on the same page in memory.\n\n\nFinally, access to\n   \n    a[7]\n   \n   causes one last TLB miss. The hardware once again consults the page table to figure out the location of this virtual page in physical memory, and updates the TLB accordingly. The final two accesses (\n   \n    a[8]\n   \n   and\n   \n    a[9]\n   \n   ) receive the benefits of this TLB update; when the hardware looks in the TLB for their translations, two more hits result.\n\n\nLet us summarize TLB activity during our ten accesses to the array:\n   **miss**\n   , hit, hit,\n   **miss**\n   , hit, hit, hit,\n   **miss**\n   , hit, hit. Thus, our TLB\n   **hit rate**\n   , which is the number of hits divided by the total number of accesses, is 70%. Although this is not too high (indeed, we desire hit rates that approach 100%), it is non-zero, which may be a surprise. Even though this is the first time the program accesses the array, the TLB improves performance due to\n   **spatial locality**\n   . The elements of the array are packed tightly into pages (i.e., they are close to one another in\n   **space**\n   ), and thus only the first access to an element on a page yields a TLB miss.\n\n\nAlso note the role that page size plays in this example. If the page size\n\n\n\n\n**TIP: USE CACHING WHEN POSSIBLE**\n\n\nCaching is one of the most fundamental performance techniques in computer systems, one that is used again and again to make the “common-case fast” [HP06]. The idea behind hardware caches is to take advantage of\n   **locality**\n   in instruction and data references. There are usually two types of locality:\n   **temporal locality**\n   and\n   **spatial locality**\n   . With temporal locality, the idea is that an instruction or data item that has been recently accessed will likely be re-accessed soon in the future. Think of loop variables or instructions in a loop; they are accessed repeatedly over time. With spatial locality, the idea is that if a program accesses memory at address\n   \n    x\n   \n   , it will likely soon access memory near\n   \n    x\n   \n   . Imagine here streaming through an array of some kind, accessing one element and then the next. Of course, these properties depend on the exact nature of the program, and thus are not hard-and-fast laws but more like rules of thumb.\n\n\nHardware caches, whether for instructions, data, or address translations (as in our TLB) take advantage of locality by keeping copies of memory in small, fast on-chip memory. Instead of having to go to a (slow) memory to satisfy a request, the processor can first check if a nearby copy exists in a cache; if it does, the processor can access it quickly (i.e., in a few CPU cycles) and avoid spending the costly time it takes to access memory (many nanoseconds).\n\n\nYou might be wondering: if caches (like the TLB) are so great, why don't we just make bigger caches and keep all of our data in them? Unfortunately, this is where we run into more fundamental laws like those of physics. If you want a fast cache, it has to be small, as issues like the speed-of-light and other physical constraints become relevant. Any large cache by definition is slow, and thus defeats the purpose. Thus, we are stuck with small, fast caches; the question that remains is how to best use them to improve performance.\n\n\nhad simply been twice as big (32 bytes, not 16), the array access would suffer even fewer misses. As typical page sizes are more like 4KB, these types of dense, array-based accesses achieve excellent TLB performance, encountering only a single miss per page of accesses.\n\n\nOne last point about TLB performance: if the program, soon after this loop completes, accesses the array again, we'd likely see an even better result, assuming that we have a big enough TLB to cache the needed translations: hit, hit, hit, hit, hit, hit, hit, hit, hit, hit, hit. In this case, the TLB hit rate would be high because of\n   **temporal locality**\n   , i.e., the quick re-referencing of memory items in\n   **time**\n   . Like any cache, TLBs rely upon both spatial and temporal locality for success, which are program properties. If the program of interest exhibits such locality (and many programs do), the TLB hit rate will likely be high.\n\n\n1  VPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2  (Success, TlbEntry) = TLB_Lookup (VPN)\n3  if (Success == True)    // TLB Hit\n4      if (CanAccess (TlbEntry.ProtectBits) == True)\n5          Offset = VirtualAddress & OFFSET_MASK\n6          PhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7          Register = AccessMemory (PhysAddr)\n8      else\n9          RaiseException (PROTECTION_FAULT)\n10 else                    // TLB Miss\n11     RaiseException (TLB_MISS)\n\nFigure 19.3: TLB Control Flow Algorithm (OS Handled)"
        },
        {
          "name": "Who Handles The TLB Miss?",
          "content": "One question that we must answer: who handles a TLB miss? Two answers are possible: the hardware, or the software (OS). In the olden days, the hardware had complex instruction sets (sometimes called\n   **CISC**\n   , for complex-instruction set computers) and the people who built the hardware didn't much trust those sneaky OS people. Thus, the hardware would handle the TLB miss entirely. To do this, the hardware has to know exactly\n   *where*\n   the page tables are located in memory (via a\n   **page-table base register**\n   , used in Line 11 in Figure 19.1), as well as their\n   *exact format*\n   ; on a miss, the hardware would “walk” the page table, find the correct page-table entry and extract the desired translation, update the TLB with the translation, and retry the instruction. An example of an “older” architecture that has\n   **hardware-managed TLBs**\n   is the Intel x86 architecture, which uses a fixed\n   **multi-level page table**\n   (see the next chapter for details); the current page table is pointed to by the CR3 register [l09].\n\n\nMore modern architectures (e.g., MIPS R10k [H93] or Sun's SPARC v9 [WG00], both\n   **RISC**\n   or reduced-instruction set computers) have what is known as a\n   **software-managed TLB**\n   . On a TLB miss, the hardware simply raises an exception (line 11 in Figure 19.3), which pauses the current instruction stream, raises the privilege level to kernel mode, and jumps to a\n   **trap handler**\n   . As you might guess, this trap handler is code within the OS that is written with the express purpose of handling TLB misses. When run, the code will lookup the translation in the page table, use special “privileged” instructions to update the TLB, and return from the trap; at this point, the hardware retries the instruction (resulting in a TLB hit).\n\n\nLet's discuss a couple of important details. First, the return-from-trap instruction needs to be a little different than the return-from-trap we saw before when servicing a system call. In the latter case, the return-from-trap should resume execution at the instruction\n   *after*\n   the trap into the OS, just as a return from a procedure call returns to the instruction immediately following the call into the procedure. In the former case, when returning from a TLB miss-handling trap, the hardware must resume execution at the instruction that\n   *caused*\n   the trap; this retry thus lets the in-\n\n\n\n\n**ASIDE: RISC VS. CISC**\n\n\nIn the 1980's, a great battle took place in the computer architecture community. On one side was the\n   **CISC**\n   camp, which stood for\n   **Complex Instruction Set Computing**\n   ; on the other side was\n   **RISC**\n   , for\n   **Reduced Instruction Set Computing**\n   [PS81]. The RISC side was spear-headed by David Patterson at Berkeley and John Hennessy at Stanford (who are also co-authors of some famous books [HP06]), although later John Cocke was recognized with a Turing award for his earliest work on RISC [CM00].\n\n\nCISC instruction sets tend to have a lot of instructions in them, and each instruction is relatively powerful. For example, you might see a string copy, which takes two pointers and a length and copies bytes from source to destination. The idea behind CISC was that instructions should be high-level primitives, to make the assembly language itself easier to use, and to make code more compact.\n\n\nRISC instruction sets are exactly the opposite. A key observation behind RISC is that instruction sets are really compiler targets, and all compilers really want are a few simple primitives that they can use to generate high-performance code. Thus, RISC proponents argued, let's rip out as much from the hardware as possible (especially the microcode), and make what's left simple, uniform, and fast.\n\n\nIn the early days, RISC chips made a huge impact, as they were noticeably faster [BC91]; many papers were written; a few companies were formed (e.g., MIPS and Sun). However, as time progressed, CISC manufacturers such as Intel incorporated many RISC techniques into the core of their processors, for example by adding early pipeline stages that transformed complex instructions into micro-instructions which could then be processed in a RISC-like manner. These innovations, plus a growing number of transistors on each chip, allowed CISC to remain competitive. The end result is that the debate died down, and today both types of processors can be made to run fast.\n\n\nstruction run again, this time resulting in a TLB hit. Thus, depending on how a trap or exception was caused, the hardware must save a different PC when trapping into the OS, in order to resume properly when the time to do so arrives.\n\n\nSecond, when running the TLB miss-handling code, the OS needs to be extra careful not to cause an infinite chain of TLB misses to occur. Many solutions exist; for example, you could keep TLB miss handlers in physical memory (where they are\n   **unmapped**\n   and not subject to address translation), or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself; these\n   **wired**\n   translations always hit in the TLB.\n\n\nThe primary advantage of the software-managed approach is\n   *flexibility*\n   : the OS can use any data structure it wants to implement the page\n\n\n**ASIDE: TLB VALID BIT\n   \n    \\neq\n   \n   PAGE TABLE VALID BIT**\nA common mistake is to confuse the valid bits found in a TLB with those found in a page table. In a page table, when a page-table entry (PTE) is marked invalid, it means that the page has not been allocated by the process, and should not be accessed by a correctly-working program. The usual response when an invalid page is accessed is to trap to the OS, which will respond by killing the process.\n\n\nA TLB valid bit, in contrast, simply refers to whether a TLB entry has a valid translation within it. When a system boots, for example, a common initial state for each TLB entry is to be set to invalid, because no address translations are yet cached there. Once virtual memory is enabled, and once programs start running and accessing their virtual address spaces, the TLB is slowly populated, and thus valid entries soon fill the TLB.\n\n\nThe TLB valid bit is quite useful when performing a context switch too, as we'll discuss further below. By setting all TLB entries to invalid, the system can ensure that the about-to-be-run process does not accidentally use a virtual-to-physical translation from a previous process.\n\n\ntable, without necessitating hardware change. Another advantage is\n   *simplicity*\n   , as seen in the TLB control flow (line 11 in Figure 19.3, in contrast to lines 11–19 in Figure 19.1). The hardware doesn't do much on a miss: just raise an exception and let the OS TLB miss handler do the rest."
        },
        {
          "name": "TLB Contents: What’s In There?",
          "content": "Let's look at the contents of the hardware TLB in more detail. A typical TLB might have 32, 64, or 128 entries and be what is called\n   **fully associative**\n   . Basically, this just means that any given translation can be anywhere in the TLB, and that the hardware will search the entire TLB in parallel to find the desired translation. A TLB entry might look like this:\n\n\nVPN | PFN | other bits\n\n\nNote that both the VPN and PFN are present in each entry, as a translation could end up in any of these locations (in hardware terms, the TLB is known as a\n   **fully-associative**\n   cache). The hardware searches the entries in parallel to see if there is a match.\n\n\nMore interesting are the “other bits”. For example, the TLB commonly has a\n   **valid**\n   bit, which says whether the entry has a valid translation or not. Also common are\n   **protection**\n   bits, which determine how a page can be accessed (as in the page table). For example, code pages might be marked\n   *read and execute*\n   , whereas heap pages might be marked\n   *read and write*\n   . There may also be a few other fields, including an\n   **address-space identifier**\n   , a\n   **dirty bit**\n   , and so forth; see below for more information."
        },
        {
          "name": "TLB Issue: Context Switches",
          "content": "With TLBs, new issues arise when switching between processes (and hence address spaces). Specifically, the TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes. As a result, when switching from one process to another, the hardware or OS (or both) must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process.\n\n\nTo understand this situation better, let's look at an example. When one process (P1) is running, it assumes the TLB might be caching translations that are valid for it, i.e., that come from P1's page table. Assume, for this example, that the 10th virtual page of P1 is mapped to physical frame 100.\n\n\nIn this example, assume another process (P2) exists, and the OS soon might decide to perform a context switch and run it. Assume here that the 10th virtual page of P2 is mapped to physical frame 170. If entries for both processes were in the TLB, the contents of the TLB would be:\n\n\n\nVPN | PFN | valid | prot\n10 | 100 | 1 | rwx\n— | — | 0 | —\n10 | 170 | 1 | rwx\n— | — | 0 | —\n\n\nIn the TLB above, we clearly have a problem: VPN 10 translates to either PFN 100 (P1) or PFN 170 (P2), but the hardware can't distinguish which entry is meant for which process. Thus, we need to do some more work in order for the TLB to correctly and efficiently support virtualization across multiple processes. And thus, a crux:\n\n\n\n\n**THE CRUX:**\n\n\n\n\n**HOW TO MANAGE TLB CONTENTS ON A CONTEXT SWITCH**\n\n\nWhen context-switching between processes, the translations in the TLB for the last process are not meaningful to the about-to-be-run process. What should the hardware or OS do in order to solve this problem?\n\n\nThere are a number of possible solutions to this problem. One approach is to simply\n   **flush**\n   the TLB on context switches, thus emptying it before running the next process. On a software-based system, this can be accomplished with an explicit (and privileged) hardware instruction; with a hardware-managed TLB, the flush could be enacted when the page-table base register is changed (note the OS must change the PTBR on a context switch anyhow). In either case, the flush operation simply sets all valid bits to 0, essentially clearing the contents of the TLB.\n\n\nBy flushing the TLB on each context switch, we now have a working solution, as a process will never accidentally encounter the wrong trans-\n\n\nlations in the TLB. However, there is a cost: each time a process runs, it must incur TLB misses as it touches its data and code pages. If the OS switches between processes frequently, this cost may be high.\n\n\nTo reduce this overhead, some systems add hardware support to enable sharing of the TLB across context switches. In particular, some hardware systems provide an\n   **address space identifier (ASID)**\n   field in the TLB. You can think of the ASID as a\n   **process identifier (PID)**\n   , but usually it has fewer bits (e.g., 8 bits for the ASID versus 32 bits for a PID).\n\n\nIf we take our example TLB from above and add ASIDs, it is clear processes can readily share the TLB: only the ASID field is needed to differentiate otherwise identical translations. Here is a depiction of a TLB with the added ASID field:\n\n\n\nVPN | PFN | valid | prot | ASID\n10 | 100 | 1 | rwX | 1\n— | — | 0 | — | —\n10 | 170 | 1 | rwX | 2\n— | — | 0 | — | —\n\n\nThus, with address-space identifiers, the TLB can hold translations from different processes at the same time without any confusion. Of course, the hardware also needs to know which process is currently running in order to perform translations, and thus the OS must, on a context switch, set some privileged register to the ASID of the current process.\n\n\nAs an aside, you may also have thought of another case where two entries of the TLB are remarkably similar. In this example, there are two entries for two different processes with two different VPNs that point to the\n   *same*\n   physical page:\n\n\n\nVPN | PFN | valid | prot | ASID\n10 | 101 | 1 | r-x | 1\n— | — | 0 | — | —\n50 | 101 | 1 | r-x | 2\n— | — | 0 | — | —\n\n\nThis situation might arise, for example, when two processes\n   *share*\n   a page (a code page, for example). In the example above, Process 1 is sharing physical page 101 with Process 2; P1 maps this page into the 10th page of its address space, whereas P2 maps it to the 50th page of its address space. Sharing of code pages (in binaries, or shared libraries) is useful as it reduces the number of physical pages in use, thus reducing memory overheads."
        },
        {
          "name": "Issue: Replacement Policy",
          "content": "As with any cache, and thus also with the TLB, one more issue that we must consider is\n   **cache replacement**\n   . Specifically, when we are installing a new entry in the TLB, we have to\n   **replace**\n   an old one, and thus the question: which one to replace?\n\n\n\n\n**THE CRUX: HOW TO DESIGN TLB REPLACEMENT POLICY**\n\n\nWhich TLB entry should be replaced when we add a new TLB entry? The goal, of course, being to minimize the\n   **miss rate**\n   (or increase\n   **hit rate**\n   ) and thus improve performance.\n\n\nWe will study such policies in some detail when we tackle the problem of swapping pages to disk; here we'll just highlight a few typical policies. One common approach is to evict the\n   **least-recently-used**\n   or\n   **LRU**\n   entry. LRU tries to take advantage of locality in the memory-reference stream, assuming it is likely that an entry that has not recently been used is a good candidate for eviction. Another typical approach is to use a\n   **random**\n   policy, which evicts a TLB mapping at random. Such a policy is useful due to its simplicity and ability to avoid corner-case behaviors; for example, a “reasonable” policy such as LRU behaves quite unreasonably when a program loops over\n   \n    n + 1\n   \n   pages with a TLB of size\n   \n    n\n   \n   ; in this case, LRU misses upon every access, whereas random does much better."
        },
        {
          "name": "A Real TLB Entry",
          "content": "Finally, let's briefly look at a real TLB. This example is from the MIPS R4000 [H93], a modern system that uses software-managed TLBs; a slightly simplified MIPS TLB entry can be seen in Figure 19.4.\n\n\nThe MIPS R4000 supports a 32-bit address space with 4KB pages. Thus, we would expect a 20-bit VPN and 12-bit offset in our typical virtual address. However, as you can see in the TLB, there are only 19 bits for the VPN; as it turns out, user addresses will only come from half the address space (the rest reserved for the kernel) and hence only 19 bits of VPN are needed. The VPN translates to up to a 24-bit physical frame number (PFN), and hence can support systems with up to 64GB of (physical) main memory (\n   \n    2^{24}\n   \n   4KB pages).\n\n\nThere are a few other interesting bits in the MIPS TLB. We see a\n   *global*\n   bit (G), which is used for pages that are globally-shared among processes. Thus, if the global bit is set, the ASID is ignored. We also see the 8-bit ASID, which the OS can use to distinguish between address spaces (as\n\n\n\n | VPN | G | ASID\n | PFN | C | D V\n\n\nFigure 19.4: A MIPS TLB Entry\n\n\n**TIP: RAM ISN'T ALWAYS RAM (CULLER'S LAW)**\nThe term\n   **random-access memory**\n   , or\n   **RAM**\n   , implies that you can access any part of RAM just as quickly as another. While it is generally good to think of RAM in this way, because of hardware/OS features such as the TLB, accessing a particular page of memory may be costly, particularly if that page isn't currently mapped by your TLB. Thus, it is always good to remember the implementation tip:\n   **RAM isn't always RAM**\n   . Sometimes randomly accessing your address space, particularly if the number of pages accessed exceeds the TLB coverage, can lead to severe performance penalties. Because one of our advisors, David Culler, used to always point to the TLB as the source of many performance problems, we name this law in his honor:\n   **Culler's Law**\n   .\n\n\ndescribed above). One question for you: what should the OS do if there are more than\n   \n    256 (2^8)\n   \n   processes running at a time? Finally, we see 3\n   *Coherence (C)*\n   bits, which determine how a page is cached by the hardware (a bit beyond the scope of these notes); a\n   *dirty*\n   bit which is marked when the page has been written to (we'll see the use of this later); a\n   *valid*\n   bit which tells the hardware if there is a valid translation present in the entry. There is also a\n   *page mask*\n   field (not shown), which supports multiple page sizes; we'll see later why having larger pages might be useful. Finally, some of the 64 bits are unused (shaded gray in the diagram).\n\n\nMIPS TLBs usually have 32 or 64 of these entries, most of which are used by user processes as they run. However, a few are reserved for the OS. A\n   *wired*\n   register can be set by the OS to tell the hardware how many slots of the TLB to reserve for the OS; the OS uses these reserved mappings for code and data that it wants to access during critical times, where a TLB miss would be problematic (e.g., in the TLB miss handler).\n\n\nBecause the MIPS TLB is software managed, there needs to be instructions to update the TLB. The MIPS provides four such instructions:\n   \n    TLBP\n   \n   , which probes the TLB to see if a particular translation is in there;\n   \n    TLBR\n   \n   , which reads the contents of a TLB entry into registers;\n   \n    TLBWI\n   \n   , which replaces a specific TLB entry; and\n   \n    TLBWR\n   \n   , which replaces a random TLB entry. The OS uses these instructions to manage the TLB's contents. It is of course critical that these instructions are\n   **privileged**\n   ; imagine what a user process could do if it could modify the contents of the TLB (hint: just about anything, including take over the machine, run its own malicious \"OS\", or even make the Sun disappear)."
        }
      ]
    },
    {
      "name": "Paging: Smaller Tables",
      "sections": [
        {
          "name": "Simple Solution: Bigger Pages",
          "content": "We could reduce the size of the page table in one simple way: use bigger pages. Take our 32-bit address space again, but this time assume 16KB pages. We would thus have an 18-bit VPN plus a 14-bit offset. Assuming the same size for each PTE (4 bytes), we now have\n   \n    2^{18}\n   \n   entries in our linear page table and thus a total size of 1MB per page table, a factor\n\n\n1\n   \n   Or indeed, you might not; this paging thing is getting out of control, no? That said, always make sure you understand the\n   *problem*\n   you are solving before moving onto the solution; indeed, if you understand the problem, you can often derive the solution yourself. Here, the problem should be clear: simple linear (array-based) page tables are too big.\n\n\n\n\n**ASIDE: MULTIPLE PAGE SIZES**\n\n\nAs an aside, do note that many architectures (e.g., MIPS, SPARC, x86-64) now support multiple page sizes. Usually, a small (4KB or 8KB) page size is used. However, if a “smart” application requests it, a single large page (e.g., of size 4MB) can be used for a specific portion of the address space, enabling such applications to place a frequently-used (and large) data structure in such a space while consuming only a single TLB entry. This type of large page usage is common in database management systems and other high-end commercial applications. The main reason for multiple page sizes is not to save page table space, however; it is to reduce pressure on the TLB, enabling a program to access more of its address space without suffering from too many TLB misses. However, as researchers have shown [N+02], using multiple page sizes makes the OS virtual memory manager notably more complex, and thus large pages are sometimes most easily used simply by exporting a new interface to applications to request large pages directly.\n\n\nof four reduction in size of the page table (not surprisingly, the reduction exactly mirrors the factor of four increase in page size).\n\n\nThe major problem with this approach, however, is that big pages lead to waste\n   *within*\n   each page, a problem known as\n   **internal fragmentation**\n   (as the waste is\n   **internal**\n   to the unit of allocation). Applications thus end up allocating pages but only using little bits and pieces of each, and memory quickly fills up with these overly-large pages. Thus, most systems use relatively small page sizes in the common case: 4KB (as in x86) or 8KB (as in SPARCV9). Our problem will not be solved so simply, alas."
        },
        {
          "name": "Hybrid Approach: Paging and Segments",
          "content": "Whenever you have two reasonable but different approaches to something in life, you should always examine the combination of the two to see if you can obtain the best of both worlds. We call such a combination a\n   **hybrid**\n   . For example, why eat just chocolate or plain peanut butter when you can instead combine the two in a lovely hybrid known as the Reese’s Peanut Butter Cup [M28]?\n\n\nYears ago, the creators of Multics (in particular Jack Dennis) chanced upon such an idea in the construction of the Multics virtual memory system [M07]. Specifically, Dennis had the idea of combining paging and segmentation in order to reduce the memory overhead of page tables. We can see why this might work by examining a typical linear page table in more detail. Assume we have an address space in which the used portions of the heap and stack are small. For the example, we use a tiny 16KB address space with 1KB pages (Figure 20.1); the page table for this address space is in Figure 20.2.\n\n\n\n\n![Diagram showing the mapping of a 16KB Virtual Address Space to 32 1KB Physical Memory pages. The Virtual Address Space is divided into code (pages 0-15), heap (pages 16-23), and stack (pages 24-31). Arrows show mappings: code page 0 to physical page 10, heap page 4 to physical page 23, and stack pages 14 and 15 to physical pages 28 and 29 respectively.](images/image_0063.jpeg)\n\n\nThe diagram illustrates the mapping between a Virtual Address Space and Physical Memory. The Virtual Address Space is divided into three segments: code (pages 0-15), heap (pages 16-23), and stack (pages 24-31). The Physical Memory consists of 32 pages (0-31). Arrows indicate the following mappings:\n\n\n  * Virtual page 0 (code) maps to Physical page 10.\n  * Virtual page 4 (heap) maps to Physical page 23.\n  * Virtual page 14 (stack) maps to Physical page 28.\n  * Virtual page 15 (stack) maps to Physical page 29.\n\n\nDiagram showing the mapping of a 16KB Virtual Address Space to 32 1KB Physical Memory pages. The Virtual Address Space is divided into code (pages 0-15), heap (pages 16-23), and stack (pages 24-31). Arrows show mappings: code page 0 to physical page 10, heap page 4 to physical page 23, and stack pages 14 and 15 to physical pages 28 and 29 respectively.\n\n\nFigure 20.1: A 16KB Address Space With 1KB Pages\n\n\n\nPFN | valid | prot | present | dirty\n10 | 1 | r-x | 1 | 0\n- | 0 | — | - | -\n- | 0 | — | - | -\n- | 0 | — | - | -\n23 | 1 | rw- | 1 | 1\n- | 0 | — | - | -\n- | 0 | — | - | -\n- | 0 | — | - | -\n- | 0 | — | - | -\n- | 0 | — | - | -\n- | 0 | — | - | -\n- | 0 | — | - | -\n28 | 1 | rw- | 1 | 1\n4 | 1 | rw- | 1 | 1\n\n\nFigure 20.2: A Page Table For 16KB Address Space\n\n\nThis example assumes the single code page (VPN 0) is mapped to physical page 10, the single heap page (VPN 4) to physical page 23, and the two stack pages at the other end of the address space (VPNs 14 and 15) to physical pages 28 and 29 respectively.\n\n\n15) are mapped to physical pages 28 and 4, respectively. As you can see from the picture,\n    *most*\n    of the page table is unused, full of\n    **invalid**\n    entries. What a waste! And this is for a tiny 16KB address space. Imagine the page table of a 32-bit address space and all the potential wasted space in there! Actually, don't imagine such a thing; it's far too gruesome.\n\n\nThus, our hybrid approach: instead of having a single page table for the entire address space of the process, why not have one per logical segment? In this example, we might thus have three page tables, one for the code, heap, and stack parts of the address space.\n\n\nNow, remember with segmentation, we had a\n    **base**\n    register that told us where each segment lived in physical memory, and a\n    **bound**\n    or\n    **limit**\n    register that told us the size of said segment. In our hybrid, we still have those structures in the MMU; here, we use the base not to point to the segment itself but rather to hold the\n    *physical address of the page table*\n    of that segment. The bounds register is used to indicate the end of the page table (i.e., how many valid pages it has).\n\n\nLet's do a simple example to clarify. Assume a 32-bit virtual address space with 4KB pages, and an address space split into four segments. We'll only use three segments for this example: one for code, one for heap, and one for stack.\n\n\nTo determine which segment an address refers to, we'll use the top two bits of the address space. Let's assume 00 is the unused segment, with 01 for code, 10 for the heap, and 11 for the stack. Thus, a virtual address looks like this:\n\n\n3 | 3 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0\n1 | 0 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nSeg | VPN | Offset\n\n\nIn the hardware, assume that there are thus three base/bounds pairs, one each for code, heap, and stack. When a process is running, the base register for each of these segments contains the physical address of a linear page table for that segment; thus, each process in the system now has\n    *three*\n    page tables associated with it. On a context switch, these registers must be changed to reflect the location of the page tables of the newly-running process.\n\n\nOn a TLB miss (assuming a hardware-managed TLB, i.e., where the hardware is responsible for handling TLB misses), the hardware uses the segment bits (SN) to determine which base and bounds pair to use. The hardware then takes the physical address therein and combines it with the VPN as follows to form the address of the page table entry (PTE):\n\n\nSN          = (VirtualAddress & SEG_MASK) >> SN_SHIFT\nVPN         = (VirtualAddress & VPN_MASK) >> VPN_SHIFT\nAddressOfPTE = Base[SN] + (VPN * sizeof(PTE))\n\n\nThis sequence should look familiar; it is virtually identical to what we saw before with linear page tables. The only difference, of course, is the use of one of three segment base registers instead of the single page table base register.\n\n\n\n\n**TIP: USE HYBRIDS**\n\n\nWhen you have two good and seemingly opposing ideas, you should always see if you can combine them into a\n   **hybrid**\n   that manages to achieve the best of both worlds. Hybrid corn species, for example, are known to be more robust than any naturally-occurring species. Of course, not all hybrids are a good idea; see the Zeedonk (or Zonkey), which is a cross of a Zebra and a Donkey. If you don't believe such a creature exists, look it up, and prepare to be amazed.\n\n\nThe critical difference in our hybrid scheme is the presence of a bounds register per segment; each bounds register holds the value of the maximum valid page in the segment. For example, if the code segment is using its first three pages (0, 1, and 2), the code segment page table will only have three entries allocated to it and the bounds register will be set to 3; memory accesses beyond the end of the segment will generate an exception and likely lead to the termination of the process. In this manner, our hybrid approach realizes a significant memory savings compared to the linear page table; unallocated pages between the stack and the heap no longer take up space in a page table (just to mark them as not valid).\n\n\nHowever, as you might notice, this approach is not without problems. First, it still requires us to use segmentation; as we discussed before, segmentation is not quite as flexible as we would like, as it assumes a certain usage pattern of the address space; if we have a large but sparsely-used heap, for example, we can still end up with a lot of page table waste. Second, this hybrid causes external fragmentation to arise again. While most of memory is managed in page-sized units, page tables now can be of arbitrary size (in multiples of PTEs). Thus, finding free space for them in memory is more complicated. For these reasons, people continued to look for better ways to implement smaller page tables."
        },
        {
          "name": "Multi-level Page Tables",
          "content": "A different approach doesn't rely on segmentation but attacks the same problem: how to get rid of all those invalid regions in the page table instead of keeping them all in memory? We call this approach a\n   **multi-level page table**\n   , as it turns the linear page table into something like a tree. This approach is so effective that many modern systems employ it (e.g., x86 [BOH10]). We now describe this approach in detail.\n\n\nThe basic idea behind a multi-level page table is simple. First, chop up the page table into page-sized units; then, if an entire page of page-table entries (PTEs) is invalid, don't allocate that page of the page table at all. To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure, called the\n   **page directory**\n   . The page directory thus either can be used to tell you where a page of the page table is, or that the entire page of the page table contains no valid pages.\n\n\n\n\n![Figure 20.3: Linear (Left) And Multi-Level (Right) Page Tables. The diagram compares a classic linear page table with a two-level multi-level page table. The linear page table on the left has a PTBR of 201 and 16 entries, with the first entry pointing to PFN 201. The multi-level page table on the right has a PDBR of 200 and two entries in the Page Directory (PFN 200 and 204). The entry for PFN 200 points to a page table for PFN 201, and the entry for PFN 204 points to a page table for PFN 204. The page tables for PFN 201 and PFN 204 each have 16 entries, with the first entry pointing to PFN 201 and PFN 204 respectively. The middle pages of the page tables are marked as 'Not Allocated'.](images/image_0064.jpeg)\n\n\n**Linear Page Table**\n\n\nPTBR: 201\n\n\n\nvalid | prot | PFN | \n1 | rx | 12 | PFN 201\n1 | rx | 13 | PFN 202\n0 | - | - | PFN 203\n1 | rw | 100 | PFN 204\n0 | - | - | \n0 | - | - | \n0 | - | - | \n0 | - | - | \n0 | - | - | \n0 | - | - | \n0 | - | - | \n0 | - | - | \n1 | rw | 86 | \n1 | rw | 15 | \n\n\n**Multi-level Page Table**\n\n\nPDBR: 200\n\n\nThe Page Directory\n\n\n\nvalid | PFN | \n1 | 201 | →\n0 | - | \n0 | - | \n1 | 204 | →\n\n\n→\n\n\n\nvalid | prot | PFN | \n1 | rx | 12 | PFN 201\n1 | rx | 13 | \n0 | - | - | [Page 1 of PT: Not Allocated]\n1 | rw | 100 | \n0 | - | - | [Page 2 of PT: Not Allocated]\n0 | - | - | \n0 | - | - | \n0 | - | - | \n0 | - | - | \n0 | - | - | \n0 | - | - | \n1 | rw | 86 | PFN 204\n1 | rw | 15 | \n\n\nFigure 20.3: Linear (Left) And Multi-Level (Right) Page Tables. The diagram compares a classic linear page table with a two-level multi-level page table. The linear page table on the left has a PTBR of 201 and 16 entries, with the first entry pointing to PFN 201. The multi-level page table on the right has a PDBR of 200 and two entries in the Page Directory (PFN 200 and 204). The entry for PFN 200 points to a page table for PFN 201, and the entry for PFN 204 points to a page table for PFN 204. The page tables for PFN 201 and PFN 204 each have 16 entries, with the first entry pointing to PFN 201 and PFN 204 respectively. The middle pages of the page tables are marked as 'Not Allocated'.\n\n\nFigure 20.3: Linear (Left) And Multi-Level (Right) Page Tables\n\n\nFigure 20.3 shows an example. On the left of the figure is the classic linear page table; even though most of the middle regions of the address space are not valid, we still require page-table space allocated for those regions (i.e., the middle two pages of the page table). On the right is a multi-level page table. The page directory marks just two pages of the page table as valid (the first and last); thus, just those two pages of the page table reside in memory. And thus you can see one way to visualize what a multi-level table is doing: it just makes parts of the linear page table disappear (freeing those frames for other uses), and tracks which pages of the page table are allocated with the page directory.\n\n\nThe page directory, in a simple two-level table, contains one entry per page of the page table. It consists of a number of\n   **page directory entries (PDE)**\n   . A PDE (minimally) has a\n   **valid bit**\n   and a\n   **page frame number (PFN)**\n   , similar to a PTE. However, as hinted at above, the meaning of this valid bit is slightly different: if the PDE is valid, it means that at least one of the pages of the page table that the entry points to (via the PFN) is valid, i.e., in at least one PTE on that page pointed to by this PDE, the valid bit in that PTE is set to one. If the PDE is not valid (i.e., equal to zero), the rest of the PDE is not defined.\n\n\nMulti-level page tables have some obvious advantages over approaches we've seen thus far. First, and perhaps most obviously, the multi-level table only allocates page-table space in proportion to the amount of address space you are using; thus it is generally compact and supports sparse address spaces.\n\n\nSecond, if carefully constructed, each portion of the page table fits neatly within a page, making it easier to manage memory; the OS can simply grab the next free page when it needs to allocate or grow a page\n\n\n\n\n**TIP: UNDERSTAND TIME-SPACE TRADE-OFFS**\n\n\nWhen building a data structure, one should always consider\n   **time-space trade-offs**\n   in its construction. Usually, if you wish to make access to a particular data structure faster, you will have to pay a space-usage penalty for the structure.\n\n\ntable. Contrast this to a simple (non-paged) linear page table\n   \n    2\n   \n   , which is just an array of PTEs indexed by VPN; with such a structure, the entire linear page table must reside contiguously in physical memory. For a large page table (say 4MB), finding such a large chunk of unused contiguous free physical memory can be quite a challenge. With a multi-level structure, we add a\n   **level of indirection**\n   through use of the page directory, which points to pieces of the page table; that indirection allows us to place page-table pages wherever we would like in physical memory.\n\n\nIt should be noted that there is a cost to multi-level tables; on a TLB miss, two loads from memory will be required to get the right translation information from the page table (one for the page directory, and one for the PTE itself), in contrast to just one load with a linear page table. Thus, the multi-level table is a small example of a\n   **time-space trade-off**\n   . We wanted smaller tables (and got them), but not for free; although in the common case (TLB hit), performance is obviously identical, a TLB miss suffers from a higher cost with this smaller table.\n\n\nAnother obvious negative is\n   *complexity*\n   . Whether it is the hardware or OS handling the page-table lookup (on a TLB miss), doing so is undoubtedly more involved than a simple linear page-table lookup. Often we are willing to increase complexity in order to improve performance or reduce overheads; in the case of a multi-level table, we make page-table lookups more complicated in order to save valuable memory.\n\n\n\n\n**A Detailed Multi-Level Example**\n\n\nTo understand the idea behind multi-level page tables better, let's do an example. Imagine a small address space of size 16KB, with 64-byte pages. Thus, we have a 14-bit virtual address space, with 8 bits for the VPN and 6 bits for the offset. A linear page table would have\n   \n    2^8\n   \n   (256) entries, even if only a small portion of the address space is in use. Figure 20.4 (page 8) presents one example of such an address space.\n\n\nIn this example, virtual pages 0 and 1 are for code, virtual pages 4 and 5 for the heap, and virtual pages 254 and 255 for the stack; the rest of the pages of the address space are unused.\n\n\nTo build a two-level page table for this address space, we start with our full linear page table and break it up into page-sized units. Recall our full table (in this example) has 256 entries; assume each PTE is 4 bytes\n\n\n2\n   \n   We are making some assumptions here, i.e., that all page tables reside in their entirety in physical memory (i.e., they are not swapped to disk); we'll soon relax this assumption.\n\n\n\n0000 0000 | code\n0000 0001 | code\n0000 0010 | (free)\n0000 0011 | (free)\n0000 0100 | heap\n0000 0101 | heap\n0000 0110 | (free)\n0000 0111 | (free)\n..... | ... all free ...\n1111 1100 | (free)\n1111 1101 | (free)\n1111 1110 | stack\n1111 1111 | stack\n\n\nFigure 20.4: A 16KB Address Space With 64-byte Pages\n\n\nin size. Thus, our page table is 1KB (\n   \n    256 \\times 4\n   \n   bytes) in size. Given that we have 64-byte pages, the 1KB page table can be divided into 16 64-byte pages; each page can hold 16 PTEs.\n\n\nWhat we need to understand now is how to take a VPN and use it to index first into the page directory and then into the page of the page table. Remember that each is an array of entries; thus, all we need to figure out is how to construct the index for each from pieces of the VPN.\n\n\nLet's first index into the page directory. Our page table in this example is small: 256 entries, spread across 16 pages. The page directory needs one entry per page of the page table; thus, it has 16 entries. As a result, we need four bits of the VPN to index into the directory; we use the top four bits of the VPN, as follows:\n\n\n\n\n![](images/image_0065.jpeg)\n\n\n| VPN | offset\n13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\n | Page Directory Index |\n\n\nOnce we extract the\n   **page-directory index**\n   (PDIIndex for short) from the VPN, we can use it to find the address of the page-directory entry (PDE) with a simple calculation:\n   \n    PDEAddr = PageDirBase + (PDIIndex \\times \\text{sizeof}(PDE))\n   \n   . This results in our page directory, which we now examine to make further progress in our translation.\n\n\nIf the page-directory entry is marked invalid, we know that the access is invalid, and thus raise an exception. If, however, the PDE is valid, we have more work to do. Specifically, we now have to fetch the page-table entry (PTE) from the page of the page table pointed to by this page-directory entry. To find this PTE, we have to index into the portion of the page table using the remaining bits of the VPN:\n\n\n\n\n![Diagram of a 14-bit virtual address space divided into VPN and offset.](images/image_0066.jpeg)\n\n\nThe diagram shows a 14-bit address space represented as a row of 14 cells, numbered 13 down to 0 from left to right. A vertical line at bit 12 separates the 'VPN' (Virtual Page Number) from the 'offset'. Below the cells, a second row of labels indicates the index ranges: 'Page Directory Index' for bits 13-10, and 'Page Table Index' for bits 9-0.\n\n\nDiagram of a 14-bit virtual address space divided into VPN and offset.\n\n\nThis\n   **page-table index**\n   (PTIndex for short) can then be used to index into the page table itself, giving us the address of our PTE:\n\n\n\\text{PTEAddr} = (\\text{PDE.PFN} \\ll \\text{SHIFT}) + (\\text{PTIndex} * \\text{sizeof(PTE)})\n\n\nNote that the page-frame number obtained from the page-directory entry must be left-shifted into place before combining it with the page-table index to form the address of the PTE.\n\n\nTo see if this all makes sense, we'll now fill in a multi-level page table with some actual values, and translate a single virtual address. Let's begin with the\n   **page directory**\n   for this example (left side of Figure 20.5).\n\n\nIn the figure, you can see that each page directory entry (PDE) describes something about a page of the page table for the address space. In this example, we have two valid regions in the address space (at the beginning and end), and a number of invalid mappings in-between.\n\n\nIn physical page 100 (the physical frame number of the 0th page of the page table), we have the first page of 16 page table entries for the first 16 VPNs in the address space. See Figure 20.5 (middle part) for the contents of this portion of the page table.\n\n\nThis page of the page table contains the mappings for the first 16 VPNs; in our example, VPNs 0 and 1 are valid (the code segment), as\n\n\n\nPage Directory | Page of PT (@PFN:100) | Page of PT (@PFN:101)\nPFN | valid? | PFN | valid | prot | PFN | valid | prot\n100 | 1 | 10 | 1 | r-x | — | 0 | —\n— | 0 | 23 | 1 | r-x | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | 80 | 1 | rw- | — | 0 | —\n— | 0 | 59 | 1 | rw- | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n— | 0 | — | 0 | — | — | 0 | —\n101 | 1 | — | 0 | — | 45 | 1 | rw-\n\n\nFigure 20.5: A Page Directory, And Pieces Of Page Table\n\n\n**TIP: BE WARY OF COMPLEXITY**\nSystem designers should be wary of adding complexity into their system. What a good systems builder does is implement the least complex system that achieves the task at hand. For example, if disk space is abundant, you shouldn't design a file system that works hard to use as few bytes as possible; similarly, if processors are fast, it is better to write a clean and understandable module within the OS than perhaps the most CPU-optimized, hand-assembled code for the task at hand. Be wary of needless complexity, in prematurely-optimized code or other forms; such approaches make systems harder to understand, maintain, and debug. As Antoine de Saint-Exupery famously wrote: \"Perfection is finally attained not when there is no longer anything to add, but when there is no longer anything to take away.\" What he didn't write: \"It's a lot easier to say something about perfection than to actually achieve it.\"\n\n\nare 4 and 5 (the heap). Thus, the table has mapping information for each of those pages. The rest of the entries are marked invalid.\n\n\nThe other valid page of the page table is found inside PFN 101. This page contains mappings for the last 16 VPNs of the address space; see Figure 20.5 (right) for details.\n\n\nIn the example, VPNs 254 and 255 (the stack) have valid mappings. Hopefully, what we can see from this example is how much space savings are possible with a multi-level indexed structure. In this example, instead of allocating the full\n   *sixteen*\n   pages for a linear page table, we allocate only\n   *three*\n   : one for the page directory, and two for the chunks of the page table that have valid mappings. The savings for large (32-bit or 64-bit) address spaces could obviously be much greater.\n\n\nFinally, let's use this information in order to perform a translation. Here is an address that refers to the 0th byte of VPN 254: 0x3F80, or 11 1111 1000 0000 in binary.\n\n\nRecall that we will use the top 4 bits of the VPN to index into the page directory. Thus, 1111 will choose the last (15th, if you start at the 0th) entry of the page directory above. This points us to a valid page of the page table located at address 101. We then use the next 4 bits of the VPN (1110) to index into that page of the page table and find the desired PTE. 1110 is the next-to-last (14th) entry on the page, and tells us that page 254 of our virtual address space is mapped at physical page 55. By concatenating PFN=55 (or hex 0x37) with offset=000000, we can thus form our desired physical address and issue the request to the memory system:\n   \n    \\text{PhysAddr} = (\\text{PTE}, \\text{PFN} \\ll \\text{SHIFT}) + \\text{offset}\n   \n\n\n   = 00 1101 1100 0000 = 0x0DC0.\n\n\nYou should now have some idea of how to construct a two-level page table, using a page directory which points to pages of the page table. Unfortunately, however, our work is not done. As we'll now discuss, sometimes two levels of page table is not enough!\n\n\n\n\n**More Than Two Levels**\n\n\nIn our example thus far, we've assumed that multi-level page tables only have two levels: a page directory and then pieces of the page table. In some cases, a deeper tree is possible (and indeed, needed).\n\n\nLet's take a simple example and use it to show why a deeper multi-level table can be useful. In this example, assume we have a 30-bit virtual address space, and a small (512 byte) page. Thus our virtual address has a 21-bit virtual page number component and a 9-bit offset.\n\n\nRemember our goal in constructing a multi-level page table: to make each piece of the page table fit within a single page. Thus far, we've only considered the page table itself; however, what if the page directory gets too big?\n\n\nTo determine how many levels are needed in a multi-level table to make all pieces of the page table fit within a page, we start by determining how many page-table entries fit within a page. Given our page size of 512 bytes, and assuming a PTE size of 4 bytes, you should see that you can fit 128 PTEs on a single page. When we index into a page of the page table, we can thus conclude we'll need the least significant 7 bits (\n   \n    \\log_2 128\n   \n   ) of the VPN as an index:\n\n\n\n\n![Diagram of a 30-bit virtual address split into VPN and offset. The VPN is 21 bits (bits 29-9) and the offset is 9 bits (bits 8-0). The VPN is further split into a Page Directory Index (bits 29-22) and a Page Table Index (bits 21-9).](images/image_0067.jpeg)\n\n\nThe diagram shows a 30-bit virtual address represented as a row of 32 bits, numbered 29 down to 0 from left to right. A vertical line at bit 21 separates the VPN (bits 29-9) from the offset (bits 8-0). Another vertical line at bit 22 separates the Page Directory Index (bits 29-22) from the Page Table Index (bits 21-9).\n\n\n\n29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nVPN | offset\nPage Directory Index | Page Table Index\n\n\nDiagram of a 30-bit virtual address split into VPN and offset. The VPN is 21 bits (bits 29-9) and the offset is 9 bits (bits 8-0). The VPN is further split into a Page Directory Index (bits 29-22) and a Page Table Index (bits 21-9).\n\n\nWhat you also might notice from the diagram above is how many bits are left into the (large) page directory: 14. If our page directory has\n   \n    2^{14}\n   \n   entries (and 4-byte PDEs), it spans not one page but 128, and our goal of making every piece of the multi-level page table fit into a page vanishes.\n\n\nTo remedy this problem, we build a further level of the tree, by splitting the page directory itself into multiple pages, and then adding another page directory on top of that, to point to the pages of the page directory. We can thus split up our virtual address as follows:\n\n\n\n\n![Diagram of a 30-bit virtual address split into VPN and offset. The VPN is 21 bits (bits 29-9) and the offset is 9 bits (bits 8-0). The VPN is further split into PD Index 0 (bits 29-22), PD Index 1 (bits 21-14), and Page Table Index (bits 13-9).](images/image_0068.jpeg)\n\n\nThe diagram shows a 30-bit virtual address represented as a row of 32 bits, numbered 29 down to 0 from left to right. A vertical line at bit 21 separates the VPN (bits 29-9) from the offset (bits 8-0). Two vertical lines at bits 22 and 14 further split the VPN into PD Index 0 (bits 29-22), PD Index 1 (bits 21-14), and Page Table Index (bits 13-9).\n\n\n\n29 | 28 | 27 | 26 | 25 | 24 | 23 | 22 | 21 | 20 | 19 | 18 | 17 | 16 | 15 | 14 | 13 | 12 | 11 | 10 | 9 | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0\nVPN | offset\nPD Index 0 | PD Index 1 | Page Table Index\n\n\nDiagram of a 30-bit virtual address split into VPN and offset. The VPN is 21 bits (bits 29-9) and the offset is 9 bits (bits 8-0). The VPN is further split into PD Index 0 (bits 29-22), PD Index 1 (bits 21-14), and Page Table Index (bits 13-9).\n\n\nNow, when indexing the upper-level page directory, we use the very top bits of the virtual address (PD Index 0 in the diagram); this index can be used to fetch the page-directory entry from the top-level page directory. If valid, the second level of the page directory is consulted by combining the physical frame number from the top-level PDE and the\n\n\n1  VPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2  (Success, TlbEntry) = TLB_Lookup (VPN)\n3  if (Success == True) // TLB Hit\n4      if (CanAccess(TlbEntry.ProtectBits) == True)\n5          Offset = VirtualAddress & OFFSET_MASK\n6          PhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7          Register = **AccessMemory** (PhysAddr)\n8      else\n9          RaiseException(PROTECTION_FAULT)\n10 else // TLB Miss\n11     // first, get page directory entry\n12     PDIIndex = (VPN & PD_MASK) >> PD_SHIFT\n13     PDEAddr = PDBR + (PDIIndex * sizeof(PDE))\n14     PDE = **AccessMemory** (PDEAddr)\n15     if (PDE.Valid == False)\n16         RaiseException(SEGMENTATION_FAULT)\n17     else\n18         // PDE is valid: now fetch PTE from page table\n19         PTIIndex = (VPN & PT_MASK) >> PT_SHIFT\n20         PTEAddr = (PDE.PFN<<SHIFT) + (PTIIndex*sizeof(PTE))\n21         PTE = **AccessMemory** (PTEAddr)\n22         if (PTE.Valid == False)\n23             RaiseException(SEGMENTATION_FAULT)\n24         else if (CanAccess(PTE.ProtectBits) == False)\n25             RaiseException(PROTECTION_FAULT)\n26         else\n27             TLB_Insert (VPN, PTE.PFN, PTE.ProtectBits)\n28             RetryInstruction()\n\nFigure 20.6: Multi-level Page Table Control Flow\n\n\nnext part of the VPN (PD Index 1). Finally, if valid, the PTE address can be formed by using the page-table index combined with the address from the second-level PDE. Whew! That's a lot of work. And all just to look something up in a multi-level table.\n\n\n\n\n**The Translation Process: Remember the TLB**\n\n\nTo summarize the entire process of address translation using a two-level page table, we once again present the control flow in algorithmic form (Figure 20.6). The figure shows what happens in hardware (assuming a hardware-managed TLB) upon\n   *every*\n   memory reference.\n\n\nAs you can see from the figure, before any of the complicated multi-level page table access occurs, the hardware first checks the TLB; upon a hit, the physical address is formed directly\n   *without*\n   accessing the page table at all, as before. Only upon a TLB miss does the hardware need to perform the full multi-level lookup. On this path, you can see the cost of our traditional two-level page table: two additional memory accesses to look up a valid translation."
        },
        {
          "name": "Inverted Page Tables",
          "content": "An even more extreme space savings in the world of page tables is found with\n   **inverted page tables**\n   . Here, instead of having many page tables (one per process of the system), we keep a single page table that has an entry for each\n   *physical page*\n   of the system. The entry tells us which process is using this page, and which virtual page of that process maps to this physical page.\n\n\nFinding the correct entry is now a matter of searching through this data structure. A linear scan would be expensive, and thus a hash table is often built over the base structure to speed up lookups. The PowerPC is one example of such an architecture [JM98].\n\n\nMore generally, inverted page tables illustrate what we’ve said from the beginning: page tables are just data structures. You can do lots of crazy things with data structures, making them smaller or bigger, making them slower or faster. Multi-level and inverted page tables are just two examples of the many things one could do."
        },
        {
          "name": "Swapping the Page Tables to Disk",
          "content": "Finally, we discuss the relaxation of one final assumption. Thus far, we have assumed that page tables reside in kernel-owned physical memory. Even with our many tricks to reduce the size of page tables, it is still possible, however, that they may be too big to fit into memory all at once. Thus, some systems place such page tables in\n   **kernel virtual memory**\n   , thereby allowing the system to\n   **swap**\n   some of these page tables to disk when memory pressure gets a little tight. We’ll talk more about this in a future chapter (namely, the case study on VAX/VMS), once we understand how to move pages in and out of memory in more detail."
        }
      ]
    },
    {
      "name": "Beyond Physical Memory: Mechanisms",
      "sections": [
        {
          "name": "Swap Space",
          "content": "The first thing we will need to do is to reserve some space on the disk for moving pages back and forth. In operating systems, we generally refer to such space as\n   **swap space**\n   , because we\n   *swap*\n   pages out of memory to it and\n   *swap*\n   pages into memory from it. Thus, we will simply assume that the OS can read from and write to the swap space, in page-sized units. To do so, the OS will need to remember the\n   **disk address**\n   of a given page.\n\n\nThe size of the swap space is important, as ultimately it determines the maximum number of memory pages that can be in use by a system at a given time. Let us assume for simplicity that it is\n   *very*\n   large for now.\n\n\nIn the tiny example (Figure 21.1), you can see a little example of a 4-page physical memory and an 8-page swap space. In the example, three processes (Proc 0, Proc 1, and Proc 2) are actively sharing physical memory; each of the three, however, only have some of their valid pages in memory, with the rest located in swap space on disk. A fourth process (Proc 3) has all of its pages swapped out to disk, and thus clearly isn't currently running. One block of swap remains free. Even from this tiny example, hopefully you can see how using swap space allows the system to pretend that memory is larger than it actually is.\n\n\nWe should note that swap space is not the only on-disk location for swapping traffic. For example, assume you are running a program binary (e.g.,\n   \n    ls\n   \n   , or your own compiled\n   \n    main\n   \n   program). The code pages from this binary are initially found on disk, and when the program runs, they are loaded into memory (either all at once when the program starts execution,\n\n\n\n\n![Diagram illustrating Physical Memory and Swap Space. Physical Memory is divided into 4 frames (PFN 0 to PFN 3). Swap Space is divided into 8 blocks (Block 0 to Block 7).](images/image_0069.jpeg)\n\n\nThe diagram shows two memory spaces.\n    **Physical Memory**\n    consists of 4 frames (PFN 0 to PFN 3). PFN 0 contains Proc 0 [VPN 0], PFN 1 contains Proc 1 [VPN 2], PFN 2 contains Proc 1 [VPN 3], and PFN 3 contains Proc 2 [VPN 0].\n    **Swap Space**\n    consists of 8 blocks (Block 0 to Block 7). Block 0 contains Proc 0 [VPN 1], Block 1 contains Proc 0 [VPN 2], Block 2 is [Free], Block 3 contains Proc 1 [VPN 0], Block 4 contains Proc 1 [VPN 1], Block 5 contains Proc 3 [VPN 0], Block 6 contains Proc 2 [VPN 1], and Block 7 contains Proc 3 [VPN 1].\n\n\nDiagram illustrating Physical Memory and Swap Space. Physical Memory is divided into 4 frames (PFN 0 to PFN 3). Swap Space is divided into 8 blocks (Block 0 to Block 7).\n\n\nFigure 21.1:\n   **Physical Memory and Swap Space**\n\n\nor, as in modern systems, one page at a time when needed). However, if the system needs to make room in physical memory for other needs, it can safely re-use the memory space for these code pages, knowing that it can later swap them in again from the on-disk binary in the file system."
        },
        {
          "name": "The Present Bit",
          "content": "Now that we have some space on the disk, we need to add some machinery higher up in the system in order to support swapping pages to and from the disk. Let us assume, for simplicity, that we have a system with a hardware-managed TLB.\n\n\nRecall first what happens on a memory reference. The running process generates virtual memory references (for instruction fetches, or data accesses), and, in this case, the hardware translates them into physical addresses before fetching the desired data from memory.\n\n\nRemember that the hardware first extracts the VPN from the virtual address, checks the TLB for a match (a\n   **TLB hit**\n   ), and if a hit, produces the resulting physical address and fetches it from memory. This is hopefully the common case, as it is fast (requiring no additional memory accesses).\n\n\nIf the VPN is not found in the TLB (i.e., a\n   **TLB miss**\n   ), the hardware locates the page table in memory (using the\n   **page table base register**\n   ) and looks up the\n   **page table entry (PTE)**\n   for this page using the VPN as an index. If the page is valid and present in physical memory, the hardware extracts the PFN from the PTE, installs it in the TLB, and retries the instruction, this time generating a TLB hit; so far, so good.\n\n\nIf we wish to allow pages to be swapped to disk, however, we must add even more machinery. Specifically, when the hardware looks in the PTE, it may find that the page is\n   *not present*\n   in physical memory. The way the hardware (or the OS, in a software-managed TLB approach) determines this is through a new piece of information in each page-table entry, known as the\n   **present bit**\n   . If the present bit is set to one, it means the page is present in physical memory and everything proceeds as above; if it is set to zero, the page is\n   *not*\n   in memory but rather on disk somewhere.\n\n\n\n\n**ASIDE: SWAPPING TERMINOLOGY AND OTHER THINGS**\n\n\nTerminology in virtual memory systems can be a little confusing and variable across machines and operating systems. For example, a\n   **page fault**\n   more generally could refer to any reference to a page table that generates a fault of some kind: this could include the type of fault we are discussing here, i.e., a page-not-present fault, but sometimes can refer to illegal memory accesses. Indeed, it is odd that we call what is definitely a legal access (to a page mapped into the virtual address space of a process, but simply not in physical memory at the time) a “fault” at all; really, it should be called a\n   **page miss**\n   . But often, when people say a program is “page faulting”, they mean that it is accessing parts of its virtual address space that the OS has swapped out to disk.\n\n\nWe suspect the reason that this behavior became known as a “fault” relates to the machinery in the operating system to handle it. When something unusual happens, i.e., when something the hardware doesn’t know how to handle occurs, the hardware simply transfers control to the OS, hoping it can make things better. In this case, a page that a process wants to access is missing from memory; the hardware does the only thing it can, which is raise an exception, and the OS takes over from there. As this is identical to what happens when a process does something illegal, it is perhaps not surprising that we term the activity a “fault.”\n\n\nThe act of accessing a page that is not in physical memory is commonly referred to as a\n   **page fault**\n   .\n\n\nUpon a page fault, the OS is invoked to service the page fault. A particular piece of code, known as a\n   **page-fault handler**\n   , runs, and must service the page fault, as we now describe."
        },
        {
          "name": "The Page Fault",
          "content": "Recall that with TLB misses, we have two types of systems: hardware-managed TLBs (where the hardware looks in the page table to find the desired translation) and software-managed TLBs (where the OS does). In either type of system, if a page is not present, the OS is put in charge to handle the page fault. The appropriately-named OS\n   **page-fault handler**\n   runs to determine what to do. Virtually all systems handle page faults in software; even with a hardware-managed TLB, the hardware trusts the OS to manage this important duty.\n\n\nIf a page is not present and has been swapped to disk, the OS will need to swap the page into memory in order to service the page fault. Thus, a question arises: how will the OS know where to find the desired page? In many systems, the page table is a natural place to store such information. Thus, the OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault\n\n\n\n\n**ASIDE: WHY HARDWARE DOESN’T HANDLE PAGE FAULTS**\n\n\nWe know from our experience with the TLB that hardware designers are loath to trust the OS to do much of anything. So why do they trust the OS to handle a page fault? There are a few main reasons. First, page faults to disk are\n   *slow*\n   ; even if the OS takes a long time to handle a fault, executing tons of instructions, the disk operation itself is traditionally so slow that the extra overheads of running software are minimal. Second, to be able to handle a page fault, the hardware would have to understand swap space, how to issue I/Os to the disk, and a lot of other details which it currently doesn’t know much about. Thus, for both reasons of performance and simplicity, the OS handles page faults, and even hardware types can be happy.\n\n\nfor a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory.\n\n\nWhen the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN field of the page-table entry (PTE) to record the in-memory location of the newly-fetched page, and retry the instruction. This next attempt may generate a TLB miss, which would then be serviced and update the TLB with the translation (one could alternately update the TLB when servicing the page fault to avoid this step). Finally, a last restart would find the translation in the TLB and thus proceed to fetch the desired data or instruction from memory at the translated physical address.\n\n\nNote that while the I/O is in flight, the process will be in the\n   **blocked**\n   state. Thus, the OS will be free to run other ready processes while the page fault is being serviced. Because I/O is expensive, this\n   **overlap**\n   of the I/O (page fault) of one process and the execution of another is yet another way a multiprogrammed system can make the most effective use of its hardware."
        },
        {
          "name": "What If Memory Is Full?In",
          "content": "In the process described above, you may notice that we assumed there is plenty of free memory in which to\n   **page in**\n   a page from swap space. Of course, this may not be the case; memory may be full (or close to it). Thus, the OS might like to first\n   **page out**\n   one or more pages to make room for the new page(s) the OS is about to bring in. The process of picking a page to kick out, or\n   **replace**\n   is known as the\n   **page-replacement policy**\n   .\n\n\nAs it turns out, a lot of thought has been put into creating a good page-replacement policy, as kicking out the wrong page can exact a great cost on program performance. Making the wrong decision can cause a program to run at disk-like speeds instead of memory-like speeds; in current technology that means a program could run 10,000 or 100,000 times\n\n\n1  VPN = (VirtualAddress & VPN_MASK) >> SHIFT\n2  (Success, TlbEntry) = TLB_Lookup(VPN)\n3  if (Success == True)  // TLB Hit\n4      if (CanAccess(TlbEntry.ProtectBits) == True)\n5          Offset = VirtualAddress & OFFSET_MASK\n6          PhysAddr = (TlbEntry.PFN << SHIFT) | Offset\n7          Register = **AccessMemory(PhysAddr)**\n8      else\n9          RaiseException(PROTECTION_FAULT)\n10 else                // TLB Miss\n11     PTEAddr = PTBR + (VPN * sizeof(PTE))\n12     PTE = **AccessMemory(PTEAddr)**\n13     if (PTE.Valid == False)\n14         RaiseException(SEGMENTATION_FAULT)\n15     else\n16         if (CanAccess(PTE.ProtectBits) == False)\n17             RaiseException(PROTECTION_FAULT)\n18         else if (PTE.Present == True)\n19             // assuming hardware-managed TLB\n20             TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n21             RetryInstruction()\n22         else if (PTE.Present == False)\n23             RaiseException(PAGE_FAULT)\n\nFigure 21.2:\n   **Page-Fault Control Flow Algorithm (Hardware)**\n\n\nslower. Thus, such a policy is something we should study in some detail; indeed, that is exactly what we will do in the next chapter. For now, it is good enough to understand that such a policy exists, built on top of the mechanisms described here."
        },
        {
          "name": "Page Fault Control Flow",
          "content": "With all of this knowledge in place, we can now roughly sketch the complete control flow of memory access. In other words, when somebody asks you “what happens when a program fetches some data from memory?”, you should have a pretty good idea of all the different possibilities. See the control flow in Figures 21.2 and 21.3 for more details; the first figure shows what the hardware does during translation, and the second what the OS does upon a page fault.\n\n\nFrom the hardware control flow diagram in Figure 21.2, notice that there are now three important cases to understand when a TLB miss occurs. First, that the page was both\n   **present**\n   and\n   **valid**\n   (Lines 18–21); in this case, the TLB miss handler can simply grab the PFN from the PTE, retry the instruction (this time resulting in a TLB hit), and thus continue as described (many times) before. In the second case (Lines 22–23), the page fault handler must be run; although this was a legitimate page for\n\n\n1 PFN = FindFreePhysicalPage()\n2 if (PFN == -1)           // no free page found\n3   PFN = EvictPage()       // replacement algorithm\n4   DiskRead(PTE.DiskAddr, PFN) // sleep (wait for I/O)\n5   PTE.present = True      // update page table:\n6   PTE.PFN    = PFN        // (present/translation)\n7   RetryInstruction()      // retry instruction\nFigure 21.3:\n   **Page-Fault Control Flow Algorithm (Software)**\n\n\nthe process to access (it is valid, after all), it is not present in physical memory. Third (and finally), the access could be to an invalid page, due for example to a bug in the program (Lines 13–14). In this case, no other bits in the PTE really matter; the hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process.\n\n\nFrom the software control flow in Figure 21.3, we can see what the OS roughly must do in order to service the page fault. First, the OS must find a physical frame for the soon-to-be-faulted-in page to reside within; if there is no such page, we'll have to wait for the replacement algorithm to run and kick some pages out of memory, thus freeing them for use here. With a physical frame in hand, the handler then issues the I/O request to read in the page from swap space. Finally, when that slow operation completes, the OS updates the page table and retries the instruction. The retry will result in a TLB miss, and then, upon another retry, a TLB hit, at which point the hardware will be able to access the desired item."
        },
        {
          "name": "When Replacements Really Occur",
          "content": "Thus far, the way we've described how replacements occur assumes that the OS waits until memory is entirely full, and only then replaces (evicts) a page to make room for some other page. As you can imagine, this is a little bit unrealistic, and there are many reasons for the OS to keep a small portion of memory free more proactively.\n\n\nTo keep a small amount of memory free, most operating systems thus have some kind of\n   **high watermark (\n    \n     HW\n    \n    )**\n   and\n   **low watermark (\n    \n     LW\n    \n    )**\n   to help decide when to start evicting pages from memory. How this works is as follows: when the OS notices that there are fewer than\n   \n    LW\n   \n   pages available, a background thread that is responsible for freeing memory runs. The thread evicts pages until there are\n   \n    HW\n   \n   pages available. The background thread, sometimes called the\n   **swap daemon**\n   or\n   **page daemon**\n\n    1\n   \n   , then goes to sleep, happy that it has freed some memory for running processes and the OS to use.\n\n\nBy performing a number of replacements at once, new performance optimizations become possible. For example, many systems will\n   **cluster**\n\n\n1\n   \n   The word “daemon”, usually pronounced “demon”, is an old term for a background thread or process that does something useful. Turns out (once again!) that the source of the term is Multics [CS94].\n\n\n**TIP: DO WORK IN THE BACKGROUND**\nWhen you have some work to do, it is often a good idea to do it in the\n   **background**\n   to increase efficiency and to allow for grouping of operations. Operating systems often do work in the background; for example, many systems buffer file writes in memory before actually writing the data to disk. Doing so has many possible benefits: increased disk efficiency, as the disk may now receive many writes at once and thus better be able to schedule them; improved latency of writes, as the application thinks the writes completed quite quickly; the possibility of work reduction, as the writes may need never to go to disk (i.e., if the file is deleted); and better use of\n   **idle time**\n   , as the background work may possibly be done when the system is otherwise idle, thus better utilizing the hardware [G+95].\n\n\nor\n   **group**\n   a number of pages and write them out at once to the swap partition, thus increasing the efficiency of the disk [LL82]; as we will see later when we discuss disks in more detail, such clustering reduces seek and rotational overheads of a disk and thus increases performance noticeably.\n\n\nTo work with the background paging thread, the control flow in Figure 21.3 should be modified slightly; instead of performing a replacement directly, the algorithm would instead simply check if there are any free pages available. If not, it would inform the background paging thread that free pages are needed; when the thread frees up some pages, it would re-awaken the original thread, which could then page in the desired page and go about its work."
        }
      ]
    },
    {
      "name": "Beyond Physical Memory: Policies",
      "sections": [
        {
          "name": "Cache Management",
          "content": "Before diving into policies, we first describe the problem we are trying to solve in more detail. Given that main memory holds some subset of all the pages in the system, it can rightly be viewed as a\n   **cache**\n   for virtual memory pages in the system. Thus, our goal in picking a replacement policy for this cache is to minimize the number of\n   **cache misses**\n   , i.e., to minimize the number of times that we have to fetch a page from disk. Alternately, one can view our goal as maximizing the number of\n   **cache hits**\n   , i.e., the number of times a page that is accessed is found in memory.\n\n\nKnowing the number of cache hits and misses let us calculate the\n   **average memory access time (AMAT)**\n   for a program (a metric computer architects compute for hardware caches [HP06]). Specifically, given these values, we can compute the AMAT of a program as follows:\n\n\nAMAT = T_M + (P_{Miss} \\cdot T_D) \\quad (22.1)\n\n\nwhere\n   \n    T_M\n   \n   represents the cost of accessing memory,\n   \n    T_D\n   \n   the cost of accessing disk, and\n   \n    P_{Miss}\n   \n   the probability of not finding the data in the cache (a miss);\n   \n    P_{Miss}\n   \n   varies from 0.0 to 1.0, and sometimes we refer to a percent miss rate instead of a probability (e.g., a 10% miss rate means\n   \n    P_{Miss} = 0.10\n   \n   ). Note you always pay the cost of accessing the data in memory; when you miss, however, you must additionally pay the cost of fetching the data from disk.\n\n\nFor example, let us imagine a machine with a (tiny) address space: 4KB, with 256-byte pages. Thus, a virtual address has two components: a 4-bit VPN (the most-significant bits) and an 8-bit offset (the least-significant bits). Thus, a process in this example can access\n   \n    2^4\n   \n   or 16 total virtual pages. In this example, the process generates the following memory references (i.e., virtual addresses): 0x000, 0x100, 0x200, 0x300, 0x400, 0x500, 0x600, 0x700, 0x800, 0x900. These virtual addresses refer to the first byte of each of the first ten pages of the address space (the page number being the first hex digit of each virtual address).\n\n\nLet us further assume that every page except virtual page 3 is already in memory. Thus, our sequence of memory references will encounter the following behavior: hit, hit, hit, miss, hit, hit, hit, hit, hit, hit. We can compute the\n   **hit rate**\n   (the percent of references found in memory): 90%, as 9 out of 10 references are in memory. The\n   **miss rate**\n   is thus 10% (\n   \n    P_{Miss} = 0.1\n   \n   ). In general,\n   \n    P_{Hit} + P_{Miss} = 1.0\n   \n   ; hit rate plus miss rate sum to 100%.\n\n\nTo calculate AMAT, we need to know the cost of accessing memory and the cost of accessing disk. Assuming the cost of accessing memory (\n   \n    T_M\n   \n   ) is around 100 nanoseconds, and the cost of accessing disk (\n   \n    T_D\n   \n   ) is about 10 milliseconds, we have the following AMAT:\n   \n    100ns + 0.1 \\cdot 10ms\n   \n   , which is\n   \n    100ns + 1ms\n   \n   , or 1.0001 ms, or about 1 millisecond. If our hit rate had instead been 99.9% (\n   \n    P_{miss} = 0.001\n   \n   ), the result is quite different: AMAT is 10.1 microseconds, or roughly 100 times faster. As the hit rate approaches 100%, AMAT approaches 100 nanoseconds.\n\n\nUnfortunately, as you can see in this example, the cost of disk access is so high in modern systems that even a tiny miss rate will quickly dominate the overall AMAT of running programs. Clearly, we need to avoid as many misses as possible or run slowly, at the rate of the disk. One way to help with this is to carefully develop a smart policy, as we now do."
        },
        {
          "name": "The Optimal Replacement Policy",
          "content": "To better understand how a particular replacement policy works, it would be nice to compare it to the best possible replacement policy. As it turns out, such an\n   **optimal**\n   policy was developed by Belady many years ago [B66] (he originally called it MIN). The optimal replacement policy leads to the fewest number of misses overall. Belady showed that a simple (but, unfortunately, difficult to implement!) approach that replaces the page that will be accessed\n   *furthest in the future*\n   is the optimal policy, resulting in the fewest-possible cache misses.\n\n\n\n\n**TIP: COMPARING AGAINST OPTIMAL IS USEFUL**\n\n\nAlthough optimal is not very practical as a real policy, it is incredibly useful as a comparison point in simulation or other studies. Saying that your fancy new algorithm has a 80% hit rate isn't meaningful in isolation; saying that optimal achieves an 82% hit rate (and thus your new approach is quite close to optimal) makes the result more meaningful and gives it context. Thus, in any study you perform, knowing what the optimal is lets you perform a better comparison, showing how much improvement is still possible, and also when you can\n   *stop*\n   making your policy better, because it is close enough to the ideal [AD03].\n\n\nHopefully, the intuition behind the optimal policy makes sense. Think about it like this: if you have to throw out some page, why not throw out the one that is needed the furthest from now? By doing so, you are essentially saying that all the other pages in the cache are more important than the one furthest out. The reason this is true is simple: you will refer to the other pages before you refer to the one furthest out.\n\n\nLet's trace through a simple example to understand the decisions the optimal policy makes. Assume a program accesses the following stream of virtual pages: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1. Figure 22.1 shows the behavior of optimal, assuming a cache that fits three pages.\n\n\nIn the figure, you can see the following actions. Not surprisingly, the first three accesses are misses, as the cache begins in an empty state; such a miss is sometimes referred to as a\n   **cold-start miss**\n   (or\n   **compulsory miss**\n   ). Then we refer again to pages 0 and 1, which both hit in the cache. Finally, we reach another miss (to page 3), but this time the cache is full; a replacement must take place! Which begs the question: which page should we replace? With the optimal policy, we examine the future for each page currently in the cache (0, 1, and 2), and see that 0 is accessed almost immediately, 1 is accessed a little later, and 2 is accessed furthest in the future. Thus the optimal policy has an easy choice: evict page 2, resulting in\n\n\n\nAccess | Hit/Miss? | Evict | Resulting Cache State\n0 | Miss |  | 0\n1 | Miss |  | 0, 1\n2 | Miss |  | 0, 1, 2\n0 | Hit |  | 0, 1, 2\n1 | Hit |  | 0, 1, 2\n3 | Miss | 2 | 0, 1, 3\n0 | Hit |  | 0, 1, 3\n3 | Hit |  | 0, 1, 3\n1 | Hit |  | 0, 1, 3\n2 | Miss | 3 | 0, 1, 2\n1 | Hit |  | 0, 1, 2\n\n\nFigure 22.1: Tracing The Optimal Policy\n\n\n\n\n**ASIDE: TYPES OF CACHE MISSES**\n\n\nIn the computer architecture world, architects sometimes find it useful to characterize misses by type, into one of three categories: compulsory, capacity, and conflict misses, sometimes called the\n   **Three C's**\n   [H87]. A\n   **compulsory miss**\n   (or\n   **cold-start miss**\n   [EF78]) occurs because the cache is empty to begin with and this is the first reference to the item; in contrast, a\n   **capacity miss**\n   occurs because the cache ran out of space and had to evict an item to bring a new item into the cache. The third type of miss (a\n   **conflict miss**\n   ) arises in hardware because of limits on where an item can be placed in a hardware cache, due to something known as\n   **set-associativity**\n   ; it does not arise in the OS page cache because such caches are always\n   **fully-associative**\n   , i.e., there are no restrictions on where in memory a page can be placed. See H&P for details [HP06].\n\n\npages 0, 1, and 3 in the cache. The next three references are hits, but then we get to page 2, which we evicted long ago, and suffer another miss. Here the optimal policy again examines the future for each page in the cache (0, 1, and 3), and sees that as long as it doesn't evict page 1 (which is about to be accessed), we'll be OK. The example shows page 3 getting evicted, although 0 would have been a fine choice too. Finally, we hit on page 1 and the trace completes.\n\n\nWe can also calculate the hit rate for the cache: with 6 hits and 5 misses, the hit rate is\n   \n    \\frac{\\text{Hits}}{\\text{Hits} + \\text{Misses}}\n   \n   which is\n   \n    \\frac{6}{6+5}\n   \n   or 54.5%. You can also compute the hit rate\n   *modulo*\n   compulsory misses (i.e., ignore the\n   *first*\n   miss to a given page), resulting in a 85.7% hit rate.\n\n\nUnfortunately, as we saw before in the development of scheduling policies, the future is not generally known; you can't build the optimal policy for a general-purpose operating system\n   \n    1\n   \n   . Thus, in developing a real, deployable policy, we will focus on approaches that find some other way to decide which page to evict. The optimal policy will thus serve only as a comparison point, to know how close we are to \"perfect\"."
        },
        {
          "name": "A Simple Policy: FIFOMany",
          "content": "Many early systems avoided the complexity of trying to approach optimal and employed very simple replacement policies. For example, some systems used\n   **FIFO**\n   (first-in, first-out) replacement, where pages were simply placed in a queue when they enter the system; when a replacement occurs, the page on the tail of the queue (the \"first-in\" page) is evicted. FIFO has one great strength: it is quite simple to implement.\n\n\nLet's examine how FIFO does on our example reference stream (Figure 22.2, page 5). We again begin our trace with three compulsory misses to\n\n\n1\n   \n   If you can, let us know! We can become rich together. Or, like the scientists who \"discovered\" cold fusion, widely scorned and mocked [FP89].\n\n\n\nAccess | Hit/Miss? | Evict | Resulting\n      \n      Cache State\n0 | Miss |  | First-in→ 0\n1 | Miss |  | First-in→ 0, 1\n2 | Miss |  | First-in→ 0, 1, 2\n0 | Hit |  | First-in→ 0, 1, 2\n1 | Hit |  | First-in→ 0, 1, 2\n3 | Miss | 0 | First-in→ 1, 2, 3\n0 | Miss | 1 | First-in→ 2, 3, 0\n3 | Hit |  | First-in→ 2, 3, 0\n1 | Miss | 2 | First-in→ 3, 0, 1\n2 | Miss | 3 | First-in→ 0, 1, 2\n1 | Hit |  | First-in→ 0, 1, 2\n\n\nFigure 22.2: Tracing The FIFO Policy\n\n\npages 0, 1, and 2, and then hit on both 0 and 1. Next, page 3 is referenced, causing a miss; the replacement decision is easy with FIFO: pick the page that was the “first one” in (the cache state in the figure is kept in FIFO order, with the first-in page on the left), which is page 0. Unfortunately, our next access is to page 0, causing another miss and replacement (of page 1). We then hit on page 3, but miss on 1 and 2, and finally hit on 1.\n\n\nComparing FIFO to optimal, FIFO does notably worse: a 36.4% hit rate (or 57.1% excluding compulsory misses). FIFO simply can’t determine the importance of blocks: even though page 0 had been accessed a number of times, FIFO still kicks it out, simply because it was the first one brought into memory.\n\n\n\n\n**ASIDE: BELADY’S ANOMALY**\n\n\nBelady (of the optimal policy) and colleagues found an interesting reference stream that behaved a little unexpectedly [BNS69]. The memory-reference stream: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5. The replacement policy they were studying was FIFO. The interesting part: how the cache hit rate changed when moving from a cache size of 3 to 4 pages.\n\n\nIn general, you would expect the cache hit rate to\n   *increase*\n   (get better) when the cache gets larger. But in this case, with FIFO, it gets worse! Calculate the hits and misses yourself and see. This odd behavior is generally referred to as\n   **Belady’s Anomaly**\n   (to the chagrin of his co-authors).\n\n\nSome other policies, such as LRU, don’t suffer from this problem. Can you guess why? As it turns out, LRU has what is known as a\n   **stack property**\n   [M+70]. For algorithms with this property, a cache of size\n   \n    N + 1\n   \n   naturally includes the contents of a cache of size\n   \n    N\n   \n   . Thus, when increasing the cache size, hit rate will either stay the same or improve. FIFO and Random (among others) clearly do not obey the stack property, and thus are susceptible to anomalous behavior.\n\n\n\nAccess | Hit/Miss? | Evict | Resulting Cache State\n0 | Miss |  | 0\n1 | Miss |  | 0, 1\n2 | Miss |  | 0, 1, 2\n0 | Hit |  | 0, 1, 2\n1 | Hit |  | 0, 1, 2\n3 | Miss | 0 | 1, 2, 3\n0 | Miss | 1 | 2, 3, 0\n3 | Hit |  | 2, 3, 0\n1 | Miss | 3 | 2, 0, 1\n2 | Hit |  | 2, 0, 1\n1 | Hit |  | 2, 0, 1\n\n\nFigure 22.3: Tracing The Random Policy"
        },
        {
          "name": "Another Simple Policy: Random",
          "content": "Another similar replacement policy is Random, which simply picks a random page to replace under memory pressure. Random has properties similar to FIFO; it is simple to implement, but it doesn't really try to be too intelligent in picking which blocks to evict. Let's look at how Random does on our famous example reference stream (see Figure 22.3).\n\n\nOf course, how Random does depends entirely upon how lucky (or unlucky) Random gets in its choices. In the example above, Random does a little better than FIFO, and a little worse than optimal. In fact, we can run the Random experiment thousands of times and determine how it does in general. Figure 22.4 shows how many hits Random achieves over 10,000 trials, each with a different random seed. As you can see, sometimes (just over 40% of the time), Random is as good as optimal, achieving 6 hits on the example trace; sometimes it does much worse, achieving 2 hits or fewer. How Random does depends on the luck of the draw.\n\n\n\n\n![Histogram showing the frequency of the number of hits achieved by the Random policy over 10,000 trials. The x-axis is 'Number of Hits' (0 to 7) and the y-axis is 'Frequency' (0 to 50). The distribution is skewed right, with the highest frequency at 6 hits (approx. 45) and the lowest at 2 hits (approx. 1).](images/image_0070.jpeg)\n\n\nNumber of Hits | Frequency (approx.)\n0 | 0\n1 | 0\n2 | 1\n3 | 3\n4 | 15\n5 | 38\n6 | 45\n7 | 0\n\n\nHistogram showing the frequency of the number of hits achieved by the Random policy over 10,000 trials. The x-axis is 'Number of Hits' (0 to 7) and the y-axis is 'Frequency' (0 to 50). The distribution is skewed right, with the highest frequency at 6 hits (approx. 45) and the lowest at 2 hits (approx. 1).\n\n\nFigure 22.4: Random Performance Over 10,000 Trials\n\n\n\nAccess | Hit/Miss? | Evict | Resulting\n      \n      Cache State\n0 | Miss |  | LRU→ 0\n1 | Miss |  | LRU→ 0, 1\n2 | Miss |  | LRU→ 0, 1, 2\n0 | Hit |  | LRU→ 1, 2, 0\n1 | Hit |  | LRU→ 2, 0, 1\n3 | Miss | 2 | LRU→ 0, 1, 3\n0 | Hit |  | LRU→ 1, 3, 0\n3 | Hit |  | LRU→ 1, 0, 3\n1 | Hit |  | LRU→ 0, 3, 1\n2 | Miss | 0 | LRU→ 3, 1, 2\n1 | Hit |  | LRU→ 3, 2, 1\n\n\nFigure 22.5: Tracing The LRU Policy"
        },
        {
          "name": "Using History: LRU",
          "content": "Unfortunately, any policy as simple as FIFO or Random is likely to have a common problem: it might kick out an important page, one that is about to be referenced again. FIFO kicks out the page that was first brought in; if this happens to be a page with important code or data structures upon it, it gets thrown out anyhow, even though it will soon be paged back in. Thus, FIFO, Random, and similar policies are not likely to approach optimal; something smarter is needed.\n\n\nAs we did with scheduling policy, to improve our guess at the future, we once again lean on the past and use\n   *history*\n   as our guide. For example, if a program has accessed a page in the near past, it is likely to access it again in the near future.\n\n\nOne type of historical information a page-replacement policy could use is\n   **frequency**\n   ; if a page has been accessed many times, perhaps it should not be replaced as it clearly has some value. A more commonly-used property of a page is its\n   **recency**\n   of access; the more recently a page has been accessed, perhaps the more likely it will be accessed again.\n\n\nThis family of policies is based on what people refer to as the\n   **principle of locality**\n   [D70], which basically is just an observation about programs and their behavior. What this principle says, quite simply, is that programs tend to access certain code sequences (e.g., in a loop) and data structures (e.g., an array accessed by the loop) quite frequently; we should thus try to use history to figure out which pages are important, and keep those pages in memory when it comes to eviction time.\n\n\nAnd thus, a family of simple historically-based algorithms are born. The\n   **Least-Frequently-Used (LFU)**\n   policy replaces the least-frequently-used page when an eviction must take place. Similarly, the\n   **Least-Recently-Used (LRU)**\n   policy replaces the least-recently-used page. These algorithms are easy to remember: once you know the name, you know exactly what it does, which is an excellent property for a name.\n\n\nTo better understand LRU, let's examine how LRU does on our exam-\n\n\n\n\n**ASIDE: TYPES OF LOCALITY**\n\n\nThere are two types of locality that programs tend to exhibit. The first is known as\n   **spatial locality**\n   , which states that if a page\n   \n    P\n   \n   is accessed, it is likely the pages around it (say\n   \n    P - 1\n   \n   or\n   \n    P + 1\n   \n   ) will also likely be accessed. The second is\n   **temporal locality**\n   , which states that pages that have been accessed in the near past are likely to be accessed again in the near future. The assumption of the presence of these types of locality plays a large role in the caching hierarchies of hardware systems, which deploy many levels of instruction, data, and address-translation caching to help programs run fast when such locality exists.\n\n\nOf course, the\n   **principle of locality**\n   , as it is often called, is no hard-and-fast rule that all programs must obey. Indeed, some programs access memory (or disk) in rather random fashion and don't exhibit much or any locality in their access streams. Thus, while locality is a good thing to keep in mind while designing caches of any kind (hardware or software), it does not\n   *guarantee*\n   success. Rather, it is a heuristic that often proves useful in the design of computer systems.\n\n\nple reference stream. Figure 22.5 (page 7) shows the results. From the figure, you can see how LRU can use history to do better than stateless policies such as Random or FIFO. In the example, LRU evicts page 2 when it first has to replace a page, because 0 and 1 have been accessed more recently. It then replaces page 0 because 1 and 3 have been accessed more recently. In both cases, LRU's decision, based on history, turns out to be correct, and the next references are thus hits. Thus, in our example, LRU does as well as possible, matching optimal in its performance\n   \n    2\n   \n   .\n\n\nWe should also note that the opposites of these algorithms exist:\n   **Most-Frequently-Used (MFU)**\n   and\n   **Most-Recently-Used (MRU)**\n   . In most cases (not all!), these policies do not work well, as they ignore the locality most programs exhibit instead of embracing it."
        },
        {
          "name": "Workload Examples",
          "content": "Let's look at a few more examples in order to better understand how some of these policies behave. Here, we'll examine more complex\n   **workloads**\n   instead of small traces. However, even these workloads are greatly simplified; a better study would include application traces.\n\n\nOur first workload has no locality, which means that each reference is to a random page within the set of accessed pages. In this simple example, the workload accesses 100 unique pages over time, choosing the next page to refer to at random; overall, 10,000 pages are accessed. In the experiment, we vary the cache size from very small (1 page) to enough to hold all the unique pages (100 pages), in order to see how each policy behaves over the range of cache sizes.\n\n\n2\n   \n   OK, we cooked the results. But sometimes cooking is necessary to prove a point.\n\n\n\n\n![Figure 22.6: The No-Locality Workload. A line graph showing Hit Rate (0% to 100%) versus Cache Size (Blocks) (0 to 100). Four policies are plotted: OPT (orange line), LRU (blue line with circles), FIFO (green line with crosses), and RAND (blue line with squares). OPT shows the highest hit rate, followed by LRU, FIFO, and RAND. All policies converge to 100% hit rate as cache size increases.](images/image_0071.jpeg)\n\n\nCache Size (Blocks) | OPT Hit Rate (%) | LRU Hit Rate (%) | FIFO Hit Rate (%) | RAND Hit Rate (%)\n0 | 0 | 0 | 0 | 0\n20 | ~55 | ~35 | ~35 | ~35\n40 | ~75 | ~45 | ~45 | ~45\n60 | ~85 | ~55 | ~55 | ~55\n80 | ~95 | ~65 | ~65 | ~65\n100 | 100 | 100 | 100 | 100\n\n\nFigure 22.6: The No-Locality Workload. A line graph showing Hit Rate (0% to 100%) versus Cache Size (Blocks) (0 to 100). Four policies are plotted: OPT (orange line), LRU (blue line with circles), FIFO (green line with crosses), and RAND (blue line with squares). OPT shows the highest hit rate, followed by LRU, FIFO, and RAND. All policies converge to 100% hit rate as cache size increases.\n\n\nFigure 22.6:\n   **The No-Locality Workload**\n\n\nFigure 22.6 plots the results of the experiment for optimal, LRU, Random, and FIFO. The y-axis of the figure shows the hit rate that each policy achieves; the x-axis varies the cache size as described above.\n\n\nWe can draw a number of conclusions from the graph. First, when there is no locality in the workload, it doesn't matter much which realistic policy you are using; LRU, FIFO, and Random all perform the same, with the hit rate exactly determined by the size of the cache. Second, when the cache is large enough to fit the entire workload, it also doesn't matter which policy you use; all policies (even Random) converge to a 100% hit rate when all the referenced blocks fit in cache. Finally, you can see that optimal performs noticeably better than the realistic policies; peeking into the future, if it were possible, does a much better job of replacement.\n\n\nThe next workload we examine is called the \"80-20\" workload, which exhibits locality: 80% of the references are made to 20% of the pages (the \"hot\" pages); the remaining 20% of the references are made to the remaining 80% of the pages (the \"cold\" pages). In our workload, there are a total 100 unique pages again; thus, \"hot\" pages are referred to most of the time, and \"cold\" pages the remainder. Figure 22.7 (page 10) shows how the policies perform with this workload.\n\n\nAs you can see from the figure, while both random and FIFO do reasonably well, LRU does better, as it is more likely to hold onto the hot pages; as those pages have been referred to frequently in the past, they are likely to be referred to again in the near future. Optimal once again does better, showing that LRU's historical information is not perfect.\n\n\n\n\n![Figure 22.7: The 80-20 Workload. A line graph showing Hit Rate (0% to 100%) versus Cache Size (Blocks) (0 to 100). Four policies are compared: OPT (orange line), LRU (blue line with circles), FIFO (green line with crosses), and RAND (purple line with squares). OPT is the most efficient, reaching 100% hit rate at a cache size of approximately 10 blocks. LRU follows, reaching 100% at approximately 100 blocks. FIFO and RAND are the least efficient, both reaching 100% at approximately 100 blocks.](images/image_0072.jpeg)\n\n\nCache Size (Blocks) | OPT Hit Rate (%) | LRU Hit Rate (%) | FIFO Hit Rate (%) | RAND Hit Rate (%)\n0 | 0 | 0 | 0 | 0\n10 | 100 | ~95 | ~85 | ~75\n20 | 100 | ~90 | ~80 | ~70\n40 | 100 | ~85 | ~75 | ~65\n60 | 100 | ~80 | ~70 | ~60\n80 | 100 | ~75 | ~65 | ~55\n100 | 100 | 100 | 100 | 100\n\n\nFigure 22.7: The 80-20 Workload. A line graph showing Hit Rate (0% to 100%) versus Cache Size (Blocks) (0 to 100). Four policies are compared: OPT (orange line), LRU (blue line with circles), FIFO (green line with crosses), and RAND (purple line with squares). OPT is the most efficient, reaching 100% hit rate at a cache size of approximately 10 blocks. LRU follows, reaching 100% at approximately 100 blocks. FIFO and RAND are the least efficient, both reaching 100% at approximately 100 blocks.\n\n\nFigure 22.7:\n   **The 80-20 Workload**\n\n\nYou might now be wondering: is LRU’s improvement over Random and FIFO really that big of a deal? The answer, as usual, is “it depends.” If each miss is very costly (not uncommon), then even a small increase in hit rate (reduction in miss rate) can make a huge difference on performance. If misses are not so costly, then of course the benefits possible with LRU are not nearly as important.\n\n\nLet’s look at one final workload. We call this one the “looping sequential” workload, as in it, we refer to 50 pages in sequence, starting at 0, then 1, ..., up to page 49, and then we loop, repeating those accesses, for a total of 10,000 accesses to 50 unique pages. The last graph in Figure 22.8 shows the behavior of the policies under this workload.\n\n\nThis workload, common in many applications (including important commercial applications such as databases [CD85]), represents a worst-case for both LRU and FIFO. These algorithms, under a looping-sequential workload, kick out older pages; unfortunately, due to the looping nature of the workload, these older pages are going to be accessed sooner than the pages that the policies prefer to keep in cache. Indeed, even with a cache of size 49, a looping-sequential workload of 50 pages results in a 0% hit rate. Interestingly, Random fares notably better, not quite approaching optimal, but at least achieving a non-zero hit rate. Turns out that random has some nice properties; one such property is not having weird corner-case behaviors.\n\n\n\n\n![Line graph titled 'The Looping-Sequential Workload' showing Hit Rate vs Cache Size (Blocks) for four policies: OPT, LRU, FIFO, and RAND.](images/image_0073.jpeg)\n\n\nThe graph shows the performance of four memory policies under a looping-sequential workload. The x-axis represents Cache Size in Blocks (0 to 100), and the y-axis represents Hit Rate (0% to 100%).\n\n\n\nCache Size (Blocks) | OPT | LRU | FIFO | RAND\n0 | 0% | 0% | 0% | 0%\n20 | ~40% | ~10% | 0% | 0%\n40 | ~80% | ~30% | 0% | 0%\n50 | 100% | ~60% | 0% | 0%\n100 | 100% | 100% | 0% | 0%\n\n\nLine graph titled 'The Looping-Sequential Workload' showing Hit Rate vs Cache Size (Blocks) for four policies: OPT, LRU, FIFO, and RAND.\n\n\n**The Looping Workload**"
        },
        {
          "name": "Implementing Historical Algorithms",
          "content": "As you can see, an algorithm such as LRU can generally do a better job than simpler policies like FIFO or Random, which may throw out important pages. Unfortunately, historical policies present us with a new challenge: how do we implement them?\n\n\nLet's take, for example, LRU. To implement it perfectly, we need to do a lot of work. Specifically, upon each\n   *page access*\n   (i.e., each memory access, whether an instruction fetch or a load or store), we must update some data structure to move this page to the front of the list (i.e., the MRU side). Contrast this to FIFO, where the FIFO list of pages is only accessed when a page is\n   *evicted*\n   (by removing the first-in page) or when a new page is added to the list (to the last-in side). To keep track of which pages have been least- and most-recently used, the system has to do some accounting work\n   *on every memory reference*\n   . Clearly, without great care, such accounting could greatly reduce performance.\n\n\nOne method that could help speed this up is to add a little bit of hardware support. For example, a machine could update, on each page access, a time field in memory (for example, this could be in the per-process page table, or just in some separate array in memory, with one entry per physical page of the system). Thus, when a page is accessed, the time field would be set, by hardware, to the current time. Then, when replacing a page, the OS could simply scan all the time fields in the system to find the least-recently-used page.\n\n\nUnfortunately, as the number of pages in a system grows, scanning a huge array of times just to find the absolute least-recently-used page is prohibitively expensive. Imagine a modern machine with 4GB of memory, chopped into 4KB pages. This machine has 1 million pages, and thus finding the LRU page will take a long time, even at modern CPU speeds. Which begs the question: do we really need to find the absolute oldest page to replace? Can we instead survive with an approximation?\n\n\n\n\n**CRUX: HOW TO IMPLEMENT AN LRU REPLACEMENT POLICY**\n\n\nGiven that it will be expensive to implement perfect LRU, can we approximate it in some way, and still obtain the desired behavior?"
        },
        {
          "name": "Approximating LRU",
          "content": "As it turns out, the answer is yes: approximating LRU is more feasible from a computational-overhead standpoint, and indeed it is what many modern systems do. The idea requires some hardware support, in the form of a\n   **use bit**\n   (sometimes called the\n   **reference bit**\n   ), the first of which was implemented in the first system with paging, the Atlas one-level store [KE+62]. There is one use bit per page of the system, and the use bits live in memory somewhere (they could be in the per-process page tables, for example, or just in an array somewhere). Whenever a page is referenced (i.e., read or written), the use bit is set by hardware to 1. The hardware never clears the bit, though (i.e., sets it to 0); that is the responsibility of the OS.\n\n\nHow does the OS employ the use bit to approximate LRU? Well, there could be a lot of ways, but with the\n   **clock algorithm**\n   [C69], one simple approach was suggested. Imagine all the pages of the system arranged in a circular list. A\n   **clock hand**\n   points to some particular page to begin with (it doesn't really matter which). When a replacement must occur, the OS checks if the currently-pointed to page\n   \n    P\n   \n   has a use bit of 1 or 0. If 1, this implies that page\n   \n    P\n   \n   was recently used and thus is\n   *not*\n   a good candidate for replacement. Thus, the use bit for\n   \n    P\n   \n   is set to 0 (cleared), and the clock hand is incremented to the next page (\n   \n    P + 1\n   \n   ). The algorithm continues until it finds a use bit that is set to 0, implying this page has not been recently used (or, in the worst case, that all pages have been and that we have now searched through the entire set of pages, clearing all the bits).\n\n\nNote that this approach is not the only way to employ a use bit to approximate LRU. Indeed, any approach which periodically clears the use bits and then differentiates between which pages have use bits of 1 versus 0 to decide which to replace would be fine. The clock algorithm of Corbato's was just one early approach which met with some success, and had the nice property of not repeatedly scanning through all of memory looking for an unused page.\n\n\n\n\n![Figure 22.9: The 80-20 Workload With Clock. A line graph showing Hit Rate (0% to 100%) versus Cache Size (Blocks) (0 to 100) for five page replacement algorithms: OPT, LRU, FIFO, RAND, and Clock. OPT is the highest, followed by LRU, Clock, RAND, and FIFO.](images/image_0074.jpeg)\n\n\nThe graph, titled \"The 80-20 Workload\", plots Hit Rate (Y-axis, 0% to 100%) against Cache Size (Blocks) (X-axis, 0 to 100). The legend identifies five algorithms: OPT (orange line), LRU (blue line with circles), FIFO (green line with crosses), RAND (purple line with pluses), and Clock (black line with dots). OPT shows the highest hit rate, reaching nearly 100% at 100 blocks. LRU follows, reaching approximately 95% at 100 blocks. The Clock algorithm, which is the focus of the caption, follows LRU, reaching about 90% at 100 blocks. RAND and FIFO show significantly lower hit rates, both reaching approximately 80% at 100 blocks.\n\n\n\nCache Size (Blocks) | OPT (%) | LRU (%) | FIFO (%) | RAND (%) | Clock (%)\n0 | 0 | 0 | 0 | 0 | 0\n20 | 80 | 70 | 60 | 50 | 55\n40 | 90 | 85 | 75 | 70 | 80\n60 | 95 | 90 | 80 | 75 | 85\n80 | 98 | 95 | 85 | 80 | 90\n100 | 100 | 98 | 88 | 82 | 92\n\n\nFigure 22.9: The 80-20 Workload With Clock. A line graph showing Hit Rate (0% to 100%) versus Cache Size (Blocks) (0 to 100) for five page replacement algorithms: OPT, LRU, FIFO, RAND, and Clock. OPT is the highest, followed by LRU, Clock, RAND, and FIFO.\n\n\nFigure 22.9: The 80-20 Workload With Clock\n\n\nThe behavior of a clock algorithm variant is shown in Figure 22.9. This variant randomly scans pages when doing a replacement; when it encounters a page with a reference bit set to 1, it clears the bit (i.e., sets it to 0); when it finds a page with the reference bit set to 0, it chooses it as its victim. As you can see, although it doesn't do quite as well as perfect LRU, it does better than approaches that don't consider history at all."
        },
        {
          "name": "Considering Dirty Pages",
          "content": "One small modification to the clock algorithm (also originally suggested by Corbato [C69]) that is commonly made is the additional consideration of whether a page has been\n   **modified**\n   or not while in memory. The reason for this: if a page has been\n   **modified**\n   and is thus\n   **dirty**\n   , it must be written back to disk to evict it, which is expensive. If it has not been modified (and is thus\n   **clean**\n   ), the eviction is free; the physical frame can simply be reused for other purposes without additional I/O. Thus, some VM systems prefer to evict clean pages over dirty pages.\n\n\nTo support this behavior, the hardware should include a\n   **modified bit**\n   (a.k.a.\n   **dirty bit**\n   ). This bit is set any time a page is written, and thus can be incorporated into the page-replacement algorithm. The clock algorithm, for example, could be changed to scan for pages that are both unused and clean to evict first; failing to find those, then for unused pages that are dirty, and so forth."
        },
        {
          "name": "Other VM Policies",
          "content": "Page replacement is not the only policy the VM subsystem employs (though it may be the most important). For example, the OS also has to decide\n   *when*\n   to bring a page into memory. This policy, sometimes called the\n   **page selection**\n   policy (as it was called by Denning [D70]), presents the OS with some different options.\n\n\nFor most pages, the OS simply uses\n   **demand paging**\n   , which means the OS brings the page into memory when it is accessed, “on demand” as it were. Of course, the OS could guess that a page is about to be used, and thus bring it in ahead of time; this behavior is known as\n   **prefetching**\n   and should only be done when there is reasonable chance of success. For example, some systems will assume that if a code page\n   \n    P\n   \n   is brought into memory, that code page\n   \n    P+1\n   \n   will likely soon be accessed and thus should be brought into memory too.\n\n\nAnother policy determines how the OS writes pages out to disk. Of course, they could simply be written out one at a time; however, many systems instead collect a number of pending writes together in memory and write them to disk in one (more efficient) write. This behavior is usually called\n   **clustering**\n   or simply\n   **grouping**\n   of writes, and is effective because of the nature of disk drives, which perform a single large write more efficiently than many small ones."
        },
        {
          "name": "Thrashing",
          "content": "Before closing, we address one final question: what should the OS do when memory is simply oversubscribed, and the memory demands of the set of running processes simply exceeds the available physical memory? In this case, the system will constantly be paging, a condition sometimes referred to as\n   **thrashing**\n   [D70].\n\n\nSome earlier operating systems had a fairly sophisticated set of mechanisms to both detect and cope with thrashing when it took place. For example, given a set of processes, a system could decide not to run a subset of processes, with the hope that the reduced set of processes’\n   **working sets**\n   (the pages that they are using actively) fit in memory and thus can make progress. This approach, generally known as\n   **admission control**\n   , states that it is sometimes better to do less work well than to try to do everything at once poorly, a situation we often encounter in real life as well as in modern computer systems (sadly).\n\n\nSome current systems take more a draconian approach to memory overload. For example, some versions of Linux run an\n   **out-of-memory killer**\n   when memory is oversubscribed; this daemon chooses a memory-intensive process and kills it, thus reducing memory in a none-too-subtle manner. While successful at reducing memory pressure, this approach can have problems, if, for example, it kills the X server and thus renders any applications requiring the display unusable."
        }
      ]
    },
    {
      "name": "Complete Virtual Memory Systems",
      "sections": [
        {
          "name": "VAX/VMS Virtual Memory",
          "content": "The VAX-11 minicomputer architecture was introduced in the late 1970's by\n   **Digital Equipment Corporation (DEC)**\n   . DEC was a massive player in the computer industry during the era of the mini-computer; unfortunately, a series of bad decisions and the advent of the PC slowly (but surely) led to their demise [C03]. The architecture was realized in a number of implementations, including the VAX-11/780 and the less powerful VAX-11/750.\n\n\nThe OS for the system was known as VAX/VMS (or just plain VMS), one of whose primary architects was Dave Cutler, who later led the effort to develop Microsoft's Windows NT [C93]. VMS had the general problem that it would be run on a broad range of machines, including very inexpensive VAXen (yes, that is the proper plural) to extremely high-end and powerful machines in the same architecture family. Thus, the OS had to have mechanisms and policies that worked (and worked well) across this huge range of systems.\n\n\nAs an additional issue, VMS is an excellent example of software innovations used to hide some of the inherent flaws of the architecture. Although the OS often relies on the hardware to build efficient abstractions and illusions, sometimes the hardware designers don't quite get everything right; in the VAX hardware, we'll see a few examples of this, and what the VMS operating system does to build an effective, working system despite these hardware flaws.\n\n\n\n\n**Memory Management Hardware**\n\n\nThe VAX-11 provided a 32-bit virtual address space per process, divided into 512-byte pages. Thus, a virtual address consisted of a 23-bit VPN and a 9-bit offset. Further, the upper two bits of the VPN were used to differentiate which segment the page resided within; thus, the system was a hybrid of paging and segmentation, as we saw previously.\n\n\nThe lower-half of the address space was known as \"process space\" and is unique to each process. In the first half of process space (known as\n   \n    P_0\n   \n   ), the user program is found, as well as a heap which grows downward. In the second half of process space (\n   \n    P_1\n   \n   ), we find the stack, which grows upwards. The upper-half of the address space is known as system space (\n   \n    S\n   \n   ), although only half of it is used. Protected OS code and data reside here, and the OS is in this way shared across processes.\n\n\nOne major concern of the VMS designers was the incredibly small size of pages in the VAX hardware (512 bytes). This size, chosen for historical reasons, has the fundamental problem of making simple linear page tables excessively large. Thus, one of the first goals of the VMS designers was to ensure that VMS would not overwhelm memory with page tables.\n\n\nThe system reduced the pressure page tables place on memory in two ways. First, by segmenting the user address space into two, the VAX-11 provides a page table for each of these regions (\n   \n    P_0\n   \n   and\n   \n    P_1\n   \n   ) per process;\n\n\n\n\n**ASIDE: THE CURSE OF GENERALITY**\n\n\nOperating systems often have a problem known as\n   **the curse of generality**\n   , where they are tasked with general support for a broad class of applications and systems. The fundamental result of the curse is that the OS is not likely to support any one installation very well. In the case of VMS, the curse was very real, as the VAX-11 architecture was realized in a number of different implementations. It is no less real today, where Linux is expected to run well on your phone, a TV set-top box, a laptop computer, desktop computer, and a high-end server running thousands of processes in a cloud-based datacenter.\n\n\nthus, no page-table space is needed for the unused portion of the address space between the stack and the heap. The base and bounds registers are used as you would expect; a base register holds the address of the page table for that segment, and the bounds holds its size (i.e., number of page-table entries).\n\n\nSecond, the OS reduces memory pressure even further by placing user page tables (for\n   \n    P_0\n   \n   and\n   \n    P_1\n   \n   , thus two per process) in kernel virtual memory. Thus, when allocating or growing a page table, the kernel allocates space out of its own virtual memory, in segment\n   \n    S\n   \n   . If memory comes under severe pressure, the kernel can swap pages of these page tables out to disk, thus making physical memory available for other uses.\n\n\nPutting page tables in kernel virtual memory means that address translation is even further complicated. For example, to translate a virtual address in\n   \n    P_0\n   \n   or\n   \n    P_1\n   \n   , the hardware has to first try to look up the page-table entry for that page in its page table (the\n   \n    P_0\n   \n   or\n   \n    P_1\n   \n   page table for that process); in doing so, however, the hardware may first have to consult the system page table (which lives in physical memory); with that translation complete, the hardware can learn the address of the page of the page table, and then finally learn the address of the desired memory access. All of this, fortunately, is made faster by the VAX's hardware-managed TLBs, which usually (hopefully) circumvent this laborious lookup.\n\n\n\n\n**A Real Address Space**\n\n\nOne neat aspect of studying VMS is that we can see how a real address space is constructed (Figure 23.1). Thus far, we have assumed a simple address space of just user code, user data, and user heap, but as we can see above, a real address space is notably more complex.\n\n\nFor example, the code segment never begins at page 0. This page, instead, is marked inaccessible, in order to provide some support for detecting\n   **null-pointer**\n   accesses. Thus, one concern when designing an address space is support for debugging, which the inaccessible zero page provides here in some form.\n\n\nPerhaps more importantly, the kernel virtual address space (i.e., its data structures and code) is a part of each user address space. On a con-\n\n\n\n\n![Diagram of the VAX/VMS Address Space showing the layout of virtual memory from address 0 to 2^32.](images/image_0075.jpeg)\n\n\nThe diagram illustrates the VAX/VMS Address Space, showing the layout of virtual memory from address 0 to\n    \n     2^{32}\n    \n    . The address space is divided into three main sections: User (P0), User (P1), and System (S).\n\n\n  * **User (P0):**\n     This section starts at address 0 and ends at\n     \n      2^{30}\n     \n     . It contains:\n       * Page 0: Invalid (black bar at address 0)\n  * User Code (light gray)\n  * User Heap (light gray)\n  * **User (P1):**\n     This section starts at address\n     \n      2^{30}\n     \n     and ends at\n     \n      2^{31}\n     \n     . It contains:\n       * User Stack (light gray)\n  * Trap Tables (dark gray)\n  * Kernel Data (dark gray)\n  * **System (S):**\n     This section starts at address\n     \n      2^{31}\n     \n     and ends at\n     \n      2^{32}\n     \n     . It contains:\n       * Kernel Code (dark gray)\n  * Kernel Heap (dark gray)\n  * Unused (hatched area from\n       \n        2^{31}\n       \n       to\n       \n        2^{32}\n       \n       )\n\n\nArrows indicate the direction of growth for the User Heap (downwards) and the User Stack (upwards). The System (S) section is shared between User (P0) and User (P1).\n\n\nDiagram of the VAX/VMS Address Space showing the layout of virtual memory from address 0 to 2^32.\n\n\nFigure 23.1:\n   **The VAX/VMS Address Space**\n\n\ntext switch, the OS changes the\n   \n    P_0\n   \n   and\n   \n    P_1\n   \n   registers to point to the appropriate page tables of the soon-to-be-run process; however, it does not change the\n   \n    S\n   \n   base and bound registers, and as a result the “same” kernel structures are mapped into each user address space.\n\n\nThe kernel is mapped into each address space for a number of reasons. This construction makes life easier for the kernel; when, for example, the OS is handed a pointer from a user program (e.g., on a\n   \n    write()\n   \n   system\n\n\n**ASIDE: WHY NULL POINTER ACCESSES CAUSE SEG FAULTS**\nYou should now have a good understanding of exactly what happens on a null-pointer dereference. A process generates a virtual address of 0, by doing something like this:\n\n\nint *p = NULL; // set p = 0\n*p = 10;       // try to store 10 to virtual addr 0\nThe hardware tries to look up the VPN (also 0 here) in the TLB, and suffers a TLB miss. The page table is consulted, and the entry for VPN 0 is found to be marked invalid. Thus, we have an invalid access, which transfers control to the OS, which likely terminates the process (on UNIX systems, processes are sent a signal which allows them to react to such a fault; if uncaught, however, the process is killed).\n\n\ncall), it is easy to copy data from that pointer to its own structures. The OS is naturally written and compiled, without worry of where the data it is accessing comes from. If in contrast the kernel were located entirely in physical memory, it would be quite hard to do things like swap pages of the page table to disk; if the kernel were given its own address space, moving data between user applications and the kernel would again be complicated and painful. With this construction (now used widely), the kernel appears almost as a library to applications, albeit a protected one.\n\n\nOne last point about this address space relates to protection. Clearly, the OS does not want user applications reading or writing OS data or code. Thus, the hardware must support different protection levels for pages to enable this. The VAX did so by specifying, in protection bits in the page table, what privilege level the CPU must be at in order to access a particular page. Thus, system data and code are set to a higher level of protection than user data and code; an attempted access to such information from user code will generate a trap into the OS, and (you guessed it) the likely termination of the offending process.\n\n\n\n\n**Page Replacement**\n\n\nThe page table entry (PTE) in VAX contains the following bits: a valid bit, a protection field (4 bits), a modify (or dirty) bit, a field reserved for OS use (5 bits), and finally a physical frame number (PFN) to store the location of the page in physical memory. The astute reader might note: no\n   **reference bit**\n   ! Thus, the VMS replacement algorithm must make do without hardware support for determining which pages are active.\n\n\nThe developers were also concerned about\n   **memory hogs**\n   , programs that use a lot of memory and make it hard for other programs to run. Most of the policies we have looked at thus far are susceptible to such hogging; for example, LRU is a\n   *global*\n   policy that doesn't share memory fairly among processes.\n\n\n\n\n**ASIDE: EMULATING REFERENCE BITS**\n\n\nAs it turns out, you don't need a hardware reference bit in order to get some notion of which pages are in use in a system. In fact, in the early 1980's, Babaoglu and Joy showed that protection bits on the VAX can be used to emulate reference bits [BJ81]. The basic idea: if you want to gain some understanding of which pages are actively being used in a system, mark all of the pages in the page table as inaccessible (but keep around the information as to which pages are really accessible by the process, perhaps in the \"reserved OS field\" portion of the page table entry). When a process accesses a page, it will generate a trap into the OS; the OS will then check if the page really should be accessible, and if so, revert the page to its normal protections (e.g., read-only, or read-write). At the time of a replacement, the OS can check which pages remain marked inaccessible, and thus get an idea of which pages have not been recently used.\n\n\nThe key to this \"emulation\" of reference bits is reducing overhead while still obtaining a good idea of page usage. The OS must not be too aggressive in marking pages inaccessible, or overhead would be too high. The OS also must not be too passive in such marking, or all pages will end up referenced; the OS will again have no good idea which page to evict.\n\n\nTo address these two problems, the developers came up with the\n   **segmented FIFO**\n   replacement policy [RL81]. The idea is simple: each process has a maximum number of pages it can keep in memory, known as its\n   **resident set size (RSS)**\n   . Each of these pages is kept on a FIFO list; when a process exceeds its RSS, the \"first-in\" page is evicted. FIFO clearly does not need any support from the hardware, and is thus easy to implement.\n\n\nOf course, pure FIFO does not perform particularly well, as we saw earlier. To improve FIFO's performance, VMS introduced two\n   **second-chance lists**\n   where pages are placed before getting evicted from memory, specifically a global\n   *clean-page free list*\n   and\n   *dirty-page list*\n   . When a process\n   \n    P\n   \n   exceeds its RSS, a page is removed from its per-process FIFO; if clean (not modified), it is placed on the end of the clean-page list; if dirty (modified), it is placed on the end of the dirty-page list.\n\n\nIf another process\n   \n    Q\n   \n   needs a free page, it takes the first free page off of the global clean list. However, if the original process\n   \n    P\n   \n   faults on that page before it is reclaimed,\n   \n    P\n   \n   reclaims it from the free (or dirty) list, thus avoiding a costly disk access. The bigger these global second-chance lists are, the closer the segmented FIFO algorithm performs to LRU [RL81].\n\n\nAnother optimization used in VMS also helps overcome the small page size in VMS. Specifically, with such small pages, disk I/O during swapping could be highly inefficient, as disks do better with large transfers. To make swapping I/O more efficient, VMS adds a number of optimizations, but most important is\n   **clustering**\n   . With clustering, VMS groups large batches of pages together from the global dirty list, and writes them\n\n\nto disk in one fell swoop (thus making them clean). Clustering is used in most modern systems, as the freedom to place pages anywhere within swap space lets the OS group pages, perform fewer and bigger writes, and thus improve performance.\n\n\n\n\n**Other Neat Tricks**\n\n\nVMS had two other now-standard tricks: demand zeroing and copy-on-write. We now describe these\n   **lazy**\n   optimizations. One form of laziness in VMS (and most modern systems) is\n   **demand zeroing**\n   of pages. To understand this better, let's consider the example of adding a page to your address space, say in your heap. In a naive implementation, the OS responds to a request to add a page to your heap by finding a page in physical memory, zeroing it (required for security; otherwise you'd be able to see what was on the page from when some other process used it!), and then mapping it into your address space (i.e., setting up the page table to refer to that physical page as desired). But the naive implementation can be costly, particularly if the page does not get used by the process.\n\n\nWith demand zeroing, the OS instead does very little work when the page is added to your address space; it puts an entry in the page table that marks the page inaccessible. If the process then reads or writes the page, a trap into the OS takes place. When handling the trap, the OS notices (usually through some bits marked in the “reserved for OS” portion of the page table entry) that this is actually a demand-zero page; at this point, the OS does the needed work of finding a physical page, zeroing it, and mapping it into the process's address space. If the process never accesses the page, all such work is avoided, and thus the virtue of demand zeroing.\n\n\nAnother cool optimization found in VMS (and again, in virtually every modern OS) is\n   **copy-on-write**\n   (COW for short). The idea, which goes at least back to the TENEX operating system [BB+72], is simple: when the OS needs to copy a page from one address space to another, instead of copying it, it can map it into the target address space and mark it read-only in both address spaces. If both address spaces only read the page, no further action is taken, and thus the OS has realized a fast copy without actually moving any data.\n\n\nIf, however, one of the address spaces does indeed try to write to the page, it will trap into the OS. The OS will then notice that the page is a COW page, and thus (lazily) allocate a new page, fill it with the data, and map this new page into the address space of the faulting process. The process then continues and now has its own private copy of the page.\n\n\nCOW is useful for a number of reasons. Certainly any sort of shared library can be mapped copy-on-write into the address spaces of many processes, saving valuable memory space. In UNIX systems, COW is even more critical, due to the semantics of\n   \n    fork()\n   \n   and\n   \n    exec()\n   \n   . As you might recall,\n   \n    fork()\n   \n   creates an exact copy of the address space of the caller; with a large address space, making such a copy is slow and data intensive. Even worse, most of the address space is immediately\n\n\n**TIP: BE LAZY**\nBeing lazy can be a virtue in both life as well as in operating systems. Laziness can put off work until later, which is beneficial within an OS for a number of reasons. First, putting off work might reduce the latency of the current operation, thus improving responsiveness; for example, operating systems often report that writes to a file succeeded immediately, and only write them to disk later in the background. Second, and more importantly, laziness sometimes obviates the need to do the work at all; for example, delaying a write until the file is deleted removes the need to do the write at all. Laziness is also good in life: for example, by putting off your OS project, you may find that the project specification bugs are worked out by your fellow classmates; however, the class project is unlikely to get canceled, so being too lazy may be problematic, leading to a late project, bad grade, and a sad professor. Don't make professors sad!\n\n\nover-written by a subsequent call to\n   \n    exec()\n   \n   , which overlays the calling process's address space with that of the soon-to-be-exec'd program. By instead performing a copy-on-write\n   \n    fork()\n   \n   , the OS avoids much of the needless copying and thus retains the correct semantics while improving performance."
        },
        {
          "name": "The Linux Virtual Memory System",
          "content": "We'll now discuss some of the more interesting aspects of the Linux VM system. Linux development has been driven forward by real engineers solving real problems encountered in production, and thus a large number of features have slowly been incorporated into what is now a fully functional, feature-filled virtual memory system.\n\n\nWhile we won't be able to discuss\n   *every*\n   aspect of Linux VM, we'll touch on the most important ones, especially where it has gone beyond what is found in classic VM systems such as VAX/VMS. We'll also try to highlight commonalities between Linux and older systems.\n\n\nFor this discussion, we'll focus on Linux for Intel x86. While Linux can and does run on many different processor architectures, Linux on x86 is its most dominant and important deployment, and thus the focus of our attention.\n\n\n\n\n**The Linux Address Space**\n\n\nMuch like other modern operating systems, and also like VAX/VMS, a Linux virtual address space\n   \n    1\n   \n   consists of a user portion (where user\n\n\n1\n   \n   Until recent changes, due to security threats, that is. Read the subsections below about Linux security for details on this modification.\n\n\n\n\n![Diagram of the Linux Address Space showing the split between User and Kernel portions.](images/image_0076.jpeg)\n\n\nThe diagram illustrates the Linux Address Space, which is divided into two main sections: User and Kernel. The User section starts at the top with a black bar labeled 'Page 0: Invalid' and ends at the address 0xC0000000. It contains three main segments: 'User Code', 'User Heap', and 'User Stack'. A downward arrow points from the 'User Heap' segment to the 'User Stack' segment. The Kernel section starts at the address 0xC0000000 and continues to the bottom. It contains two segments: 'Kernel (Logical)' and 'Kernel (Virtual)'. A vertical line on the left marks the boundary at 0xC0000000. Brackets on the right side group the 'User' and 'Kernel' sections.\n\n\nDiagram of the Linux Address Space showing the split between User and Kernel portions.\n\n\nFigure 23.2:\n   **The Linux Address Space**\n\n\nprogram code, stack, heap, and other parts reside) and a kernel portion (where kernel code, stacks, heap, and other parts reside). Like those other systems, upon a context switch, the user portion of the currently-running address space changes; the kernel portion is the same across processes. Like those other systems, a program running in user mode cannot access kernel virtual pages; only by trapping into the kernel and transitioning to privileged mode can such memory be accessed.\n\n\nIn classic 32-bit Linux (i.e., Linux with a 32-bit virtual address space), the split between user and kernel portions of the address space takes place at address 0xC0000000, or three-quarters of the way through the address space. Thus, virtual addresses 0 through 0xBFFFFFFF are user virtual addresses; the remaining virtual addresses (0xC0000000 through 0xFFFFFFFF) are in the kernel's virtual address space. 64-bit Linux has a similar split but at slightly different points. Figure 23.2 shows a depiction of a typical (simplified) address space.\n\n\nOne slightly interesting aspect of Linux is that it contains two types of kernel virtual addresses. The first are known as\n   **kernel logical addresses**\n   [O16]. This is what you would consider the normal virtual address space of the kernel; to get more memory of this type, kernel code merely needs to call\n   \n    kmalloc\n   \n   . Most kernel data structures live here, such as page tables, per-process kernel stacks, and so forth. Unlike most other memory in the system, kernel logical memory\n   *cannot*\n   be swapped to disk.\n\n\nThe most interesting aspect of kernel logical addresses is their connection to physical memory. Specifically, there is a direct mapping between kernel logical addresses and the first portion of physical memory. Thus, kernel logical address 0xC0000000 translates to physical address\n\n\n0x00000000, 0xC0000FFF to 0x00000FFF, and so forth. This direct mapping has two implications. The first is that it is simple to translate back and forth between kernel logical addresses and physical addresses; as a result, these addresses are often treated as if they are indeed physical. The second is that if a chunk of memory is contiguous in kernel logical address space, it is also contiguous in physical memory. This makes memory allocated in this part of the kernel's address space suitable for operations which need contiguous physical memory to work correctly, such as I/O transfers to and from devices via\n   **direct memory access (DMA)**\n   (something we'll learn about in the third part of this book).\n\n\nThe other type of kernel address is a\n   **kernel virtual address**\n   . To get memory of this type, kernel code calls a different allocator,\n   \n    vmalloc\n   \n   , which returns a pointer to a virtually contiguous region of the desired size. Unlike kernel logical memory, kernel virtual memory is usually not contiguous; each kernel virtual page may map to non-contiguous physical pages (and is thus not suitable for DMA). However, such memory is easier to allocate as a result, and thus used for large buffers where finding a contiguous large chunk of physical memory would be challenging.\n\n\nIn 32-bit Linux, one other reason for the existence of kernel virtual addresses is that they enable the kernel to address more than (roughly) 1 GB of memory. Years ago, machines had much less memory than this, and enabling access to more than 1 GB was not an issue. However, technology progressed, and soon there was a need to enable the kernel to use larger amounts of memory. Kernel virtual addresses, and their disconnection from a strict one-to-one mapping to physical memory, make this possible. However, with the move to 64-bit Linux, the need is less urgent, because the kernel is not confined to only the last 1 GB of the virtual address space.\n\n\n\n\n**Page Table Structure**\n\n\nBecause we are focused on Linux for x86, our discussion will center on the type of page-table structure provided by x86, as it determines what Linux can and cannot do. As mentioned before, x86 provides a hardware-managed, multi-level page table structure, with one page table per process; the OS simply sets up mappings in its memory, points a privileged register at the start of the page directory, and the hardware handles the rest. The OS gets involved, as expected, at process creation, deletion, and upon context switches, making sure in each case that the correct page table is being used by the hardware MMU to perform translations.\n\n\nProbably the biggest change in recent years is the move from 32-bit x86 to 64-bit x86, as briefly mentioned above. As seen in the VAX/VMS system, 32-bit address spaces have been around for a long time, and as technology changed, they were finally starting to become a real limit for programs. Virtual memory makes it easy to program systems, but with modern systems containing many GB of memory, 32 bits were no longer enough to refer to each of them. Thus, the next leap became necessary.\n\n\nMoving to a 64-bit address affects page table structure in x86 in the expected manner. Because x86 uses a multi-level page table, current 64-bit systems use a four-level table. The full 64-bit nature of the virtual address space is not yet in use, however, rather only the bottom 48 bits. Thus, a virtual address can be viewed as follows:\n\n\n\n\n![Diagram of a 64-bit virtual address structure showing bit ranges: 63-47 (Unused), 47-31 (P1), 31-15 (P2), 15-0 (P3, P4, Offset).](images/image_0077.jpeg)\n\n\nThe diagram illustrates the bit structure of a 64-bit virtual address. It shows a horizontal bar representing the 64 bits, with vertical lines indicating the boundaries between different fields. The fields are labeled above the bar at bit positions 63, 47, 31, 15, and 0. Below the bar, the fields are labeled: 'Unused' (bits 63-47), 'P1' (bits 47-31), 'P2' (bits 31-15), 'P3' (bits 15-0), 'P4' (bits 15-0), and 'Offset' (bits 0-0). The 'P3' and 'P4' labels are placed under the same bit range (bits 15-0), indicating they are part of the same level in the page table hierarchy.\n\n\nDiagram of a 64-bit virtual address structure showing bit ranges: 63-47 (Unused), 47-31 (P1), 31-15 (P2), 15-0 (P3, P4, Offset).\n\n\nAs you can see in the picture, the top 16 bits of a virtual address are unused (and thus play no role in translation), the bottom 12 bits (due to the 4-KB page size) are used as the offset (and hence just used directly, and not translated), leaving the middle 36 bits of virtual address to take part in the translation. The P1 portion of the address is used to index into the topmost page directory, and the translation proceeds from there, one level at a time, until the actual page of the page table is indexed by P4, yielding the desired page table entry.\n\n\nAs system memories grow even larger, more parts of this voluminous address space will become enabled, leading to five-level and eventually six-level page-table tree structures. Imagine that: a simple page table lookup requiring six levels of translation, just to figure out where in memory a certain piece of data resides.\n\n\n\n\n**Large Page Support**\n\n\nIntel x86 allows for the use of multiple page sizes, not just the standard 4-KB page. Specifically, recent designs support 2-MB and even 1-GB pages in hardware. Thus, over time, Linux has evolved to allow applications to utilize these\n   **huge pages**\n   (as they are called in the world of Linux).\n\n\nUsing huge pages, as hinted at earlier, leads to numerous benefits. As seen in VAX/VMS, doing so reduces the number of mappings that are needed in the page table; the larger the pages, the fewer the mappings. However, fewer page-table entries is not the driving force behind huge pages; rather, it's better TLB behavior and related performance gains.\n\n\nWhen a process actively uses a large amount of memory, it quickly fills up the TLB with translations. If those translations are for 4-KB pages, only a small amount of total memory can be accessed without inducing TLB misses. The result, for modern “big memory” workloads running on machines with many GBs of memory, is a noticeable performance cost; recent research shows that some applications spend 10% of their cycles servicing TLB misses [B+13].\n\n\nHuge pages allow a process to access a large tract of memory without TLB misses, by using fewer slots in the TLB, and thus is the main advantage. However, there are other benefits to huge pages: there is a shorter TLB-miss path, meaning that when a TLB miss does occur, it is\n\n\n\n\n**TIP: CONSIDER INCREMENTALISM**\n\n\nMany times in life, you are encouraged to be a revolutionary. “Think big!”, they say. “Change the world!”, they scream. And you can see why it is appealing; in some cases, big changes are needed, and thus pushing hard for them makes a lot of sense. And, if you try it this way, at least they might stop yelling at you.\n\n\nHowever, in many cases, a slower, more incremental approach might be the right thing to do. The Linux huge page example in this chapter is an example of engineering incrementalism; instead of taking the stance of a fundamentalist and insisting large pages were the way of the future, developers took the measured approach of first introducing specialized support for it, learning more about its upsides and downsides, and, only when there was real reason for it, adding more generic support for all applications.\n\n\nIncrementalism, while sometimes scorned, often leads to slow, thoughtful, and sensible progress. When building systems, such an approach might just be the thing you need. Indeed, this may be true in life as well.\n\n\nserviced more quickly. In addition, allocation can be quite fast (in certain scenarios), a small but sometimes important benefit.\n\n\nOne interesting aspect of Linux support for huge pages is how it was done incrementally. At first, Linux developers knew such support was only important for a few applications, such as large databases with stringent performance demands. Thus, the decision was made to allow applications to explicitly request memory allocations with large pages (either through the\n   \n    mmap()\n   \n   or\n   \n    shmget()\n   \n   calls). In this way, most applications would be unaffected (and continue to use only 4-KB pages); a few demanding applications would have to be changed to use these interfaces, but for them it would be worth the pain.\n\n\nMore recently, as the need for better TLB behavior is more common among many applications, Linux developers have added\n   **transparent**\n   huge page support. When this feature is enabled, the operating system automatically looks for opportunities to allocate huge pages (usually 2 MB, but on some systems, 1 GB) without requiring application modification.\n\n\nHuge pages are not without their costs. The biggest potential cost is\n   **internal fragmentation**\n   , i.e., a page that is large but sparsely used. This form of waste can fill memory with large but little used pages. Swapping, if enabled, also does not work well with huge pages, sometimes greatly amplifying the amount of I/O a system does. Overhead of allocation can also be bad (in some other cases). Overall, one thing is clear: the 4-KB page size which served systems so well for so many years is not the universal solution it once was; growing memory sizes demand that we consider large pages and other solutions as part of a necessary evolution of VM systems. Linux’s slow adoption of this hardware-based technology is evidence of the coming change.\n\n\n\n\n**The Page Cache**\n\n\nTo reduce costs of accessing persistent storage (the focus of the third part of this book), most systems use aggressive\n   **caching**\n   subsystems to keep popular data items in memory. Linux, in this regard, is no different than traditional operating systems.\n\n\nThe Linux\n   **page cache**\n   is unified, keeping pages in memory from three primary sources:\n   **memory-mapped files**\n   , file data and metadata from devices (usually accessed by directing\n   \n    read()\n   \n   and\n   \n    write()\n   \n   calls to the file system), and heap and stack pages that comprise each process (sometimes called\n   **anonymous memory**\n   , because there is no named file underneath of it, but rather swap space). These entities are kept in a\n   **page cache hash table**\n   , allowing for quick lookup when said data is needed.\n\n\nThe page cache tracks if entries are\n   **clean**\n   (read but not updated) or\n   **dirty**\n   (a.k.a.,\n   **modified**\n   ). Dirty data is periodically written to the backing store (i.e., to a specific file for file data, or to swap space for anonymous regions) by background threads (called\n   \n    pdflush\n   \n   ), thus ensuring that modified data eventually is written back to persistent storage. This background activity either takes place after a certain time period or if too many pages are considered dirty (both configurable parameters).\n\n\nIn some cases, a system runs low on memory, and Linux has to decide which pages to kick out of memory to free up space. To do so, Linux uses a modified form of\n   **2Q**\n   replacement [JS94], which we describe here.\n\n\nThe basic idea is simple: standard LRU replacement is effective, but can be subverted by certain common access patterns. For example, if a process repeatedly accesses a large file (especially one that is nearly the size of memory, or larger), LRU will kick every other file out of memory. Even worse: retaining portions of this file in memory isn't useful, as they are never re-referenced before getting kicked out of memory.\n\n\nThe Linux version of the 2Q replacement algorithm solves this problem by keeping two lists, and dividing memory between them. When accessed for the first time, a page is placed on one queue (called\n   \n    A_1\n   \n   in the original paper, but the\n   **inactive list**\n   in Linux); when it is re-referenced, the page is promoted to the other queue (called\n   \n    A_q\n   \n   in the original, but the\n   **active list**\n   in Linux). When replacement needs to take place, the candidate for replacement is taken from the inactive list. Linux also periodically moves pages from the bottom of the active list to the inactive list, keeping the active list to about two-thirds of the total page cache size [G04].\n\n\nLinux would ideally manage these lists in perfect LRU order, but, as discussed in earlier chapters, doing so is costly. Thus, as with many OSes, an approximation of LRU (similar to\n   **clock**\n   replacement) is used.\n\n\nThis 2Q approach generally behaves quite a bit like LRU, but notably handles the case where a cyclic large-file access occurs by confining the pages of that cyclic access to the inactive list. Because said pages are never re-referenced before getting kicked out of memory, they do not flush out other useful pages found in the active list.\n\n\n\n\n**ASIDE: THE UBIQUITY OF MEMORY-MAPPING**\n\n\nMemory mapping predates Linux by some years, and is used in many places within Linux and other modern systems. The idea is simple: by calling\n   \n    mmap()\n   \n   on an already opened file descriptor, a process is returned a pointer to the beginning of a region of virtual memory where the contents of the file seem to be located. By then using that pointer, a process can access any part of the file with a simple pointer dereference.\n\n\nAccesses to parts of a memory-mapped file that have not yet been brought into memory trigger\n   **page faults**\n   , at which point the OS will page in the relevant data and make it accessible by updating the page table of the process accordingly (i.e.,\n   **demand paging**\n   ).\n\n\nEvery regular Linux process uses memory-mapped files, even though the code in\n   \n    main()\n   \n   does not call\n   \n    mmap()\n   \n   directly, because of how Linux loads code from the executable and shared library code into memory. Below is the (highly abbreviated) output of the\n   \n    pmap\n   \n   command line tool, which shows what different mappings comprise the virtual address space of a running program (the shell, in this example,\n   \n    tcsh\n   \n   ). The output shows four columns: the virtual address of the mapping, its size, the protection bits of the region, and the source of the mapping:\n\n\n0000000000400000    372K r-x--  tcsh\n00000000019d5000   1780K rw---  [anon ]\n00007f4e7cf06000   1792K r-x--  libc-2.23.so\n00007f4e7d2d0000    36K r-x--  libcrypto-2.23.so\n00007f4e7d508000   148K r-x--  libtinfo.so.5.9\n00007f4e7d731000   152K r-x--  ld-2.23.so\n00007f4e7d932000    16K rw---  [stack ]\nAs you can see from this output, the code from the\n   \n    tcsh\n   \n   binary, as well as code from\n   \n    libc\n   \n   ,\n   \n    libcrypto\n   \n   ,\n   \n    libtinfo\n   \n   , and code from the dynamic linker itself (\n   \n    ld.so\n   \n   ) are all mapped into the address space. Also present are two anonymous regions, the heap (the second entry, labeled\n   \n    anon\n   \n   ) and the stack (labeled\n   \n    stack\n   \n   ). Memory-mapped files provide a straightforward and efficient way for the OS to construct a modern address space.\n\n\n\n\n**Security And Buffer Overflows**\n\n\nProbably the biggest difference between modern VM systems (Linux, Solaris, or one of the BSD variants) and ancient ones (VAX/VMS) is the emphasis on security in the modern era. Protection has always been a serious concern for operating systems, but with machines more interconnected than ever, it is no surprise that developers have implemented a variety of defensive countermeasures to halt those wily hackers from gaining control of systems.\n\n\nOne major threat is found in\n   **buffer overflow**\n   attacks\n   \n    2\n   \n   , which can be used against normal user programs and even the kernel itself. The idea of these attacks is to find a bug in the target system which lets the attacker inject arbitrary data into the target's address space. Such vulnerabilities sometime arise because the developer assumes (erroneously) that an input will not be overly long, and thus (trustingly) copies the input into a buffer; because the input is in fact too long, it overflows the buffer, thus overwriting memory of the target. Code as innocent as the below can be the source of the problem:\n\n\nint some_function(char *input) {\n    char dest_buffer[100];\n    strcpy(dest_buffer, input); // oops, unbounded copy!\n}\nIn many cases, such an overflow is not catastrophic, e.g., bad input innocently given to a user program or even the OS will probably cause it to crash, but no worse. However, malicious programmers can carefully craft the input that overflows the buffer so as to inject their own code into the targeted system, essentially allowing them to take it over and do their own bidding. If successful upon a network-connected user program, attackers can run arbitrary computations or even rent out cycles on the compromised system; if successful upon the operating system itself, the attack can access even more resources, and is a form of what is called\n   **privilege escalation**\n   (i.e., user code gaining kernel access rights). If you can't guess, these are all Bad Things.\n\n\nThe first and most simple defense against buffer overflow is to prevent execution of any code found within certain regions of an address space (e.g., within the stack). The\n   **NX bit**\n   (for No-eXecute), introduced by AMD into their version of x86 (a similar XD bit is now available on Intel's), is one such defense; it just prevents execution from any page which has this bit set in its corresponding page table entry. The approach prevents code, injected by an attacker into the target's stack, from being executed, and thus mitigates the problem.\n\n\nHowever, clever attackers are ... clever, and even when injected code cannot be added explicitly by the attacker, arbitrary code sequences can be executed by malicious code. The idea is known, in its most general form, as\n   **return-oriented programming (ROP)**\n   [S07], and really it is quite brilliant. The observation behind ROP is that there are lots of bits of code (\n   **gadgets**\n   , in ROP terminology) within any program's address space, especially C programs that link with the voluminous C library. Thus, an attacker can overwrite the stack such that the return address in the currently executing function points to a desired malicious instruction (or se-\n\n\n2\n   \n   See\n   https://en.wikipedia.org/wiki/Buffer_overflow\n   for some details and links about this topic, including a reference to the famous article by the security hacker Elias Levy, also known as \"Aleph One\".\n\n\nries of instructions), followed by a return instruction. By stringing together a large number of gadgets (i.e., ensuring each return jumps to the next gadget), the attacker can execute arbitrary code. Amazing!\n\n\nTo defend against ROP (including its earlier form, the\n   **return-to-libc attack**\n   [S+04]), Linux (and other systems) add another defense, known as\n   **address space layout randomization (ASLR)**\n   . Instead of placing code, stack, and the heap at fixed locations within the virtual address space, the OS randomizes their placement, thus making it quite challenging to craft the intricate code sequence required to implement this class of attacks. Most attacks on vulnerable user programs will thus cause crashes, but not be able to gain control of the running program.\n\n\nInterestingly, you can observe this randomness in practice rather easily. Here's a piece of code that demonstrates it on a modern Linux system:\n\n\nint main(int argc, char *argv[]) {\n    int stack = 0;\n    printf(\"%p\\n\", &stack);\n    return 0;\n}\nThis code just prints out the (virtual) address of a variable on the stack. In older non-ASLR systems, this value would be the same each time. But, as you can see below, the value changes with each run:\n\n\nprompt> ./random\n0x7ffd3e55d2b4\nprompt> ./random\n0x7ffe1033b8f4\nprompt> ./random\n0x7ffe45522e94\nASLR is such a useful defense for user-level programs that it has also been incorporated into the kernel, in a feature unimaginatively called\n   **kernel address space layout randomization (KASLR)**\n   . However, it turns out the kernel may have even bigger problems to handle, as we discuss next.\n\n\n\n\n**Other Security Problems: Meltdown And Spectre**\n\n\nAs we write these words (August, 2018), the world of systems security has been turned upside down by two new and related attacks. The first is called\n   **Meltdown**\n   , and the second\n   **Spectre**\n   . They were discovered at about the same time by four different groups of researchers/engineers, and have led to deep questioning of the fundamental protections offered by computer hardware and the OS above. See\n   spectreattack.com\n   for papers describing each attack in detail; Spectre is considered the more problematic of the two.\n\n\nThe general weakness exploited in each of these attacks is that the CPUs found in modern systems perform all sorts of crazy behind-the-scenes tricks to improve performance. One class of technique that lies at the core of the problem is called\n   **speculative execution**\n   , in which the CPU guesses which instructions will soon be executed in the future, and starts executing them ahead of time. If the guesses are correct, the program runs faster; if not, the CPU undoes their effects on architectural state (e.g., registers) and tries again, this time going down the right path.\n\n\nThe problem with speculation is that it tends to leave traces of its execution in various parts of the system, such as processor caches, branch predictors, etc. And thus the problem: as the authors of the attacks show, such state can make vulnerable the contents of memory, even memory that we thought was protected by the MMU.\n\n\nOne avenue to increasing kernel protection was thus to remove as much of the kernel address space from each user process and instead have a separate kernel page table for most kernel data (called\n   **kernel page-table isolation**\n   , or\n   **KPTI**\n   ) [G+17]. Thus, instead of mapping the kernel’s code and data structures into each process, only the barest minimum is kept therein; when switching into the kernel, then, a switch to the kernel page table is now needed. Doing so improves security and avoids some attack vectors, but at a cost: performance. Switching page tables is costly. Ah, the costs of security: convenience\n   *and*\n   performance.\n\n\nUnfortunately, KPTI doesn’t solve all of the security problems laid out above, just some of them. And simple solutions, such as turning off speculation, would make little sense, because systems would run thousands of times slower. Thus, it is an interesting time to be alive, if systems security is your thing.\n\n\nTo truly understand these attacks, you’ll (likely) have to learn a lot more first. Begin by understanding modern computer architecture, as found in advanced books on the topic, focusing on speculation and all the mechanisms needed to implement it. Definitely read about the Meltdown and Spectre attacks, at the websites mentioned above; they actually also include a useful primer on speculation, so perhaps are not a bad place to start. And study the operating system for further vulnerabilities. Who knows what problems remain?"
        }
      ]
    },
    {
      "name": "Concurrency: An Introduction",
      "sections": [
        {
          "name": "Why Use Threads?",
          "content": "Before getting into the details of threads and some of the problems you might have in writing multi-threaded programs, let's first answer a more simple question. Why should you use threads at all?\n\n\nAs it turns out, there are at least two major reasons you should use threads. The first is simple:\n   **parallelism**\n   . Imagine you are writing a program that performs operations on very large arrays, for example, adding two large arrays together, or incrementing the value of each element in the array by some amount. If you are running on just a single processor, the task is straightforward: just perform each operation and be done. However, if you are executing the program on a system with multiple processors, you have the potential of speeding up this process considerably by using the processors to each perform a portion of the work. The task of transforming your standard\n   **single-threaded**\n   program into a program that does this sort of work on multiple CPUs is called\n   **parallelization**\n   , and using a thread per CPU to do this work is a natural and typical way to make programs run faster on modern hardware.\n\n\nThe second reason is a bit more subtle: to avoid blocking program progress due to slow I/O. Imagine that you are writing a program that performs different types of I/O: either waiting to send or receive a message, for an explicit disk I/O to complete, or even (implicitly) for a page fault to finish. Instead of waiting, your program may wish to do something else, including utilizing the CPU to perform computation, or even issuing further I/O requests. Using threads is a natural way to avoid getting stuck; while one thread in your program waits (i.e., is blocked waiting for I/O), the CPU scheduler can switch to other threads, which are ready to run and do something useful. Threading enables\n   **overlap**\n   of I/O with other activities\n   *within*\n   a single program, much like\n   **multprogramming**\n   did for processes\n   *across*\n   programs; as a result, many modern server-based applications (web servers, database management systems, and the like) make use of threads in their implementations.\n\n\nOf course, in either of the cases mentioned above, you could use multiple\n   *processes*\n   instead of threads. However, threads share an address space and thus make it easy to share data, and hence are a natural choice when constructing these types of programs. Processes are a more sound choice for logically separate tasks where little sharing of data structures in memory is needed."
        },
        {
          "name": "An Example: Thread Creation",
          "content": "Let's get into some of the details. Say we wanted to run a program that creates two threads, each of which does some independent work, in this case printing \"A\" or \"B\". The code is shown in Figure 26.2 (page 4).\n\n\nThe main program creates two threads, each of which will run the function\n   \n    mythread()\n   \n   , though with different arguments (the string A or B). Once a thread is created, it may start running right away (depending on the whims of the scheduler); alternately, it may be put in a \"ready\" but not \"running\" state and thus not run yet. Of course, on a multiprocessor, the threads could even be running at the same time, but let's not worry about this possibility quite yet.\n\n\n1 #include <stdio.h>\n2 #include <assert.h>\n3 #include <pthread.h>\n4 #include \"common.h\"\n5 #include \"common_threads.h\"\n6\n7 void *mythread(void *arg) {\n8     printf(\"%s\\n\", (char *) arg);\n9     return NULL;\n10 }\n11\n12 int\n13 main(int argc, char *argv[]) {\n14     pthread_t p1, p2;\n15     int rc;\n16     printf(\"main: begin\\n\");\n17     Pthread_create(&p1, NULL, mythread, \"A\");\n18     Pthread_create(&p2, NULL, mythread, \"B\");\n19     // join waits for the threads to finish\n20     Pthread_join(p1, NULL);\n21     Pthread_join(p2, NULL);\n22     printf(\"main: end\\n\");\n23     return 0;\n24 }\nFigure 26.2: Simple Thread Creation Code (\n   **t0.c**\n   )\n\n\nAfter creating the two threads (let's call them T1 and T2), the main thread calls\n   \n    pthread_join()\n   \n   , which waits for a particular thread to complete. It does so twice, thus ensuring T1 and T2 will run and complete before finally allowing the main thread to run again; when it does, it will print \"main: end\" and exit. Overall, three threads were employed during this run: the main thread, T1, and T2.\n\n\nLet us examine the possible execution ordering of this little program. In the execution diagram (Figure 26.3, page 5), time increases in the downwards direction, and each column shows when a different thread (the main one, or Thread 1, or Thread 2) is running.\n\n\nNote, however, that this ordering is not the only possible ordering. In fact, given a sequence of instructions, there are quite a few, depending on which thread the scheduler decides to run at a given point. For example, once a thread is created, it may run immediately, which would lead to the execution shown in Figure 26.4 (page 5).\n\n\nWe also could even see \"B\" printed before \"A\", if, say, the scheduler decided to run Thread 2 first even though Thread 1 was created earlier; there is no reason to assume that a thread that is created first will run first. Figure 26.5 (page 6) shows this final execution ordering, with Thread 2 getting to strut its stuff before Thread 1.\n\n\nAs you might be able to see, one way to think about thread creation\n\n\n\nmain | Thread 1 | Thread2\nstarts running |  | \nprints \"main: begin\" |  | \ncreates Thread 1 |  | \ncreates Thread 2 |  | \nwaits for T1 |  | \n | runs | \n | prints \"A\" | \n | returns | \nwaits for T2 |  | \n |  | runs\n |  | prints \"B\"\n |  | returns\nprints \"main: end\" |  | \n\n\n**Thread Trace (1)**\n\nmain | Thread 1 | Thread2\nstarts running |  | \nprints \"main: begin\" |  | \ncreates Thread 1 |  | \n | runs | \n | prints \"A\" | \n | returns | \ncreates Thread 2 |  | \n |  | runs\n |  | prints \"B\"\n |  | returns\nwaits for T1 |  | \nreturns immediately; T1 is done |  | \nwaits for T2 |  | \nreturns immediately; T2 is done |  | \nprints \"main: end\" |  | \n\n\n**Thread Trace (2)**\nis that it is a bit like making a function call; however, instead of first executing the function and then returning to the caller, the system instead creates a new thread of execution for the routine that is being called, and it runs independently of the caller, perhaps before returning from the create, but perhaps much later. What runs next is determined by the OS\n   **scheduler**\n   , and although the scheduler likely implements some sensible algorithm, it is hard to know what will run at any given moment in time.\n\n\nAs you also might be able to tell from this example, threads make life complicated: it is already hard to tell what will run when! Computers are hard enough to understand without concurrency. Unfortunately, with concurrency, it simply gets worse. Much worse.\n\n\n\nmain | Thread 1 | Thread2\nstarts running |  | \nprints \"main: begin\" |  | \ncreates Thread 1 |  | \ncreates Thread 2 |  | \n |  | runs\n |  | prints \"B\"\n |  | returns\nwaits for T1 |  | \n | runs | \n | prints \"A\" | \n | returns | \nwaits for T2 |  | \nreturns immediately; T2 is done |  | \nprints \"main: end\" |  | \n\n\n**Thread Trace (3)**"
        },
        {
          "name": "Why It Gets Worse: Shared Data",
          "content": "The simple thread example we showed above was useful in showing how threads are created and how they can run in different orders depending on how the scheduler decides to run them. What it doesn't show you, though, is how threads interact when they access shared data.\n\n\nLet us imagine a simple example where two threads wish to update a global shared variable. The code we'll study is in Figure 26.6 (page 7).\n\n\nHere are a few notes about the code. First, as Stevens suggests [SR05], we wrap the thread creation and join routines to simply exit on failure; for a program as simple as this one, we want to at least notice an error occurred (if it did), but not do anything very smart about it (e.g., just exit). Thus,\n   \n    Pthread_create()\n   \n   simply calls\n   \n    pthread_create()\n   \n   and makes sure the return code is 0; if it isn't,\n   \n    Pthread_create()\n   \n   just prints a message and exits.\n\n\nSecond, instead of using two separate function bodies for the worker threads, we just use a single piece of code, and pass the thread an argument (in this case, a string) so we can have each thread print a different letter before its messages.\n\n\nFinally, and most importantly, we can now look at what each worker is trying to do: add a number to the shared variable\n   \n    counter\n   \n   , and do so 10 million times (\n   \n    1e7\n   \n   ) in a loop. Thus, the desired final result is: 20,000,000.\n\n\nWe now compile and run the program, to see how it behaves. Sometimes, everything works how we might expect:\n\n\nprompt> gcc -o main main.c -Wall -pthread; ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 20000000)\n1 #include <stdio.h>\n2 #include <pthread.h>\n3 #include \"common.h\"\n4 #include \"common_threads.h\"\n5\n6 static volatile int counter = 0;\n7\n8 // mythread()\n9 //\n10 // Simply adds 1 to counter repeatedly, in a loop\n11 // No, this is not how you would add 10,000,000 to\n12 // a counter, but it shows the problem nicely.\n13 //\n14 void *mythread(void *arg) {\n15     printf(\"%s: begin\\n\", (char *) arg);\n16     int i;\n17     for (i = 0; i < 1e7; i++) {\n18         counter = counter + 1;\n19     }\n20     printf(\"%s: done\\n\", (char *) arg);\n21     return NULL;\n22 }\n23\n24 // main()\n25 //\n26 // Just launches two threads (pthread_create)\n27 // and then waits for them (pthread_join)\n28 //\n29 int main(int argc, char *argv[]) {\n30     pthread_t p1, p2;\n31     printf(\"main: begin (counter = %d)\\n\", counter);\n32     Pthread_create(&p1, NULL, mythread, \"A\");\n33     Pthread_create(&p2, NULL, mythread, \"B\");\n34\n35     // join waits for the threads to finish\n36     Pthread_join(p1, NULL);\n37     Pthread_join(p2, NULL);\n38     printf(\"main: done with both (counter = %d)\\n\",\n39             counter);\n40     return 0;\n41 }\nFigure 26.6: Sharing Data: Uh Oh (t1.c)\n\n\nUnfortunately, when we run this code, even on a single processor, we don't necessarily get the desired result. Sometimes, we get:\n\n\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 19345221)\nLet's try it one more time, just to see if we've gone crazy. After all, aren't computers supposed to produce\n   **deterministic**\n   results, as you have been taught?! Perhaps your professors have been lying to you? (\n   *gasp*\n   )\n\n\nprompt> ./main\nmain: begin (counter = 0)\nA: begin\nB: begin\nA: done\nB: done\nmain: done with both (counter = 19221041)\nNot only is each run wrong, but also yields a\n   *different*\n   result! A big question remains: why does this happen?\n\n\n\n\n**TIP: KNOW AND USE YOUR TOOLS**\n\n\nYou should always learn new tools that help you write, debug, and understand computer systems. Here, we use a neat tool called a\n   **disassembler**\n   . When you run a disassembler on an executable, it shows you what assembly instructions make up the program. For example, if we wish to understand the low-level code to update a counter (as in our example), we run\n   \n    objdump\n   \n   (Linux) to see the assembly code:\n\n\nprompt> objdump -d main\nDoing so produces a long listing of all the instructions in the program, neatly labeled (particularly if you compiled with the\n   \n    -g\n   \n   flag), which includes symbol information in the program. The\n   \n    objdump\n   \n   program is just one of many tools you should learn how to use; a debugger like\n   \n    gdb\n   \n   , memory profilers like\n   \n    valgrind\n   \n   or\n   \n    purify\n   \n   , and of course the compiler itself are others that you should spend time to learn more about; the better you are at using your tools, the better systems you'll be able to build."
        },
        {
          "name": "The Heart Of The Problem: Uncontrolled Scheduling",
          "content": "To understand why this happens, we must understand the code sequence that the compiler generates for the update to\n   \n    counter\n   \n   . In this case, we wish to simply add a number (1) to\n   \n    counter\n   \n   . Thus, the code sequence for doing so might look something like this (in x86);\n\n\nmov 0x8049a1c, %eax\nadd $0x1, %eax\nmov %eax, 0x8049a1c\nThis example assumes that the variable\n   \n    counter\n   \n   is located at address 0x8049a1c. In this three-instruction sequence, the x86\n   \n    mov\n   \n   instruction is used first to get the memory value at the address and put it into register\n   \n    eax\n   \n   . Then, the\n   \n    add\n   \n   is performed, adding 1 (0x1) to the contents of the\n   \n    eax\n   \n   register, and finally, the contents of\n   \n    eax\n   \n   are stored back into memory at the same address.\n\n\nLet us imagine one of our two threads (Thread 1) enters this region of code, and is thus about to increment\n   \n    counter\n   \n   by one. It loads the value of\n   \n    counter\n   \n   (let's say it's 50 to begin with) into its register\n   \n    eax\n   \n   . Thus,\n   \n    eax\n   \n   =50 for Thread 1. Then it adds one to the register; thus\n   \n    eax\n   \n   =51. Now, something unfortunate happens: a timer interrupt goes off; thus, the OS saves the state of the currently running thread (its PC, its registers including\n   \n    eax\n   \n   , etc.) to the thread's TCB.\n\n\nNow something worse happens: Thread 2 is chosen to run, and it enters this same piece of code. It also executes the first instruction, getting the value of\n   \n    counter\n   \n   and putting it into its\n   \n    eax\n   \n   (remember: each thread when running has its own private registers; the registers are\n   **virtualized**\n   by the context-switch code that saves and restores them). The value of\n   \n    counter\n   \n   is still 50 at this point, and thus Thread 2 has\n   \n    eax\n   \n   =50. Let's then assume that Thread 2 executes the next two instructions, incrementing\n   \n    eax\n   \n   by 1 (thus\n   \n    eax\n   \n   =51), and then saving the contents of\n   \n    eax\n   \n   into\n   \n    counter\n   \n   (address 0x8049a1c). Thus, the global variable\n   \n    counter\n   \n   now has the value 51.\n\n\nFinally, another context switch occurs, and Thread 1 resumes running. Recall that it had just executed the\n   \n    mov\n   \n   and\n   \n    add\n   \n   , and is now about to perform the final\n   \n    mov\n   \n   instruction. Recall also that\n   \n    eax\n   \n   =51. Thus, the final\n   \n    mov\n   \n   instruction executes, and saves the value to memory; the counter is set to 51 again.\n\n\nPut simply, what has happened is this: the code to increment\n   \n    counter\n   \n   has been run twice, but\n   \n    counter\n   \n   , which started at 50, is now only equal to 51. A “correct” version of this program should have resulted in the variable\n   \n    counter\n   \n   equal to 52.\n\n\nLet's look at a detailed execution trace to understand the problem better. Assume, for this example, that the above code is loaded at address 100 in memory, like the following sequence (note for those of you used to\n   \n    nice\n   \n   , RISC-like instruction sets: x86 has variable-length instructions; this\n   \n    mov\n   \n   instruction takes up 5 bytes of memory, and the\n   \n    add\n   \n   only 3):\n\n\n\nOS | Thread 1 | Thread 2 | (after instruction)\nPC | eax | counter\nbefore critical section\n | mov 8049a1c, %eax |  | 100 | 0 | 50\n | add $0x1, %eax |  | 105 | 50 | 50\n |  |  | 108 | 51 | 50\ninterrupt | save T1 |  |  |  | \n | restore T2 |  |  |  | \n |  | mov 8049a1c, %eax | 100 | 0 | 50\n |  | add $0x1, %eax | 105 | 50 | 50\n |  | mov %eax, 8049a1c | 108 | 51 | 50\n |  |  | 113 | 51 | 51\ninterrupt | save T2 |  |  |  | \n | restore T1 |  |  |  | \n | mov %eax, 8049a1c |  | 108 | 51 | 51\n |  |  | 113 | 51 | 51\n\n\nFigure 26.7:\n   **The Problem: Up Close and Personal**\n\n\n100 mov    0x8049a1c, %eax\n105 add    $0x1, %eax\n108 mov    %eax, 0x8049a1c\nWith these assumptions, what happens is shown in Figure 26.7 (page 10). Assume the counter starts at value 50, and trace through this example to make sure you understand what is going on.\n\n\nWhat we have demonstrated here is called a\n   **race condition**\n   (or, more specifically, a\n   **data race**\n   ): the results depend on the timing of the code’s execution. With some bad luck (i.e., context switches that occur at untimely points in the execution), we get the wrong result. In fact, we may get a different result each time; thus, instead of a nice\n   **deterministic**\n   computation (which we are used to from computers), we call this result\n   **indeterminate**\n   , where it is not known what the output will be and it is indeed likely to be different across runs.\n\n\nBecause multiple threads executing this code can result in a race condition, we call this code a\n   **critical section**\n   . A critical section is a piece of code that accesses a shared variable (or more generally, a shared resource) and must not be concurrently executed by more than one thread.\n\n\nWhat we really want for this code is what we call\n   **mutual exclusion**\n   . This property guarantees that if one thread is executing within the critical section, the others will be prevented from doing so.\n\n\nVirtually all of these terms, by the way, were coined by Edsger Dijkstra, who was a pioneer in the field and indeed won the Turing Award because of this and other work; see his 1968 paper on “Cooperating Sequential Processes” [D68] for an amazingly clear description of the problem. We’ll be hearing more about Dijkstra in this section of the book.\n\n\n\n\n**TIP: USE ATOMIC OPERATIONS**\n\n\nAtomic operations are one of the most powerful underlying techniques in building computer systems, from the computer architecture, to concurrent code (what we are studying here), to file systems (which we'll study soon enough), database management systems, and even distributed systems [L+93].\n\n\nThe idea behind making a series of actions\n   **atomic**\n   is simply expressed with the phrase “all or nothing”; it should either appear as if all of the actions you wish to group together occurred, or that none of them occurred, with no in-between state visible. Sometimes, the grouping of many actions into a single atomic action is called a\n   **transaction**\n   , an idea developed in great detail in the world of databases and transaction processing [GR92].\n\n\nIn our theme of exploring concurrency, we'll be using synchronization primitives to turn short sequences of instructions into atomic blocks of execution, but the idea of atomicity is much bigger than that, as we will see. For example, file systems use techniques such as journaling or copy-on-write in order to atomically transition their on-disk state, critical for operating correctly in the face of system failures. If that doesn't make sense, don't worry — it will, in some future chapter."
        },
        {
          "name": "The Wish For Atomicity",
          "content": "One way to solve this problem would be to have more powerful instructions that, in a single step, did exactly whatever we needed done and thus removed the possibility of an untimely interrupt. For example, what if we had a super instruction that looked like this:\n\n\nmemory-add 0x8049a1c, $0x1\nAssume this instruction adds a value to a memory location, and the hardware guarantees that it executes\n   **atomically**\n   ; when the instruction executed, it would perform the update as desired. It could not be interrupted mid-instruction, because that is precisely the guarantee we receive from the hardware: when an interrupt occurs, either the instruction has not run at all, or it has run to completion; there is no in-between state. Hardware can be a beautiful thing, no?\n\n\nAtomically, in this context, means “as a unit”, which sometimes we take as “all or none.” What we'd like is to execute the three instruction sequence atomically:\n\n\nmov 0x8049a1c, %eax\nadd $0x1, %eax\nmov %eax, 0x8049a1c\nAs we said, if we had a single instruction to do this, we could just issue that instruction and be done. But in the general case, we won't have such an instruction. Imagine we were building a concurrent B-tree, and wished to update it; would we really want the hardware to support an \"atomic update of B-tree\" instruction? Probably not, at least in a sane instruction set.\n\n\nThus, what we will instead do is ask the hardware for a few useful instructions upon which we can build a general set of what we call\n   **synchronization primitives**\n   . By using this hardware support, in combination with some help from the operating system, we will be able to build multi-threaded code that accesses critical sections in a synchronized and controlled manner, and thus reliably produces the correct result despite the challenging nature of concurrent execution. Pretty awesome, right?\n\n\nThis is the problem we will study in this section of the book. It is a wonderful and hard problem, and should make your mind hurt (a bit). If it doesn't, then you don't understand! Keep working until your head hurts; you then know you're headed in the right direction. At that point, take a break; we don't want your head hurting too much.\n\n\n\n\n**THE CRUX: HOW TO SUPPORT SYNCHRONIZATION**\n\n\nWhat support do we need from the hardware in order to build useful synchronization primitives? What support do we need from the OS? How can we build these primitives correctly and efficiently? How can programs use them to get the desired results?"
        },
        {
          "name": "One More Problem: Waiting For Another",
          "content": "This chapter has set up the problem of concurrency as if only one type of interaction occurs between threads, that of accessing shared variables and the need to support atomicity for critical sections. As it turns out, there is another common interaction that arises, where one thread must wait for another to complete some action before it continues. This interaction arises, for example, when a process performs a disk I/O and is put to sleep; when the I/O completes, the process needs to be roused from its slumber so it can continue.\n\n\nThus, in the coming chapters, we'll be not only studying how to build support for synchronization primitives to support atomicity but also for mechanisms to support this type of sleeping/waking interaction that is common in multi-threaded programs. If this doesn't make sense right now, that is OK! It will soon enough, when you read the chapter on\n   **condition variables**\n   . If it doesn't by then, well, then it is less OK, and you should read that chapter again (and again) until it does make sense.\n\n\nASIDE: KEY CONCURRENCY TERMS\n   \n\n   CRITICAL SECTION, RACE CONDITION,\n   \n\n   INDETERMINATE, MUTUAL EXCLUSION\n\n\nThese four terms are so central to concurrent code that we thought it worth while to call them out explicitly. See some of Dijkstra's early work [D65,D68] for more details.\n\n\n  * • A\n    **critical section**\n    is a piece of code that accesses a\n    *shared*\n    resource, usually a variable or data structure.\n  * • A\n    **race condition**\n    (or\n    **data race**\n    [NM92]) arises if multiple threads of execution enter the critical section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps undesirable) outcome.\n  * • An\n    **indeterminate**\n    program consists of one or more race conditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not\n    **deterministic**\n    , something we usually expect from computer systems.\n  * • To avoid these problems, threads should use some kind of\n    **mutual exclusion**\n    primitives; doing so guarantees that only a single thread ever enters a critical section, thus avoiding races, and resulting in deterministic program outputs."
        },
        {
          "name": "Summary: Why in OS Class?",
          "content": "Before wrapping up, one question that you might have is: why are we studying this in OS class? “History” is the one-word answer; the OS was the first concurrent program, and many techniques were created for use\n   *within*\n   the OS. Later, with multi-threaded processes, application programmers also had to consider such things.\n\n\nFor example, imagine the case where there are two processes running. Assume they both call\n   \n    write()\n   \n   to write to the file, and both wish to append the data to the file (i.e., add the data to the end of the file, thus increasing its length). To do so, both must allocate a new block, record in the inode of the file where this block lives, and change the size of the file to reflect the new larger size (among other things; we'll learn more about files in the third part of the book). Because an interrupt may occur at any time, the code that updates these shared structures (e.g., a bitmap for allocation, or the file's inode) are critical sections; thus, OS designers, from the very beginning of the introduction of the interrupt, had to worry about how the OS updates internal structures. An untimely interrupt causes all of the problems described above. Not surprisingly, page tables, process lists, file system structures, and virtually every kernel data structure has to be carefully accessed, with the proper synchronization primitives, to work correctly.\n\n\n\n\n**References**\n\n\n[D65] \"Solution of a problem in concurrent programming control\" by E. W. Dijkstra.\n   *Communications of the ACM*\n   , 8(9):569, September 1965.\n   *Pointed to as the first paper of Dijkstra's where he outlines the mutual exclusion problem and a solution. The solution, however, is not widely used; advanced hardware and OS support is needed, as we will see in the coming chapters.*\n\n\n[D68] \"Cooperating sequential processes\" by Edsger W. Dijkstra. 1968. Available at this site:\n   http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF\n   .\n   *Dijkstra has an amazing number of his old papers, notes, and thoughts recorded (for posterity) on this website at the last place he worked, the University of Texas. Much of his foundational work, however, was done years earlier while he was at the Technische Hogeschool Eindhoven (THE), including this famous paper on \"cooperating sequential processes\", which basically outlines all of the thinking that has to go into writing multi-threaded programs. Dijkstra discovered much of this while working on an operating system named after his school: the \"THE\" operating system (said \"T\", \"H\", \"E\", and not like the word \"the\").*\n\n\n[GR92] \"Transaction Processing: Concepts and Techniques\" by Jim Gray and Andreas Reuter. Morgan Kaufmann, September 1992.\n   *This book is the bible of transaction processing, written by one of the legends of the field, Jim Gray. It is, for this reason, also considered Jim Gray's \"brain dump\", in which he wrote down everything he knows about how database management systems work. Sadly, Gray passed away tragically a few years back, and many of us lost a friend and great mentor, including the co-authors of said book, who were lucky enough to interact with Gray during their graduate school years.*\n\n\n[L+93] \"Atomic Transactions\" by Nancy Lynch, Michael Merritt, William Weihl, Alan Fekete. Morgan Kaufmann, August 1993.\n   *A nice text on some of the theory and practice of atomic transactions for distributed systems. Perhaps a bit formal for some, but lots of good material is found herein.*\n\n\n[NM92] \"What Are Race Conditions? Some Issues and Formalizations\" by Robert H. B. Netzer and Barton P. Miller.\n   *ACM Letters on Programming Languages and Systems*\n   , Volume 1:1, March 1992.\n   *An excellent discussion of the different types of races found in concurrent programs. In this chapter (and the next few), we focus on data races, but later we will broaden to discuss\n    **general races**\n    as well.*\n\n\n[SR05] \"Advanced Programming in the UNIX Environment\" by W. Richard Stevens and Stephen A. Rago. Addison-Wesley, 2005.\n   *As we've said many times, buy this book, and read it, in little chunks, preferably before going to bed. This way, you will actually fall asleep more quickly; more importantly, you learn a little more about how to become a serious UNIX programmer.*\n\n\n\n\n**Homework (Simulation)**\n\n\nThis program,\n   \n    x86.py\n   \n   , allows you to see how different thread interleavings either cause or avoid race conditions. See the README for details on how the program works, then answer the questions below.\n\n\n\n\n**Questions**\n\n\n  * 1. Let's examine a simple program, \"loop.s\". First, just read and understand it. Then, run it with these arguments (\n    \n     ./x86.py -t 1 -p loop.s -i 100 -R dx\n    \n    ) This specifies a single thread, an interrupt every 100 instructions, and tracing of register\n    \n     %dx\n    \n    . What will\n    \n     %dx\n    \n    be during the run? Use the\n    \n     -c\n    \n    flag to check your answers; the answers, on the left, show the value of the register (or memory value)\n    *after*\n    the instruction on the right has run.\n  * 2. Same code, different flags: (\n    \n     ./x86.py -p loop.s -t 2 -i 100 -a dx=3, dx=3 -R dx\n    \n    ) This specifies two threads, and initializes each\n    \n     %dx\n    \n    to 3. What values will\n    \n     %dx\n    \n    see? Run with\n    \n     -c\n    \n    to check. Does the presence of multiple threads affect your calculations? Is there a race in this code?\n  * 3. Run this:\n    \n     ./x86.py -p loop.s -t 2 -i 3 -r -R dx -a dx=3, dx=3\n    \n    This makes the interrupt interval small/random; use different seeds (\n    \n     -s\n    \n    ) to see different interleavings. Does the interrupt frequency change anything?\n  * 4. Now, a different program,\n    \n     looping-race-nolock.s\n    \n    , which accesses a shared variable located at address 2000; we'll call this variable\n    \n     value\n    \n    . Run it with a single thread to confirm your understanding:\n    \n     ./x86.py -p looping-race-nolock.s -t 1 -M 2000\n    \n    What is\n    \n     value\n    \n    (i.e., at memory address 2000) throughout the run? Use\n    \n     -c\n    \n    to check.\n  * 5. Run with multiple iterations/threads:\n    \n     ./x86.py -p looping-race-nolock.s -t 2 -a bx=3 -M 2000\n    \n    Why does each thread loop three times? What is final value of\n    \n     value\n    \n    ?\n  * 6. Run with random interrupt intervals:\n    \n     ./x86.py -p looping-race-nolock.s -t 2 -M 2000 -i 4 -r -s 0\n    \n    with different seeds (\n    \n     -s 1\n    \n    ,\n    \n     -s 2\n    \n    , etc.) Can you tell by looking at the thread interleaving what the final value of\n    \n     value\n    \n    will be? Does the timing of the interrupt matter? Where can it safely occur? Where not? In other words, where is the critical section exactly?\n\n\n  * 7. Now examine fixed interrupt intervals:\n    \n     ./x86.py -p looping-race-nolock.s -a bx=1 -t 2 -M 2000 -i 1\n    \n    What will the final value of the shared variable\n    \n     value\n    \n    be? What about when you change\n    \n     -i 2\n    \n    ,\n    \n     -i 3\n    \n    , etc.? For which interrupt intervals does the program give the “correct” answer?\n  * 8. Run the same for more loops (e.g., set\n    \n     -a bx=100\n    \n    ). What interrupt intervals (\n    \n     -i\n    \n    ) lead to a correct outcome? Which intervals are surprising?\n  * 9. One last program:\n    \n     wait-for-me.s\n    \n    . Run:\n    \n     ./x86.py -p wait-for-me.s -a ax=1,ax=0 -R ax -M 2000\n    \n    This sets the\n    \n     %ax\n    \n    register to 1 for thread 0, and 0 for thread 1, and watches\n    \n     %ax\n    \n    and memory location 2000. How should the code behave? How is the value at location 2000 being used by the threads? What will its final value be?\n  * 10. Now switch the inputs:\n    \n     ./x86.py -p wait-for-me.s -a ax=0,ax=1 -R ax -M 2000\n    \n    How do the threads behave? What is thread 0 doing? How would changing the interrupt interval (e.g.,\n    \n     -i 1000\n    \n    , or perhaps to use random intervals) change the trace outcome? Is the program efficiently using the CPU?\n\n\n\n\n**Interlude: Thread API**\n\n\nThis chapter briefly covers the main portions of the thread API. Each part will be explained further in the subsequent chapters, as we show how to use the API. More details can be found in various books and online sources [B89, B97, B+96, K+96]. We should note that the subsequent chapters introduce the concepts of locks and condition variables more slowly, with many examples; this chapter is thus better used as a reference.\n\n\n\n\n**CRUX: HOW TO CREATE AND CONTROL THREADS**\n\n\nWhat interfaces should the OS present for thread creation and control? How should these interfaces be designed to enable ease of use as well as utility?"
        }
      ]
    },
    {
      "name": "Interlude: Thread API",
      "sections": [
        {
          "name": "Thread Creation",
          "content": "The first thing you have to be able to do to write a multi-threaded program is to create new threads, and thus some kind of thread creation interface must exist. In POSIX, it is easy:\n\n\n#include <pthread.h>\nint\npthread_create(pthread_t        *thread,\n               const pthread_attr_t *attr,\n               void            *(*start_routine) (void*),\n               void            *arg);\nThis declaration might look a little complex (particularly if you haven't used function pointers in C), but actually it's not too bad. There are four arguments:\n   \n    thread\n   \n   ,\n   \n    attr\n   \n   ,\n   \n    start_routine\n   \n   , and\n   \n    arg\n   \n   . The first,\n   \n    thread\n   \n   , is a pointer to a structure of type\n   \n    pthread_t\n   \n   ; we'll use this structure to interact with this thread, and thus we need to pass it to\n   \n    pthread_create()\n   \n   in order to initialize it.\n\n\nThe second argument,\n   \n    attr\n   \n   , is used to specify any attributes this thread might have. Some examples include setting the stack size or perhaps information about the scheduling priority of the thread. An attribute is initialized with a separate call to\n   \n    pthread_attr_init()\n   \n   ; see the manual page for details. However, in most cases, the defaults will be fine; in this case, we will simply pass the value\n   \n    NULL\n   \n   in.\n\n\nThe third argument is the most complex, but is really just asking: which function should this thread start running in? In C, we call this a\n   **function pointer**\n   , and this one tells us the following is expected: a function name (\n   \n    start_routine\n   \n   ), which is passed a single argument of type\n   \n    void *\n   \n   (as indicated in the parentheses after\n   \n    start_routine\n   \n   ), and which returns a value of type\n   \n    void *\n   \n   (i.e., a\n   **void pointer**\n   ).\n\n\nIf this routine instead required an integer argument, instead of a void pointer, the declaration would look like this:\n\n\nint pthread_create(..., // first two args are the same\n                    void *(*start_routine)(int),\n                    int arg);\nIf instead the routine took a void pointer as an argument, but returned an integer, it would look like this:\n\n\nint pthread_create(..., // first two args are the same\n                    int (*start_routine)(void *),\n                    void *arg);\nFinally, the fourth argument,\n   \n    arg\n   \n   , is exactly the argument to be passed to the function where the thread begins execution. You might ask: why do we need these void pointers? Well, the answer is quite simple: having a void pointer as an argument to the function\n   \n    start_routine\n   \n   allows us to pass in\n   *any*\n   type of argument; having it as a return value allows the thread to return\n   *any*\n   type of result.\n\n\nLet's look at an example in Figure 27.1. Here we just create a thread that is passed two arguments, packaged into a single type we define ourselves (\n   \n    myarg_t\n   \n   ). The thread, once created, can simply cast its argument to the type it expects and thus unpack the arguments as desired.\n\n\nAnd there it is! Once you create a thread, you really have another live executing entity, complete with its own call stack, running within the\n   *same*\n   address space as all the currently existing threads in the program. The fun thus begins!"
        },
        {
          "name": "Thread Completion",
          "content": "The example above shows how to create a thread. However, what happens if you want to wait for a thread to complete? You need to do something special in order to wait for completion; in particular, you must call the routine\n   \n    pthread_join()\n   \n   .\n\n\nint pthread_join(pthread_t thread, void **value_ptr);\n1 #include <stdio.h>\n2 #include <pthread.h>\n3\n4 typedef struct {\n5     int a;\n6     int b;\n7 } myarg_t;\n8\n9 void *mythread(void *arg) {\n10    myarg_t *args = (myarg_t *) arg;\n11    printf(\"%d %d\\n\", args->a, args->b);\n12    return NULL;\n13 }\n14\n15 int main(int argc, char *argv[]) {\n16    pthread_t p;\n17    myarg_t args = { 10, 20 };\n18\n19    int rc = pthread_create(&p, NULL, mythread, &args);\n20    ...\n21 }\nFigure 27.1:\n   **Creating a Thread**\n\n\nThis routine takes two arguments. The first is of type\n   \n    pthread_t\n   \n   , and is used to specify which thread to wait for. This variable is initialized by the thread creation routine (when you pass a pointer to it as an argument to\n   \n    pthread_create()\n   \n   ); if you keep it around, you can use it to wait for that thread to terminate.\n\n\nThe second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is defined to return a pointer to\n   \n    void\n   \n   ; because the\n   \n    pthread_join()\n   \n   routine\n   *changes*\n   the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself.\n\n\nLet's look at another example (Figure 27.2, page 4). In the code, a single thread is again created, and passed a couple of arguments via the\n   \n    myarg_t\n   \n   structure. To return values, the\n   \n    myret_t\n   \n   type is used. Once the thread is finished running, the main thread, which has been waiting inside of the\n   \n    pthread_join()\n   \n   routine\n   \n    1\n   \n   , then returns, and we can access the values returned from the thread, namely whatever is in\n   \n    myret_t\n   \n   .\n\n\nA few things to note about this example. First, often times we don't have to do all of this painful packing and unpacking of arguments. For example, if we just create a thread with no arguments, we can pass\n   \n    NULL\n   \n   in as an argument when the thread is created. Similarly, we can pass\n   \n    NULL\n   \n   into\n   \n    pthread_join()\n   \n   if we don't care about the return value.\n\n\n1\n   \n   Note we use wrapper functions here; specifically, we call\n   \n    Malloc()\n   \n   ,\n   \n    Pthread_join()\n   \n   , and\n   \n    Pthread_create()\n   \n   , which just call their similarly-named lower-case versions and make sure the routines did not return anything unexpected.\n\n\n1  typedef struct { int a; int b; } myarg_t;\n2  typedef struct { int x; int y; } myret_t;\n3\n4  void *mythread(void *arg) {\n5      myret_t *rvs = Malloc(sizeof(myret_t));\n6      rvs->x = 1;\n7      rvs->y = 2;\n8      return (void *) rvs;\n9  }\n10\n11 int main(int argc, char *argv[]) {\n12     pthread_t p;\n13     myret_t *rvs;\n14     myarg_t args = { 10, 20 };\n15     Pthread_create(&p, NULL, mythread, &args);\n16     Pthread_join(p, (void **) &rvs);\n17     printf(\"returned %d %d\\n\", rvs->x, rvs->y);\n18     free(rvs);\n19     return 0;\n20 }\nFigure 27.2: Waiting for Thread Completion\n\n\nSecond, if we are just passing in a single value (e.g., a\n   \n    long long int\n   \n   ), we don't have to package it up as an argument. Figure 27.3 (page 5) shows an example. In this case, life is a bit simpler, as we don't have to package arguments and return values inside of structures.\n\n\nThird, we should note that one has to be extremely careful with how values are returned from a thread. Specifically, never return a pointer which refers to something allocated on the thread's call stack. If you do, what do you think will happen? (think about it!) Here is an example of a dangerous piece of code, modified from the example in Figure 27.2.\n\n\n1  void *mythread(void *arg) {\n2      myarg_t *args = (myarg_t *) arg;\n3      printf(\"%d %d\\n\", args->a, args->b);\n4      myret_t oops; // ALLOCATED ON STACK: BAD!\n5      oops.x = 1;\n6      oops.y = 2;\n7      return (void *) &oops;\n8  }\nIn this case, the variable\n   \n    oops\n   \n   is allocated on the stack of\n   \n    mythread\n   \n   . However, when it returns, the value is automatically deallocated (that's why the stack is so easy to use, after all!), and thus, passing back a pointer to a now deallocated variable will lead to all sorts of bad results. Cer-\n\n\nvoid *mythread(void *arg) {\n    long long int value = (long long int) arg;\n    printf(\"%lld\\n\", value);\n    return (void *) (value + 1);\n}\n\nint main(int argc, char *argv[]) {\n    pthread_t p;\n    long long int rvalue;\n    Pthread_create(&p, NULL, mythread, (void *) 100);\n    Pthread_join(p, (void **) &rvalue);\n    printf(\"returned %lld\\n\", rvalue);\n    return 0;\n}\nFigure 27.3:\n   **Simpler Argument Passing to a Thread**\n\n\ntainly, when you print out the values you think you returned, you'll probably (but not necessarily!) be surprised. Try it and find out for yourself\n   \n    2\n   \n   !\n\n\nFinally, you might notice that the use of\n   \n    pthread_create()\n   \n   to create a thread, followed by an immediate call to\n   \n    pthread_join()\n   \n   , is a pretty strange way to create a thread. In fact, there is an easier way to accomplish this exact task; it's called a\n   **procedure call**\n   . Clearly, we'll usually be creating more than just one thread and waiting for it to complete, otherwise there is not much purpose to using threads at all.\n\n\nWe should note that not all code that is multi-threaded uses the join routine. For example, a multi-threaded web server might create a number of worker threads, and then use the main thread to accept requests and pass them to the workers, indefinitely. Such long-lived programs thus may not need to join. However, a parallel program that creates threads to execute a particular task (in parallel) will likely use join to make sure all such work completes before exiting or moving onto the next stage of computation."
        },
        {
          "name": "Locks",
          "content": "Beyond thread creation and join, probably the next most useful set of functions provided by the POSIX threads library are those for providing mutual exclusion to a critical section via\n   **locks**\n   . The most basic pair of routines to use for this purpose is provided by the following:\n\n\nint pthread_mutex_lock(pthread_mutex_t *mutex);\nint pthread_mutex_unlock(pthread_mutex_t *mutex);\n2\n   \n   Fortunately the compiler\n   \n    gcc\n   \n   will likely complain when you write code like this, which is yet another reason to pay attention to compiler warnings.\n\n\nThe routines should be easy to understand and use. When you have a region of code that is a\n   **critical section**\n   , and thus needs to be protected to ensure correct operation, locks are quite useful. You can probably imagine what the code looks like:\n\n\npthread_mutex_t lock;\npthread_mutex_lock(&lock);\nx = x + 1; // or whatever your critical section is\npthread_mutex_unlock(&lock);\nThe intent of the code is as follows: if no other thread holds the lock when\n   \n    pthread_mutex_lock()\n   \n   is called, the thread will acquire the lock and enter the critical section. If another thread does indeed hold the lock, the thread trying to grab the lock will not return from the call until it has acquired the lock (implying that the thread holding the lock has released it via the unlock call). Of course, many threads may be stuck waiting inside the lock acquisition function at a given time; only the thread with the lock acquired, however, should call unlock.\n\n\nUnfortunately, this code is broken, in two important ways. The first problem is a\n   **lack of proper initialization**\n   . All locks must be properly initialized in order to guarantee that they have the correct values to begin with and thus work as desired when lock and unlock are called.\n\n\nWith POSIX threads, there are two ways to initialize locks. One way to do this is to use\n   \n    PTHREAD_MUTEX_INITIALIZER\n   \n   , as follows:\n\n\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\nDoing so sets the lock to the default values and thus makes the lock usable. The dynamic way to do it (i.e., at run time) is to make a call to\n   \n    pthread_mutex_init()\n   \n   , as follows:\n\n\nint rc = pthread_mutex_init(&lock, NULL);\nassert(rc == 0); // always check success!\nThe first argument to this routine is the address of the lock itself, whereas the second is an optional set of attributes. Read more about the attributes yourself; passing\n   \n    NULL\n   \n   in simply uses the defaults. Either way works, but we usually use the dynamic (latter) method. Note that a corresponding call to\n   \n    pthread_mutex_destroy()\n   \n   should also be made, when you are done with the lock; see the manual page for all of the details.\n\n\nThe second problem with the code above is that it fails to check error codes when calling lock and unlock. Just like virtually any library routine you call in a UNIX system, these routines can also fail! If your code doesn't properly check error codes, the failure will happen silently, which in this case could allow multiple threads into a critical section. Minimally, use wrappers, which assert that the routine succeeded, as shown in Figure 27.4 (page 7); more sophisticated (non-toy) programs, which can't simply exit when something goes wrong, should check for failure and do something appropriate when a call does not succeed.\n\n\n// Keeps code clean; only use if exit() OK upon failure\nvoid Pthread_mutex_lock(pthread_mutex_t *mutex) {\n    int rc = pthread_mutex_lock(mutex);\n    assert(rc == 0);\n}\nFigure 27.4:\n   **An Example Wrapper**\n\n\nThe lock and unlock routines are not the only routines within the\n   \n    pthread\n   \n   library to interact with locks. Two other routines of interest:\n\n\nint pthread_mutex_trylock(pthread_mutex_t *mutex);\nint pthread_mutex_timedlock(pthread_mutex_t *mutex,\n                            struct timespec *abs_timeout);\nThese two calls are used in lock acquisition. The\n   \n    trylock\n   \n   version returns failure if the lock is already held; the\n   \n    timedlock\n   \n   version of acquiring a lock returns after a timeout or after acquiring the lock, whichever happens first. Thus, the\n   \n    timedlock\n   \n   with a timeout of zero degenerates to the\n   \n    trylock\n   \n   case. Both of these versions should generally be avoided; however, there are a few cases where avoiding getting stuck (perhaps indefinitely) in a lock acquisition routine can be useful, as we'll see in future chapters (e.g., when we study deadlock)."
        },
        {
          "name": "Condition Variables",
          "content": "The other major component of any threads library, and certainly the case with POSIX threads, is the presence of a\n   **condition variable**\n   . Condition variables are useful when some kind of signaling must take place between threads, if one thread is waiting for another to do something before it can continue. Two primary routines are used by programs wishing to interact in this way:\n\n\nint pthread_cond_wait(pthread_cond_t *cond,\n                      pthread_mutex_t *mutex);\nint pthread_cond_signal(pthread_cond_t *cond);\nTo use a condition variable, one has to in addition have a lock that is associated with this condition. When calling either of the above routines, this lock should be held.\n\n\nThe first routine,\n   \n    pthread_cond_wait()\n   \n   , puts the calling thread to sleep, and thus waits for some other thread to signal it, usually when something in the program has changed that the now-sleeping thread might care about. A typical usage looks like this:\n\n\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\npthread_cond_t cond = PTHREAD_COND_INITIALIZER;\n\nPthread_mutex_lock(&lock);\nwhile (ready == 0)\n    Pthread_cond_wait(&cond, &lock);\nPthread_mutex_unlock(&lock);\nIn this code, after initialization of the relevant lock and condition\n   \n    3\n   \n   , a thread checks to see if the variable\n   \n    ready\n   \n   has yet been set to something other than zero. If not, the thread simply calls the wait routine in order to sleep until some other thread wakes it.\n\n\nThe code to wake a thread, which would run in some other thread, looks like this:\n\n\nPthread_mutex_lock(&lock);\nready = 1;\nPthread_cond_signal(&cond);\nPthread_mutex_unlock(&lock);\nA few things to note about this code sequence. First, when signaling (as well as when modifying the global variable\n   \n    ready\n   \n   ), we always make sure to have the lock held. This ensures that we don't accidentally introduce a race condition into our code.\n\n\nSecond, you might notice that the wait call takes a lock as its second parameter, whereas the signal call only takes a condition. The reason for this difference is that the wait call, in addition to putting the calling thread to sleep,\n   *releases*\n   the lock when putting said caller to sleep. Imagine if it did not: how could the other thread acquire the lock and signal it to wake up? However,\n   *before*\n   returning after being woken, the\n   \n    pthread_cond_wait()\n   \n   re-acquires the lock, thus ensuring that any time the waiting thread is running between the lock acquire at the beginning of the wait sequence, and the lock release at the end, it holds the lock.\n\n\nOne last oddity: the waiting thread re-checks the condition in a while loop, instead of a simple if statement. We'll discuss this issue in detail when we study condition variables in a future chapter, but in general, using a while loop is the simple and safe thing to do. Although it rechecks the condition (perhaps adding a little overhead), there are some pthread implementations that could spuriously wake up a waiting thread; in such a case, without rechecking, the waiting thread will continue thinking that the condition has changed even though it has not. It is safer thus to view waking up as a hint that something might have changed, rather than an absolute fact.\n\n\nNote that sometimes it is tempting to use a simple flag to signal between two threads, instead of a condition variable and associated lock. For example, we could rewrite the waiting code above to look more like this in the waiting code:\n\n\nwhile (ready == 0)\n  ; // spin\nThe associated signaling code would look like this:\n\n\nready = 1;\n3\n   \n   One can use\n   \n    pthread_cond_init()\n   \n   (and\n   \n    pthread_cond_destroy()\n   \n   ) instead of the static initializer\n   \n    PTHREAD_COND_INITIALIZER\n   \n   . Sound like more work? It is.\n\n\nDon't ever do this, for the following reasons. First, it performs poorly in many cases (spinning for a long time just wastes CPU cycles). Second, it is error prone. As recent research shows [X+10], it is surprisingly easy to make mistakes when using flags (as above) to synchronize between threads; in that study, roughly half the uses of these\n   *ad hoc*\n   synchronizations were buggy! Don't be lazy; use condition variables even when you think you can get away without doing so.\n\n\nIf condition variables sound confusing, don't worry too much (yet) – we'll be covering them in great detail in a subsequent chapter. Until then, it should suffice to know that they exist and to have some idea how and why they are used."
        },
        {
          "name": "Compiling and Running",
          "content": "All of the code examples in this chapter are relatively easy to get up and running. To compile them, you must include the header\n   \n    pthread.h\n   \n   in your code. On the link line, you must also explicitly link with the\n   \n    pthread\n   \n   library, by adding the\n   \n    -pthread\n   \n   flag.\n\n\nFor example, to compile a simple multi-threaded program, all you have to do is the following:\n\n\nprompt> gcc -o main main.c -Wall -pthread\nAs long as\n   \n    main.c\n   \n   includes the\n   \n    pthread\n   \n   header, you have now successfully compiled a concurrent program. Whether it works or not, as usual, is a different matter entirely."
        }
      ]
    },
    {
      "name": "Locks",
      "sections": [
        {
          "name": "Locks: The Basic Idea",
          "content": "As an example, assume our critical section looks like this, the canonical update of a shared variable:\n\n\nbalance = balance + 1;\nOf course, other critical sections are possible, such as adding an element to a linked list or other more complex updates to shared structures, but we'll just keep to this simple example for now. To use a lock, we add some code around the critical section like this:\n\n\n1 lock_t mutex; // some globally-allocated lock 'mutex'\n2 ...\n3 lock(&mutex);\n4 balance = balance + 1;\n5 unlock(&mutex);\nA lock is just a variable, and thus to use one, you must declare a\n   **lock variable**\n   of some kind (such as\n   \n    mutex\n   \n   above). This lock variable (or just \"lock\" for short) holds the state of the lock at any instant in time. It is either\n   **available**\n   (or\n   **unlocked**\n   or\n   **free**\n   ) and thus no thread holds the lock, or\n   **acquired**\n   (or\n   **locked**\n   or\n   **held**\n   ), and thus exactly one thread holds the lock and presumably is in a critical section. We could store other information in the data type as well, such as which thread holds the lock, or a queue\n\n\nfor ordering lock acquisition, but information like that is hidden from the user of the lock.\n\n\nThe semantics of the\n   \n    lock()\n   \n   and\n   \n    unlock()\n   \n   routines are simple. Calling the routine\n   \n    lock()\n   \n   tries to acquire the lock; if no other thread holds the lock (i.e., it is free), the thread will acquire the lock and enter the critical section; this thread is sometimes said to be the\n   **owner**\n   of the lock. If another thread then calls\n   \n    lock()\n   \n   on that same lock variable (\n   \n    mutex\n   \n   in this example), it will not return while the lock is held by another thread; in this way, other threads are prevented from entering the critical section while the first thread that holds the lock is in there.\n\n\nOnce the owner of the lock calls\n   \n    unlock()\n   \n   , the lock is now available (free) again. If no other threads are waiting for the lock (i.e., no other thread has called\n   \n    lock()\n   \n   and is stuck therein), the state of the lock is simply changed to free. If there are waiting threads (stuck in\n   \n    lock()\n   \n   ), one of them will (eventually) notice (or be informed of) this change of the lock's state, acquire the lock, and enter the critical section.\n\n\nLocks provide some minimal amount of control over scheduling to programmers. In general, we view threads as entities created by the programmer but scheduled by the OS, in any fashion that the OS chooses. Locks yield some of that control back to the programmer; by putting a lock around a section of code, the programmer can guarantee that no more than a single thread can ever be active within that code. Thus locks help transform the chaos that is traditional OS scheduling into a more controlled activity."
        },
        {
          "name": "Pthread Locks",
          "content": "The name that the POSIX library uses for a lock is a\n   **mutex**\n   , as it is used to provide\n   **mutual exclusion**\n   between threads, i.e., if one thread is in the critical section, it excludes the others from entering until it has completed the section. Thus, when you see the following POSIX threads code, you should understand that it is doing the same thing as above (we again use our wrappers that check for errors upon lock and unlock):\n\n\n1 pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\n2\n3 Pthread_mutex_lock(&lock); // wrapper; exits on failure\n4 balance = balance + 1;\n5 Pthread_mutex_unlock(&lock);\nYou might also notice here that the POSIX version passes a variable to lock and unlock, as we may be using\n   *different*\n   locks to protect different variables. Doing so can increase concurrency: instead of one big lock that is used any time any critical section is accessed (a\n   **coarse-grained**\n   locking strategy), one will often protect different data and data structures with different locks, thus allowing more threads to be in locked code at once (a more\n   **fine-grained**\n   approach)."
        },
        {
          "name": "Building A Lock",
          "content": "By now, you should have some understanding of how a lock works, from the perspective of a programmer. But how should we build a lock? What hardware support is needed? What OS support? It is this set of questions we address in the rest of this chapter.\n\n\n\n\n**THE CRUX: HOW TO BUILD A LOCK**\n\n\nHow can we build an efficient lock? Efficient locks provide mutual exclusion at low cost, and also might attain a few other properties we discuss below. What hardware support is needed? What OS support?\n\n\nTo build a working lock, we will need some help from our old friend, the hardware, as well as our good pal, the OS. Over the years, a number of different hardware primitives have been added to the instruction sets of various computer architectures; while we won't study how these instructions are implemented (that, after all, is the topic of a computer architecture class), we will study how to use them in order to build a mutual exclusion primitive like a lock. We will also study how the OS gets involved to complete the picture and enable us to build a sophisticated locking library."
        },
        {
          "name": "Evaluating Locks",
          "content": "Before building any locks, we should first understand what our goals are, and thus we ask how to evaluate the efficacy of a particular lock implementation. To evaluate whether a lock works (and works well), we should establish some basic criteria. The first is whether the lock does its basic task, which is to provide\n   **mutual exclusion**\n   . Basically, does the lock work, preventing multiple threads from entering a critical section?\n\n\nThe second is\n   **fairness**\n   . Does each thread contending for the lock get a fair shot at acquiring it once it is free? Another way to look at this is by examining the more extreme case: does any thread contending for the lock\n   **starve**\n   while doing so, thus never obtaining it?\n\n\nThe final criterion is\n   **performance**\n   , specifically the time overheads added by using the lock. There are a few different cases that are worth considering here. One is the case of no contention; when a single thread is running and grabs and releases the lock, what is the overhead of doing so? Another is the case where multiple threads are contending for the lock on a single CPU; in this case, are there performance concerns? Finally, how does the lock perform when there are multiple CPUs involved, and threads on each contending for the lock? By comparing these different scenarios, we can better understand the performance impact of using various locking techniques, as described below."
        },
        {
          "name": "Controlling Interrupts",
          "content": "One of the earliest solutions used to provide mutual exclusion was to disable interrupts for critical sections; this solution was invented for single-processor systems. The code would look like this:\n\n\n1 void lock() {\n2     DisableInterrupts();\n3 }\n4 void unlock() {\n5     EnableInterrupts();\n6 }\nAssume we are running on such a single-processor system. By turning off interrupts (using some kind of special hardware instruction) before entering a critical section, we ensure that the code inside the critical section will\n   *not*\n   be interrupted, and thus will execute as if it were atomic. When we are finished, we re-enable interrupts (again, via a hardware instruction) and thus the program proceeds as usual.\n\n\nThe main positive of this approach is its simplicity. You certainly don't have to scratch your head too hard to figure out why this works. Without interruption, a thread can be sure that the code it executes will execute and that no other thread will interfere with it.\n\n\nThe negatives, unfortunately, are many. First, this approach requires us to allow any calling thread to perform a\n   *privileged*\n   operation (turning interrupts on and off), and thus\n   *trust*\n   that this facility is not abused. As you already know, any time we are required to trust an arbitrary program, we are probably in trouble. Here, the trouble manifests in numerous ways: a greedy program could call\n   \n    lock()\n   \n   at the beginning of its execution and thus monopolize the processor; worse, an errant or malicious program could call\n   \n    lock()\n   \n   and go into an endless loop. In this latter case, the OS never regains control of the system, and there is only one recourse: restart the system. Using interrupt disabling as a general-purpose synchronization solution requires too much trust in applications.\n\n\nSecond, the approach does not work on multiprocessors. If multiple threads are running on different CPUs, and each try to enter the same critical section, it does not matter whether interrupts are disabled; threads will be able to run on other processors, and thus could enter the critical section. As multiprocessors are now commonplace, our general solution will have to do better than this.\n\n\nThird, turning off interrupts for extended periods of time can lead to interrupts becoming lost, which can lead to serious systems problems. Imagine, for example, if the CPU missed the fact that a disk device has finished a read request. How will the OS know to wake the process waiting for said read?\n\n\n1  typedef struct __lock_t { int flag; } lock_t;\n2\n3  void init(lock_t *mutex) {\n4      // 0 -> lock is available, 1 -> held\n5      mutex->flag = 0;\n6  }\n7\n8  void lock(lock_t *mutex) {\n9      while (mutex->flag == 1) // TEST the flag\n10     ; // spin-wait (do nothing)\n11     mutex->flag = 1;         // now SET it!\n12 }\n13\n14 void unlock(lock_t *mutex) {\n15     mutex->flag = 0;\n16 }\n\nFigure 28.1:\n   **First Attempt: A Simple Flag**\n\n\nFor these reasons, turning off interrupts is only used in limited contexts as a mutual-exclusion primitive. For example, in some cases an operating system itself will use interrupt masking to guarantee atomicity when accessing its own data structures, or at least to prevent certain messy interrupt handling situations from arising. This usage makes sense, as the trust issue disappears inside the OS, which always trusts itself to perform privileged operations anyhow."
        },
        {
          "name": "A Failed Attempt: Just Using Loads/Stores",
          "content": "To move beyond interrupt-based techniques, we will have to rely on CPU hardware and the instructions it provides us to build a proper lock. Let's first try to build a simple lock by using a single flag variable. In this failed attempt, we'll see some of the basic ideas needed to build a lock, and (hopefully) see why just using a single variable and accessing it via normal loads and stores is insufficient.\n\n\nIn this first attempt (Figure 28.1), the idea is quite simple: use a simple variable (\n   \n    flag\n   \n   ) to indicate whether some thread has possession of a lock. The first thread that enters the critical section will call\n   \n    lock()\n   \n   , which\n   **tests**\n   whether the flag is equal to 1 (in this case, it is not), and then\n   **sets**\n   the flag to 1 to indicate that the thread now\n   **holds**\n   the lock. When finished with the critical section, the thread calls\n   \n    unlock()\n   \n   and clears the flag, thus indicating that the lock is no longer held.\n\n\nIf another thread happens to call\n   \n    lock()\n   \n   while that first thread is in the critical section, it will simply\n   **spin-wait**\n   in the while loop for that thread to call\n   \n    unlock()\n   \n   and clear the flag. Once that first thread does so, the waiting thread will fall out of the while loop, set the flag to 1 for itself, and proceed into the critical section.\n\n\nUnfortunately, the code has two problems: one of correctness, and an-\n\n\n\nThread 1 | Thread 2\ncall lock ()\nwhile (flag == 1)\ninterrupt: switch to Thread 2 | call lock ()\nwhile (flag == 1)\nflag = 1;\ninterrupt: switch to Thread 1\nflag = 1; // set flag to 1 (too!)\n\n\nFigure 28.2: Trace: No Mutual Exclusion\n\n\nother of performance. The correctness problem is simple to see once you get used to thinking about concurrent programming. Imagine the code interleaving in Figure 28.2; assume\n   \n    flag=0\n   \n   to begin.\n\n\nAs you can see from this interleaving, with timely (untimely?) interrupts, we can easily produce a case where\n   *both*\n   threads set the flag to 1 and both threads are thus able to enter the critical section. This behavior is what professionals call “bad” – we have obviously failed to provide the most basic requirement: providing mutual exclusion.\n\n\nThe performance problem, which we will address more later on, is the fact that the way a thread waits to acquire a lock that is already held: it endlessly checks the value of\n   \n    flag\n   \n   , a technique known as\n   **spin-waiting**\n   . Spin-waiting wastes time waiting for another thread to release a lock. The waste is exceptionally high on a uniprocessor, where the thread that the waiter is waiting for cannot even run (at least, until a context switch occurs)! Thus, as we move forward and develop more sophisticated solutions, we should also consider ways to avoid this kind of waste."
        },
        {
          "name": "BuildingWorking Spin Locks with Test-And-Set",
          "content": "Because disabling interrupts does not work on multiple processors, and because simple approaches using loads and stores (as shown above) don't work, system designers started to invent hardware support for locking. The earliest multiprocessor systems, such as the Burroughs B5000 in the early 1960's [M82], had such support; today all systems provide this type of support, even for single CPU systems.\n\n\nThe simplest bit of hardware support to understand is known as a\n   **test-and-set**\n   (or\n   **atomic exchange**\n\n    1\n   \n   ) instruction. We define what the test-and-set instruction does via the following C code snippet:\n\n\n1 int TestAndSet(int *old_ptr, int new) {\n2     int old = *old_ptr; // fetch old value at old_ptr\n3     *old_ptr = new;     // store 'new' into old_ptr\n4     return old;         // return the old value\n5 }\n1\n   \n   Each architecture that supports test-and-set calls it by a different name. On SPARC it is called the load/store unsigned byte instruction (\n   \n    ldstub\n   \n   ); on x86 it is the locked version of the atomic exchange (\n   \n    xchg\n   \n   ).\n\n\n\n\n**ASIDE: DEKKER'S AND PETERSON'S ALGORITHMS**\n\n\nIn the 1960's, Dijkstra posed the concurrency problem to his friends, and one of them, a mathematician named Theodorus Jozef Dekker, came up with a solution [D68]. Unlike the solutions we discuss here, which use special hardware instructions and even OS support,\n   **Dekker's algorithm**\n   uses just loads and stores (assuming they are atomic with respect to each other, which was true on early hardware).\n\n\nDekker's approach was later refined by Peterson [P81]. Once again, just loads and stores are used, and the idea is to ensure that two threads never enter a critical section at the same time. Here is\n   **Peterson's algorithm**\n   (for two threads); see if you can understand the code. What are the\n   \n    flag\n   \n   and\n   \n    turn\n   \n   variables used for?\n\n\nint flag[2];\nint turn;\n\nvoid init() {\n    // indicate you intend to hold the lock w/ 'flag'\n    flag[0] = flag[1] = 0;\n    // whose turn is it? (thread 0 or 1)\n    turn = 0;\n}\nvoid lock() {\n    // 'self' is the thread ID of caller\n    flag[self] = 1;\n    // make it other thread's turn\n    turn = 1 - self;\n    while ((flag[1-self] == 1) && (turn == 1 - self))\n        ; // spin-wait while it's not your turn\n}\nvoid unlock() {\n    // simply undo your intent\n    flag[self] = 0;\n}\nFor some reason, developing locks that work without special hardware support became all the rage for a while, giving theory-types a lot of problems to work on. Of course, this line of work became quite useless when people realized it is much easier to assume a little hardware support (and indeed that support had been around from the earliest days of multiprocessing). Further, algorithms like the ones above don't work on modern hardware (due to relaxed memory consistency models), thus making them even less useful than they were before. Yet more research relegated to the dustbin of history...\n\n\n1  typedef struct __lock_t {\n2      int flag;\n3  } lock_t;\n4\n5  void init(lock_t *lock) {\n6      // 0: lock is available, 1: lock is held\n7      lock->flag = 0;\n8  }\n9\n10 void lock(lock_t *lock) {\n11     while (TestAndSet(&lock->flag, 1) == 1)\n12         ; // spin-wait (do nothing)\n13 }\n14\n15 void unlock(lock_t *lock) {\n16     lock->flag = 0;\n17 }\n\nFigure 28.3: A Simple Spin Lock Using Test-and-set\n\n\nWhat the test-and-set instruction does is as follows. It returns the old value pointed to by the\n   \n    old_ptr\n   \n   , and simultaneously updates said value to\n   \n    new\n   \n   . The key, of course, is that this sequence of operations is performed\n   **atomically**\n   . The reason it is called “test and set” is that it enables you to “test” the old value (which is what is returned) while simultaneously “setting” the memory location to a new value; as it turns out, this slightly more powerful instruction is enough to build a simple\n   **spin lock**\n   , as we now examine in Figure 28.3. Or better yet: figure it out first yourself!\n\n\nLet’s make sure we understand why this lock works. Imagine first the case where a thread calls\n   \n    lock()\n   \n   and no other thread currently holds the lock; thus,\n   \n    flag\n   \n   should be 0. When the thread calls\n   \n    TestAndSet(flag, 1)\n   \n   , the routine will return the old value of\n   \n    flag\n   \n   , which is 0; thus, the calling thread, which is\n   *testing*\n   the value of\n   \n    flag\n   \n   , will not get caught spinning in the while loop and will acquire the lock. The thread will also atomically\n   *set*\n   the value to 1, thus indicating that the lock is now held. When the thread is finished with its critical section, it calls\n   \n    unlock()\n   \n   to set the\n   \n    flag\n   \n   back to zero.\n\n\nThe second case we can imagine arises when one thread already has the lock held (i.e.,\n   \n    flag\n   \n   is 1). In this case, this thread will call\n   \n    lock()\n   \n   and then call\n   \n    TestAndSet(flag, 1)\n   \n   as well. This time,\n   \n    TestAndSet()\n   \n   will return the old value at\n   \n    flag\n   \n   , which is 1 (because the lock is held), while simultaneously setting it to 1 again. As long as the lock is held by another thread,\n   \n    TestAndSet()\n   \n   will repeatedly return 1, and thus this thread will spin and spin until the lock is finally released. When the\n   \n    flag\n   \n   is finally set to 0 by some other thread, this thread will call\n   \n    TestAndSet()\n   \n   again, which will now return 0 while atomically setting the value to 1 and thus acquire the lock and enter the critical section.\n\n\nBy making both the\n   **test**\n   (of the old lock value) and\n   **set**\n   (of the new\n\n\n**TIP: THINK ABOUT CONCURRENCY AS A MALICIOUS SCHEDULER**\nFrom this example, you might get a sense of the approach you need to take to understand concurrent execution. What you should try to do is to pretend you are a\n   **malicious scheduler**\n   , one that interrupts threads at the most inopportune of times in order to foil their feeble attempts at building synchronization primitives. What a mean scheduler you are! Although the exact sequence of interrupts may be\n   *improbable*\n   , it is\n   *possible*\n   , and that is all we need to demonstrate that a particular approach does not work. It can be useful to think maliciously! (at least, sometimes)\n\n\nvalue) a single atomic operation, we ensure that only one thread acquires the lock. And that's how to build a working mutual exclusion primitive!\n\n\nYou may also now understand why this type of lock is usually referred to as a\n   **spin lock**\n   . It is the simplest type of lock to build, and simply spins, using CPU cycles, until the lock becomes available. To work correctly on a single processor, it requires a\n   **preemptive scheduler**\n   (i.e., one that will interrupt a thread via a timer, in order to run a different thread, from time to time). Without preemption, spin locks don't make much sense on a single CPU, as a thread spinning on a CPU will never relinquish it."
        },
        {
          "name": "Evaluating Spin Locks",
          "content": "Given our basic spin lock, we can now evaluate how effective it is along our previously described axes. The most important aspect of a lock is\n   **correctness**\n   : does it provide mutual exclusion? The answer here is yes: the spin lock only allows a single thread to enter the critical section at a time. Thus, we have a correct lock.\n\n\nThe next axis is\n   **fairness**\n   . How fair is a spin lock to a waiting thread? Can you guarantee that a waiting thread will ever enter the critical section? The answer here, unfortunately, is bad news: spin locks don't provide any fairness guarantees. Indeed, a thread spinning may spin forever, under contention. Simple spin locks (as discussed thus far) are not fair and may lead to starvation.\n\n\nThe final axis is\n   **performance**\n   . What are the costs of using a spin lock? To analyze this more carefully, we suggest thinking about a few different cases. In the first, imagine threads competing for the lock on a single processor; in the second, consider threads spread out across many CPUs.\n\n\nFor spin locks, in the single CPU case, performance overheads can be quite painful; imagine the case where the thread holding the lock is preempted within a critical section. The scheduler might then run every other thread (imagine there are\n   \n    N - 1\n   \n   others), each of which tries to acquire the lock. In this case, each of those threads will spin for the duration of a time slice before giving up the CPU, a waste of CPU cycles.\n\n\nHowever, on multiple CPUs, spin locks work reasonably well (if the number of threads roughly equals the number of CPUs). The thinking\n\n\n1 int CompareAndSwap(int *ptr, int expected, int new) {\n2     int original = *ptr;\n3     if (original == expected)\n4         *ptr = new;\n5     return original;\n6 }\n\nFigure 28.4:\n   **Compare-and-swap**\n\n\ngoes as follows: imagine Thread A on CPU 1 and Thread B on CPU 2, both contending for a lock. If Thread A (CPU 1) grabs the lock, and then Thread B tries to, B will spin (on CPU 2). However, presumably the critical section is short, and thus soon the lock becomes available, and is acquired by Thread B. Spinning to wait for a lock held on another processor doesn't waste many cycles in this case, and thus can be effective."
        },
        {
          "name": "Compare-And-Swap",
          "content": "Another hardware primitive that some systems provide is known as the\n   **compare-and-swap**\n   instruction (as it is called on SPARC, for example), or\n   **compare-and-exchange**\n   (as it called on x86). The C pseudocode for this single instruction is found in Figure 28.4.\n\n\nThe basic idea is for compare-and-swap to test whether the value at the address specified by\n   \n    ptr\n   \n   is equal to\n   \n    expected\n   \n   ; if so, update the memory location pointed to by\n   \n    ptr\n   \n   with the new value. If not, do nothing. In either case, return the original value at that memory location, thus allowing the code calling compare-and-swap to know whether it succeeded or not.\n\n\nWith the compare-and-swap instruction, we can build a lock in a manner quite similar to that with test-and-set. For example, we could just replace the\n   \n    lock()\n   \n   routine above with the following:\n\n\n1 void lock(lock_t *lock) {\n2     while (CompareAndSwap(&lock->flag, 0, 1) == 1)\n3         ; // spin\n4 }\n\nThe rest of the code is the same as the test-and-set example above. This code works quite similarly; it simply checks if the flag is 0 and if so, atomically swaps in a 1 thus acquiring the lock. Threads that try to acquire the lock while it is held will get stuck spinning until the lock is finally released.\n\n\nIf you want to see how to really make a C-callable x86-version of compare-and-swap, the code sequence (from [S05]) might be useful\n   \n    2\n   \n   .\n\n\nFinally, as you may have sensed, compare-and-swap is a more powerful instruction than test-and-set. We will make some use of this power in\n\n\n2\n   \ngithub.com/remzi-arpacidusseau/ostep-code/tree/master/threads-locks\n\n\nthe future when we briefly delve into topics such as\n   **lock-free synchronization**\n   [H91]. However, if we just build a simple spin lock with it, its behavior is identical to the spin lock we analyzed above."
        },
        {
          "name": "Load-Linked and Store-Conditional",
          "content": "Some platforms provide a pair of instructions that work in concert to help build critical sections. On the MIPS architecture [H93], for example, the\n   **load-linked**\n   and\n   **store-conditional**\n   instructions can be used in tandem to build locks and other concurrent structures. The C pseudocode for these instructions is as found in Figure 28.5. Alpha, PowerPC, and ARM provide similar instructions [W09].\n\n\nThe load-linked operates much like a typical load instruction, and simply fetches a value from memory and places it in a register. The key difference comes with the store-conditional, which only succeeds (and updates the value stored at the address just load-linked from) if no intervening store to the address has taken place. In the case of success, the store-conditional returns 1 and updates the value at\n   \n    ptr\n   \n   to\n   \n    value\n   \n   ; if it fails, the value at\n   \n    ptr\n   \n   is\n   *not*\n   updated and 0 is returned.\n\n\nAs a challenge to yourself, try thinking about how to build a lock using load-linked and store-conditional. Then, when you are finished, look at the code below which provides one simple solution. Do it! The solution is in Figure 28.6.\n\n\nThe\n   \n    lock()\n   \n   code is the only interesting piece. First, a thread spins waiting for the flag to be set to 0 (and thus indicate the lock is not held). Once so, the thread tries to acquire the lock via the store-conditional; if it succeeds, the thread has atomically changed the flag's value to 1 and thus can proceed into the critical section.\n\n\nNote how failure of the store-conditional might arise. One thread calls\n   \n    lock()\n   \n   and executes the load-linked, returning 0 as the lock is not held. Before it can attempt the store-conditional, it is interrupted and another thread enters the lock code, also executing the load-linked instruction,\n\n\n1 int LoadLinked(int *ptr) {\n2     return *ptr;\n3 }\n4\n5 int StoreConditional(int *ptr, int value) {\n6     if (no update to *ptr since LL to this addr) {\n7         *ptr = value;\n8         return 1; // success!\n9     } else {\n10        return 0; // failed to update\n11    }\n12 }\nFigure 28.5:\n   **Load-linked And Store-conditional**\n\n\n1 void lock(lock_t *lock) {\n2     while (1) {\n3         while (LoadLinked(&lock->flag) == 1)\n4             ; // spin until it's zero\n5         if (StoreConditional(&lock->flag, 1) == 1)\n6             return; // if set-to-1 was success: done\n7         // otherwise: try again\n8     }\n9 }\n10\n11 void unlock(lock_t *lock) {\n12     lock->flag = 0;\n13 }\nFigure 28.6: Using LL/SC To Build A Lock\n\n\nand also getting a 0 and continuing. At this point, two threads have each executed the load-linked and each are about to attempt the store-conditional. The key feature of these instructions is that only one of these threads will succeed in updating the flag to 1 and thus acquire the lock; the second thread to attempt the store-conditional will fail (because the other thread updated the value of flag between its load-linked and store-conditional) and thus have to try to acquire the lock again.\n\n\nIn class a few years ago, undergraduate student David Capel suggested a more concise form of the above, for those of you who enjoy short-circuiting boolean conditionals. See if you can figure out why it is equivalent. It certainly is shorter!\n\n\n1 void lock(lock_t *lock) {\n2     while (LoadLinked(&lock->flag) ||\n3         !StoreConditional(&lock->flag, 1))\n4         ; // spin\n5 }"
        },
        {
          "name": "Fetch-And-Add",
          "content": "One final hardware primitive is the\n   **fetch-and-add**\n   instruction, which atomically increments a value while returning the old value at a particular address. The C pseudocode for the fetch-and-add instruction looks like this:\n\n\n1 int FetchAndAdd(int *ptr) {\n2     int old = *ptr;\n3     *ptr = old + 1;\n4     return old;\n5 }\n**TIP: LESS CODE IS BETTER CODE (LAUER'S LAW)**\nProgrammers tend to brag about how much code they wrote to do something. Doing so is fundamentally broken. What one should brag about, rather, is how\n   *little*\n   code one wrote to accomplish a given task. Short, concise code is always preferred; it is likely easier to understand and has fewer bugs. As Hugh Lauer said, when discussing the construction of the Pilot operating system: “If the same people had twice as much time, they could produce as good of a system in half the code.” [L81] We’ll call this\n   **Lauer’s Law**\n   , and it is well worth remembering. So next time you’re bragging about how much code you wrote to finish the assignment, think again, or better yet, go back, rewrite, and make the code as clear and concise as possible.\n\n\nIn this example, we’ll use fetch-and-add to build a more interesting\n   **ticket lock**\n   , as introduced by Mellor-Crummey and Scott [MS91]. The lock and unlock code is found in Figure 28.7 (page 14).\n\n\nInstead of a single value, this solution uses a ticket and turn variable in combination to build a lock. The basic operation is pretty simple: when a thread wishes to acquire a lock, it first does an atomic fetch-and-add on the ticket value; that value is now considered this thread’s “turn” (\n   \n    myturn\n   \n   ). The globally shared\n   \n    lock->turn\n   \n   is then used to determine which thread’s turn it is; when (\n   \n    myturn == turn\n   \n   ) for a given thread, it is that thread’s turn to enter the critical section. Unlock is accomplished simply by incrementing the turn such that the next waiting thread (if there is one) can now enter the critical section.\n\n\nNote one important difference with this solution versus our previous attempts: it ensures progress for all threads. Once a thread is assigned its ticket value, it will be scheduled at some point in the future (once those in front of it have passed through the critical section and released the lock). In our previous attempts, no such guarantee existed; a thread spinning on test-and-set (for example) could spin forever even as other threads acquire and release the lock."
        },
        {
          "name": "What Now?Our hardware-based locks",
          "content": "Our hardware-based locks are simple (only a few lines of code) and they work (you could even prove that if you’d like to, by writing some code), which are two excellent properties of any system or code. However, in some cases, these solutions can be quite inefficient. Imagine you are running two threads on a single processor. Now imagine that one thread (thread 0) is in a critical section and thus has a lock held, and unfortunately gets interrupted. The second thread (thread 1) now tries to acquire the lock, but finds that it is held. Thus, it begins to spin. And spin. Then it spins some more. And finally, a timer interrupt goes off, thread 0 is run again, which releases the lock, and finally (the next time\n\n\n1  typedef struct __lock_t {\n2      int ticket;\n3      int turn;\n4  } lock_t;\n5\n6  void lock_init(lock_t *lock) {\n7      lock->ticket = 0;\n8      lock->turn   = 0;\n9  }\n10\n11 void lock(lock_t *lock) {\n12     int myturn = FetchAndAdd(&lock->ticket);\n13     while (lock->turn != myturn)\n14         ; // spin\n15 }\n16\n17 void unlock(lock_t *lock) {\n18     lock->turn = lock->turn + 1;\n19 }\nFigure 28.7:\n   **Ticket Locks**\n\n\nit runs, say), thread 1 won't have to spin so much and will be able to acquire the lock. Thus, any time a thread gets caught spinning in a situation like this, it wastes an entire time slice doing nothing but checking a value that isn't going to change! The problem gets worse with\n   \n    N\n   \n   threads contending for a lock;\n   \n    N - 1\n   \n   time slices may be wasted in a similar manner, simply spinning and waiting for a single thread to release the lock. And thus, our next problem:\n\n\nTHE CRUX: HOW TO AVOID SPINNING\n\n\nHow can we develop a lock that doesn't needlessly waste time spinning on the CPU?\n\n\nHardware support alone cannot solve the problem. We'll need OS support too! Let's now figure out just how that might work."
        },
        {
          "name": "A Simple Approach: Just Yield, Baby",
          "content": "Hardware support got us pretty far: working locks, and even (as with the case of the ticket lock) fairness in lock acquisition. However, we still have a problem: what to do when a context switch occurs in a critical section, and threads start to spin endlessly, waiting for the interrupted (lock-holding) thread to be run again?\n\n\nOur first try is a simple and friendly approach: when you are going to spin, instead give up the CPU to another thread. As Al Davis might say, \"just yield, baby!\" [D91]. Figure 28.8 (page 15) shows the approach.\n\n\n1 void init() {\n2     flag = 0;\n3 }\n4\n5 void lock() {\n6     while (TestAndSet(&flag, 1) == 1)\n7         yield(); // give up the CPU\n8 }\n9\n10 void unlock() {\n11     flag = 0;\n12 }\nFigure 28.8: Lock With Test-and-set And Yield\n\n\nIn this approach, we assume an operating system primitive\n   \n    yield()\n   \n   which a thread can call when it wants to give up the CPU and let another thread run. A thread can be in one of three states (running, ready, or blocked);\n   \n    yield\n   \n   is simply a system call that moves the caller from the\n   **running**\n   state to the\n   **ready**\n   state, and thus promotes another thread to running. Thus, the yielding thread essentially\n   **deschedules**\n   itself.\n\n\nThink about the example with two threads on one CPU; in this case, our yield-based approach works quite well. If a thread happens to call\n   \n    lock()\n   \n   and find a lock held, it will simply yield the CPU, and thus the other thread will run and finish its critical section. In this simple case, the yielding approach works well.\n\n\nLet us now consider the case where there are many threads (say 100) contending for a lock repeatedly. In this case, if one thread acquires the lock and is preempted before releasing it, the other 99 will each call\n   \n    lock()\n   \n   , find the lock held, and yield the CPU. Assuming some kind of round-robin scheduler, each of the 99 will execute this run-and-yield pattern before the thread holding the lock gets to run again. While better than our spinning approach (which would waste 99 time slices spinning), this approach is still costly; the cost of a context switch can be substantial, and there is thus plenty of waste.\n\n\nWorse, this approach does not address starvation. A thread may get caught in an endless yield loop while other threads repeatedly enter and exit the critical section. We clearly will need an approach that addresses starvation directly."
        },
        {
          "name": "Using Queues: Sleeping Instead Of Spinning",
          "content": "The real problem with some previous approaches (other than the ticket lock) is that they leave too much to chance. The scheduler determines which thread runs next; if the scheduler makes a bad choice, a thread that runs must either spin waiting for the lock (our first approach), or yield the CPU immediately (our second approach). Either way, there is potential for waste and no prevention of starvation.\n\n\n1  typedef struct __lock_t {\n2      int flag;\n3      int guard;\n4      queue_t *q;\n5  } lock_t;\n6\n7  void lock_init(lock_t *m) {\n8      m->flag = 0;\n9      m->guard = 0;\n10     queue_init(m->q);\n11 }\n12\n13 void lock(lock_t *m) {\n14     while (TestAndSet(&m->guard, 1) == 1)\n15         ; //acquire guard lock by spinning\n16     if (m->flag == 0) {\n17         m->flag = 1; // lock is acquired\n18         m->guard = 0;\n19     } else {\n20         queue_add(m->q, gettid());\n21         m->guard = 0;\n22         park();\n23     }\n24 }\n25\n26 void unlock(lock_t *m) {\n27     while (TestAndSet(&m->guard, 1) == 1)\n28         ; //acquire guard lock by spinning\n29     if (queue_empty(m->q))\n30         m->flag = 0; // let go of lock; no one wants it\n31     else\n32         unpark(queue_remove(m->q)); // hold lock\n33                                   // (for next thread!)\n34     m->guard = 0;\n35 }\nFigure 28.9:\n   **Lock With Queues, Test-and-set, Yield, And Wakeup**\n\n\nThus, we must explicitly exert some control over which thread next gets to acquire the lock after the current holder releases it. To do this, we will need a little more OS support, as well as a queue to keep track of which threads are waiting to acquire the lock.\n\n\nFor simplicity, we will use the support provided by Solaris, in terms of two calls:\n   \n    park()\n   \n   to put a calling thread to sleep, and\n   \n    unpark(threadID)\n   \n   to wake a particular thread as designated by\n   \n    threadID\n   \n   . These two routines can be used in tandem to build a lock that puts a caller to sleep if it tries to acquire a held lock and wakes it when the lock is free. Let's look at the code in Figure 28.9 to understand one possible use of such primitives.\n\n\n\n\n**ASIDE: MORE REASON TO AVOID SPINNING: PRIORITY INVERSION**\n\n\nOne good reason to avoid spin locks is performance: as described in the main text, if a thread is interrupted while holding a lock, other threads that use spin locks will spend a large amount of CPU time just waiting for the lock to become available. However, it turns out there is another interesting reason to avoid spin locks on some systems: correctness. The problem to be wary of is known as\n   **priority inversion**\n   , which unfortunately is an intergalactic scourge, occurring on Earth [M15] and Mars [R97]!\n\n\nLet's assume there are two threads in a system. Thread 2 (T2) has a high scheduling priority, and Thread 1 (T1) has lower priority. In this example, let's assume that the CPU scheduler will always run T2 over T1, if indeed both are runnable; T1 only runs when T2 is not able to do so (e.g., when T2 is blocked on I/O).\n\n\nNow, the problem. Assume T2 is blocked for some reason. So T1 runs, grabs a spin lock, and enters a critical section. T2 now becomes unblocked (perhaps because an I/O completed), and the CPU scheduler immediately schedules it (thus descheduling T1). T2 now tries to acquire the lock, and because it can't (T1 holds the lock), it just keeps spinning. Because the lock is a spin lock, T2 spins forever, and the system is hung.\n\n\nJust avoiding the use of spin locks, unfortunately, does not avoid the problem of inversion (alas). Imagine three threads, T1, T2, and T3, with T3 at the highest priority, and T1 the lowest. Imagine now that T1 grabs a lock. T3 then starts, and because it is higher priority than T1, runs immediately (preempting T1). T3 tries to acquire the lock that T1 holds, but gets stuck waiting, because T1 still holds it. If T2 starts to run, it will have higher priority than T1, and thus it will run. T3, which is higher priority than T2, is stuck waiting for T1, which may never run now that T2 is running. Isn't it sad that the mighty T3 can't run, while lowly T2 controls the CPU? Having high priority just ain't what it used to be.\n\n\nYou can address the priority inversion problem in a number of ways. In the specific case where spin locks cause the problem, you can avoid using spin locks (described more below). More generally, a higher-priority thread waiting for a lower-priority thread can temporarily boost the lower thread's priority, thus enabling it to run and overcoming the inversion, a technique known as\n   **priority inheritance**\n   . A last solution is simplest: ensure all threads have the same priority.\n\n\nWe do a couple of interesting things in this example. First, we combine the old test-and-set idea with an explicit queue of lock waiters to make a more efficient lock. Second, we use a queue to help control who gets the lock next and thus avoid starvation.\n\n\nYou might notice how the guard is used (Figure 28.9, page 16), basically as a spin-lock around the flag and queue manipulations the lock is using. This approach thus doesn't avoid spin-waiting entirely; a thread\n\n\nmight be interrupted while acquiring or releasing the lock, and thus cause other threads to spin-wait for this one to run again. However, the time spent spinning is quite limited (just a few instructions inside the lock and unlock code, instead of the user-defined critical section), and thus this approach may be reasonable.\n\n\nYou might also observe that in\n   \n    lock()\n   \n   , when a thread can not acquire the lock (it is already held), we are careful to add ourselves to a queue (by calling the\n   \n    gettid()\n   \n   function to get the thread ID of the current thread), set\n   \n    guard\n   \n   to 0, and yield the CPU. A question for the reader: What would happen if the release of the guard lock came\n   *after*\n   the\n   \n    park()\n   \n   , and not before? Hint: something bad.\n\n\nYou might further detect that the flag does not get set back to 0 when another thread gets woken up. Why is this? Well, it is not an error, but rather a necessity! When a thread is woken up, it will be as if it is returning from\n   \n    park()\n   \n   ; however, it does not hold the guard at that point in the code and thus cannot even try to set the flag to 1. Thus, we just pass the lock directly from the thread releasing the lock to the next thread acquiring it;\n   \n    flag\n   \n   is not set to 0 in-between.\n\n\nFinally, you might notice the perceived race condition in the solution, just before the call to\n   \n    park()\n   \n   . With just the wrong timing, a thread will be about to park, assuming that it should sleep until the lock is no longer held. A switch at that time to another thread (say, a thread holding the lock) could lead to trouble, for example, if that thread then released the lock. The subsequent park by the first thread would then sleep forever (potentially), a problem sometimes called the\n   **wakeup/waiting race**\n   .\n\n\nSolaris solves this problem by adding a third system call:\n   \n    setpark()\n   \n   . By calling this routine, a thread can indicate it is\n   *about to*\n   park. If it then happens to be interrupted and another thread calls\n   \n    unpark\n   \n   before\n   \n    park\n   \n   is actually called, the subsequent\n   \n    park\n   \n   returns immediately instead of sleeping. The code modification, inside of\n   \n    lock()\n   \n   , is quite small:\n\n\n1   queue_add(m->q, gettid());\n2   setpark(); // new code\n3   m->guard = 0;\nA different solution could pass the guard into the kernel. In that case, the kernel could take precautions to atomically release the lock and dequeue the running thread."
        },
        {
          "name": "Different OS, Different Support",
          "content": "We have thus far seen one type of support that an OS can provide in order to build a more efficient lock in a thread library. Other OS's provide similar support; the details vary.\n\n\nFor example, Linux provides a\n   **futex**\n   which is similar to the Solaris interface but provides more in-kernel functionality. Specifically, each futex has associated with it a specific physical memory location, as well as a\n\n\n1 void mutex_lock (int *mutex) {\n2     int v;\n3     // Bit 31 was clear, we got the mutex (fastpath)\n4     if (atomic_bit_test_set (mutex, 31) == 0)\n5         return;\n6     atomic_increment (mutex);\n7     while (1) {\n8         if (atomic_bit_test_set (mutex, 31) == 0) {\n9             atomic_decrement (mutex);\n10            return;\n11        }\n12        // Have to waitFirst to make sure futex value\n13        // we are monitoring is negative (locked).\n14        v = *mutex;\n15        if (v >= 0)\n16            continue;\n17        futex_wait (mutex, v);\n18    }\n19 }\n20\n21 void mutex_unlock (int *mutex) {\n22     // Adding 0x80000000 to counter results in 0 if and\n23     // only if there are not other interested threads\n24     if (atomic_add_zero (mutex, 0x80000000))\n25         return;\n26\n27     // There are other threads waiting for this mutex,\n28     // wake one of them up.\n29     futex_wake (mutex);\n30 }\n\nFigure 28.10:\n   **Linux-based Futex Locks**\n\n\nper-futex in-kernel queue. Callers can use futex calls (described below) to sleep and wake as need be.\n\n\nSpecifically, two calls are available. The call to\n   \n    futex_wait(address, expected)\n   \n   puts the calling thread to sleep, assuming the value at the address\n   \n    address\n   \n   is equal to\n   \n    expected\n   \n   . If it is\n   *not*\n   equal, the call returns immediately. The call to the routine\n   \n    futex_wake(address)\n   \n   wakes one thread that is waiting on the queue. The usage of these calls in a Linux mutex is shown in Figure 28.10 (page 19).\n\n\nThis code snippet from\n   \n    lowlevellock.h\n   \n   in the\n   \n    nptl\n   \n   library (part of the\n   \n    gnu libc\n   \n   library) [L09] is interesting for a few reasons. First, it uses a single integer to track both whether the lock is held or not (the high bit of the integer) and the number of waiters on the lock (all the other bits). Thus, if the lock is negative, it is held (because the high bit is set and that bit determines the sign of the integer).\n\n\nSecond, the code snippet shows how to optimize for the common case,\n\n\nspecifically when there is no contention for the lock; with only one thread acquiring and releasing a lock, very little work is done (the atomic bit test-and-set to lock and an atomic add to release the lock).\n\n\nSee if you can puzzle through the rest of this “real-world” lock to understand how it works. Do it and become a master of Linux locking, or at least somebody who listens when a book tells you to do something\n   \n    3\n   \n   ."
        },
        {
          "name": "Two-Phase Locks",
          "content": "One final note: the Linux approach has the flavor of an old approach that has been used on and off for years, going at least as far back to Dahm Locks in the early 1960’s [M82], and is now referred to as a\n   **two-phase lock**\n   . A two-phase lock realizes that spinning can be useful, particularly if the lock is about to be released. So in the first phase, the lock spins for a while, hoping that it can acquire the lock.\n\n\nHowever, if the lock is not acquired during the first spin phase, a second phase is entered, where the caller is put to sleep, and only woken up when the lock becomes free later. The Linux lock above is a form of such a lock, but it only spins once; a generalization of this could spin in a loop for a fixed amount of time before using\n   **futex**\n   support to sleep.\n\n\nTwo-phase locks are yet another instance of a\n   **hybrid**\n   approach, where combining two good ideas may indeed yield a better one. Of course, whether it does depends strongly on many things, including the hardware environment, number of threads, and other workload details. As always, making a single general-purpose lock, good for all possible use cases, is quite a challenge."
        }
      ]
    },
    {
      "name": "Lock-based Concurrent Data Structures",
      "sections": [
        {
          "name": "Concurrent Counters",
          "content": "One of the simplest data structures is a counter. It is a structure that is commonly used and has a simple interface. We define a simple non-concurrent counter in Figure 29.1.\n\n\n\n\n**Simple But Not Scalable**\n\n\nAs you can see, the non-synchronized counter is a trivial data structure, requiring a tiny amount of code to implement. We now have our next challenge: how can we make this code\n   **thread safe**\n   ? Figure 29.2 shows how we do so.\n\n\n1 typedef struct __counter_t {\n2     int value;\n3 } counter_t;\n4\n5 void init(counter_t *c) {\n6     c->value = 0;\n7 }\n8\n9 void increment(counter_t *c) {\n10    c->value++;\n11 }\n12\n13 void decrement(counter_t *c) {\n14    c->value--;\n15 }\n16\n17 int get(counter_t *c) {\n18     return c->value;\n19 }\nFigure 29.1:\n   **A Counter Without Locks**\n\n\nThis concurrent counter is simple and works correctly. In fact, it follows a design pattern common to the simplest and most basic concurrent data structures: it simply adds a single lock, which is acquired when calling a routine that manipulates the data structure, and is released when returning from the call. In this manner, it is similar to a data structure built with\n   **monitors**\n   [BH73], where locks are acquired and released automatically as you call and return from object methods.\n\n\nAt this point, you have a working concurrent data structure. The problem you might have is performance. If your data structure is too slow, you'll have to do more than just add a single lock; such optimizations, if needed, are thus the topic of the rest of the chapter. Note that if the data structure is\n   *not*\n   too slow, you are done! No need to do something fancy if something simple will work.\n\n\nTo understand the performance costs of the simple approach, we run a benchmark in which each thread updates a single shared counter a fixed number of times; we then vary the number of threads. Figure 29.5 shows the total time taken, with one to four threads active; each thread updates the counter one million times. This experiment was run upon an iMac with four Intel 2.7 GHz i5 CPUs; with more CPUs active, we hope to get more total work done per unit time.\n\n\nFrom the top line in the figure (labeled 'Precise'), you can see that the performance of the synchronized counter scales poorly. Whereas a single thread can complete the million counter updates in a tiny amount of time (roughly 0.03 seconds), having two threads each update the counter one million times concurrently leads to a massive slowdown (taking over 5 seconds!). It only gets worse with more threads.\n\n\n1  typedef struct __counter_t {\n2      int             value;\n3      pthread_mutex_t lock;\n4  } counter_t;\n5\n6  void init(counter_t *c) {\n7      c->value = 0;\n8      Pthread_mutex_init(&c->lock, NULL);\n9  }\n10\n11 void increment(counter_t *c) {\n12     Pthread_mutex_lock(&c->lock);\n13     c->value++;\n14     Pthread_mutex_unlock(&c->lock);\n15 }\n16\n17 void decrement(counter_t *c) {\n18     Pthread_mutex_lock(&c->lock);\n19     c->value--;\n20     Pthread_mutex_unlock(&c->lock);\n21 }\n22\n23 int get(counter_t *c) {\n24     Pthread_mutex_lock(&c->lock);\n25     int rc = c->value;\n26     Pthread_mutex_unlock(&c->lock);\n27     return rc;\n28 }\n\nFigure 29.2:\n   **A Counter With Locks**\n\n\nIdeally, you'd like to see the threads complete just as quickly on multiple processors as the single thread does on one. Achieving this end is called\n   **perfect scaling**\n   ; even though more work is done, it is done in parallel, and hence the time taken to complete the task is not increased.\n\n\n\n\n**Scalable Counting**\n\n\nAmazingly, researchers have studied how to build more scalable counters for years [MS04]. Even more amazing is the fact that scalable counters matter, as recent work in operating system performance analysis has shown [B+10]; without scalable counting, some workloads running on Linux suffer from serious scalability problems on multicore machines.\n\n\nMany techniques have been developed to attack this problem. We'll describe one approach known as an\n   **approximate counter**\n   [C06].\n\n\nThe approximate counter works by representing a single logical counter via numerous\n   *local*\n   physical counters, one per CPU core, as well as a single\n   *global*\n   counter. Specifically, on a machine with four CPUs, there are four\n\n\n\nTime | L_1 | L_2 | L_3 | L_4 | G\n0 | 0 | 0 | 0 | 0 | 0\n1 | 0 | 0 | 1 | 1 | 0\n2 | 1 | 0 | 2 | 1 | 0\n3 | 2 | 0 | 3 | 1 | 0\n4 | 3 | 0 | 3 | 2 | 0\n5 | 4 | 1 | 3 | 3 | 0\n6 | 5\n      \n       \\to\n      \n      0 | 1 | 3 | 4 | 5 (from\n      \n       L_1\n      \n      )\n7 | 0 | 2 | 4 | 5\n      \n       \\to\n      \n      0 | 10 (from\n      \n       L_4\n      \n      )\n\n\nFigure 29.3: Tracing the Approximate Counters\n\n\nlocal counters and one global one. In addition to these counters, there are also locks: one for each local counter\n   \n    1\n   \n   , and one for the global counter.\n\n\nThe basic idea of approximate counting is as follows. When a thread running on a given core wishes to increment the counter, it increments its local counter; access to this local counter is synchronized via the corresponding local lock. Because each CPU has its own local counter, threads across CPUs can update local counters without contention, and thus updates to the counter are scalable.\n\n\nHowever, to keep the global counter up to date (in case a thread wishes to read its value), the local values are periodically transferred to the global counter, by acquiring the global lock and incrementing it by the local counter's value; the local counter is then reset to zero.\n\n\nHow often this local-to-global transfer occurs is determined by a threshold\n   \n    S\n   \n   . The smaller\n   \n    S\n   \n   is, the more the counter behaves like the non-scalable counter above; the bigger\n   \n    S\n   \n   is, the more scalable the counter, but the further off the global value might be from the actual count. One could simply acquire all the local locks and the global lock (in a specified order, to avoid deadlock) to get an exact value, but that is not scalable.\n\n\nTo make this clear, let's look at an example (Figure 29.3). In this example, the threshold\n   \n    S\n   \n   is set to 5, and there are threads on each of four CPUs updating their local counters\n   \n    L_1 \\dots L_4\n   \n   . The global counter value (\n   \n    G\n   \n   ) is also shown in the trace, with time increasing downward. At each time step, a local counter may be incremented; if the local value reaches the threshold\n   \n    S\n   \n   , the local value is transferred to the global counter and the local counter is reset.\n\n\nThe lower line in Figure 29.5 (labeled 'Approximate', on page 6) shows the performance of approximate counters with a threshold\n   \n    S\n   \n   of 1024. Performance is excellent; the time taken to update the counter four million times on four processors is hardly higher than the time taken to update it one million times on one processor.\n\n\n1\n   \n   We need the local locks because we assume there may be more than one thread on each core. If, instead, only one thread ran on each core, no local lock would be needed.\n\n\n1  typedef struct __counter_t {\n2      int         global;           // global count\n3      pthread_mutex_t glock;       // global lock\n4      int         local[NUMCPUS];  // per-CPU count\n5      pthread_mutex_t llock[NUMCPUS]; // ... and locks\n6      int         threshold;       // update freq\n7  } counter_t;\n8\n9  // init: record threshold, init locks, init values\n10 //          of all local counts and global count\n11 void init(counter_t *c, int threshold) {\n12     c->threshold = threshold;\n13     c->global = 0;\n14     pthread_mutex_init(&c->glock, NULL);\n15     int i;\n16     for (i = 0; i < NUMCPUS; i++) {\n17         c->local[i] = 0;\n18         pthread_mutex_init(&c->llock[i], NULL);\n19     }\n20 }\n21\n22 // update: usually, just grab local lock and update\n23 // local amount; once it has risen 'threshold',\n24 // grab global lock and transfer local values to it\n25 void update(counter_t *c, int threadID, int amt) {\n26     int cpu = threadID % NUMCPUS;\n27     pthread_mutex_lock(&c->llock[cpu]);\n28     c->local[cpu] += amt;\n29     if (c->local[cpu] >= c->threshold) {\n30         // transfer to global (assumes amt>0)\n31         pthread_mutex_lock(&c->glock);\n32         c->global += c->local[cpu];\n33         pthread_mutex_unlock(&c->glock);\n34         c->local[cpu] = 0;\n35     }\n36     pthread_mutex_unlock(&c->llock[cpu]);\n37 }\n38\n39 // get: just return global amount (approximate)\n40 int get(counter_t *c) {\n41     pthread_mutex_lock(&c->glock);\n42     int val = c->global;\n43     pthread_mutex_unlock(&c->glock);\n44     return val; // only approximate!\n45 }\n\nFigure 29.4: Approximate Counter Implementation\n\n\n\n\n![Figure 29.5: Performance of Traditional vs. Approximate Counters. A line graph showing Time (seconds) on the Y-axis (0 to 15) versus Threads on the X-axis (1 to 4). The 'Precise' counter (marked with 'x') shows a linear increase in time as the number of threads increases. The 'Approximate' counter (marked with 'o') remains at zero time for all thread counts.](images/image_0078.jpeg)\n\n\nThreads | Precise (seconds) | Approximate (seconds)\n1 | 0 | 0\n2 | 5.5 | 0\n3 | 9.0 | 0\n4 | 12.0 | 0\n\n\nFigure 29.5: Performance of Traditional vs. Approximate Counters. A line graph showing Time (seconds) on the Y-axis (0 to 15) versus Threads on the X-axis (1 to 4). The 'Precise' counter (marked with 'x') shows a linear increase in time as the number of threads increases. The 'Approximate' counter (marked with 'o') remains at zero time for all thread counts.\n\n\nFigure 29.5:\n   **Performance of Traditional vs. Approximate Counters**\n\n\n\n\n![Figure 29.6: Scaling Approximate Counters. A line graph showing Time (seconds) on the Y-axis (0 to 15) versus Approximation Factor (S) on the X-axis (1 to 1024). The time decreases sharply as the approximation factor S increases, leveling off at a very low time for S values greater than 16.](images/image_0079.jpeg)\n\n\nApproximation Factor (S) | Time (seconds)\n1 | 11.0\n2 | 5.5\n4 | 2.8\n8 | 1.2\n16 | 0.5\n32 | 0.2\n64 | 0.1\n128 | 0.05\n256 | 0.02\n512 | 0.01\n1024 | 0.005\n\n\nFigure 29.6: Scaling Approximate Counters. A line graph showing Time (seconds) on the Y-axis (0 to 15) versus Approximation Factor (S) on the X-axis (1 to 1024). The time decreases sharply as the approximation factor S increases, leveling off at a very low time for S values greater than 16.\n\n\nFigure 29.6:\n   **Scaling Approximate Counters**\n\n\nFigure 29.6 shows the importance of the threshold value\n   \n    S\n   \n   , with four threads each incrementing the counter 1 million times on four CPUs. If\n   \n    S\n   \n   is low, performance is poor (but the global count is always quite accurate); if\n   \n    S\n   \n   is high, performance is excellent, but the global count lags (by at most the number of CPUs multiplied by\n   \n    S\n   \n   ). This accuracy/performance trade-off is what approximate counters enable.\n\n\nA rough version of an approximate counter is found in Figure 29.4 (page 5). Read it, or better yet, run it yourself in some experiments to better understand how it works.\n\n\n**TIP: MORE CONCURRENCY ISN'T NECESSARILY FASTER**\nIf the scheme you design adds a lot of overhead (for example, by acquiring and releasing locks frequently, instead of once), the fact that it is more concurrent may not be important. Simple schemes tend to work well, especially if they use costly routines rarely. Adding more locks and complexity can be your downfall. All of that said, there is one way to really know: build both alternatives (simple but less concurrent, and complex but more concurrent) and measure how they do. In the end, you can't cheat on performance; your idea is either faster, or it isn't."
        },
        {
          "name": "Concurrent Linked Lists",
          "content": "We next examine a more complicated structure, the linked list. Let's start with a basic approach once again. For simplicity, we'll omit some of the obvious routines that such a list would have and just focus on concurrent insert and lookup; we'll leave it to the reader to think about delete, etc. Figure 29.7 shows the code for this rudimentary data structure.\n\n\nAs you can see in the code, the code simply acquires a lock in the insert routine upon entry, and releases it upon exit. One small tricky issue arises if\n   \n    malloc()\n   \n   happens to fail (a rare case); in this case, the code must also release the lock before failing the insert.\n\n\nThis kind of exceptional control flow has been shown to be quite error prone; a recent study of Linux kernel patches found that a huge fraction of bugs (nearly 40%) are found on such rarely-taken code paths (indeed, this observation sparked some of our own research, in which we removed all memory-failing paths from a Linux file system, resulting in a more robust system [S+11]).\n\n\nThus, a challenge: can we rewrite the insert and lookup routines to remain correct under concurrent insert but avoid the case where the failure path also requires us to add the call to unlock?\n\n\nThe answer, in this case, is yes. Specifically, we can rearrange the code a bit so that the lock and release only surround the actual critical section in the insert code, and that a common exit path is used in the lookup code. The former works because part of the insert actually need not be locked; assuming that\n   \n    malloc()\n   \n   itself is thread-safe, each thread can call into it without worry of race conditions or other concurrency bugs. Only when updating the shared list does a lock need to be held. See Figure 29.8 for the details of these modifications.\n\n\nAs for the lookup routine, it is a simple code transformation to jump out of the main search loop to a single return path. Doing so again reduces the number of lock acquire/release points in the code, and thus decreases the chances of accidentally introducing bugs (such as forgetting to unlock before returning) into the code.\n\n\n1 // basic node structure\n2 typedef struct __node_t {\n3     int          key;\n4     struct __node_t *next;\n5 } node_t;\n6\n7 // basic list structure (one used per list)\n8 typedef struct __list_t {\n9     node_t       *head;\n10    pthread_mutex_t lock;\n11 } list_t;\n12\n13 void List_Init(list_t *L) {\n14     L->head = NULL;\n15     pthread_mutex_init(&L->lock, NULL);\n16 }\n17\n18 int List_Insert(list_t *L, int key) {\n19     pthread_mutex_lock(&L->lock);\n20     node_t *new = malloc(sizeof(node_t));\n21     if (new == NULL) {\n22         perror(\"malloc\");\n23         pthread_mutex_unlock(&L->lock);\n24         return -1; // fail\n25     }\n26     new->key = key;\n27     new->next = L->head;\n28     L->head = new;\n29     pthread_mutex_unlock(&L->lock);\n30     return 0; // success\n31 }\n32\n33 int List_Lookup(list_t *L, int key) {\n34     pthread_mutex_lock(&L->lock);\n35     node_t *curr = L->head;\n36     while (curr) {\n37         if (curr->key == key) {\n38             pthread_mutex_unlock(&L->lock);\n39             return 0; // success\n40         }\n41         curr = curr->next;\n42     }\n43     pthread_mutex_unlock(&L->lock);\n44     return -1; // failure\n45 }\nFigure 29.7: Concurrent Linked List\n\n\n1 void List_Init(list_t *L) {\n2     L->head = NULL;\n3     pthread_mutex_init(&L->lock, NULL);\n4 }\n5\n6 int List_Insert(list_t *L, int key) {\n7     // synchronization not needed\n8     node_t *new = malloc(sizeof(node_t));\n9     if (new == NULL) {\n10         perror(\"malloc\");\n11         return -1;\n12     }\n13     new->key = key;\n14     // just lock critical section\n15     pthread_mutex_lock(&L->lock);\n16     new->next = L->head;\n17     L->head = new;\n18     pthread_mutex_unlock(&L->lock);\n19     return 0; // success\n20 }\n21\n22 int List_Lookup(list_t *L, int key) {\n23     int rv = -1;\n24     pthread_mutex_lock(&L->lock);\n25     node_t *curr = L->head;\n26     while (curr) {\n27         if (curr->key == key) {\n28             rv = 0;\n29             break;\n30         }\n31         curr = curr->next;\n32     }\n33     pthread_mutex_unlock(&L->lock);\n34     return rv; // now both success and failure\n35 }\nFigure 29.8:\n   **Concurrent Linked List: Rewritten**\n\n\n\n\n**Scaling Linked Lists**\n\n\nThough we again have a basic concurrent linked list, once again we are in a situation where it does not scale particularly well. One technique that researchers have explored to enable more concurrency within a list is something called\n   **hand-over-hand locking**\n   (a.k.a.\n   **lock coupling**\n   ) [MS04].\n\n\nThe idea is pretty simple. Instead of having a single lock for the entire list, you instead add a lock per node of the list. When traversing the list, the code first grabs the next node's lock and then releases the current node's lock (which inspires the name hand-over-hand).\n\n\n**TIP: BE WARY OF LOCKS AND CONTROL FLOW**\nA general design tip, which is useful in concurrent code as well as elsewhere, is to be wary of control flow changes that lead to function returns, exits, or other similar error conditions that halt the execution of a function. Because many functions will begin by acquiring a lock, allocating some memory, or doing other similar stateful operations, when errors arise, the code has to undo all of the state before returning, which is error-prone. Thus, it is best to structure code to minimize this pattern.\n\n\nConceptually, a hand-over-hand linked list makes some sense; it enables a high degree of concurrency in list operations. However, in practice, it is hard to make such a structure faster than the simple single lock approach, as the overheads of acquiring and releasing locks for each node of a list traversal is prohibitive. Even with very large lists, and a large number of threads, the concurrency enabled by allowing multiple ongoing traversals is unlikely to be faster than simply grabbing a single lock, performing an operation, and releasing it. Perhaps some kind of hybrid (where you grab a new lock every so many nodes) would be worth investigating."
        },
        {
          "name": "Concurrent Queues",
          "content": "As you know by now, there is always a standard method to make a concurrent data structure: add a big lock. For a queue, we'll skip that approach, assuming you can figure it out.\n\n\nInstead, we'll take a look at a slightly more concurrent queue designed by Michael and Scott [MS98]. The data structures and code used for this queue are found in Figure 29.9 (page 11).\n\n\nIf you study this code carefully, you'll notice that there are two locks, one for the head of the queue, and one for the tail. The goal of these two locks is to enable concurrency of enqueue and dequeue operations. In the common case, the enqueue routine will only access the tail lock, and dequeue only the head lock.\n\n\nOne trick used by Michael and Scott is to add a dummy node (allocated in the queue initialization code); this dummy enables the separation of head and tail operations. Study the code, or better yet, type it in, run it, and measure it, to understand how it works deeply.\n\n\nQueues are commonly used in multi-threaded applications. However, the type of queue used here (with just locks) often does not completely meet the needs of such programs. A more fully developed bounded queue, that enables a thread to wait if the queue is either empty or overly full, is the subject of our intense study in the next chapter on condition variables. Watch for it!\n\n\n1  typedef struct __node_t {\n2      int          value;\n3      struct __node_t *next;\n4  } node_t;\n5\n6  typedef struct __queue_t {\n7      node_t      *head;\n8      node_t      *tail;\n9      pthread_mutex_t head_lock, tail_lock;\n10 } queue_t;\n11\n12 void Queue_Init(queue_t *q) {\n13     node_t *tmp = malloc(sizeof(node_t));\n14     tmp->next = NULL;\n15     q->head = q->tail = tmp;\n16     pthread_mutex_init(&q->head_lock, NULL);\n17     pthread_mutex_init(&q->tail_lock, NULL);\n18 }\n19\n20 void Queue_Enqueue(queue_t *q, int value) {\n21     node_t *tmp = malloc(sizeof(node_t));\n22     assert(tmp != NULL);\n23     tmp->value = value;\n24     tmp->next = NULL;\n25\n26     pthread_mutex_lock(&q->tail_lock);\n27     q->tail->next = tmp;\n28     q->tail = tmp;\n29     pthread_mutex_unlock(&q->tail_lock);\n30 }\n31\n32 int Queue_Dequeue(queue_t *q, int *value) {\n33     pthread_mutex_lock(&q->head_lock);\n34     node_t *tmp = q->head;\n35     node_t *new_head = tmp->next;\n36     if (new_head == NULL) {\n37         pthread_mutex_unlock(&q->head_lock);\n38         return -1; // queue was empty\n39     }\n40     *value = new_head->value;\n41     q->head = new_head;\n42     pthread_mutex_unlock(&q->head_lock);\n43     free(tmp);\n44     return 0;\n45 }\nFigure 29.9: Michael and Scott Concurrent Queue\n\n\n1 #define BUCKETS (101)\n2\n3 typedef struct __hash_t {\n4     list_t lists[BUCKETS];\n5 } hash_t;\n6\n7 void Hash_Init(hash_t *H) {\n8     int i;\n9     for (i = 0; i < BUCKETS; i++)\n10        List_Init(&H->lists[i]);\n11 }\n12\n13 int Hash_Insert(hash_t *H, int key) {\n14     return List_Insert(&H->lists[key % BUCKETS], key);\n15 }\n16\n17 int Hash_Lookup(hash_t *H, int key) {\n18     return List_Lookup(&H->lists[key % BUCKETS], key);\n19 }\nFigure 29.10:\n   **A Concurrent Hash Table**"
        },
        {
          "name": "Concurrent Hash Table",
          "content": "We end our discussion with a simple and widely applicable concurrent data structure, the hash table. We'll focus on a simple hash table that does not resize; a little more work is required to handle resizing, which we leave as an exercise for the reader (sorry!).\n\n\nThis concurrent hash table (Figure 29.10) is straightforward, is built using the concurrent lists we developed earlier, and works incredibly well. The reason for its good performance is that instead of having a single lock for the entire structure, it uses a lock per hash bucket (each of which is represented by a list). Doing so enables many concurrent operations to take place.\n\n\nFigure 29.11 (page 13) shows the performance of the hash table under concurrent updates (from 10,000 to 50,000 concurrent updates from each of four threads, on the same iMac with four CPUs). Also shown, for the sake of comparison, is the performance of a linked list (with a single lock). As you can see from the graph, this simple concurrent hash table scales magnificently; the linked list, in contrast, does not."
        }
      ]
    },
    {
      "name": "Condition Variables",
      "sections": [
        {
          "name": "Definition and Routines",
          "content": "To wait for a condition to become true, a thread can make use of what is known as a\n   **condition variable**\n   . A\n   **condition variable**\n   is an explicit queue that threads can put themselves on when some state of execution (i.e., some\n   **condition**\n   ) is not as desired (by\n   **waiting**\n   on the condition); some other thread, when it changes said state, can then wake one (or more) of those waiting threads and thus allow them to continue (by\n   **signaling**\n   on the condition). The idea goes back to Dijkstra's use of \"private semaphores\" [D68]; a similar idea was later named a \"condition variable\" by Hoare in his work on monitors [H74].\n\n\nTo declare such a condition variable, one simply writes something like this:\n   \n    pthread_cond_t c;\n   \n   , which declares\n   \n    c\n   \n   as a condition variable (note: proper initialization is also required). A condition variable has two operations associated with it:\n   \n    wait()\n   \n   and\n   \n    signal()\n   \n   . The\n   \n    wait()\n   \n   call is executed when a thread wishes to put itself to sleep; the\n   \n    signal()\n   \n   call\n\n\n1 int done = 0;\n2 pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;\n3 pthread_cond_t c = PTHREAD_COND_INITIALIZER;\n4\n5 void thr_exit() {\n6     Pthread_mutex_lock(&m);\n7     done = 1;\n8     Pthread_cond_signal(&c);\n9     Pthread_mutex_unlock(&m);\n10 }\n11\n12 void *child(void *arg) {\n13     printf(\"child\\n\");\n14     thr_exit();\n15     return NULL;\n16 }\n17\n18 void thr_join() {\n19     Pthread_mutex_lock(&m);\n20     while (done == 0)\n21         Pthread_cond_wait(&c, &m);\n22     Pthread_mutex_unlock(&m);\n23 }\n24\n25 int main(int argc, char *argv[]) {\n26     printf(\"parent: begin\\n\");\n27     pthread_t p;\n28     Pthread_create(&p, NULL, child, NULL);\n29     thr_join();\n30     printf(\"parent: end\\n\");\n31     return 0;\n32 }\nFigure 30.3:\n   **Parent Waiting For Child: Use A Condition Variable**\n\n\nis executed when a thread has changed something in the program and thus wants to wake a sleeping thread waiting on this condition. Specifically, the POSIX calls look like this:\n\n\npthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);\npthread_cond_signal(pthread_cond_t *c);\nWe will often refer to these as\n   \n    wait()\n   \n   and\n   \n    signal()\n   \n   for simplicity. One thing you might notice about the\n   \n    wait()\n   \n   call is that it also takes a mutex as a parameter; it assumes that this mutex is locked when\n   \n    wait()\n   \n   is called. The responsibility of\n   \n    wait()\n   \n   is to release the lock and put the calling thread to sleep (atomically); when the thread wakes up (after some other thread has signaled it), it must re-acquire the lock before returning to the caller. This complexity stems from the desire to prevent certain\n\n\nrace conditions from occurring when a thread is trying to put itself to sleep. Let's take a look at the solution to the join problem (Figure 30.3) to understand this better.\n\n\nThere are two cases to consider. In the first, the parent creates the child thread but continues running itself (assume we have only a single processor) and thus immediately calls into\n   \n    thr_join()\n   \n   to wait for the child thread to complete. In this case, it will acquire the lock, check if the child is done (it is not), and put itself to sleep by calling\n   \n    wait()\n   \n   (hence releasing the lock). The child will eventually run, print the message \"child\", and call\n   \n    thr_exit()\n   \n   to wake the parent thread; this code just grabs the lock, sets the state variable\n   \n    done\n   \n   , and signals the parent thus waking it. Finally, the parent will run (returning from\n   \n    wait()\n   \n   with the lock held), unlock the lock, and print the final message \"parent: end\".\n\n\nIn the second case, the child runs immediately upon creation, sets\n   \n    done\n   \n   to 1, calls\n   \n    signal\n   \n   to wake a sleeping thread (but there is none, so it just returns), and is done. The parent then runs, calls\n   \n    thr_join()\n   \n   , sees that\n   \n    done\n   \n   is 1, and thus does not wait and returns.\n\n\nOne last note: you might observe the parent uses a\n   \n    while\n   \n   loop instead of just an\n   \n    if\n   \n   statement when deciding whether to wait on the condition. While this does not seem strictly necessary per the logic of the program, it is always a good idea, as we will see below.\n\n\nTo make sure you understand the importance of each piece of the\n   \n    thr_exit()\n   \n   and\n   \n    thr_join()\n   \n   code, let's try a few alternate implementations. First, you might be wondering if we need the state variable\n   \n    done\n   \n   . What if the code looked like the example below? (Figure 30.4)\n\n\nUnfortunately this approach is broken. Imagine the case where the child runs immediately and calls\n   \n    thr_exit()\n   \n   immediately; in this case, the child will signal, but there is no thread asleep on the condition. When the parent runs, it will simply call\n   \n    wait\n   \n   and be stuck; no thread will ever wake it. From this example, you should appreciate the importance of the state variable\n   \n    done\n   \n   ; it records the value the threads are interested in knowing. The sleeping, waking, and locking all are built around it.\n\n\n1 void thr_exit() {\n2     Pthread_mutex_lock(&m);\n3     Pthread_cond_signal(&c);\n4     Pthread_mutex_unlock(&m);\n5 }\n6\n7 void thr_join() {\n8     Pthread_mutex_lock(&m);\n9     Pthread_cond_wait(&c, &m);\n10    Pthread_mutex_unlock(&m);\n11 }\nFigure 30.4: Parent Waiting: No State Variable\n\n\n1 void thr_exit() {\n2     done = 1;\n3     Pthread_cond_signal(&c);\n4 }\n5\n6 void thr_join() {\n7     if (done == 0)\n8         Pthread_cond_wait(&c);\n9 }\nFigure 30.5:\n   **Parent Waiting: No Lock**\n\n\nHere (Figure 30.5) is another poor implementation. In this example, we imagine that one does not need to hold a lock in order to signal and wait. What problem could occur here? Think about it\n   \n    1\n   \n   !\n\n\nThe issue here is a subtle race condition. Specifically, if the parent calls\n   \n    thr_join()\n   \n   and then checks the value of\n   \n    done\n   \n   , it will see that it is 0 and thus try to go to sleep. But just before it calls\n   \n    wait\n   \n   to go to sleep, the parent is interrupted, and the child runs. The child changes the state variable\n   \n    done\n   \n   to 1 and signals, but no thread is waiting and thus no thread is woken. When the parent runs again, it sleeps forever, which is sad.\n\n\nHopefully, from this simple join example, you can see some of the basic requirements of using condition variables properly. To make sure you understand, we now go through a more complicated example: the\n   **producer/consumer**\n   or\n   **bounded-buffer**\n   problem.\n\n\n**TIP: ALWAYS HOLD THE LOCK WHILE SIGNALING**\n\n\nAlthough it is strictly not necessary in all cases, it is likely simplest and best to hold the lock while signaling when using condition variables. The example above shows a case where you\n   *must*\n   hold the lock for correctness; however, there are some other cases where it is likely OK not to, but probably is something you should avoid. Thus, for simplicity,\n   **hold the lock when calling signal**\n   .\n\n\nThe converse of this tip, i.e., hold the lock when calling\n   \n    wait\n   \n   , is not just a tip, but rather mandated by the semantics of\n   \n    wait\n   \n   , because\n   \n    wait\n   \n   always (a) assumes the lock is held when you call it, (b) releases said lock when putting the caller to sleep, and (c) re-acquires the lock just before returning. Thus, the generalization of this tip is correct:\n   **hold the lock when calling signal or wait**\n   , and you will always be in good shape.\n\n\n1\n   \n   Note that this example is not “real” code, because the call to\n   \n    pthread_cond_wait()\n   \n   always requires a mutex as well as a condition variable; here, we just pretend that the interface does not do so for the sake of the negative example.\n\n\n1  int buffer;\n2  int count = 0; // initially, empty\n3\n4  void put(int value) {\n5      assert(count == 0);\n6      count = 1;\n7      buffer = value;\n8  }\n9\n10 int get() {\n11     assert(count == 1);\n12     count = 0;\n13     return buffer;\n14 }\nFigure 30.6:\n   **The Put And Get Routines (v1)**"
        },
        {
          "name": "The Producer/Consumer (Bounded Buffer) Problem",
          "content": "The next synchronization problem we will confront in this chapter is known as the\n   **producer/consumer**\n   problem, or sometimes as the\n   **bounded buffer**\n   problem, which was first posed by Dijkstra [D72]. Indeed, it was this very producer/consumer problem that led Dijkstra and his co-workers to invent the generalized semaphore (which can be used as either a lock or a condition variable) [D01]; we will learn more about semaphores later.\n\n\nImagine one or more producer threads and one or more consumer threads. Producers generate data items and place them in a buffer; consumers grab said items from the buffer and consume them in some way.\n\n\nThis arrangement occurs in many real systems. For example, in a multi-threaded web server, a producer puts HTTP requests into a work queue (i.e., the bounded buffer); consumer threads take requests out of this queue and process them.\n\n\nA bounded buffer is also used when you pipe the output of one program into another, e.g.,\n   \n    grep foo file.txt | wc -l\n   \n   . This example runs two processes concurrently;\n   \n    grep\n   \n   writes lines from\n   \n    file.txt\n   \n   with the string\n   \n    foo\n   \n   in them to what it thinks is standard output; the UNIX shell redirects the output to what is called a UNIX pipe (created by the\n   **pipe**\n   system call). The other end of this pipe is connected to the standard input of the process\n   \n    wc\n   \n   , which simply counts the number of lines in the input stream and prints out the result. Thus, the\n   \n    grep\n   \n   process is the producer; the\n   \n    wc\n   \n   process is the consumer; between them is an in-kernel bounded buffer; you, in this example, are just the happy user.\n\n\nBecause the bounded buffer is a shared resource, we must of course require synchronized access to it, lest\n   \n    2\n   \n   a race condition arise. To begin to understand this problem better, let us examine some actual code.\n\n\nThe first thing we need is a shared buffer, into which a producer puts data, and out of which a consumer takes data. Let's just use a single\n\n\n2\n   \n   This is where we drop some serious Old English on you, and the subjunctive form.\n\n\n1 void *producer(void *arg) {\n2     int i;\n3     int loops = (int) arg;\n4     for (i = 0; i < loops; i++) {\n5         put(i);\n6     }\n7 }\n8\n9 void *consumer(void *arg) {\n10    while (1) {\n11        int tmp = get();\n12        printf(\"%d\\n\", tmp);\n13    }\n14 }\nFigure 30.7:\n   **Producer/Consumer Threads (v1)**\n\n\ninteger for simplicity (you can certainly imagine placing a pointer to a data structure into this slot instead), and the two inner routines to put a value into the shared buffer, and to get a value out of the buffer. See Figure 30.6 (page 6) for details.\n\n\nPretty simple, no? The\n   \n    put()\n   \n   routine assumes the buffer is empty (and checks this with an assertion), and then simply puts a value into the shared buffer and marks it full by setting\n   \n    count\n   \n   to 1. The\n   \n    get()\n   \n   routine does the opposite, setting the buffer to empty (i.e., setting\n   \n    count\n   \n   to 0) and returning the value. Don't worry that this shared buffer has just a single entry; later, we'll generalize it to a queue that can hold multiple entries, which will be even more fun than it sounds.\n\n\nNow we need to write some routines that know when it is OK to access the buffer to either put data into it or get data out of it. The conditions for this should be obvious: only put data into the buffer when\n   \n    count\n   \n   is zero (i.e., when the buffer is empty), and only get data from the buffer when\n   \n    count\n   \n   is one (i.e., when the buffer is full). If we write the synchronization code such that a producer puts data into a full buffer, or a consumer gets data from an empty one, we have done something wrong (and in this code, an assertion will fire).\n\n\nThis work is going to be done by two types of threads, one set of which we'll call the\n   **producer**\n   threads, and the other set which we'll call\n   **consumer**\n   threads. Figure 30.7 shows the code for a producer that puts an integer into the shared buffer\n   \n    loops\n   \n   number of times, and a consumer that gets the data out of that shared buffer (forever), each time printing out the data item it pulled from the shared buffer.\n\n\n\n\n**A Broken Solution**\n\n\nNow imagine that we have just a single producer and a single consumer. Obviously the\n   \n    put()\n   \n   and\n   \n    get()\n   \n   routines have critical sections within them, as\n   \n    put()\n   \n   updates the buffer, and\n   \n    get()\n   \n   reads from it. However, putting a lock around the code doesn't work; we need something more.\n\n\n1  int loops; // must initialize somewhere...\n2  cond_t cond;\n3  mutex_t mutex;\n4\n5  void *producer(void *arg) {\n6      int i;\n7      for (i = 0; i < loops; i++) {\n8          Pthread_mutex_lock(&mutex);           // p1\n9          if (count == 1)                       // p2\n10         Pthread_cond_wait(&cond, &mutex);    // p3\n11         put(i);                               // p4\n12         Pthread_cond_signal(&cond);          // p5\n13         Pthread_mutex_unlock(&mutex);        // p6\n14     }\n15 }\n16\n17 void *consumer(void *arg) {\n18     int i;\n19     for (i = 0; i < loops; i++) {\n20         Pthread_mutex_lock(&mutex);          // c1\n21         if (count == 0)                       // c2\n22             Pthread_cond_wait(&cond, &mutex); // c3\n23         int tmp = get();                      // c4\n24         Pthread_cond_signal(&cond);          // c5\n25         Pthread_mutex_unlock(&mutex);        // c6\n26         printf(\"%d\\n\", tmp);\n27     }\n28 }\n\nFigure 30.8:\n   **Producer/Consumer: Single CV And If Statement**\n\n\nNot surprisingly, that something more is some condition variables. In this (broken) first try (Figure 30.8), we have a single condition variable\n   \n    cond\n   \n   and associated lock\n   \n    mutex\n   \n   .\n\n\nLet's examine the signaling logic between producers and consumers. When a producer wants to fill the buffer, it waits for it to be empty (p1–p3). The consumer has the exact same logic, but waits for a different condition: fullness (c1–c3).\n\n\nWith just a single producer and a single consumer, the code in Figure 30.8 works. However, if we have more than one of these threads (e.g., two consumers), the solution has two critical problems. What are they?\n\n\n... (\n   *pause here to think*\n   ) ...\n\n\nLet's understand the first problem, which has to do with the\n   \n    if\n   \n   statement before the wait. Assume there are two consumers (\n   \n    T_{c1}\n   \n   and\n   \n    T_{c2}\n   \n   ) and one producer (\n   \n    T_p\n   \n   ). First, a consumer (\n   \n    T_{c1}\n   \n   ) runs; it acquires the lock (c1), checks if any buffers are ready for consumption (c2), and finding that none are, waits (c3) (which releases the lock).\n\n\nThen the producer (\n   \n    T_p\n   \n   ) runs. It acquires the lock (p1), checks if all\n\n\n\nT_{c1} | State | T_{c2} | State | T_p | State | Count | Comment\nc1 | Run |  | Ready |  | Ready | 0 | \nc2 | Run |  | Ready |  | Ready | 0 | \nc3 | Sleep |  | Ready |  | Ready | 0 | Nothing to get\n | Sleep |  | Ready | p1 | Run | 0 | \n | Sleep |  | Ready | p2 | Run | 0 | \n | Sleep |  | Ready | p4 | Run | 1 | Buffer now full\n | Ready |  | Ready | p5 | Run | 1 | T_{c1}\n      \n      awoken\n | Ready |  | Ready | p6 | Run | 1 | \n | Ready |  | Ready | p1 | Run | 1 | \n | Ready |  | Ready | p2 | Run | 1 | \n | Ready |  | Ready | p3 | Sleep | 1 | Buffer full; sleep\n | Ready | c1 | Run |  | Sleep | 1 | T_{c2}\n      \n      sneaks in ...\n | Ready | c2 | Run |  | Sleep | 1 | \n | Ready | c4 | Run |  | Sleep | 0 | ... and grabs data\n | Ready | c5 | Run |  | Ready | 0 | T_p\n      \n      awoken\n | Ready | c6 | Run |  | Ready | 0 | \nc4 | Run |  | Ready |  | Ready | 0 | Oh oh! No data\n\n\n**Thread Trace: Broken Solution (v1)**\nbuffers are full (p2), and finding that not to be the case, goes ahead and fills the buffer (p4). The producer then signals that a buffer has been filled (p5). Critically, this moves the first consumer (\n   \n    T_{c1}\n   \n   ) from sleeping on a condition variable to the ready queue;\n   \n    T_{c1}\n   \n   is now able to run (but not yet running). The producer then continues until realizing the buffer is full, at which point it sleeps (p6, p1–p3).\n\n\nHere is where the problem occurs: another consumer (\n   \n    T_{c2}\n   \n   ) sneaks in and consumes the one existing value in the buffer (c1, c2, c4, c5, c6, skipping the wait at c3 because the buffer is full). Now assume\n   \n    T_{c1}\n   \n   runs; just before returning from the wait, it re-acquires the lock and then returns. It then calls\n   \n    get()\n   \n   (c4), but there are no buffers to consume! An assertion triggers, and the code has not functioned as desired. Clearly, we should have somehow prevented\n   \n    T_{c1}\n   \n   from trying to consume because\n   \n    T_{c2}\n   \n   snuck in and consumed the one value in the buffer that had been produced. Figure 30.9 shows the action each thread takes, as well as its scheduler state (Ready, Running, or Sleeping) over time.\n\n\nThe problem arises for a simple reason: after the producer woke\n   \n    T_{c1}\n   \n   , but\n   *before*\n\n    T_{c1}\n   \n   ever ran, the state of the bounded buffer changed (thanks to\n   \n    T_{c2}\n   \n   ). Signaling a thread only wakes them up; it is thus a\n   *hint*\n   that the state of the world has changed (in this case, that a value has been placed in the buffer), but there is no guarantee that when the woken thread runs, the state will\n   *still*\n   be as desired. This interpretation of what a signal means is often referred to as\n   **Mesa semantics**\n   , after the first research that built a condition variable in such a manner [LR80]; the contrast, referred to as\n\n\n1  int loops;\n2  cond_t cond;\n3  mutex_t mutex;\n4\n5  void *producer(void *arg) {\n6      int i;\n7      for (i = 0; i < loops; i++) {\n8          Pthread_mutex_lock(&mutex);           // p1\n9          while (count == 1)                   // p2\n10         Pthread_cond_wait(&cond, &mutex);    // p3\n11         put(i);                               // p4\n12         Pthread_cond_signal(&cond);          // p5\n13         Pthread_mutex_unlock(&mutex);        // p6\n14     }\n15 }\n16\n17 void *consumer(void *arg) {\n18     int i;\n19     for (i = 0; i < loops; i++) {\n20         Pthread_mutex_lock(&mutex);          // c1\n21         while (count == 0)                   // c2\n22             Pthread_cond_wait(&cond, &mutex); // c3\n23         int tmp = get();                     // c4\n24         Pthread_cond_signal(&cond);          // c5\n25         Pthread_mutex_unlock(&mutex);        // c6\n26         printf(\"%d\\n\", tmp);\n27     }\n28 }\n\nFigure 30.10:\n   **Producer/Consumer: Single CV And While**\n\n\n**Hoare semantics**\n   , is harder to build but provides a stronger guarantee that the woken thread will run immediately upon being woken [H74]. Virtually every system ever built employs Mesa semantics.\n\n\n\n\n**Better, But Still Broken: While, Not If**\n\n\nFortunately, this fix is easy (Figure 30.10): change the\n   \n    if\n   \n   to a\n   \n    while\n   \n   . Think about why this works; now consumer\n   \n    T_{c1}\n   \n   wakes up and (with the lock held) immediately re-checks the state of the shared variable (c2). If the buffer is empty at that point, the consumer simply goes back to sleep (c3). The corollary\n   \n    if\n   \n   is also changed to a\n   \n    while\n   \n   in the producer (p2).\n\n\nThanks to Mesa semantics, a simple rule to remember with condition variables is to\n   **always use while loops**\n   . Sometimes you don't have to re-check the condition, but it is always safe to do so; just do it and be happy.\n\n\nHowever, this code still has a bug, the second of two problems mentioned above. Can you see it? It has something to do with the fact that there is only one condition variable. Try to figure out what the problem is, before reading ahead. DO IT! (\n   *pause for you to think, or close your eyes...*\n   )\n\n\n\nT_{c1} | State | T_{c2} | State | T_p | State | Count | Comment\nc1 | Run |  | Ready |  | Ready | 0 | \nc2 | Run |  | Ready |  | Ready | 0 | \nc3 | Sleep |  | Ready |  | Ready | 0 | Nothing to get\n | Sleep | c1 | Run |  | Ready | 0 | \n | Sleep | c2 | Run |  | Ready | 0 | \n | Sleep | c3 | Sleep |  | Ready | 0 | Nothing to get\n | Sleep |  | Sleep | p1 | Run | 0 | \n | Sleep |  | Sleep | p2 | Run | 0 | \n | Sleep |  | Sleep | p4 | Run | 1 | Buffer now full\n | Sleep |  | Sleep | p5 | Run | 1 | T_{c1}\n      \n      awoken\n | Ready |  | Sleep | p6 | Run | 1 | \n | Ready |  | Sleep | p1 | Run | 1 | \n | Ready |  | Sleep | p2 | Run | 1 | \n | Ready |  | Sleep | p3 | Sleep | 1 | Must sleep (full)\nc2 | Run |  | Sleep |  | Sleep | 1 | Recheck condition\nc4 | Run |  | Sleep |  | Sleep | 0 | T_{c1}\n      \n      grabs data\nc5 | Run |  | Ready |  | Sleep | 0 | Oops! Woke\n      \n       T_{c2}\nc6 | Run |  | Ready |  | Sleep | 0 | \nc1 | Run |  | Ready |  | Sleep | 0 | \nc2 | Run |  | Ready |  | Sleep | 0 | \nc3 | Sleep |  | Ready |  | Sleep | 0 | Nothing to get\n | Sleep | c2 | Run |  | Sleep | 0 | \n | Sleep | c3 | Sleep |  | Sleep | 0 | Everyone asleep...\n\n\n**Thread Trace: Broken Solution (v2)**\nLet's confirm you figured it out correctly, or perhaps let's confirm that you are now awake and reading this part of the book. The problem occurs when two consumers run first (\n   \n    T_{c1}\n   \n   and\n   \n    T_{c2}\n   \n   ) and both go to sleep (c3). Then, the producer runs, puts a value in the buffer, and wakes one of the consumers (say\n   \n    T_{c1}\n   \n   ). The producer then loops back (releasing and reacquiring the lock along the way) and tries to put more data in the buffer; because the buffer is full, the producer instead waits on the condition (thus sleeping). Now, one consumer is ready to run (\n   \n    T_{c1}\n   \n   ), and two threads are sleeping on a condition (\n   \n    T_{c2}\n   \n   and\n   \n    T_p\n   \n   ). We are about to cause a problem: things are getting exciting!\n\n\nThe consumer\n   \n    T_{c1}\n   \n   then wakes by returning from\n   \n    wait()\n   \n   (c3), re-checks the condition (c2), and finding the buffer full, consumes the value (c4). This consumer then, critically, signals on the condition (c5), waking\n   *only one*\n   thread that is sleeping. However, which thread should it wake?\n\n\nBecause the consumer has emptied the buffer, it clearly should wake the producer. However, if it wakes the consumer\n   \n    T_{c2}\n   \n   (which is definitely possible, depending on how the wait queue is managed), we have a problem. Specifically, the consumer\n   \n    T_{c2}\n   \n   will wake up and find the buffer empty (c2), and go back to sleep (c3). The producer\n   \n    T_p\n   \n   , which has a value\n\n\n1  cond_t  empty, fill;\n2  mutex_t mutex;\n3\n4  void *producer(void *arg) {\n5      int i;\n6      for (i = 0; i < loops; i++) {\n7          Pthread_mutex_lock(&mutex);\n8          while (count == 1)\n9              Pthread_cond_wait(&empty, &mutex);\n10         put(i);\n11         Pthread_cond_signal(&fill);\n12         Pthread_mutex_unlock(&mutex);\n13     }\n14 }\n15\n16 void *consumer(void *arg) {\n17     int i;\n18     for (i = 0; i < loops; i++) {\n19         Pthread_mutex_lock(&mutex);\n20         while (count == 0)\n21             Pthread_cond_wait(&fill, &mutex);\n22         int tmp = get();\n23         Pthread_cond_signal(&empty);\n24         Pthread_mutex_unlock(&mutex);\n25         printf(\"%d\\n\", tmp);\n26     }\n27 }\n\nFigure 30.12:\n   **Producer/Consumer: Two CVs And While**\n\n\nto put into the buffer, is left sleeping. The other consumer thread,\n   \n    T_{c1}\n   \n   , also goes back to sleep. All three threads are left sleeping, a clear bug; see Figure 30.11 for the brutal step-by-step of this terrible calamity.\n\n\nSignaling is clearly needed, but must be more directed. A consumer should not wake other consumers, only producers, and vice-versa.\n\n\n\n\n**The Single Buffer Producer/Consumer Solution**\n\n\nThe solution here is once again a small one: use\n   *two*\n   condition variables, instead of one, in order to properly signal which type of thread should wake up when the state of the system changes. Figure 30.12 shows the resulting code.\n\n\nIn the code, producer threads wait on the condition\n   **empty**\n   , and signals\n   **fill**\n   . Conversely, consumer threads wait on\n   **fill**\n   and signal\n   **empty**\n   . By doing so, the second problem above is avoided by design: a consumer can never accidentally wake a consumer, and a producer can never accidentally wake a producer.\n\n\n1  int buffer[MAX];\n2  int fill_ptr = 0;\n3  int use_ptr  = 0;\n4  int count     = 0;\n5\n6  void put(int value) {\n7      buffer[fill_ptr] = value;\n8      fill_ptr = (fill_ptr + 1) % MAX;\n9      count++;\n10 }\n11\n12 int get() {\n13     int tmp = buffer[use_ptr];\n14     use_ptr = (use_ptr + 1) % MAX;\n15     count--;\n16     return tmp;\n17 }\n\nFigure 30.13: The Correct Put And Get Routines\n\n\n1  cond_t empty, fill;\n2  mutex_t mutex;\n3\n4  void *producer(void *arg) {\n5      int i;\n6      for (i = 0; i < loops; i++) {\n7          Pthread_mutex_lock(&mutex);           // p1\n8          while (count == MAX)                 // p2\n9              Pthread_cond_wait(&empty, &mutex); // p3\n10         put(i);                               // p4\n11         Pthread_cond_signal(&fill);          // p5\n12         Pthread_mutex_unlock(&mutex);        // p6\n13     }\n14 }\n15\n16 void *consumer(void *arg) {\n17     int i;\n18     for (i = 0; i < loops; i++) {\n19         Pthread_mutex_lock(&mutex);          // c1\n20         while (count == 0)                   // c2\n21             Pthread_cond_wait(&fill, &mutex); // c3\n22         int tmp = get();                     // c4\n23         Pthread_cond_signal(&empty);         // c5\n24         Pthread_mutex_unlock(&mutex);        // c6\n25         printf(\"%d\\n\", tmp);\n26     }\n27 }\n\nFigure 30.14: The Correct Producer/Consumer Synchronization\n\n\n**TIP: USE WHILE (NOT IF) FOR CONDITIONS**\nWhen checking for a condition in a multi-threaded program, using a\n   \n    while\n   \n   loop is always correct; using an\n   \n    if\n   \n   statement only might be, depending on the semantics of signaling. Thus, always use\n   \n    while\n   \n   and your code will behave as expected.\n\n\nUsing\n   \n    while\n   \n   loops around conditional checks also handles the case where\n   **spurious wakeups**\n   occur. In some thread packages, due to details of the implementation, it is possible that two threads get woken up though just a single signal has taken place [L11]. Spurious wakeups are further reason to re-check the condition a thread is waiting on.\n\n\n\n\n**The Correct Producer/Consumer Solution**\n\n\nWe now have a working producer/consumer solution, albeit not a fully general one. The last change we make is to enable more concurrency and efficiency; specifically, we add more buffer slots, so that multiple values can be produced before sleeping, and similarly multiple values can be consumed before sleeping. With just a single producer and consumer, this approach is more efficient as it reduces context switches; with multiple producers or consumers (or both), it even allows concurrent producing or consuming to take place, thus increasing concurrency. Fortunately, it is a small change from our current solution.\n\n\nThe first change for this correct solution is within the buffer structure itself and the corresponding\n   \n    put()\n   \n   and\n   \n    get()\n   \n   (Figure 30.13). We also slightly change the conditions that producers and consumers check in order to determine whether to sleep or not. We also show the correct waiting and signaling logic (Figure 30.14). A producer only sleeps if all buffers are currently filled (\n   \n    p2\n   \n   ); similarly, a consumer only sleeps if all buffers are currently empty (\n   \n    c2\n   \n   ). And thus we solve the producer/consumer problem; time to sit back and drink a cold one."
        },
        {
          "name": "Covering Conditions",
          "content": "We'll now look at one more example of how condition variables can be used. This code study is drawn from Lampson and Redell's paper on Pilot [LR80], the same group who first implemented the\n   **Mesa semantics**\n   described above (the language they used was Mesa, hence the name).\n\n\nThe problem they ran into is best shown via simple example, in this case in a simple multi-threaded memory allocation library. Figure 30.15 shows a code snippet which demonstrates the issue.\n\n\nAs you might see in the code, when a thread calls into the memory allocation code, it might have to wait in order for more memory to become free. Conversely, when a thread frees memory, it signals that more memory is free. However, our code above has a problem: which waiting thread (there can be more than one) should be woken up?\n\n\n1 // how many bytes of the heap are free?\n2 int bytesLeft = MAX_HEAP_SIZE;\n3\n4 // need lock and condition too\n5 cond_t c;\n6 mutex_t m;\n7\n8 void *\n9 allocate(int size) {\n10     Pthread_mutex_lock(&m);\n11     while (bytesLeft < size)\n12         Pthread_cond_wait(&c, &m);\n13     void *ptr = ...; // get mem from heap\n14     bytesLeft -= size;\n15     Pthread_mutex_unlock(&m);\n16     return ptr;\n17 }\n18\n19 void free(void *ptr, int size) {\n20     Pthread_mutex_lock(&m);\n21     bytesLeft += size;\n22     Pthread_cond_signal(&c); // whom to signal??\n23     Pthread_mutex_unlock(&m);\n24 }\nFigure 30.15:\n   **Covering Conditions: An Example**\n\n\nConsider the following scenario. Assume there are zero bytes free; thread\n   \n    T_a\n   \n   calls\n   \n    allocate(100)\n   \n   , followed by thread\n   \n    T_b\n   \n   , which asks for less memory by calling\n   \n    allocate(10)\n   \n   . Both\n   \n    T_a\n   \n   and\n   \n    T_b\n   \n   thus wait on the condition and go to sleep; there aren't enough free bytes to satisfy either of these requests.\n\n\nAt that point, assume a third thread,\n   \n    T_c\n   \n   , calls\n   \n    free(50)\n   \n   . Unfortunately, when it calls\n   \n    signal\n   \n   to wake a waiting thread, it might not wake the correct waiting thread,\n   \n    T_b\n   \n   , which is waiting for only 10 bytes to be freed;\n   \n    T_a\n   \n   should remain waiting, as not enough memory is yet free. Thus, the code in the figure does not work, as the thread waking other threads does not know which thread (or threads) to wake up.\n\n\nThe solution suggested by Lampson and Redell is straightforward: replace the\n   \n    Pthread_cond_signal()\n   \n   call in the code above with a call to\n   \n    Pthread_cond_broadcast()\n   \n   , which wakes up\n   *all*\n   waiting threads. By doing so, we guarantee that any threads that should be woken are. The downside, of course, can be a negative performance impact, as we might needlessly wake up many other waiting threads that shouldn't (yet) be awake. Those threads will simply wake up, re-check the condition, and then go immediately back to sleep.\n\n\nLampson and Redell call such a condition a\n   **covering condition**\n   , as it covers all the cases where a thread needs to wake up (conservatively); the cost, as we've discussed, is that too many threads might be woken.\n\n\nThe astute reader might also have noticed we could have used this approach earlier (see the producer/consumer problem with only a single condition variable). However, in that case, a better solution was available to us, and thus we used it. In general, if you find that your program only works when you change your signals to broadcasts (but you don't think it should need to), you probably have a bug; fix it! But in cases like the memory allocator above, broadcast may be the most straightforward solution available."
        }
      ]
    },
    {
      "name": "Semaphores",
      "sections": [
        {
          "name": "Semaphores: A Definition",
          "content": "A semaphore is an object with an integer value that we can manipulate with two routines; in the POSIX standard, these routines are\n   \n    sem_wait()\n   \n   and\n   \n    sem_post()\n   \n\n    1\n   \n   . Because the initial value of the semaphore determines its behavior, before calling any other routine to interact with the semaphore, we must first initialize it to some value, as the code in Figure 31.1 does.\n\n\n1\n   \n   Historically,\n   \n    sem_wait()\n   \n   was called\n   \n    P()\n   \n   by Dijkstra and\n   \n    sem_post()\n   \n   called\n   \n    V()\n   \n   . These shortened forms come from Dutch words; interestingly,\n   *which*\n   Dutch words they supposedly derive from has changed over time. Originally,\n   \n    P()\n   \n   came from “passering” (to pass) and\n   \n    V()\n   \n   from “vrijgave” (release); later, Dijkstra wrote\n   \n    P()\n   \n   was from “prolaag”, a contraction of “probeer” (Dutch for “try”) and “verlaag” (“decrease”), and\n   \n    V()\n   \n   from “verhoog” which means “increase”. Sometimes, people call them down and up. Use the Dutch versions to impress your friends, or confuse them, or both. See\n   https://news.ycombinator.com/item?id=8761539\n   for details.\n\n\n1 #include <semaphore.h>\n2 sem_t s;\n3 sem_init(&s, 0, 1);\nFigure 31.1:\n   **Initializing A Semaphore**\n\n\nIn the figure, we declare a semaphore\n   \n    s\n   \n   and initialize it to the value 1 by passing 1 in as the third argument. The second argument to\n   \n    sem_init()\n   \n   will be set to 0 in all of the examples we'll see; this indicates that the semaphore is shared between threads in the same process. See the man page for details on other usages of semaphores (namely, how they can be used to synchronize access across\n   *different*\n   processes), which require a different value for that second argument.\n\n\nAfter a semaphore is initialized, we can call one of two functions to interact with it,\n   \n    sem_wait()\n   \n   or\n   \n    sem_post()\n   \n   . The behavior of these two functions is seen in Figure 31.2.\n\n\nFor now, we are not concerned with the implementation of these routines, which clearly requires some care; with multiple threads calling into\n   \n    sem_wait()\n   \n   and\n   \n    sem_post()\n   \n   , there is the obvious need for managing these critical sections. We will now focus on how to\n   *use*\n   these primitives; later we may discuss how they are built.\n\n\nWe should discuss a few salient aspects of the interfaces here. First, we can see that\n   \n    sem_wait()\n   \n   will either return right away (because the value of the semaphore was one or higher when we called\n   \n    sem_wait()\n   \n   ), or it will cause the caller to suspend execution waiting for a subsequent post. Of course, multiple calling threads may call into\n   \n    sem_wait()\n   \n   , and thus all be queued waiting to be woken.\n\n\nSecond, we can see that\n   \n    sem_post()\n   \n   does not wait for some particular condition to hold like\n   \n    sem_wait()\n   \n   does. Rather, it simply increments the value of the semaphore and then, if there is a thread waiting to be woken, wakes one of them up.\n\n\nThird, the value of the semaphore, when negative, is equal to the number of waiting threads [D68b]. Though the value generally isn't seen by users of the semaphores, this invariant is worth knowing and perhaps can help you remember how a semaphore functions.\n\n\n1 int sem_wait(sem_t *s) {\n2     decrement the value of semaphore s by one\n3     wait if value of semaphore s is negative\n4 }\n5\n6 int sem_post(sem_t *s) {\n7     increment the value of semaphore s by one\n8     if there are one or more threads waiting, wake one\n9 }\nFigure 31.2:\n   **Semaphore: Definitions Of Wait And Post**\n\n\n1 sem_t m;\n2 sem_init(&m, 0, X); // init to X; what should X be?\n3\n4 sem_wait(&m);\n5 // critical section here\n6 sem_post(&m);\n\nFigure 31.3:\n   **A Binary Semaphore (That Is, A Lock)**\n\n\nDon't worry (yet) about the seeming race conditions possible within the semaphore; assume that the actions they make are performed atomically. We will soon use locks and condition variables to do just this."
        },
        {
          "name": "Binary Semaphores (Locks)We",
          "content": "We are now ready to use a semaphore. Our first use will be one with which we are already familiar: using a semaphore as a lock. See Figure 31.3 for a code snippet; therein, you'll see that we simply surround the critical section of interest with a\n   \n    sem_wait()/sem_post()\n   \n   pair. Critical to making this work, though, is the initial value of the semaphore\n   \n    m\n   \n   (initialized to\n   \n    X\n   \n   in the figure). What should\n   \n    X\n   \n   be?\n\n\n... (\n   *Try thinking about it before going on*\n   ) ...\n\n\nLooking back at definition of the\n   \n    sem_wait()\n   \n   and\n   \n    sem_post()\n   \n   routines above, we can see that the initial value should be 1.\n\n\nTo make this clear, let's imagine a scenario with two threads. The first thread (Thread 0) calls\n   \n    sem_wait()\n   \n   ; it will first decrement the value of the semaphore, changing it to 0. Then, it will wait only if the value is\n   *not*\n   greater than or equal to 0. Because the value is 0,\n   \n    sem_wait()\n   \n   will simply return and the calling thread will continue; Thread 0 is now free to enter the critical section. If no other thread tries to acquire the lock while Thread 0 is inside the critical section, when it calls\n   \n    sem_post()\n   \n   , it will simply restore the value of the semaphore to 1 (and not wake a waiting thread, because there are none). Figure 31.4 shows a trace of this scenario.\n\n\nA more interesting case arises when Thread 0 \"holds the lock\" (i.e., it has called\n   \n    sem_wait()\n   \n   but not yet called\n   \n    sem_post()\n   \n   ), and another thread (Thread 1) tries to enter the critical section by calling\n   \n    sem_wait()\n   \n   . In this case, Thread 1 will decrement the value of the semaphore to -1, and\n\n\n\nValue of Semaphore | Thread 0 | Thread 1\n1 |  | \n1 | call\n      \n       sem_wait() | \n0 | sem_wait()\n      \n      returns | \n0 | (crit sect) | \n0 | call\n      \n       sem_post() | \n1 | sem_post()\n      \n      returns | \n\n\nFigure 31.4:\n   **Thread Trace: Single Thread Using A Semaphore**\n\n\n\nVal | Thread 0 | State | Thread 1 | State\n1 |  | Run |  | Ready\n1 | call sem.wait() | Run |  | Ready\n0 | sem.wait() returns | Run |  | Ready\n0 | (crit sect begin) | Run |  | Ready\n0 | Interrupt; Switch→T1 | Ready |  | Run\n0 |  | Ready | call sem.wait() | Run\n-1 |  | Ready | decr sem | Run\n-1 |  | Ready | (sem<0)→sleep | Sleep\n-1 |  | Ready | Switch→T0 | Sleep\n-1 |  | Run |  | Sleep\n-1 | (crit sect end) | Run |  | Sleep\n-1 | call sem.post() | Run |  | Sleep\n0 | incr sem | Run |  | Sleep\n0 | wake(T1) | Run |  | Ready\n0 | sem.post() returns | Run |  | Ready\n0 | Interrupt; Switch→T1 | Ready |  | Run\n0 |  | Ready | sem.wait() returns | Run\n0 |  | Ready | (crit sect) | Run\n0 |  | Ready | call sem.post() | Run\n1 |  | Ready | sem.post() returns | Run\n\n\nFigure 31.5:\n   **Thread Trace: Two Threads Using A Semaphore**\n\n\nthus wait (putting itself to sleep and relinquishing the processor). When Thread 0 runs again, it will eventually call\n   \n    sem.post()\n   \n   , incrementing the value of the semaphore back to zero, and then wake the waiting thread (Thread 1), which will then be able to acquire the lock for itself. When Thread 1 finishes, it will again increment the value of the semaphore, restoring it to 1 again.\n\n\nFigure 31.5 shows a trace of this example. In addition to thread actions, the figure shows the\n   **scheduler state**\n   of each thread: Run (the thread is running), Ready (i.e., runnable but not running), and Sleep (the thread is blocked). Note that Thread 1 goes into the sleeping state when it tries to acquire the already-held lock; only when Thread 0 runs again can Thread 1 be awoken and potentially run again.\n\n\nIf you want to work through your own example, try a scenario where multiple threads queue up waiting for a lock. What would the value of the semaphore be during such a trace?\n\n\nThus we are able to use semaphores as locks. Because locks only have two states (held and not held), we sometimes call a semaphore used as a lock a\n   **binary semaphore**\n   . Note that if you are using a semaphore only in this binary fashion, it could be implemented in a simpler manner than the generalized semaphores we present here."
        },
        {
          "name": "Semaphores For Ordering",
          "content": "Semaphores are also useful to order events in a concurrent program. For example, a thread may wish to wait for a list to become non-empty,\n\n\n1 sem_t s;\n2\n3 void *child(void *arg) {\n4     printf(\"child\\n\");\n5     sem_post(&s); // signal here: child is done\n6     return NULL;\n7 }\n8\n9 int main(int argc, char *argv[]) {\n10    sem_init(&s, 0, X); // what should X be?\n11    printf(\"parent: begin\\n\");\n12    pthread_t c;\n13    Pthread_create(&c, NULL, child, NULL);\n14    sem_wait(&s); // wait here for child\n15    printf(\"parent: end\\n\");\n16    return 0;\n17 }\n\nFigure 31.6: A Parent Waiting For Its Child\n\n\nso it can delete an element from it. In this pattern of usage, we often find one thread\n   *waiting*\n   for something to happen, and another thread making that something happen and then\n   *signaling*\n   that it has happened, thus waking the waiting thread. We are thus using the semaphore as an\n   **ordering**\n   primitive (similar to our use of\n   **condition variables**\n   earlier).\n\n\nA simple example is as follows. Imagine a thread creates another thread and then wants to wait for it to complete its execution (Figure 31.6). When this program runs, we would like to see the following:\n\n\nparent: begin\nchild\nparent: end\n\nThe question, then, is how to use a semaphore to achieve this effect; as it turns out, the answer is relatively easy to understand. As you can see in the code, the parent simply calls\n   \n    sem_wait()\n   \n   and the child\n   \n    sem_post()\n   \n   to wait for the condition of the child finishing its execution to become true. However, this raises the question: what should the initial value of this semaphore be?\n\n\n*(Again, think about it here, instead of reading ahead)*\n\n\nThe answer, of course, is that the value of the semaphore should be set to is 0. There are two cases to consider. First, let us assume that the parent creates the child but the child has not run yet (i.e., it is sitting in a ready queue but not running). In this case (Figure 31.7, page 6), the parent will call\n   \n    sem_wait()\n   \n   before the child has called\n   \n    sem_post()\n   \n   ; we'd like the parent to wait for the child to run. The only way this will happen is if the value of the semaphore is not greater than 0; hence, 0 is the initial value. The parent runs, decrements the semaphore (to -1), then waits (sleeping). When the child finally runs, it will call\n   \n    sem_post()\n   \n   , increment the value\n\n\n\nVal | Parent | State | Child | State\n0 | create(Child) | Run | (Child exists, can run) | Ready\n0 | call sem.wait() | Run |  | Ready\n-1 | decr sem | Run |  | Ready\n-1 | (sem<0)→sleep | Sleep |  | Ready\n-1 | Switch\n      \n      →Child | Sleep | child runs | Run\n-1 |  | Sleep | call sem.post() | Run\n0 |  | Sleep | inc sem | Run\n0 |  | Ready | wake(Parent) | Run\n0 |  | Ready | sem.post() returns | Run\n0 |  | Ready | Interrupt\n      \n      →Parent | Ready\n0 | sem.wait() returns | Run |  | Ready\n\n\nFigure 31.7: Thread Trace: Parent Waiting For Child (Case 1)\n\n\n\nVal | Parent | State | Child | State\n0 | create(Child) | Run | (Child exists; can run) | Ready\n0 | Interrupt\n      \n      →Child | Ready | child runs | Run\n0 |  | Ready | call sem.post() | Run\n1 |  | Ready | inc sem | Run\n1 |  | Ready | wake(nobody) | Run\n1 |  | Ready | sem.post() returns | Run\n1 | parent runs | Run | Interrupt\n      \n      →Parent | Ready\n1 | call sem.wait() | Run |  | Ready\n0 | decrement sem | Run |  | Ready\n0 | (sem≥0)→awake | Run |  | Ready\n0 | sem.wait() returns | Run |  | Ready\n\n\nFigure 31.8: Thread Trace: Parent Waiting For Child (Case 2)\n\n\nof the semaphore to 0, and wake the parent, which will then return from\n   \n    sem.wait()\n   \n   and finish the program.\n\n\nThe second case (Figure 31.8) occurs when the child runs to completion before the parent gets a chance to call\n   \n    sem.wait()\n   \n   . In this case, the child will first call\n   \n    sem.post()\n   \n   , thus incrementing the value of the semaphore from 0 to 1. When the parent then gets a chance to run, it will call\n   \n    sem.wait()\n   \n   and find the value of the semaphore to be 1; the parent will thus decrement the value (to 0) and return from\n   \n    sem.wait()\n   \n   without waiting, also achieving the desired effect."
        },
        {
          "name": "The Producer /Consumer (Bounded Buffer) Problem",
          "content": "The next problem we will confront in this chapter is known as the\n   **producer/consumer**\n   problem, or sometimes as the\n   **bounded buffer**\n   problem [D72]. This problem is described in detail in the previous chapter on condition variables; see there for details.\n\n\n**ASIDE: SETTING THE VALUE OF A SEMAPHORE**\nWe've now seen two examples of initializing a semaphore. In the first case, we set the value to 1 to use the semaphore as a lock; in the second, to 0, to use the semaphore for ordering. So what's the general rule for semaphore initialization?\n\n\nOne simple way to think about it, thanks to Perry Kivolowitz, is to consider the number of resources you are willing to give away immediately after initialization. With the lock, it was 1, because you are willing to have the lock locked (given away) immediately after initialization. With the ordering case, it was 0, because there is nothing to give away at the start; only when the child thread is done is the resource created, at which point, the value is incremented to 1. Try this line of thinking on future semaphore problems, and see if it helps.\n\n\n**First Attempt**\nOur first attempt at solving the problem introduces two semaphores,\n   \n    empty\n   \n   and\n   \n    full\n   \n   , which the threads will use to indicate when a buffer entry has been emptied or filled, respectively. The code for the\n   \n    put\n   \n   and\n   \n    get\n   \n   routines is in Figure 31.9, and our attempt at solving the producer and consumer problem is in Figure 31.10 (page 8).\n\n\nIn this example, the producer first waits for a buffer to become empty in order to put data into it, and the consumer similarly waits for a buffer to become filled before using it. Let us first imagine that\n   \n    MAX=1\n   \n   (there is only one buffer in the array), and see if this works.\n\n\nImagine again there are two threads, a producer and a consumer. Let us examine a specific scenario on a single CPU. Assume the consumer gets to run first. Thus, the consumer will hit Line C1 in Figure 31.10, calling\n   \n    sem_wait(&full)\n   \n   . Because\n   \n    full\n   \n   was initialized to the value 0,\n\n\n1 int buffer[MAX];\n2 int fill = 0;\n3 int use  = 0;\n4\n5 void put(int value) {\n6     buffer[fill] = value;    // Line F1\n7     fill = (fill + 1) % MAX; // Line F2\n8 }\n9\n10 int get() {\n11     int tmp = buffer[use];   // Line G1\n12     use = (use + 1) % MAX;   // Line G2\n13     return tmp;\n14 }\nFigure 31.9: The Put And Get Routines\n\n\n1  sem_t empty;\n2  sem_t full;\n3\n4  void *producer(void *arg) {\n5      int i;\n6      for (i = 0; i < loops; i++) {\n7          sem_wait(&empty);      // Line P1\n8          put(i);                // Line P2\n9          sem_post(&full);       // Line P3\n10     }\n11 }\n12\n13 void *consumer(void *arg) {\n14     int tmp = 0;\n15     while (tmp != -1) {\n16         sem_wait(&full);        // Line C1\n17         tmp = get();            // Line C2\n18         sem_post(&empty);       // Line C3\n19         printf(\"%d\\n\", tmp);\n20     }\n21 }\n22\n23 int main(int argc, char *argv[]) {\n24     // ...\n25     sem_init(&empty, 0, MAX);   // MAX are empty\n26     sem_init(&full, 0, 0);      // 0 are full\n27     // ...\n28 }\n\nFigure 31.10:\n   **Adding The Full And Empty Conditions**\n\n\nthe call will decrement\n   \n    full\n   \n   (to -1), block the consumer, and wait for another thread to call\n   \n    sem_post()\n   \n   on\n   \n    full\n   \n   , as desired.\n\n\nAssume the producer then runs. It will hit Line P1, thus calling the\n   \n    sem_wait(&empty)\n   \n   routine. Unlike the consumer, the producer will continue through this line, because\n   \n    empty\n   \n   was initialized to the value\n   \n    MAX\n   \n   (in this case, 1). Thus,\n   \n    empty\n   \n   will be decremented to 0 and the producer will put a data value into the first entry of buffer (Line P2). The producer will then continue on to P3 and call\n   \n    sem_post(&full)\n   \n   , changing the value of the\n   \n    full\n   \n   semaphore from -1 to 0 and waking the consumer (e.g., move it from blocked to ready).\n\n\nIn this case, one of two things could happen. If the producer continues to run, it will loop around and hit Line P1 again. This time, however, it would block, as the\n   \n    empty\n   \n   semaphore's value is 0. If the producer instead was interrupted and the consumer began to run, it would return from\n   \n    sem_wait(&full)\n   \n   (Line C1), find that the buffer was full, and consume it. In either case, we achieve the desired behavior.\n\n\nYou can try this same example with more threads (e.g., multiple producers, and multiple consumers). It should still work.\n\n\n1 void *producer(void *arg) {\n2     int i;\n3     for (i = 0; i < loops; i++) {\n4         sem_wait(&mutex);           // Line P0 (NEW LINE)\n5         sem_wait(&empty);          // Line P1\n6         put(i);                     // Line P2\n7         sem_post(&full);           // Line P3\n8         sem_post(&mutex);          // Line P4 (NEW LINE)\n9     }\n10 }\n11\n12 void *consumer(void *arg) {\n13     int i;\n14     for (i = 0; i < loops; i++) {\n15         sem_wait(&mutex);           // Line C0 (NEW LINE)\n16         sem_wait(&full);           // Line C1\n17         int tmp = get();           // Line C2\n18         sem_post(&empty);          // Line C3\n19         sem_post(&mutex);          // Line C4 (NEW LINE)\n20         printf(\"%d\\n\", tmp);\n21     }\n22 }\n\nFigure 31.11:\n   **Adding Mutual Exclusion (Incorrectly)**\n\n\nLet us now imagine that\n   \n    MAX\n   \n   is greater than 1 (say\n   \n    MAX=10\n   \n   ). For this example, let us assume that there are multiple producers and multiple consumers. We now have a problem: a race condition. Do you see where it occurs? (take some time and look for it) If you can't see it, here's a hint: look more closely at the\n   \n    put()\n   \n   and\n   \n    get()\n   \n   code.\n\n\nOK, let's understand the issue. Imagine two producers (\n   \n    Pa\n   \n   and\n   \n    Pb\n   \n   ) both calling\n   \n    put()\n   \n   at roughly the same time. Assume producer\n   \n    Pa\n   \n   gets to run first, and just starts to fill the first buffer entry (\n   \n    fill=0\n   \n   at Line F1). Before\n   \n    Pa\n   \n   gets a chance to increment the\n   \n    fill\n   \n   counter to 1, it is interrupted. Producer\n   \n    Pb\n   \n   starts to run, and at Line F1 it also puts its data into the 0th element of buffer, which means that the old data there is overwritten! This action is a no-no; we don't want any data from the producer to be lost.\n\n\n\n\n**A Solution: Adding Mutual Exclusion**\n\n\nAs you can see, what we've forgotten here is\n   *mutual exclusion*\n   . The filling of a buffer and incrementing of the index into the buffer is a critical section, and thus must be guarded carefully. So let's use our friend the binary semaphore and add some locks. Figure 31.11 shows our attempt.\n\n\nNow we've added some locks around the entire\n   \n    put()/get()\n   \n   parts of the code, as indicated by the\n   \n    NEW LINE\n   \n   comments. That seems like the right idea, but it also doesn't work. Why? Deadlock. Why does deadlock occur? Take a moment to consider it; try to find a case where deadlock arises. What sequence of steps must happen for the program to deadlock?\n\n\n1 void *producer(void *arg) {\n2     int i;\n3     for (i = 0; i < loops; i++) {\n4         sem_wait(&empty);           // Line P1\n5         sem_wait(&mutex);          // Line P1.5 (lock)\n6         put(i);                    // Line P2\n7         sem_post(&mutex);          // Line P2.5 (unlock)\n8         sem_post(&full);           // Line P3\n9     }\n10 }\n11\n12 void *consumer(void *arg) {\n13     int i;\n14     for (i = 0; i < loops; i++) {\n15         sem_wait(&full);           // Line C1\n16         sem_wait(&mutex);          // Line C1.5 (lock)\n17         int tmp = get();          // Line C2\n18         sem_post(&mutex);          // Line C2.5 (unlock)\n19         sem_post(&empty);          // Line C3\n20         printf(\"%d\\n\", tmp);\n21     }\n22 }\nFigure 31.12:\n   **Adding Mutual Exclusion (Correctly)**\n\n\n\n\n**Avoiding Deadlock**\n\n\nOK, now that you figured it out, here is the answer. Imagine two threads, one producer and one consumer. The consumer gets to run first. It acquires the mutex (Line C0), and then calls\n   \n    sem_wait()\n   \n   on the full semaphore (Line C1); because there is no data yet, this call causes the consumer to block and thus yield the CPU; importantly, though, the consumer still holds the lock.\n\n\nA producer then runs. It has data to produce and if it were able to run, it would be able to wake the consumer thread and all would be good. Unfortunately, the first thing it does is call\n   \n    sem_wait()\n   \n   on the binary mutex semaphore (Line P0). The lock is already held. Hence, the producer is now stuck waiting too.\n\n\nThere is a simple cycle here. The consumer\n   *holds*\n   the mutex and is\n   *waiting*\n   for someone to signal full. The producer could\n   *signal*\n   full but is\n   *waiting*\n   for the mutex. Thus, the producer and consumer are each stuck waiting for each other: a classic deadlock.\n\n\n\n\n**At Last, A Working Solution**\n\n\nTo solve this problem, we simply must reduce the scope of the lock. Figure 31.12 (page 10) shows the correct solution. As you can see, we simply move the mutex acquire and release to be just around the critical section;\n\n\nthe full and empty wait and signal code is left outside\n   \n    2\n   \n   . The result is a simple and working bounded buffer, a commonly-used pattern in multi-threaded programs. Understand it now; use it later. You will thank us for years to come. Or at least, you will thank us when the same question is asked on the final exam, or during a job interview."
        },
        {
          "name": "Reader-Writer Locks",
          "content": "Another classic problem stems from the desire for a more flexible locking primitive that admits that different data structure accesses might require different kinds of locking. For example, imagine a number of concurrent list operations, including inserts and simple lookups. While inserts change the state of the list (and thus a traditional critical section makes sense), lookups simply\n   *read*\n   the data structure; as long as we can guarantee that no insert is on-going, we can allow many lookups to proceed concurrently. The special type of lock we will now develop to support this type of operation is known as a\n   **reader-writer lock**\n   [CHP71]. The code for such a lock is available in Figure 31.13 (page 12).\n\n\nThe code is pretty simple. If some thread wants to update the data structure in question, it should call the new pair of synchronization operations:\n   \n    rwlock_acquire_writelock()\n   \n   , to acquire a write lock, and\n   \n    rwlock_release_writelock()\n   \n   , to release it. Internally, these simply use the\n   \n    writelock\n   \n   semaphore to ensure that only a single writer can acquire the lock and thus enter the critical section to update the data structure in question.\n\n\nMore interesting is the pair of routines to acquire and release read locks. When acquiring a read lock, the reader first acquires\n   \n    lock\n   \n   and then increments the\n   \n    readers\n   \n   variable to track how many readers are currently inside the data structure. The important step then taken within\n   \n    rwlock_acquire_readlock()\n   \n   occurs when the first reader acquires the lock; in that case, the reader also acquires the write lock by calling\n   \n    sem_wait()\n   \n   on the\n   \n    writelock\n   \n   semaphore, and then releasing the lock by calling\n   \n    sem_post()\n   \n   .\n\n\nThus, once a reader has acquired a read lock, more readers will be allowed to acquire the read lock too; however, any thread that wishes to acquire the write lock will have to wait until\n   *all*\n   readers are finished; the last one to exit the critical section calls\n   \n    sem_post()\n   \n   on “\n   \n    writelock\n   \n   ” and thus enables a waiting writer to acquire the lock.\n\n\nThis approach works (as desired), but does have some negatives, especially when it comes to fairness. In particular, it would be relatively easy for readers to starve writers. More sophisticated solutions to this problem exist; perhaps you can think of a better implementation? Hint: think about what you would need to do to prevent more readers from entering the lock once a writer is waiting.\n\n\n2\n   \n   Indeed, it may have been more natural to place the\n   \n    mutex\n   \n   acquire/release inside the\n   \n    put()\n   \n   and\n   \n    get()\n   \n   functions for the purposes of modularity.\n\n\n1  typedef struct _rwlock_t {\n2      sem_t lock;          // binary semaphore (basic lock)\n3      sem_t writelock;    // allow ONE writer/MANY readers\n4      int   readers;      // #readers in critical section\n5  } rwlock_t;\n6\n7  void rwlock_init(rwlock_t *rw) {\n8      rw->readers = 0;\n9      sem_init(&rw->lock, 0, 1);\n10     sem_init(&rw->writelock, 0, 1);\n11 }\n12\n13 void rwlock_acquire_readlock(rwlock_t *rw) {\n14     sem_wait(&rw->lock);\n15     rw->readers++;\n16     if (rw->readers == 1) // first reader gets writelock\n17         sem_wait(&rw->writelock);\n18     sem_post(&rw->lock);\n19 }\n20\n21 void rwlock_release_readlock(rwlock_t *rw) {\n22     sem_wait(&rw->lock);\n23     rw->readers--;\n24     if (rw->readers == 0) // last reader lets it go\n25         sem_post(&rw->writelock);\n26     sem_post(&rw->lock);\n27 }\n28\n29 void rwlock_acquire_writelock(rwlock_t *rw) {\n30     sem_wait(&rw->writelock);\n31 }\n32\n33 void rwlock_release_writelock(rwlock_t *rw) {\n34     sem_post(&rw->writelock);\n35 }\nFigure 31.13: A Simple Reader-Writer Lock\n\n\nFinally, it should be noted that reader-writer locks should be used with some caution. They often add more overhead (especially with more sophisticated implementations), and thus do not end up speeding up performance as compared to just using simple and fast locking primitives [CB08]. Either way, they showcase once again how we can use semaphores in an interesting and useful way.\n\n\n**TIP: SIMPLE AND DUMB CAN BE BETTER (HILL'S LAW)**\nYou should never underestimate the notion that the simple and dumb approach can be the best one. With locking, sometimes a simple spin lock works best, because it is easy to implement and fast. Although something like reader/writer locks sounds cool, they are complex, and complex can mean slow. Thus, always try the simple and dumb approach first.\n\n\nThis idea, of appealing to simplicity, is found in many places. One early source is Mark Hill's dissertation [H87], which studied how to design caches for CPUs. Hill found that simple direct-mapped caches worked better than fancy set-associative designs (one reason is that in caching, simpler designs enable faster lookups). As Hill succinctly summarized his work: \"Big and dumb is better.\" And thus we call this similar advice Hill's Law."
        },
        {
          "name": "The Dining Philosophers",
          "content": "One of the most famous concurrency problems posed, and solved, by Dijkstra, is known as the\n   **dining philosopher's problem**\n   [D71]. The problem is famous because it is fun and somewhat intellectually interesting; however, its practical utility is low. However, its fame forces its inclusion here; indeed, you might be asked about it on some interview, and you'd really hate your OS professor if you miss that question and don't get the job. Conversely, if you get the job, please feel free to send your OS professor a nice note, or some stock options.\n\n\nThe basic setup for the problem is this (as shown in Figure 31.14): assume there are five \"philosophers\" sitting around a table. Between each pair of philosophers is a single fork (and thus, five total). The philosophers each have times where they think, and don't need any forks, and times where they eat. In order to eat, a philosopher needs two forks, both the one on their left and the one on their right. The contention for these forks, and the synchronization problems that ensue, are what makes this a problem we study in concurrent programming.\n\n\nHere is the basic loop of each philosopher, assuming each has a unique thread identifier\n   \n    p\n   \n   from 0 to 4 (inclusive):\n\n\nwhile (1) {\n    think();\n    get_forks(p);\n    eat();\n    put_forks(p);\n}\nThe key challenge, then, is to write the routines\n   \n    get_forks()\n   \n   and\n   \n    put_forks()\n   \n   such that there is no deadlock, no philosopher starves and\n\n\n\n\n![Diagram illustrating the Dining Philosophers problem. Five philosophers (P0, P1, P2, P3, P4) are arranged in a circle. Each philosopher is represented by a circle. Between each pair of adjacent philosophers is a fork, represented by a rectangle. The forks are labeled f0, f1, f2, f3, and f4 in a clockwise direction starting from the top. P0 is at the top right, P1 at the top, P2 at the top left, P3 at the bottom left, and P4 at the bottom.](images/image_0080.jpeg)\n\n\nDiagram illustrating the Dining Philosophers problem. Five philosophers (P0, P1, P2, P3, P4) are arranged in a circle. Each philosopher is represented by a circle. Between each pair of adjacent philosophers is a fork, represented by a rectangle. The forks are labeled f0, f1, f2, f3, and f4 in a clockwise direction starting from the top. P0 is at the top right, P1 at the top, P2 at the top left, P3 at the bottom left, and P4 at the bottom.\n\n\nFigure 31.14:\n   **The Dining Philosophers**\n\n\nnever gets to eat, and concurrency is high (i.e., as many philosophers can eat at the same time as possible).\n\n\nFollowing Downey's solutions [D08], we'll use a few helper functions to get us towards a solution. They are:\n\n\nint left(int p) { return p; }\nint right(int p) { return (p + 1) % 5; }\nWhen philosopher\n   \n    p\n   \n   wishes to refer to the fork on their left, they simply call\n   \n    left(p)\n   \n   . Similarly, the fork on the right of a philosopher\n   \n    p\n   \n   is referred to by calling\n   \n    right(p)\n   \n   ; the modulo operator therein handles the one case where the last philosopher (\n   \n    p=4\n   \n   ) tries to grab the fork on their right, which is fork 0.\n\n\nWe'll also need some semaphores to solve this problem. Let us assume we have five, one for each fork:\n   \n    sem_t forks[5]\n   \n   .\n\n\n\n\n**Broken Solution**\n\n\nWe attempt our first solution to the problem. Assume we initialize each semaphore (in the\n   \n    forks\n   \n   array) to a value of 1. Assume also that each philosopher knows its own number (\n   \n    p\n   \n   ). We can thus write the\n   \n    get_forks()\n   \n   and\n   \n    put_forks()\n   \n   routine (Figure 31.15, page 15).\n\n\nThe intuition behind this (broken) solution is as follows. To acquire the forks, we simply grab a \"lock\" on each one: first the one on the left,\n\n\n1 void get_forks(int p) {\n2     sem_wait(&forks[left(p)]);\n3     sem_wait(&forks[right(p)]);\n4 }\n5\n6 void put_forks(int p) {\n7     sem_post(&forks[left(p)]);\n8     sem_post(&forks[right(p)]);\n9 }\nFigure 31.15:\n   **The\n    \n     get_forks()\n    \n    And\n    \n     put_forks()\n    \n    Routines**\n\n\n1 void get_forks(int p) {\n2     if (p == 4) {\n3         sem_wait(&forks[right(p)]);\n4         sem_wait(&forks[left(p)]);\n5     } else {\n6         sem_wait(&forks[left(p)]);\n7         sem_wait(&forks[right(p)]);\n8     }\n9 }\nFigure 31.16:\n   **Breaking The Dependency In\n    \n     get_forks()**\n\n\nand then the one on the right. When we are done eating, we release them. Simple, no? Unfortunately, in this case, simple means broken. Can you see the problem that arises? Think about it.\n\n\nThe problem is\n   **deadlock**\n   . If each philosopher happens to grab the fork on their left before any philosopher can grab the fork on their right, each will be stuck holding one fork and waiting for another, forever. Specifically, philosopher 0 grabs fork 0, philosopher 1 grabs fork 1, philosopher 2 grabs fork 2, philosopher 3 grabs fork 3, and philosopher 4 grabs fork 4; all the forks are acquired, and all the philosophers are stuck waiting for a fork that another philosopher possesses. We'll study deadlock in more detail soon; for now, it is safe to say that this is not a working solution.\n\n\n\n\n**A Solution: Breaking The Dependency**\n\n\nThe simplest way to attack this problem is to change how forks are acquired by at least one of the philosophers; indeed, this is how Dijkstra himself solved the problem. Specifically, let's assume that philosopher 4 (the highest numbered one) gets the forks in a\n   *different*\n   order than the others (Figure 31.16); the\n   \n    put_forks()\n   \n   code remains the same.\n\n\nBecause the last philosopher tries to grab right before left, there is no situation where each philosopher grabs one fork and is stuck waiting for another; the cycle of waiting is broken. Think through the ramifications of this solution, and convince yourself that it works.\n\n\nThere are other “famous” problems like this one, e.g., the\n   **cigarette smoker’s problem**\n   or the\n   **sleeping barber problem**\n   . Most of them are just excuses to think about concurrency; some of them have fascinating names. Look them up if you are interested in learning more, or just getting more practice thinking in a concurrent manner [D08]."
        },
        {
          "name": "Thread Throttling",
          "content": "One other simple use case for semaphores arises on occasion, and thus we present it here. The specific problem is this: how can a programmer prevent “too many” threads from doing something at once and bogging the system down? Answer: decide upon a threshold for “too many”, and then use a semaphore to limit the number of threads concurrently executing the piece of code in question. We call this approach\n   **throttling**\n   [T99], and consider it a form of\n   **admission control**\n   .\n\n\nLet’s consider a more specific example. Imagine that you create hundreds of threads to work on some problem in parallel. However, in a certain part of the code, each thread acquires a large amount of memory to perform part of the computation; let’s call this part of the code the\n   *memory-intensive region*\n   . If\n   *all*\n   of the threads enter the memory-intensive region at the same time, the sum of all the memory allocation requests will exceed the amount of physical memory on the machine. As a result, the machine will start thrashing (i.e., swapping pages to and from the disk), and the entire computation will slow to a crawl.\n\n\nA simple semaphore can solve this problem. By initializing the value of the semaphore to the maximum number of threads you wish to enter the memory-intensive region at once, and then putting a\n   \n    sem_wait()\n   \n   and\n   \n    sem_post()\n   \n   around the region, a semaphore can naturally throttle the number of threads that are ever concurrently in the dangerous region of the code."
        },
        {
          "name": "How To Implement Semaphores",
          "content": "Finally, let’s use our low-level synchronization primitives, locks and condition variables, to build our own version of semaphores called ... (\n   *drum roll here*\n   ) ...\n   **Zemaphores**\n   . This task is fairly straightforward, as you can see in Figure 31.17 (page 17).\n\n\nIn the code above, we use just one lock and one condition variable, plus a state variable to track the value of the semaphore. Study the code for yourself until you really understand it. Do it!\n\n\nOne subtle difference between our Zemaphore and pure semaphores as defined by Dijkstra is that we don’t maintain the invariant that the value of the semaphore, when negative, reflects the number of waiting threads; indeed, the value will never be lower than zero. This behavior is easier to implement and matches the current Linux implementation.\n\n\n1  typedef struct __Zem_t {\n2      int value;\n3      pthread_cond_t cond;\n4      pthread_mutex_t lock;\n5  } Zem_t;\n6\n7  // only one thread can call this\n8  void Zem_init(Zem_t *s, int value) {\n9      s->value = value;\n10     Cond_init(&s->cond);\n11     Mutex_init(&s->lock);\n12 }\n13\n14 void Zem_wait(Zem_t *s) {\n15     Mutex_lock(&s->lock);\n16     while (s->value <= 0)\n17         Cond_wait(&s->cond, &s->lock);\n18     s->value--;\n19     Mutex_unlock(&s->lock);\n20 }\n21\n22 void Zem_post(Zem_t *s) {\n23     Mutex_lock(&s->lock);\n24     s->value++;\n25     Cond_signal(&s->cond);\n26     Mutex_unlock(&s->lock);\n27 }\nFigure 31.17:\n   **Implementing Zemaphores With Locks And CVs**\n\n\nCuriously, building condition variables out of semaphores is a much trickier proposition. Some highly experienced concurrent programmers tried to do this in the Windows environment, and many different bugs ensued [B04]. Try it yourself, and see if you can figure out why building condition variables out of semaphores is more challenging of a problem than it might appear."
        }
      ]
    },
    {
      "name": "Common Concurrency Problems",
      "sections": [
        {
          "name": "What Types Of Bugs Exist?",
          "content": "The first, and most obvious, question is this: what types of concurrency bugs manifest in complex, concurrent programs? This question is difficult to answer in general, but fortunately, some others have done the work for us. Specifically, we rely upon a study by Lu et al. [L+08], which analyzes a number of popular concurrent applications in great detail to understand what types of bugs arise in practice.\n\n\nThe study focuses on four major and important open-source applications: MySQL (a popular database management system), Apache (a well-known web server), Mozilla (the famous web browser), and OpenOffice (a free version of the MS Office suite, which some people actually use). In the study, the authors examine concurrency bugs that have been found and fixed in each of these code bases, turning the developers' work into a quantitative bug analysis; understanding these results can help you understand what types of problems actually occur in mature code bases.\n\n\nFigure 32.1 shows a summary of the bugs Lu and colleagues studied. From the figure, you can see that there were 105 total bugs, most of which\n\n\n\nApplication | What it does | Non-Deadlock | Deadlock\nMySQL | Database Server | 14 | 9\nApache | Web Server | 13 | 4\nMozilla | Web Browser | 41 | 16\nOpenOffice | Office Suite | 6 | 2\nTotal |  | 74 | 31\n\n\nFigure 32.1:\n   **Bugs In Modern Applications**\n\n\nwere not deadlock (74); the remaining 31 were deadlock bugs. Further, you can see the number of bugs studied from each application; while OpenOffice only had 8 total concurrency bugs, Mozilla had nearly 60.\n\n\nWe now dive into these different classes of bugs (non-deadlock, deadlock) a bit more deeply. For the first class of non-deadlock bugs, we use examples from the study to drive our discussion. For the second class of deadlock bugs, we discuss the long line of work that has been done in either preventing, avoiding, or handling deadlock."
        },
        {
          "name": "Non-Deadlock Bugs",
          "content": "Non-deadlock bugs make up a majority of concurrency bugs, according to Lu’s study. But what types of bugs are these? How do they arise? How can we fix them? We now discuss the two major types of non-deadlock bugs found by Lu et al.:\n   **atomicity violation**\n   bugs and\n   **order violation**\n   bugs.\n\n\n\n\n**Atomicity-Violation Bugs**\n\n\nThe first type of problem encountered is referred to as an\n   **atomicity violation**\n   . Here is a simple example, found in MySQL. Before reading the explanation, try figuring out what the bug is. Do it!\n\n\n1 Thread 1::\n2 if (thd->proc_info) {\n3     fputs(thd->proc_info, ...);\n4 }\n5\n6 Thread 2::\n7 thd->proc_info = NULL;\nFigure 32.2:\n   **Atomicity Violation (atomicity.c)**\n\n\nIn the example, two different threads access the field\n   \n    proc_info\n   \n   in the structure\n   \n    thd\n   \n   . The first thread checks if the value is non-NULL and then prints its value; the second thread sets it to NULL. Clearly, if the first thread performs the check but then is interrupted before the call to\n   \n    fputs\n   \n   , the second thread could run in-between, thus setting the pointer to NULL; when the first thread resumes, it will crash, as a NULL pointer will be dereferenced by\n   \n    fputs\n   \n   .\n\n\nThe more formal definition of an atomicity violation, according to Lu et al, is this: “The desired serializability among multiple memory accesses is violated (i.e. a code region is intended to be atomic, but the atomicity is not enforced during execution).” In our example above, the code has an\n   *atomicity assumption*\n   (in Lu’s words) about the check for non-NULL of\n   \n    proc_info\n   \n   and the usage of\n   \n    proc_info\n   \n   in the\n   \n    fputs()\n   \n   call; when the assumption is incorrect, the code will not work as desired.\n\n\nFinding a fix for this type of problem is often (but not always) straightforward. Can you think of how to fix the code above?\n\n\nIn this solution (Figure 32.3), we simply add locks around the shared-variable references, ensuring that when either thread accesses the\n   \n    proc_info\n   \n   field, it has a lock held (\n   \n    proc_info_lock\n   \n   ). Of course, any other code that accesses the structure should also acquire this lock before doing so.\n\n\n1 pthread_mutex_t proc_info_lock = PTHREAD_MUTEX_INITIALIZER;\n2\n3 Thread 1::\n4 pthread_mutex_lock(&proc_info_lock);\n5 if (thd->proc_info) {\n6     fputs(thd->proc_info, ...);\n7 }\n8 pthread_mutex_unlock(&proc_info_lock);\n9\n10 Thread 2::\n11 pthread_mutex_lock(&proc_info_lock);\n12 thd->proc_info = NULL;\n13 pthread_mutex_unlock(&proc_info_lock);\nFigure 32.3:\n   **Atomicity Violation Fixed**\n   (\n   \n    atomicity_fixed.c\n   \n   )\n\n\n\n\n**Order-Violation Bugs**\n\n\nAnother common type of non-deadlock bug found by Lu et al. is known as an\n   **order violation**\n   . Here is another simple example; once again, see if you can figure out why the code below has a bug in it.\n\n\n1 Thread 1::\n2 void init() {\n3     mThread = PR_CreateThread(mMain, ...);\n4 }\n5\n6 Thread 2::\n7 void mMain(...) {\n8     mState = mThread->State;\n9 }\nFigure 32.4:\n   **Ordering Bug**\n   (\n   \n    ordering.c\n   \n   )\n\n\nAs you probably figured out, the code in Thread 2 seems to assume that the variable\n   \n    mThread\n   \n   has already been initialized (and is not NULL);\n\n\n1  pthread_mutex_t mtLock = PTHREAD_MUTEX_INITIALIZER;\n2  pthread_cond_t mtCond = PTHREAD_COND_INITIALIZER;\n3  int mtInit           = 0;\n4\n5  Thread 1::\n6  void init() {\n7      ...\n8      mThread = PR_CreateThread(mMain, ...);\n9\n10     // signal that the thread has been created...\n11     pthread_mutex_lock(&mtLock);\n12     mtInit = 1;\n13     pthread_cond_signal(&mtCond);\n14     pthread_mutex_unlock(&mtLock);\n15     ...\n16 }\n17\n18 Thread 2::\n19 void mMain(...) {\n20     ...\n21     // wait for the thread to be initialized...\n22     pthread_mutex_lock(&mtLock);\n23     while (mtInit == 0)\n24         pthread_cond_wait(&mtCond, &mtLock);\n25     pthread_mutex_unlock(&mtLock);\n26\n27     mState = mThread->State;\n28     ...\n29 }\n\nFigure 32.5: Fixing The Ordering Violation (\n   \n    ordering_fixed.c\n   \n   )\n\n\nhowever, if Thread 2 runs immediately once created, the value of\n   \n    mThread\n   \n   will not be set when it is accessed within\n   \n    mMain()\n   \n   in Thread 2, and will likely crash with a NULL-pointer dereference. Note that we assume the value of\n   \n    mThread\n   \n   is initially NULL; if not, even stranger things could happen as arbitrary memory locations are accessed through the dereference in Thread 2.\n\n\nThe more formal definition of an order violation is the following: “The desired order between two (groups of) memory accesses is flipped (i.e.,\n   *A*\n   should always be executed before\n   *B*\n   , but the order is not enforced during execution)” [L+08].\n\n\nThe fix to this type of bug is generally to enforce ordering. As discussed previously, using\n   **condition variables**\n   is an easy and robust way to add this style of synchronization into modern code bases. In the example above, we could thus rewrite the code as seen in Figure 32.5.\n\n\nIn this fixed-up code sequence, we have added a condition variable (\n   \n    mtCond\n   \n   ) and corresponding lock (\n   \n    mtLock\n   \n   ), as well as a state variable\n\n\n(\n   \n    mtInit\n   \n   ). When the initialization code runs, it sets the state of\n   \n    mtInit\n   \n   to 1 and signals that it has done so. If Thread 2 had run before this point, it will be waiting for this signal and corresponding state change; if it runs later, it will check the state and see that the initialization has already occurred (i.e.,\n   \n    mtInit\n   \n   is set to 1), and thus continue as is proper. Note that we could likely use\n   \n    mThread\n   \n   as the state variable itself, but do not do so for the sake of simplicity here. When ordering matters between threads, condition variables (or semaphores) can come to the rescue.\n\n\n\n\n**Non-Deadlock Bugs: Summary**\n\n\nA large fraction (97%) of non-deadlock bugs studied by Lu et al. are either atomicity or order violations. Thus, by carefully thinking about these types of bug patterns, programmers can likely do a better job of avoiding them. Moreover, as more automated code-checking tools develop, they should likely focus on these two types of bugs as they constitute such a large fraction of non-deadlock bugs found in deployment.\n\n\nUnfortunately, not all bugs are as easily fixed as the examples we looked at above. Some require a deeper understanding of what the program is doing, or a larger amount of code or data structure reorganization to fix. Read Lu et al.’s excellent (and readable) paper for more details."
        },
        {
          "name": "Deadlock Bugs",
          "content": "Beyond the concurrency bugs mentioned above, a classic problem that arises in many concurrent systems with complex locking protocols is known as\n   **deadlock**\n   . Deadlock occurs, for example, when a thread (say Thread 1) is holding a lock (\n   \n    L1\n   \n   ) and waiting for another one (\n   \n    L2\n   \n   ); unfortunately, the thread (Thread 2) that holds lock\n   \n    L2\n   \n   is waiting for\n   \n    L1\n   \n   to be released. Here is a code snippet that demonstrates such a potential deadlock:\n\n\nThread 1:                Thread 2:\npthread_mutex_lock(L1);  pthread_mutex_lock(L2);\npthread_mutex_lock(L2);  pthread_mutex_lock(L1);\nFigure 32.6: Simple Deadlock (\n   \n    deadlock.c\n   \n   )\n\n\nNote that if this code runs, deadlock does not necessarily occur; rather, it may occur, if, for example, Thread 1 grabs lock\n   \n    L1\n   \n   and then a context switch occurs to Thread 2. At that point, Thread 2 grabs\n   \n    L2\n   \n   , and tries to acquire\n   \n    L1\n   \n   . Thus we have a deadlock, as each thread is waiting for the other and neither can run. See Figure 32.7 for a graphical depiction; the presence of a\n   **cycle**\n   in the graph is indicative of the deadlock.\n\n\nThe figure should make the problem clear. How should programmers write code so as to handle deadlock in some way?\n\n\n\n\n![Deadlock Dependency Graph showing Thread 1 holding Lock L1 and waiting for Lock L2, while Thread 2 holds Lock L2 and waits for Lock L1.](images/image_0081.jpeg)\n\n\ngraph TD\n    T1((Thread 1)) -- Holds --> L1[Lock L1]\n    L1 -- Wanted by --> T2((Thread 2))\n    T2 -- Holds --> L2[Lock L2]\n    L2 -- Wanted by --> T1\n  \nDeadlock Dependency Graph showing Thread 1 holding Lock L1 and waiting for Lock L2, while Thread 2 holds Lock L2 and waits for Lock L1.\n\n\nFigure 32.7: The Deadlock Dependency Graph\n\n\n\n\n**CRUX: HOW TO DEAL WITH DEADLOCK**\n\n\nHow should we build systems to prevent, avoid, or at least detect and recover from deadlock? Is this a real problem in systems today?\n\n\n\n\n**Why Do Deadlocks Occur?**\n\n\nAs you may be thinking, simple deadlocks such as the one above seem readily avoidable. For example, if Thread 1 and 2 both made sure to grab locks in the same order, the deadlock would never arise. So why do deadlocks happen?\n\n\nOne reason is that in large code bases, complex dependencies arise between components. Take the operating system, for example. The virtual memory system might need to access the file system in order to page in a block from disk; the file system might subsequently require a page of memory to read the block into and thus contact the virtual memory system. Thus, the design of locking strategies in large systems must be carefully done to avoid deadlock in the case of circular dependencies that may occur naturally in the code.\n\n\nAnother reason is due to the nature of\n   **encapsulation**\n   . As software developers, we are taught to hide details of implementations and thus make software easier to build in a modular way. Unfortunately, such modularity does not mesh well with locking. As Julia et al. point out [J+08], some seemingly innocuous interfaces almost invite you to deadlock. For example, take the Java\n   \n    Vector\n   \n   class and the method\n   \n    AddAll()\n   \n   . This routine would be called as follows:\n\n\nVector v1, v2;\nv1.AddAll(v2);\nInternally, because the method needs to be multi-thread safe, locks for both the vector being added to (\n   \n    v1\n   \n   ) and the parameter (\n   \n    v2\n   \n   ) need to be acquired. The routine acquires said locks in some arbitrary order (say\n   \n    v1\n   \n   then\n   \n    v2\n   \n   ) in order to add the contents of\n   \n    v2\n   \n   to\n   \n    v1\n   \n   . If some other thread calls\n   \n    v2.AddAll(v1)\n   \n   at nearly the same time, we have the potential for deadlock, all in a way that is quite hidden from the calling application.\n\n\n\n\n**Conditions for Deadlock**\n\n\nFour conditions need to hold for a deadlock to occur [C+71]:\n\n\n  * •\n    **Mutual exclusion:**\n    Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock).\n  * •\n    **Hold-and-wait:**\n    Threads hold resources allocated to them (e.g., locks that they have already acquired) while waiting for additional resources (e.g., locks that they wish to acquire).\n  * •\n    **No preemption:**\n    Resources (e.g., locks) cannot be forcibly removed from threads that are holding them.\n  * •\n    **Circular wait:**\n    There exists a circular chain of threads such that each thread holds one or more resources (e.g., locks) that are being requested by the next thread in the chain.\n\n\nIf any of these four conditions are not met, deadlock cannot occur. Thus, we first explore techniques to\n   *prevent*\n   deadlock; each of these strategies seeks to prevent one of the above conditions from arising and thus is one approach to handling the deadlock problem.\n\n\n\n\n**Prevention**\n\n\n\n\n**Circular Wait**\n\n\nProbably the most practical prevention technique (and certainly one that is frequently employed) is to write your locking code such that you never induce a circular wait. The most straightforward way to do that is to provide a\n   **total ordering**\n   on lock acquisition. For example, if there are only two locks in the system (\n   \n    L1\n   \n   and\n   \n    L2\n   \n   ), you can prevent deadlock by always acquiring\n   \n    L1\n   \n   before\n   \n    L2\n   \n   . Such strict ordering ensures that no cyclical wait arises; hence, no deadlock.\n\n\nOf course, in more complex systems, more than two locks will exist, and thus total lock ordering may be difficult to achieve (and perhaps is unnecessary anyhow). Thus, a\n   **partial ordering**\n   can be a useful way to structure lock acquisition so as to avoid deadlock. An excellent real example of partial lock ordering can be seen in the memory mapping code in Linux [T+94] (v5.2); the comment at the top of the source code reveals ten different groups of lock acquisition orders, including simple\n\n\n**TIP: ENFORCE LOCK ORDERING BY LOCK ADDRESS**\nIn some cases, a function must grab two (or more) locks; thus, we know we must be careful or deadlock could arise. Imagine a function that is called as follows:\n   \n    do_something(mutex_t *m1, mutex_t *m2)\n   \n   . If the code always grabs\n   \n    m1\n   \n   before\n   \n    m2\n   \n   (or always\n   \n    m2\n   \n   before\n   \n    m1\n   \n   ), it could deadlock, because one thread could call\n   \n    do_something(L1, L2)\n   \n   while another thread could call\n   \n    do_something(L2, L1)\n   \n   .\n\n\nTo avoid this particular issue, the clever programmer can use the\n   *address*\n   of each lock as a way of ordering lock acquisition. By acquiring locks in either high-to-low or low-to-high address order,\n   \n    do_something()\n   \n   can guarantee that it always acquires locks in the same order, regardless of which order they are passed in. The code would look something like this:\n\n\nif (m1 > m2) { // grab in high-to-low address order\n    pthread_mutex_lock(m1);\n    pthread_mutex_lock(m2);\n} else {\n    pthread_mutex_lock(m2);\n    pthread_mutex_lock(m1);\n}\n// Code assumes that m1 != m2 (not the same lock)\nBy using this simple technique, a programmer can ensure a simple and efficient deadlock-free implementation of multi-lock acquisition.\n\n\nones such as “\n   \n    i_mutex\n   \n   before\n   \n    i_mmap_rwsem\n   \n   ” and more complex orders such as “\n   \n    i_mmap_rwsem\n   \n   before\n   \n    private_lock\n   \n   before\n   \n    swap_lock\n   \n   before\n   \n    i_pages_lock\n   \n   ”.\n\n\nAs you can imagine, both total and partial ordering require careful design of locking strategies and must be constructed with great care. Further, ordering is just a convention, and a sloppy programmer can easily ignore the locking protocol and potentially cause deadlock. Finally, lock ordering requires a deep understanding of the code base, and how various routines are called; just one mistake could result in the “D” word\n   \n    1\n   \n   .\n\n\n**Hold-and-wait**\nThe hold-and-wait requirement for deadlock can be avoided by acquiring all locks at once, atomically. In practice, this could be achieved as follows:\n\n\n1   pthread_mutex_lock(prevention);    // begin acquisition\n2   pthread_mutex_lock(L1);\n3   pthread_mutex_lock(L2);\n4   ...\n5   pthread_mutex_unlock(prevention); // end\n1\n   \n   Hint: “D” stands for “Deadlock”.\n\n\nBy first grabbing the lock\n   \n    prevention\n   \n   , this code guarantees that no untimely thread switch can occur in the midst of lock acquisition and thus deadlock can once again be avoided. Of course, it requires that any time any thread grabs a lock, it first acquires the global prevention lock. For example, if another thread was trying to grab locks\n   \n    L1\n   \n   and\n   \n    L2\n   \n   in a different order, it would be OK, because it would be holding the prevention lock while doing so.\n\n\nNote that the solution is problematic for a number of reasons. As before, encapsulation works against us: when calling a routine, this approach requires us to know exactly which locks must be held and to acquire them ahead of time. This technique also is likely to decrease concurrency as all locks must be acquired early on (at once) instead of when they are truly needed.\n\n\n\n\n**No Preemption**\n\n\nBecause we generally view locks as held until unlock is called, multiple lock acquisition often gets us into trouble because when waiting for one lock we are holding another. Many thread libraries provide a more flexible set of interfaces to help avoid this situation. Specifically, the routine\n   \n    pthread_mutex_trylock()\n   \n   either grabs the lock (if it is available) and returns success or returns an error code indicating the lock is held; in the latter case, you can try again later if you want to grab that lock.\n\n\nSuch an interface could be used as follows to build a deadlock-free, ordering-robust lock acquisition protocol:\n\n\n1 top:\n2     pthread_mutex_lock(L1);\n3     if (pthread_mutex_trylock(L2) != 0) {\n4         pthread_mutex_unlock(L1);\n5         goto top;\n6     }\nNote that another thread could follow the same protocol but grab the locks in the other order (\n   \n    L2\n   \n   then\n   \n    L1\n   \n   ) and the program would still be deadlock free. One new problem does arise, however:\n   **livelock**\n   . It is possible (though perhaps unlikely) that two threads could both be repeatedly attempting this sequence and repeatedly failing to acquire both locks. In this case, both systems are running through this code sequence over and over again (and thus it is not a deadlock), but progress is not being made, hence the name livelock. There are solutions to the livelock problem, too: for example, one could add a random delay before looping back and trying the entire thing over again, thus decreasing the odds of repeated interference among competing threads.\n\n\nOne point about this solution: it skirts around the hard parts of using a trylock approach. The first problem that would likely exist again arises due to encapsulation: if one of these locks is buried in some routine that is getting called, the jump back to the beginning becomes more complex to implement. If the code had acquired some resources (other than\n   \n    L1\n   \n   )\n\n\nalong the way, it must make sure to carefully release them as well; for example, if after acquiring\n   \n    L_1\n   \n   , the code had allocated some memory, it would have to release that memory upon failure to acquire\n   \n    L_2\n   \n   , before jumping back to the top to try the entire sequence again. However, in limited circumstances (e.g., the Java vector method mentioned earlier), this type of approach could work well.\n\n\nYou might also notice that this approach doesn't really\n   *add*\n   preemption (the forcible action of taking a lock away from a thread that owns it), but rather uses the trylock approach to allow a developer to back out of lock ownership (i.e., preempt their own ownership) in a graceful way. However, it is a practical approach, and thus we include it here, despite its imperfection in this regard.\n\n\n\n\n**Mutual Exclusion**\n\n\nThe final prevention technique would be to avoid the need for mutual exclusion at all. In general, we know this is difficult, because the code we wish to run does indeed have critical sections. So what can we do?\n\n\nHerlihy had the idea that one could design various data structures without locks at all [H91, H93]. The idea behind these\n   **lock-free**\n   (and related\n   **wait-free**\n   ) approaches here is simple: using powerful hardware instructions, you can build data structures in a manner that does not require explicit locking.\n\n\nAs a simple example, let us assume we have a compare-and-swap instruction, which as you may recall is an atomic instruction provided by the hardware that does the following:\n\n\n1 int CompareAndSwap(int *address, int expected, int new) {\n2     if (*address == expected) {\n3         *address = new;\n4         return 1; // success\n5     }\n6     return 0; // failure\n7 }\nImagine we now wanted to atomically increment a value by a certain amount, using compare-and-swap. We could do so with the following simple function:\n\n\n1 void AtomicIncrement(int *value, int amount) {\n2     do {\n3         int old = *value;\n4     } while (CompareAndSwap(value, old, old + amount) == 0);\n5 }\nInstead of acquiring a lock, doing the update, and then releasing it, we have instead built an approach that repeatedly tries to update the value to the new amount and uses the compare-and-swap to do so. In this manner,\n\n\nno lock is acquired, and no deadlock can arise (though livelock is still a possibility, and thus a robust solution will be more complex than the simple code snippet above).\n\n\nLet us consider a slightly more complex example: list insertion. Here is code that inserts at the head of a list:\n\n\n1 void insert(int value) {\n2     node_t *n = malloc(sizeof(node_t));\n3     assert(n != NULL);\n4     n->value = value;\n5     n->next  = head;\n6     head     = n;\n7 }\nThis code performs a simple insertion, but if called by multiple threads at the “same time”, has a race condition. Can you figure out why? (draw a picture of what could happen to a list if two concurrent insertions take place, assuming, as always, a malicious scheduling interleaving). Of course, we could solve this by surrounding this code with a lock acquire and release:\n\n\n1 void insert(int value) {\n2     node_t *n = malloc(sizeof(node_t));\n3     assert(n != NULL);\n4     n->value = value;\n5     pthread_mutex_lock(listlock);    // begin critical section\n6     n->next  = head;\n7     head     = n;\n8     pthread_mutex_unlock(listlock); // end critical section\n9 }\nIn this solution, we are using locks in the traditional manner\n   \n    2\n   \n   . Instead, let us try to perform this insertion in a lock-free manner simply using the compare-and-swap instruction. Here is one possible approach:\n\n\n1 void insert(int value) {\n2     node_t *n = malloc(sizeof(node_t));\n3     assert(n != NULL);\n4     n->value = value;\n5     do {\n6         n->next = head;\n7     } while (CompareAndSwap(&head, n->next, n) == 0);\n8 }\n2\n   \n   The astute reader might be asking why we grabbed the lock so late, instead of right when entering\n   \n    insert()\n   \n   ; can you, astute reader, figure out why that is likely correct? What assumptions does the code make, for example, about the call to\n   \n    malloc()\n   \n   ?\n\n\nThe code here updates the next pointer to point to the current head, and then tries to swap the newly-created node into position as the new head of the list. However, this will fail if some other thread successfully swapped in a new head in the meanwhile, causing this thread to retry again with the new head.\n\n\nOf course, building a useful list requires more than just a list insert, and not surprisingly building a list that you can insert into, delete from, and perform lookups on in a lock-free manner is non-trivial. Read the rich literature on lock-free and wait-free synchronization to learn more [H01, H91, H93].\n\n\n\n\n**Deadlock Avoidance via Scheduling**\n\n\nInstead of deadlock prevention, in some scenarios deadlock\n   **avoidance**\n   is preferable. Avoidance requires some global knowledge of which locks various threads might grab during their execution, and subsequently schedules said threads in a way as to guarantee no deadlock can occur.\n\n\nFor example, assume we have two processors and four threads which must be scheduled upon them. Assume further we know that Thread 1 (T1) grabs locks\n   \n    L_1\n   \n   and\n   \n    L_2\n   \n   (in some order, at some point during its execution), T2 grabs\n   \n    L_1\n   \n   and\n   \n    L_2\n   \n   as well, T3 grabs just\n   \n    L_2\n   \n   , and T4 grabs no locks at all. We can show these lock acquisition demands of the threads in tabular form:\n\n\n\n | T1 | T2 | T3 | T4\nL_1 | yes | yes | no | no\nL_2 | yes | yes | yes | no\n\n\nA smart scheduler could thus compute that as long as T1 and T2 are not run at the same time, no deadlock could ever arise. Here is one such schedule:\n\n\n\nCPU 1 | T3 | T4\nCPU 2 | T1 | T2\n\n\nNote that it is OK for (T3 and T1) or (T3 and T2) to overlap. Even though T3 grabs lock\n   \n    L_2\n   \n   , it can never cause a deadlock by running concurrently with other threads because it only grabs one lock.\n\n\nLet's look at one more example. In this one, there is more contention for the same resources (again, locks\n   \n    L_1\n   \n   and\n   \n    L_2\n   \n   ), as indicated by the following contention table:\n\n\n\n | T1 | T2 | T3 | T4\nL_1 | yes | yes | yes | no\nL_2 | yes | yes | yes | no\n\n\n**TIP: DON'T ALWAYS DO IT PERFECTLY (TOM WEST'S LAW)**\nTom West, famous as the subject of the classic computer-industry book\n   *Soul of a New Machine*\n   [K81], says famously: \"Not everything worth doing is worth doing well\", which is a terrific engineering maxim. If a bad thing happens rarely, certainly one should not spend a great deal of effort to prevent it, particularly if the cost of the bad thing occurring is small. If, on the other hand, you are building a space shuttle, and the cost of something going wrong is the space shuttle blowing up, well, perhaps you should ignore this piece of advice.\n\n\nSome readers object: \"This sounds like you are suggesting mediocrity as a solution!\" Perhaps they are right, that we should be careful with advice such as this. However, our experience tells us that in the world of engineering, with pressing deadlines and other real-world concerns, one will always have to decide which aspects of a system to build well and which to put aside for another day. The hard part is knowing which to do when, a bit of insight only gained through experience and dedication to the task at hand.\n\n\nIn particular, threads T1, T2, and T3 all need to grab both locks L1 and L2 at some point during their execution. Here is a possible schedule that guarantees that no deadlock could ever occur:\n\n\n\n\n![A diagram showing task scheduling on two CPUs. CPU 1 runs task T4. CPU 2 runs tasks T1, T2, and T3 sequentially. T1 and T2 are in light gray boxes, while T3 is in a black box.](images/image_0082.jpeg)\n\n\nCPU 1 | T4\nCPU 2 | T1\n | T2\n | T3\n\n\nA diagram showing task scheduling on two CPUs. CPU 1 runs task T4. CPU 2 runs tasks T1, T2, and T3 sequentially. T1 and T2 are in light gray boxes, while T3 is in a black box.\n\n\nAs you can see, static scheduling leads to a conservative approach where T1, T2, and T3 are all run on the same processor, and thus the total time to complete the jobs is lengthened considerably. Though it may have been possible to run these tasks concurrently, the fear of deadlock prevents us from doing so, and the cost is performance.\n\n\nOne famous example of an approach like this is Dijkstra's Banker's Algorithm [D64], and many similar approaches have been described in the literature. Unfortunately, they are only useful in very limited environments, for example, in an embedded system where one has full knowledge of the entire set of tasks that must be run and the locks that they need. Further, such approaches can limit concurrency, as we saw in the second example above. Thus, avoidance of deadlock via scheduling is not a widely-used general-purpose solution.\n\n\n\n\n**Detect and Recover**\n\n\nOne final general strategy is to allow deadlocks to occasionally occur, and then take some action once such a deadlock has been detected. For exam-\n\n\nple, if an OS froze once a year, you would just reboot it and get happily (or grumpily) on with your work. If deadlocks are rare, such a non-solution is indeed quite pragmatic.\n\n\nMany database systems employ deadlock detection and recovery techniques. A deadlock detector runs periodically, building a resource graph and checking it for cycles. In the event of a cycle (deadlock), the system needs to be restarted. If more intricate repair of data structures is first required, a human being may be involved to ease the process.\n\n\nMore detail on database concurrency, deadlock, and related issues can be found elsewhere [B+87, K87]. Read these works, or better yet, take a course on databases to learn more about this rich and interesting topic."
        }
      ]
    },
    {
      "name": "Event-based Concurrency (Advanced)",
      "sections": [
        {
          "name": "The Basic Idea: An Event Loop",
          "content": "The basic approach we'll use, as stated above, is called\n   **event-based concurrency**\n   . The approach is quite simple: you simply wait for something (i.e., an \"event\") to occur; when it does, you check what type of\n\n\nevent it is and do the small amount of work it requires (which may include issuing I/O requests, or scheduling other events for future handling, etc.). That's it!\n\n\nBefore getting into the details, let's first examine what a canonical event-based server looks like. Such applications are based around a simple construct known as the\n   **event loop**\n   . Pseudocode for an event loop looks like this:\n\n\nwhile (1) {\n    events = getEvents();\n    for (e in events)\n        processEvent(e);\n}\nIt's really that simple. The main loop simply waits for something to do (by calling\n   \n    getEvents()\n   \n   in the code above) and then, for each event returned, processes them, one at a time; the code that processes each event is known as an\n   **event handler**\n   . Importantly, when a handler processes an event, it is the only activity taking place in the system; thus, deciding which event to handle next is equivalent to scheduling. This explicit control over scheduling is one of the fundamental advantages of the event-based approach.\n\n\nBut this discussion leaves us with a bigger question: how exactly does an event-based server determine which events are taking place, in particular with regards to network and disk I/O? Specifically, how can an event server tell if a message has arrived for it?"
        },
        {
          "name": "An Important API: select() (or poll())",
          "content": "With that basic event loop in mind, we next must address the question of how to receive events. In most systems, a basic API is available, via either the\n   \n    select()\n   \n   or\n   \n    poll()\n   \n   system calls.\n\n\nWhat these interfaces enable a program to do is simple: check whether there is any incoming I/O that should be attended to. For example, imagine that a network application (such as a web server) wishes to check whether any network packets have arrived, in order to service them. These system calls let you do exactly that.\n\n\nTake\n   \n    select()\n   \n   for example. The manual page (on a Mac) describes the API in this manner:\n\n\nint select(int nfds,\n           fd_set *restrict readfds,\n           fd_set *restrict writefds,\n           fd_set *restrict errorfds,\n           struct timeval *restrict timeout);\nThe actual description from the man page:\n   *select()\n    \n    examines the I/O descriptor sets whose addresses are passed in\n    \n     readfds\n    \n    ,\n    \n     writefds\n    \n    , and\n    \n     errorfds\n    \n    to see*\n\n\n\n\n**ASIDE: BLOCKING VS. NON-BLOCKING INTERFACES**\n\n\nBlocking (or\n   **synchronous**\n   ) interfaces do all of their work before returning to the caller; non-blocking (or\n   **asynchronous**\n   ) interfaces begin some work but return immediately, thus letting whatever work that needs to be done get done in the background.\n\n\nThe usual culprit in blocking calls is I/O of some kind. For example, if a call must read from disk in order to complete, it might block, waiting for the I/O request that has been sent to the disk to return.\n\n\nNon-blocking interfaces can be used in any style of programming (e.g., with threads), but are essential in the event-based approach, as a call that blocks will halt all progress.\n\n\n*if some of their descriptors are ready for reading, are ready for writing, or have an exceptional condition pending, respectively. The first nfds descriptors are checked in each set, i.e., the descriptors from 0 through nfds-1 in the descriptor sets are examined. On return, select() replaces the given descriptor sets with subsets consisting of those descriptors that are ready for the requested operation. select() returns the total number of ready descriptors in all the sets.*\n\n\nA couple of points about\n   \n    select()\n   \n   . First, note that it lets you check whether descriptors can be\n   *read*\n   from as well as\n   *written*\n   to; the former lets a server determine that a new packet has arrived and is in need of processing, whereas the latter lets the service know when it is OK to reply (i.e., the outbound queue is not full).\n\n\nSecond, note the timeout argument. One common usage here is to set the timeout to\n   \n    NULL\n   \n   , which causes\n   \n    select()\n   \n   to block indefinitely, until some descriptor is ready. However, more robust servers will usually specify some kind of timeout; one common technique is to set the timeout to zero, and thus use the call to\n   \n    select()\n   \n   to return immediately.\n\n\nThe\n   \n    poll()\n   \n   system call is quite similar. See its manual page, or Stevens and Rago [SR05], for details.\n\n\nEither way, these basic primitives give us a way to build a non-blocking event loop, which simply checks for incoming packets, reads from sockets with messages upon them, and replies as needed."
        },
        {
          "name": "Using select()",
          "content": "To make this more concrete, let's examine how to use\n   \n    select()\n   \n   to see which network descriptors have incoming messages upon them. Figure 33.1 shows a simple example.\n\n\nThis code is actually fairly simple to understand. After some initialization, the server enters an infinite loop. Inside the loop, it uses the\n   \n    FD_ZERO()\n   \n   macro to first clear the set of file descriptors, and then uses\n   \n    FD_SET()\n   \n   to include all of the file descriptors from\n   \n    minFD\n   \n   to\n   \n    maxFD\n   \n   in\n\n\n1 #include <stdio.h>\n2 #include <stdlib.h>\n3 #include <sys/time.h>\n4 #include <sys/types.h>\n5 #include <unistd.h>\n6\n7 int main(void) {\n8     // open and set up a bunch of sockets (not shown)\n9     // main loop\n10    while (1) {\n11        // initialize the fd_set to all zero\n12        fd_set readFDs;\n13        FD_ZERO(&readFDs);\n14\n15        // now set the bits for the descriptors\n16        // this server is interested in\n17        // (for simplicity, all of them from min to max)\n18        int fd;\n19        for (fd = minFD; fd < maxFD; fd++)\n20            FD_SET(fd, &readFDs);\n21\n22        // do the select\n23        int rc = select(maxFD+1, &readFDs, NULL, NULL, NULL);\n24\n25        // check which actually have data using FD_ISSET()\n26        int fd;\n27        for (fd = minFD; fd < maxFD; fd++)\n28            if (FD_ISSET(fd, &readFDs))\n29                processFD(fd);\n30    }\n31 }\nFigure 33.1: Simple Code Using\n   \n    select()\n\n\nthe set. This set of descriptors might represent, for example, all of the network sockets to which the server is paying attention. Finally, the server calls\n   \n    select()\n   \n   to see which of the connections have data available upon them. By then using\n   \n    FD_ISSET()\n   \n   in a loop, the event server can see which of the descriptors have data ready and process the incoming data.\n\n\nOf course, a real server would be more complicated than this, and require logic to use when sending messages, issuing disk I/O, and many other details. For further information, see Stevens and Rago [SR05] for API information, or Pai et. al or Welsh et al. for a good overview of the general flow of event-based servers [PDZ99, WCB01].\n\n\n**TIP: DON'T BLOCK IN EVENT-BASED SERVERS**\nEvent-based servers enable fine-grained control over scheduling of tasks. However, to maintain such control, no call that blocks the execution of the caller can ever be made; failing to obey this design tip will result in a blocked event-based server, frustrated clients, and serious questions as to whether you ever read this part of the book."
        },
        {
          "name": "Why Simpler? No Locks Needed",
          "content": "With a single CPU and an event-based application, the problems found in concurrent programs are no longer present. Specifically, because only one event is being handled at a time, there is no need to acquire or release locks; the event-based server cannot be interrupted by another thread because it is decidedly single threaded. Thus, concurrency bugs common in threaded programs do not manifest in the basic event-based approach."
        },
        {
          "name": "A Problem: Blocking System Calls",
          "content": "Thus far, event-based programming sounds great, right? You program a simple loop, and handle events as they arise. You don't even need to think about locking! But there is an issue: what if an event requires that you issue a system call that might block?\n\n\nFor example, imagine a request comes from a client into a server to read a file from disk and return its contents to the requesting client (much like a simple HTTP request). To service such a request, some event handler will eventually have to issue an\n   \n    open()\n   \n   system call to open the file, followed by a series of\n   \n    read()\n   \n   calls to read the file. When the file is read into memory, the server will likely start sending the results to the client.\n\n\nBoth the\n   \n    open()\n   \n   and\n   \n    read()\n   \n   calls may issue I/O requests to the storage system (when the needed metadata or data is not in memory already), and thus may take a long time to service. With a thread-based server, this is no issue: while the thread issuing the I/O request suspends (waiting for the I/O to complete), other threads can run, thus enabling the server to make progress. Indeed, this natural\n   **overlap**\n   of I/O and other computation is what makes thread-based programming quite natural and straightforward.\n\n\nWith an event-based approach, however, there are no other threads to run: just the main event loop. And this implies that if an event handler issues a call that blocks, the\n   *entire*\n   server will do just that: block until the call completes. When the event loop blocks, the system sits idle, and thus is a huge potential waste of resources. We thus have a rule that must be obeyed in event-based systems: no blocking calls are allowed."
        },
        {
          "name": "A Solution: Asynchronous I/O",
          "content": "To overcome this limit, many modern operating systems have introduced new ways to issue I/O requests to the disk system, referred to generically as\n   **asynchronous I/O**\n   . These interfaces enable an application to issue an I/O request and return control immediately to the caller, before the I/O has completed; additional interfaces enable an application to determine whether various I/Os have completed.\n\n\nFor example, let us examine the interface provided on a Mac (other systems have similar APIs). The APIs revolve around a basic structure, the\n   **struct aio_cb**\n   or\n   **AIO control block**\n   in common terminology. A simplified version of the structure looks like this (see the manual pages for more information):\n\n\nstruct aio_cb {\n    int aio_fildes; // File descriptor\n    off_t aio_offset; // File offset\n    volatile void *aio_buf; // Location of buffer\n    size_t aio_nbytes; // Length of transfer\n};\nTo issue an asynchronous read to a file, an application should first fill in this structure with the relevant information: the file descriptor of the file to be read (\n   \n    aio_fildes\n   \n   ), the offset within the file (\n   \n    aio_offset\n   \n   ) as well as the length of the request (\n   \n    aio_nbytes\n   \n   ), and finally the target memory location into which the results of the read should be copied (\n   \n    aio_buf\n   \n   ).\n\n\nAfter this structure is filled in, the application must issue the asynchronous call to read the file; on a Mac, this API is simply the\n   **asynchronous read API**\n   :\n\n\nint aio_read(struct aio_cb *aio_cbp);\nThis call tries to issue the I/O; if successful, it simply returns right away and the application (i.e., the event-based server) can continue with its work.\n\n\nThere is one last piece of the puzzle we must solve, however. How can we tell when an I/O is complete, and thus that the buffer (pointed to by\n   \n    aio_buf\n   \n   ) now has the requested data within it?\n\n\nOne last API is needed. On a Mac, it is referred to (somewhat confusingly) as\n   \n    aio_error()\n   \n   . The API looks like this:\n\n\nint aio_error(const struct aio_cb *aio_cbp);\nThis system call checks whether the request referred to by\n   \n    aio_cbp\n   \n   has completed. If it has, the routine returns success (indicated by a zero); if not,\n   \n    EINPROGRESS\n   \n   is returned. Thus, for every outstanding asynchronous I/O, an application can periodically\n   **poll**\n   the system via a call to\n   \n    aio_error()\n   \n   to determine whether said I/O has yet completed.\n\n\nOne thing you might have noticed is that it is painful to check whether an I/O has completed; if a program has tens or hundreds of I/Os issued at a given point in time, should it simply keep checking each of them repeatedly, or wait a little while first, or ... ?\n\n\nTo remedy this issue, some systems provide an approach based on the\n   **interrupt**\n   . This method uses UNIX\n   **signals**\n   to inform applications when an asynchronous I/O completes, thus removing the need to repeatedly ask the system. This polling vs. interrupts issue is seen in devices too, as you will see (or already have seen) in the chapter on I/O devices.\n\n\nIn systems without asynchronous I/O, the pure event-based approach cannot be implemented. However, clever researchers have derived methods that work fairly well in their place. For example, Pai et al. [PDZ99] describe a hybrid approach in which events are used to process network packets, and a thread pool is used to manage outstanding I/Os. Read their paper for details."
        },
        {
          "name": "Another Problem: State Management",
          "content": "Another issue with the event-based approach is that such code is generally more complicated to write than traditional thread-based code. The reason is as follows: when an event handler issues an asynchronous I/O, it must package up some program state for the next event handler to use when the I/O finally completes; this additional work is not needed in thread-based programs, as the state the program needs is on the stack of the thread. Adya et al. call this work\n   **manual stack management**\n   , and it is fundamental to event-based programming [A+02].\n\n\nTo make this point more concrete, let's look at a simple example in which a thread-based server needs to read from a file descriptor (\n   \n    fd\n   \n   ) and, once complete, write the data that it read from the file to a network socket descriptor (\n   \n    sd\n   \n   ). The code (ignoring error checking) looks like this:\n\n\nint rc = read(fd, buffer, size);\nrc = write(sd, buffer, size);\nAs you can see, in a multi-threaded program, doing this kind of work is trivial; when the\n   \n    read()\n   \n   finally returns, the code immediately knows which socket to write to because that information is on the stack of the thread (in the variable\n   \n    sd\n   \n   ).\n\n\nIn an event-based system, life is not so easy. To perform the same task, we'd first issue the read asynchronously, using the AIO calls described above. Let's say we then periodically check for completion of the read using the\n   \n    aio_error()\n   \n   call; when that call informs us that the read is complete, how does the event-based server know what to do?\n\n\n\n\n**ASIDE: UNIX SIGNALS**\n\n\nA huge and fascinating infrastructure known as\n   **signals**\n   is present in all modern UNIX variants. At its simplest, signals provide a way to communicate with a process. Specifically, a signal can be delivered to an application; doing so stops the application from whatever it is doing to run a\n   **signal handler**\n   , i.e., some code in the application to handle that signal. When finished, the process just resumes its previous behavior.\n\n\nEach signal has a name, such as\n   **HUP**\n   (hang up),\n   **INT**\n   (interrupt),\n   **SEGV**\n   (segmentation violation), etc.; see the man page for details. Interestingly, sometimes it is the kernel itself that does the signaling. For example, when your program encounters a segmentation violation, the OS sends it a\n   **SIGSEGV**\n   (prepending\n   **SIG**\n   to signal names is common); if your program is configured to catch that signal, you can actually run some code in response to this erroneous program behavior (which is helpful for debugging). When a signal is sent to a process not configured to handle a signal, the default behavior is enacted; for\n   **SEGV**\n   , the process is killed.\n\n\nHere is a simple program that goes into an infinite loop, but has first set up a signal handler to catch\n   **SIGHUP**\n   :\n\n\nvoid handle(int arg) {\n    printf(\"stop wakin' me up...\\n\");\n}\n\nint main(int argc, char *argv[]) {\n    signal(SIGHUP, handle);\n    while (1)\n        ; // doin' nothin' except catchin' some sigs\n    return 0;\n}\nYou can send signals to it with the\n   **kill**\n   command line tool (yes, this is an odd and aggressive name). Doing so will interrupt the main while loop in the program and run the handler code\n   \n    handle()\n   \n   :\n\n\nprompt> ./main &\n[3] 36705\nprompt> kill -HUP 36705\nstop wakin' me up...\nprompt> kill -HUP 36705\nstop wakin' me up...\nThere is a lot more to learn about signals, so much that a single chapter, much less a single page, does not nearly suffice. As always, there is one great source: Stevens and Rago [SR05]. Read more if interested.\n\n\nThe solution, as described by Adya et al. [A+02], is to use an old programming language construct known as a\n   **continuation**\n   [FHK84]. Though it sounds complicated, the idea is rather simple: basically, record the needed information to finish processing this event in some data structure; when the event happens (i.e., when the disk I/O completes), look up the needed information and process the event.\n\n\nIn this specific case, the solution would be to record the socket descriptor (\n   \n    sd\n   \n   ) in some kind of data structure (e.g., a hash table), indexed by the file descriptor (\n   \n    fd\n   \n   ). When the disk I/O completes, the event handler would use the file descriptor to look up the continuation, which will return the value of the socket descriptor to the caller. At this point (finally), the server can then do the last bit of work to write the data to the socket."
        },
        {
          "name": "What Is Still DifficultWith Events",
          "content": "There are a few other difficulties with the event-based approach that we should mention. For example, when systems moved from a single CPU to multiple CPUs, some of the simplicity of the event-based approach disappeared. Specifically, in order to utilize more than one CPU, the event server has to run multiple event handlers in parallel; when doing so, the usual synchronization problems (e.g., critical sections) arise, and the usual solutions (e.g., locks) must be employed. Thus, on modern multicore systems, simple event handling without locks is no longer possible.\n\n\nAnother problem with the event-based approach is that it does not integrate well with certain kinds of systems activity, such as\n   **paging**\n   . For example, if an event-handler page faults, it will block, and thus the server will not make progress until the page fault completes. Even though the server has been structured to avoid\n   *explicit*\n   blocking, this type of\n   *implicit*\n   blocking due to page faults is hard to avoid and thus can lead to large performance problems when prevalent.\n\n\nA third issue is that event-based code can be hard to manage over time, as the exact semantics of various routines changes [A+02]. For example, if a routine changes from non-blocking to blocking, the event handler that calls that routine must also change to accommodate its new nature, by ripping itself into two pieces. Because blocking is so disastrous for event-based servers, a programmer must always be on the lookout for such changes in the semantics of the APIs each event uses.\n\n\nFinally, though asynchronous disk I/O is now possible on most platforms, it has taken a long time to get there [PDZ99], and it never quite integrates with asynchronous network I/O in as simple and uniform a manner as you might think. For example, while one would simply like to use the\n   \n    select()\n   \n   interface to manage all outstanding I/Os, usually some combination of\n   \n    select()\n   \n   for networking and the AIO calls for disk I/O are required."
        }
      ]
    },
    {
      "name": "I/O Devices",
      "sections": [
        {
          "name": "System Architecture",
          "content": "To begin our discussion, let's look at a \"classical\" diagram of a typical system (Figure 36.1, page 2). The picture shows a single CPU attached to the main memory of the system via some kind of\n   **memory bus**\n   or interconnect. Some devices are connected to the system via a general\n   **I/O bus**\n   , which in many modern systems would be\n   **PCI**\n   (or one of its many derivatives); graphics and some other higher-performance I/O devices might be found here. Finally, even lower down are one or more of what we call a\n   **peripheral bus**\n   , such as\n   **SCSI**\n   ,\n   **SATA**\n   , or\n   **USB**\n   . These connect slow devices to the system, including\n   **disks**\n   ,\n   **mice**\n   , and\n   **keyboards**\n   .\n\n\nOne question you might ask is: why do we need a hierarchical structure like this? Put simply: physics, and cost. The faster a bus is, the shorter it must be; thus, a high-performance memory bus does not have much room to plug devices and such into it. In addition, engineering a bus for high performance is quite costly. Thus, system designers have adopted this hierarchical approach, where components that demand high performance (such as the graphics card) are nearer the CPU. Lower performance components are further away. The benefits of placing disks and other slow devices on a peripheral bus are manifold; in particular, you can place a large number of devices on it.\n\n\n\n\n![Diagram of a prototypical system architecture showing CPU, Memory, Graphics, and Peripheral I/O Bus connections.](images/image_0083.jpeg)\n\n\nThe diagram illustrates a prototypical system architecture. At the top, a CPU block and a Memory block are connected to a central vertical bus. The CPU connects to a horizontal line labeled 'Memory Bus (proprietary)'. The Memory block connects to a horizontal line labeled 'General I/O Bus (e.g., PCI)'. Below the central vertical bus is a 'Graphics' block. The central vertical bus connects to a horizontal line labeled 'Peripheral I/O Bus (e.g., SCSI, SATA, USB)'. Four cylinder-shaped icons representing storage devices are connected to the 'Peripheral I/O Bus'.\n\n\nDiagram of a prototypical system architecture showing CPU, Memory, Graphics, and Peripheral I/O Bus connections.\n\n\nFigure 36.1:\n   **Prototypical System Architecture**\n\n\nOf course, modern systems increasingly use specialized chipsets and faster point-to-point interconnects to improve performance. Figure 36.2 (page 3) shows an approximate diagram of Intel's Z270 Chipset [H17]. Along the top, the CPU connects most closely to the memory system, but also has a high-performance connection to the graphics card (and thus, the display) to enable gaming (oh, the horror!) and other graphics-intensive applications.\n\n\nThe CPU connects to an I/O chip via Intel's proprietary\n   **DMI (Direct Media Interface)**\n   , and the rest of the devices connect to this chip via a number of different interconnects. On the right, one or more hard drives connect to the system via the\n   **eSATA**\n   interface;\n   **ATA**\n   (the\n   **AT Attachment**\n   , in reference to providing connection to the IBM PC AT), then\n   **SATA**\n   (for\n   **Serial ATA**\n   ), and now\n   **eSATA**\n   (for\n   **external SATA**\n   ) represent an evolution of storage interfaces over the past decades, with each step forward increasing performance to keep pace with modern storage devices.\n\n\nBelow the I/O chip are a number of\n   **USB (Universal Serial Bus)**\n   connections, which in this depiction enable a keyboard and mouse to be attached to the computer. On many modern systems, USB is used for low performance devices such as these.\n\n\nFinally, on the left, other higher performance devices can be connected\n\n\n\n\n![Figure 36.2: Modern System Architecture. This block diagram illustrates the internal components and their interconnections in a modern computer system. At the top, a 'CPU' block is connected to a 'Graphics' block via a 'PCIe Graphics' bus, and to a 'Memory' block via a 'Memory Interconnect' bus. Below the CPU, a 'DMI' (Direct Media Interface) bus connects the CPU to an 'I/O Chip' block. The 'I/O Chip' is the central hub for peripheral devices. It connects to a 'Network' block via a 'PCIe' bus. It connects to a 'Disk' block via an 'eSATA' bus. It connects to a 'Keyboard' and a 'Mouse' via a 'USB' bus. There are also three smaller 'Di' blocks (likely representing other disk or storage controllers) connected to the I/O Chip via a 'USB' bus.](images/image_0084.jpeg)\n\n\nFigure 36.2: Modern System Architecture. This block diagram illustrates the internal components and their interconnections in a modern computer system. At the top, a 'CPU' block is connected to a 'Graphics' block via a 'PCIe Graphics' bus, and to a 'Memory' block via a 'Memory Interconnect' bus. Below the CPU, a 'DMI' (Direct Media Interface) bus connects the CPU to an 'I/O Chip' block. The 'I/O Chip' is the central hub for peripheral devices. It connects to a 'Network' block via a 'PCIe' bus. It connects to a 'Disk' block via an 'eSATA' bus. It connects to a 'Keyboard' and a 'Mouse' via a 'USB' bus. There are also three smaller 'Di' blocks (likely representing other disk or storage controllers) connected to the I/O Chip via a 'USB' bus.\n\n\nFigure 36.2:\n   **Modern System Architecture**\n\n\nto the system via\n   **PCIe**\n   (\n   **P**\n   eripheral\n   **C**\n   omponent\n   **I**\n   nterconnect\n   **E**\n   xpress). In this diagram, a network interface is attached to the system here; higher performance storage devices (such as\n   **NVMe**\n   persistent storage devices) are often connected here."
        },
        {
          "name": "A Canonical Device",
          "content": "Let us now look at a canonical device (not a real one), and use this device to drive our understanding of some of the machinery required to make device interaction efficient. From Figure 36.3 (page 4), we can see that a device has two important components. The first is the hardware\n   **interface**\n   it presents to the rest of the system. Just like a piece of software, hardware must also present some kind of interface that allows the system software to control its operation. Thus, all devices have some specified interface and protocol for typical interaction.\n\n\nThe second part of any device is its\n   **internal structure**\n   . This part of the device is implementation specific and is responsible for implementing the abstraction the device presents to the system. Very simple devices will have one or a few hardware chips to implement their functionality; more complex devices will include a simple CPU, some general purpose memory, and other device-specific chips to get their job done. For example, modern RAID controllers might consist of hundreds of thousands of lines of\n   **firmware**\n   (i.e., software within a hardware device) to implement its functionality.\n\n\n\n\n![Diagram of a Canonical Device showing its internal components and interface registers.](images/image_0085.jpeg)\n\n\nThe diagram illustrates a Canonical Device. It is divided into two main sections: 'Interface' and 'Internals'. The 'Interface' section at the top contains four registers: 'Registers' (a label for a group of four boxes), 'Status', 'Command', and 'Data'. The 'Internals' section below is separated by a dashed line and contains 'Micro-controller (CPU)', 'Memory (DRAM or SRAM or both)', and 'Other Hardware-specific Chips'.\n\n\nDiagram of a Canonical Device showing its internal components and interface registers.\n\n\nFigure 36.3: A Canonical Device"
        },
        {
          "name": "The Canonical Protocol",
          "content": "In the picture above, the (simplified) device interface is comprised of three registers: a\n   **status**\n   register, which can be read to see the current status of the device; a\n   **command**\n   register, to tell the device to perform a certain task; and a\n   **data**\n   register to pass data to the device, or get data from the device. By reading and writing these registers, the operating system can control device behavior.\n\n\nLet us now describe a typical interaction that the OS might have with the device in order to get the device to do something on its behalf. The protocol is as follows:\n\n\nWhile (STATUS == BUSY)\n  ; // wait until device is not busy\nWrite data to DATA register\nWrite command to COMMAND register\n  (starts the device and executes the command)\nWhile (STATUS == BUSY)\n  ; // wait until device is done with your request\nThe protocol has four steps. In the first, the OS waits until the device is ready to receive a command by repeatedly reading the status register; we call this\n   **polling**\n   the device (basically, just asking it what is going on). Second, the OS sends some data down to the data register; one can imagine that if this were a disk, for example, that multiple writes would need to take place to transfer a disk block (say 4KB) to the device. When the main CPU is involved with the data movement (as in this example protocol), we refer to it as\n   **programmed I/O (PIO)**\n   . Third, the OS writes a command to the command register; doing so implicitly lets the device know that both the data is present and that it should begin working on the command. Finally, the OS waits for the device to finish by again polling it in a loop, waiting to see if it is finished (it may then get an error code to indicate success or failure).\n\n\nThis basic protocol has the positive aspect of being simple and working. However, there are some inefficiencies and inconveniences involved. The first problem you might notice in the protocol is that polling seems inefficient; specifically, it wastes a great deal of CPU time just waiting for the (potentially slow) device to complete its activity, instead of switching to another ready process and thus better utilizing the CPU.\n\n\n\n\n**THE CRUX: HOW TO AVOID THE COSTS OF POLLING**\n\n\nHow can the OS check device status without frequent polling, and thus lower the CPU overhead required to manage the device?"
        },
        {
          "name": "Lowering CPU OverheadWith Interrupts",
          "content": "The invention that many engineers came upon years ago to improve this interaction is something we've seen already: the\n   **interrupt**\n   . Instead of polling the device repeatedly, the OS can issue a request, put the calling process to sleep, and context switch to another task. When the device is finally finished with the operation, it will raise a hardware interrupt, causing the CPU to jump into the OS at a predetermined\n   **interrupt service routine (ISR)**\n   or more simply an\n   **interrupt handler**\n   . The handler is just a piece of operating system code that will finish the request (for example, by reading data and perhaps an error code from the device) and wake the process waiting for the I/O, which can then proceed as desired.\n\n\nInterrupts thus allow for\n   **overlap**\n   of computation and I/O, which is key for improved utilization. This timeline shows the problem:\n\n\n\n\n![](images/image_0086.jpeg)\n\n\nCPU | 1 | 1 | 1 | 1 | 1 | p | p | p | p | p | 1 | 1 | 1 | 1 | 1\nDisk |  |  |  |  |  | 1 | 1 | 1 | 1 | 1 |  |  |  |  |\n\n\nIn the diagram, Process 1 runs on the CPU for some time (indicated by a repeated 1 on the CPU line), and then issues an I/O request to the disk to read some data. Without interrupts, the system simply spins, polling the status of the device repeatedly until the I/O is complete (indicated by a p). The disk services the request and finally Process 1 can run again.\n\n\nIf instead we utilize interrupts and allow for overlap, the OS can do something else while waiting for the disk:\n\n\n\n\n![](images/image_0087.jpeg)\n\n\nCPU | 1 | 1 | 1 | 1 | 1 | 2 | 2 | 2 | 2 | 2 | 1 | 1 | 1 | 1 | 1\nDisk |  |  |  |  |  | 1 | 1 | 1 | 1 | 1 |  |  |  |  |\n\n\nIn this example, the OS runs Process 2 on the CPU while the disk services Process 1's request. When the disk request is finished, an interrupt occurs, and the OS wakes up Process 1 and runs it again. Thus,\n   *both*\n   the CPU and the disk are properly utilized during the middle stretch of time.\n\n\nNote that using interrupts is not\n   *always*\n   the best solution. For example, imagine a device that performs its tasks very quickly: the first poll usually finds the device to be done with task. Using an interrupt in this case will actually\n   *slow down*\n   the system: switching to another process, handling the interrupt, and switching back to the issuing process is expensive. Thus, if a device is fast, it may be best to poll; if it is slow, interrupts, which allow\n\n\n**TIP: INTERRUPTS NOT ALWAYS BETTER THAN POLLING**\nAlthough interrupts allow for overlap of computation and I/O, they only really make sense for slow devices. Otherwise, the cost of interrupt handling and context switching may outweigh the benefits interrupts provide. There are also cases where a flood of interrupts may overload a system and lead it to livelock [MR96]; in such cases, polling provides more control to the OS in its scheduling and thus is again useful.\n\n\noverlap, are best. If the speed of the device is not known, or sometimes fast and sometimes slow, it may be best to use a\n   **hybrid**\n   that polls for a little while and then, if the device is not yet finished, uses interrupts. This\n   **two-phased**\n   approach may achieve the best of both worlds.\n\n\nAnother reason not to use interrupts arises in networks [MR96]. When a huge stream of incoming packets each generate an interrupt, it is possible for the OS to\n   **livelock**\n   , that is, find itself only processing interrupts and never allowing a user-level process to run and actually service the requests. For example, imagine a web server that experiences a load burst because it became the top-ranked entry on hacker news [H18]. In this case, it is better to occasionally use polling to better control what is happening in the system and allow the web server to service some requests before going back to the device to check for more packet arrivals.\n\n\nAnother interrupt-based optimization is\n   **coalescing**\n   . In such a setup, a device which needs to raise an interrupt first waits for a bit before delivering the interrupt to the CPU. While waiting, other requests may soon complete, and thus multiple interrupts can be coalesced into a single interrupt delivery, thus lowering the overhead of interrupt processing. Of course, waiting too long will increase the latency of a request, a common trade-off in systems. See Ahmad et al. [A+11] for an excellent summary."
        },
        {
          "name": "More Efficient Data MovementWith DMA",
          "content": "Unfortunately, there is one other aspect of our canonical protocol that requires our attention. In particular, when using programmed I/O (PIO) to transfer a large chunk of data to a device, the CPU is once again overburdened with a rather trivial task, and thus wastes a lot of time and effort that could better be spent running other processes. This timeline illustrates the problem:\n\n\n\n\n![](images/image_0088.jpeg)\n\n\nCPU | 1 | 1 | 1 | 1 | 1 | c | c | c | 2 | 2 | 2 | 2 | 2 | 1 | 1\nDisk |  |  |  |  |  |  |  |  |  | 1 | 1 | 1 | 1 | 1\n\n\nIn the timeline, Process 1 is running and then wishes to write some data to the disk. It then initiates the I/O, which must copy the data from memory to the device explicitly, one word at a time (marked\n   *c*\n   in the diagram). When the copy is complete, the I/O begins on the disk and the CPU can finally be used for something else.\n\n\n**THE CRUX: HOW TO LOWER PIO OVERHEADS**\nWith PIO, the CPU spends too much time moving data to and from devices by hand. How can we offload this work and thus allow the CPU to be more effectively utilized?\n\n\nThe solution to this problem is something we refer to as\n   **Direct Memory Access (DMA)**\n   . A DMA engine is essentially a very specific device within a system that can orchestrate transfers between devices and main memory without much CPU intervention.\n\n\nDMA works as follows. To transfer data to the device, for example, the OS would program the DMA engine by telling it where the data lives in memory, how much data to copy, and which device to send it to. At that point, the OS is done with the transfer and can proceed with other work. When the DMA is complete, the DMA controller raises an interrupt, and the OS thus knows the transfer is complete. The revised timeline:\n\n\n\n\n![](images/image_0089.jpeg)\n\n\nCPU | 1 | 1 | 1 | 1 | 1 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 1 | 1\nDMA |  |  |  |  |  | C | C | C |  |  |  |  |  |  | \nDisk |  |  |  |  |  |  |  |  | 1 | 1 | 1 | 1 | 1 |  |\n\n\nFrom the timeline, you can see that the copying of data is now handled by the DMA controller. Because the CPU is free during that time, the OS can do something else, here choosing to run Process 2. Process 2 thus gets to use more CPU before Process 1 runs again."
        },
        {
          "name": "Methods Of Device Interaction",
          "content": "Now that we have some sense of the efficiency issues involved with performing I/O, there are a few other problems we need to handle to incorporate devices into modern systems. One problem you may have noticed thus far: we have not really said anything about how the OS actually communicates with the device! Thus, the problem:\n\n\n**THE CRUX: HOW TO COMMUNICATE WITH DEVICES**\nHow should the hardware communicate with a device? Should there be explicit instructions? Or are there other ways to do it?\n\n\nOver time, two primary methods of device communication have developed. The first, oldest method (used by IBM mainframes for many years) is to have explicit\n   **I/O instructions**\n   . These instructions specify a way for the OS to send data to specific device registers and thus allow the construction of the protocols described above.\n\n\nFor example, on x86, the\n   \n    in\n   \n   and\n   \n    out\n   \n   instructions can be used to communicate with devices. For example, to send data to a device, the caller specifies a register with the data in it, and a specific\n   *port*\n   which names the device. Executing the instruction leads to the desired behavior.\n\n\nSuch instructions are usually\n   **privileged**\n   . The OS controls devices, and the OS thus is the only entity allowed to directly communicate with them. Imagine if any program could read or write the disk, for example: total chaos (as always), as any user program could use such a loophole to gain complete control over the machine.\n\n\nThe second method to interact with devices is known as\n   **memory-mapped I/O**\n   . With this approach, the hardware makes device registers available as if they were memory locations. To access a particular register, the OS issues a load (to read) or store (to write) the address; the hardware then routes the load/store to the device instead of main memory.\n\n\nThere is not some great advantage to one approach or the other. The memory-mapped approach is nice in that no new instructions are needed to support it, but both approaches are still in use today."
        },
        {
          "name": "Fitting Into The OS: The Device Driver",
          "content": "One final problem we will discuss: how to fit devices, each of which have very specific interfaces, into the OS, which we would like to keep as general as possible. For example, consider a file system. We'd like to build a file system that worked on top of SCSI disks, IDE disks, USB keychain drives, and so forth, and we'd like the file system to be relatively oblivious to all of the details of how to issue a read or write request to these different types of drives. Thus, our problem:\n\n\n\n\n**THE CRUX: HOW TO BUILD A DEVICE-NEUTRAL OS**\n\n\nHow can we keep most of the OS device-neutral, thus hiding the details of device interactions from major OS subsystems?\n\n\nThe problem is solved through the age-old technique of\n   **abstraction**\n   . At the lowest level, a piece of software in the OS must know in detail how a device works. We call this piece of software a\n   **device driver**\n   , and any specifics of device interaction are encapsulated within.\n\n\nLet us see how this abstraction might help OS design and implementation by examining the Linux file system software stack. Figure 36.4 is a rough and approximate depiction of the Linux software organization. As you can see from the diagram, a file system (and certainly, an application above) is completely oblivious to the specifics of which disk class it is using; it simply issues block read and write requests to the generic block layer, which routes them to the appropriate device driver, which handles the details of issuing the specific request. Although simplified, the diagram shows how such detail can be hidden from most of the OS.\n\n\n\n\n![Diagram of the File System Stack showing the layers from Application down to Device Driver, with a raw interface bypassing the File System layer.](images/image_0090.jpeg)\n\n\nThe diagram illustrates the File System Stack, showing the layers of software that manage I/O operations. It is divided into two main sections:\n    **user**\n    (top) and\n    **kernel mode**\n    (bottom), separated by a horizontal line.\n\n\n  * **Application**\n     (user mode)\n  * **POSIX API [open, read, write, close, etc.]**\n     (user mode)\n  * **File System**\n     (kernel mode) - This layer is bypassed by a\n     **Raw**\n     interface.\n  * **Generic Block Interface [block read/write]**\n     (kernel mode)\n  * **Generic Block Layer**\n     (kernel mode)\n  * **Specific Block Interface [protocol-specific read/write]**\n     (kernel mode)\n  * **Device Driver [SCSI, ATA, etc.]**\n     (kernel mode)\n\n\nA vertical line labeled\n    **Raw**\n    connects the\n    **POSIX API**\n    layer directly to the\n    **Generic Block Interface**\n    layer, bypassing the\n    **File System**\n    layer.\n\n\nDiagram of the File System Stack showing the layers from Application down to Device Driver, with a raw interface bypassing the File System layer.\n\n\nFigure 36.4:\n   **The File System Stack**\n\n\nThe diagram also shows a\n   **raw interface**\n   to devices, which enables special applications (such as a\n   **file-system checker**\n   , described later [AD14], or a\n   **disk defragmentation**\n   tool) to directly read and write blocks without using the file abstraction. Most systems provide this type of interface to support these low-level storage management applications.\n\n\nNote that the encapsulation seen above can have its downside as well. For example, if there is a device that has many special capabilities, but has to present a generic interface to the rest of the kernel, those special capabilities will go unused. This situation arises, for example, in Linux with SCSI devices, which have very rich error reporting; because other block devices (e.g., ATA/IDE) have much simpler error handling, all that higher levels of software ever receive is a generic\n   **EIO**\n   (generic IO error) error code; any extra detail that SCSI may have provided is thus lost to the file system [G08].\n\n\nInterestingly, because device drivers are needed for any device you might plug into your system, over time they have come to represent a huge percentage of kernel code. Studies of the Linux kernel reveal that over 70% of OS code is found in device drivers [C01]; for Windows-based systems, it is likely quite high as well. Thus, when people tell you that the OS has millions of lines of code, what they are really saying is that the OS has millions of lines of device-driver code. Of course, for any given installation, most of that code may not be active (i.e., only a few devices are connected to the system at a time). Perhaps more depressingly, as drivers are often written by “amateurs” (instead of full-time kernel developers), they tend to have many more bugs and thus are a primary contributor to kernel crashes [S03]."
        },
        {
          "name": "Case Study: A Simple IDE Disk Driver",
          "content": "To dig a little deeper here, let's take a quick look at an actual device: an IDE disk drive [L94]. We summarize the protocol as described in this reference [W10]; we'll also peek at the xv6 source code for a simple example of a working IDE driver [CK+08].\n\n\nControl Register:\n   \n\n   Address 0x3F6 = 0x08 (0000 1RE0): R=reset,\n   \n\n   E=0 means \"enable interrupt\"\n\n\nCommand Block Registers:\n   \n\n   Address 0x1F0 = Data Port\n   \n\n   Address 0x1F1 = Error\n   \n\n   Address 0x1F2 = Sector Count\n   \n\n   Address 0x1F3 = LBA low byte\n   \n\n   Address 0x1F4 = LBA mid byte\n   \n\n   Address 0x1F5 = LBA hi byte\n   \n\n   Address 0x1F6 = 1B1D TOP4LBA: B=LBA, D=drive\n   \n\n   Address 0x1F7 = Command/status\n\n\nStatus Register (Address 0x1F7):\n   \n\n   7   6   5   4   3   2   1   0\n   \n\n   BUSY READY FAULT SEEK DRQ CORR IDDEX ERROR\n\n\nError Register (Address 0x1F1): (check when ERROR==1)\n   \n\n   7   6   5   4   3   2   1   0\n   \n\n   BBK UNC MC IDNF MCR ABRT TONF AMNF\n\n\nBBK = Bad Block\n   \n\n   UNC = Uncorrectable data error\n   \n\n   MC = Media Changed\n   \n\n   IDNF = ID mark Not Found\n   \n\n   MCR = Media Change Requested\n   \n\n   ABRT = Command aborted\n   \n\n   TONF = Track 0 Not Found\n   \n\n   AMNF = Address Mark Not Found\n\n\nFigure 36.5: The IDE Interface\n\n\nAn IDE disk presents a simple interface to the system, consisting of four types of register: control, command block, status, and error. These registers are available by reading or writing to specific \"I/O addresses\" (such as 0x3F6 below) using (on x86) the\n   \n    in\n   \n   and\n   \n    out\n   \n   I/O instructions.\n\n\nThe basic protocol to interact with the device is as follows, assuming it has already been initialized.\n\n\n  * •\n    **Wait for drive to be ready.**\n    Read Status Register (0x1F7) until drive is READY and not BUSY.\n  * •\n    **Write parameters to command registers.**\n    Write the sector count, logical block address (LBA) of the sectors to be accessed, and drive number (master=0x00 or slave=0x10, as IDE permits just two drives) to command registers (0x1F2-0x1F6).\n  * •\n    **Start the I/O.**\n    Write READ | WRITE command to command register (0x1F7).\n\n\n  * •\n    **Data transfer (for writes):**\n    Wait until drive status is READY and DRQ (drive request for data); write data to data port.\n  * •\n    **Handle interrupts.**\n    In the simplest case, handle an interrupt for each sector transferred; more complex approaches allow batching and thus one final interrupt when the entire transfer is complete.\n  * •\n    **Error handling.**\n    After each operation, read the status register. If the ERROR bit is on, read the error register for details.\n\n\nMost of this protocol is found in the xv6 IDE driver (Figure 36.6), which (after initialization) works through four primary functions. The first is\n   \n    ide_rw()\n   \n   , which queues a request (if there are others pending), or issues it directly to the disk (via\n   \n    ide_start_request()\n   \n   ); in either case, the routine waits for the request to complete and the calling process is put to sleep. The second is\n   \n    ide_start_request()\n   \n   , which is used to send a request (and perhaps data, in the case of a write) to the disk; the\n   \n    in\n   \n   and\n   \n    out\n   \n   x86 instructions are called to read and write device registers, respectively. The start request routine uses the third function,\n   \n    ide_wait_ready()\n   \n   , to ensure the drive is ready before issuing a request to it. Finally,\n   \n    ide_intr()\n   \n   is invoked when an interrupt takes place; it reads data from the device (if the request is a read, not a write), wakes the process waiting for the I/O to complete, and (if there are more requests in the I/O queue), launches the next I/O via\n   \n    ide_start_request()\n   \n   ."
        },
        {
          "name": "Historical Notes",
          "content": "Before ending, we include a brief historical note on the origin of some of these fundamental ideas. If you are interested in learning more, read Smotherman’s excellent summary [S08].\n\n\nInterrupts are an ancient idea, existing on the earliest of machines. For example, the UNIVAC in the early 1950’s had some form of interrupt vectoring, although it is unclear in exactly which year this feature was available [S08]. Sadly, even in its infancy, we are beginning to lose the origins of computing history.\n\n\nThere is also some debate as to which machine first introduced the idea of DMA. For example, Knuth and others point to the DYSEAC (a “mobile” machine, which at the time meant it could be hauled in a trailer), whereas others think the IBM SAGE may have been the first [S08]. Either way, by the mid 50’s, systems with I/O devices that communicated directly with memory and interrupted the CPU when finished existed.\n\n\nThe history here is difficult to trace because the inventions are tied to real, and sometimes obscure, machines. For example, some think that the Lincoln Labs TX-2 machine was first with vectored interrupts [S08], but this is hardly clear.\n\n\nstatic int ide_wait_ready() {\n    while (((int r = inb(0x1f7)) & IDE_BSY) || !(r & IDE_DRDY))\n        ; // loop until drive isn't busy\n    // return -1 on error, or 0 otherwise\n}\n\nstatic void ide_start_request(struct buf *b) {\n    ide_wait_ready();\n    outb(0x3f6, 0);           // generate interrupt\n    outb(0x1f2, 1);           // how many sectors?\n    outb(0x1f3, b->sector & 0xff); // LBA goes here ...\n    outb(0x1f4, (b->sector >> 8) & 0xff); // ... and here\n    outb(0x1f5, (b->sector >> 16) & 0xff); // ... and here!\n    outb(0x1f6, 0xe0 | ((b->dev&1) << 4) | ((b->sector >> 24) & 0x0f));\n    if(b->flags & B_DIRTY){\n        outb(0x1f7, IDE_CMD_WRITE); // this is a WRITE\n        outsl(0x1f0, b->data, 512/4); // transfer data too!\n    } else {\n        outb(0x1f7, IDE_CMD_READ); // this is a READ (no data)\n    }\n}\n\nvoid ide_rw(struct buf *b) {\n    acquire(&ide_lock);\n    for (struct buf **pp = &ide_queue; *pp; pp=&(*pp)->qnext)\n        ; // walk queue\n    *pp = b; // add request to end\n    if (ide_queue == b) // if q is empty\n        ide_start_request(b);\n    while ((b->flags & (B_VALID|B_DIRTY)) != B_VALID)\n        sleep(b, &ide_lock); // wait for completion\n    release(&ide_lock);\n}\n\nvoid ide_intr() {\n    struct buf *b;\n    acquire(&ide_lock);\n    if (!(b->flags & B_DIRTY) && ide_wait_ready() >= 0)\n        insl(0x1f0, b->data, 512/4); // if READ: get data\n    b->flags |= B_VALID;\n    b->flags &= ~B_DIRTY;\n    wakeup(b); // wake waiting process\n    if ((ide_queue = b->qnext) != 0) // start next request\n        ide_start_request(ide_queue); // (if one exists)\n    release(&ide_lock);\n}\n\nFigure 36.6: The xv6 IDE Disk Driver (Simplified)\n\n\nBecause the ideas are relatively obvious — no Einsteinian leap is required to come up with the idea of letting the CPU do something else while a slow I/O is pending — perhaps our focus on “who first?” is misguided. What is certainly clear: as people built these early machines, it became obvious that I/O support was needed. Interrupts, DMA, and related ideas are all direct outcomes of the nature of fast CPUs and slow devices; if you were there at the time, you might have had similar ideas."
        }
      ]
    },
    {
      "name": "Hard Disk Drives",
      "sections": [
        {
          "name": "The Interface",
          "content": "Let's start by understanding the interface to a modern disk drive. The basic interface for all modern drives is straightforward. The drive consists of a large number of sectors (512-byte blocks), each of which can be read or written. The sectors are numbered from 0 to\n   \n    n - 1\n   \n   on a disk with\n   \n    n\n   \n   sectors. Thus, we can view the disk as an array of sectors; 0 to\n   \n    n - 1\n   \n   is thus the\n   **address space**\n   of the drive.\n\n\nMulti-sector operations are possible; indeed, many file systems will read or write 4KB at a time (or more). However, when updating the disk, the only guarantee drive manufacturers make is that a single 512-byte write is\n   **atomic**\n   (i.e., it will either complete in its entirety or it won't complete at all); thus, if an untimely power loss occurs, only a portion of a larger write may complete (sometimes called a\n   **torn write**\n   ).\n\n\nThere are some assumptions most clients of disk drives make, but that are not specified directly in the interface; Schlosser and Ganger have\n\n\n\n\n![Diagram of a disk with a single track. It shows a central spindle with concentric circles representing tracks. The outermost circle is labeled with numbers 1 through 11 around its circumference, representing sectors. The inner circle is labeled 'Spindle'.](images/image_0091.jpeg)\n\n\nDiagram of a disk with a single track. It shows a central spindle with concentric circles representing tracks. The outermost circle is labeled with numbers 1 through 11 around its circumference, representing sectors. The inner circle is labeled 'Spindle'.\n\n\nFigure 37.1: A Disk With Just A Single Track\n\n\ncalled this the “unwritten contract” of disk drives [SG04]. Specifically, one can usually assume that accessing two blocks\n   \n    1\n   \n   near one-another within the drive’s address space will be faster than accessing two blocks that are far apart. One can also usually assume that accessing blocks in a contiguous chunk (i.e., a sequential read or write) is the fastest access mode, and usually much faster than any more random access pattern."
        },
        {
          "name": "Basic Geometry",
          "content": "Let’s start to understand some of the components of a modern disk. We start with a\n   **platter**\n   , a circular hard surface on which data is stored persistently by inducing magnetic changes to it. A disk may have one or more platters; each platter has 2 sides, each of which is called a\n   **surface**\n   . These platters are usually made of some hard material (such as aluminum), and then coated with a thin magnetic layer that enables the drive to persistently store bits even when the drive is powered off.\n\n\nThe platters are all bound together around the\n   **spindle**\n   , which is connected to a motor that spins the platters around (while the drive is powered on) at a constant (fixed) rate. The rate of rotation is often measured in\n   **rotations per minute (RPM)**\n   , and typical modern values are in the 7,200 RPM to 15,000 RPM range. Note that we will often be interested in the time of a single rotation, e.g., a drive that rotates at 10,000 RPM means that a single rotation takes about 6 milliseconds (6 ms).\n\n\nData is encoded on each surface in concentric circles of sectors; we call one such concentric circle a\n   **track**\n   . A single surface contains many thousands and thousands of tracks, tightly packed together, with hundreds of tracks fitting into the width of a human hair.\n\n\nTo read and write from the surface, we need a mechanism that allows us to either sense (i.e., read) the magnetic patterns on the disk or to induce a change in (i.e., write) them. This process of reading and writing is accomplished by the\n   **disk head**\n   ; there is one such head per surface of the drive. The disk head is attached to a single\n   **disk arm**\n   , which moves across the surface to position the head over the desired track.\n\n\n1\n   \n   We, and others, often use the terms\n   **block**\n   and\n   **sector**\n   interchangeably, assuming the reader will know exactly what is meant per context. Sorry about this!\n\n\n\n\n![Diagram of a single track on a disk platter. The platter is a circle with a central spindle. A single track is shown as a ring of 12 sectors numbered 0 through 11 clockwise. A disk head is attached to a disk arm, positioned over sector 6. An arrow at the top indicates the platter rotates counter-clockwise.](images/image_0092.jpeg)\n\n\nDiagram of a single track on a disk platter. The platter is a circle with a central spindle. A single track is shown as a ring of 12 sectors numbered 0 through 11 clockwise. A disk head is attached to a disk arm, positioned over sector 6. An arrow at the top indicates the platter rotates counter-clockwise.\n\n\nFigure 37.2: A Single Track Plus A Head"
        },
        {
          "name": "A Simple Disk Drive",
          "content": "Let's understand how disks work by building up a model one track at a time. Assume we have a simple disk with a single track (Figure 37.1). This track has just 12 sectors, each of which is 512 bytes in size (our typical sector size, recall) and addressed therefore by the numbers 0 through 11. The single platter we have here rotates around the spindle, to which a motor is attached.\n\n\nOf course, the track by itself isn't too interesting; we want to be able to read or write those sectors, and thus we need a disk head, attached to a disk arm, as we now see (Figure 37.2). In the figure, the disk head, attached to the end of the arm, is positioned over sector 6, and the surface is rotating counter-clockwise.\n\n\n\n\n**Single-track Latency: The Rotational Delay**\n\n\nTo understand how a request would be processed on our simple, one-track disk, imagine we now receive a request to read block 0. How should the disk service this request?\n\n\nIn our simple disk, the disk doesn't have to do much. In particular, it must just wait for the desired sector to rotate under the disk head. This wait happens often enough in modern drives, and is an important enough component of I/O service time, that it has a special name:\n   **rotational delay**\n   (sometimes\n   **rotation delay**\n   , though that sounds weird). In the example, if the full rotational delay is\n   \n    R\n   \n   , the disk has to incur a rotational delay of about\n   \n    \\frac{R}{2}\n   \n   to wait for 0 to come under the read/write head (if we start at 6). A worst-case request on this single track would be to sector 5, causing nearly a full rotational delay in order to service such a request.\n\n\n\n\n**Multiple Tracks: Seek Time**\n\n\nSo far our disk just has a single track, which is not too realistic; modern disks of course have many millions. Let's thus look at an ever-so-slightly more realistic disk surface, this one with three tracks (Figure 37.3, left).\n\n\nIn the figure, the head is currently positioned over the innermost track (which contains sectors 24 through 35); the next track over contains the\n\n\n\n\n![Figure 37.3: Two diagrams of a disk platter showing tracks and sectors. The left diagram shows three tracks with sectors numbered 0-35. The right diagram shows the same platter with a disk arm moving from track 12 to track 9, labeled 'Seek'. A label 'Rotates this way' with an arrow indicates counter-clockwise rotation. A label 'Remaining rotation' points to the sectors between the current head position and the target sector.](images/image_0093.jpeg)\n\n\nFigure 37.3: Two diagrams of a disk platter showing tracks and sectors. The left diagram shows three tracks with sectors numbered 0-35. The right diagram shows the same platter with a disk arm moving from track 12 to track 9, labeled 'Seek'. A label 'Rotates this way' with an arrow indicates counter-clockwise rotation. A label 'Remaining rotation' points to the sectors between the current head position and the target sector.\n\n\nFigure 37.3:\n   **Three Tracks Plus A Head (Right: With Seek)**\n\n\nnext set of sectors (12 through 23), and the outermost track contains the first sectors (0 through 11).\n\n\nTo understand how the drive might access a given sector, we now trace what would happen on a request to a distant sector, e.g., a read to sector 11. To service this read, the drive has to first move the disk arm to the correct track (in this case, the outermost one), in a process known as a\n   **seek**\n   .\n   **Seeks**\n   , along with rotations, are one of the most costly disk operations.\n\n\nThe seek, it should be noted, has many phases: first an\n   *acceleration*\n   phase as the disk arm gets moving; then\n   *coasting*\n   as the arm is moving at full speed, then\n   *deceleration*\n   as the arm slows down; finally\n   *settling*\n   as the head is carefully positioned over the correct track. The\n   **settling time**\n   is often quite significant, e.g., 0.5 to 2 ms, as the drive must be certain to find the right track (imagine if it just got close instead!).\n\n\nAfter the seek, the disk arm has positioned the head over the right track. A depiction of the seek is found in Figure 37.3 (right).\n\n\nAs we can see, during the seek, the arm has been moved to the desired track, and the platter of course has rotated, in this case about 3 sectors. Thus, sector 9 is just about to pass under the disk head, and we must only endure a short rotational delay to complete the transfer.\n\n\nWhen sector 11 passes under the disk head, the final phase of I/O will take place, known as the\n   **transfer**\n   , where data is either read from or written to the surface. And thus, we have a complete picture of I/O time: first a seek, then waiting for the rotational delay, and finally the transfer.\n\n\n\n\n**Some Other Details**\n\n\nThough we won't spend too much time on it, there are some other interesting details about how hard drives operate. Many drives employ some kind of\n   **track skew**\n   to make sure that sequential reads can be properly serviced even when crossing track boundaries. In our simple example disk, this might appear as seen in Figure 37.4 (page 5).\n\n\n\n\n![Diagram illustrating track skew on a hard disk drive. It shows three concentric tracks with sectors numbered 0-35. The disk rotates counter-clockwise as indicated by an arrow and the text 'Rotates this way'. A read/write head is shown moving from sector 26 on the inner track to sector 22 on the outer track. The text 'Track skew: 2 blocks' is displayed below the diagram.](images/image_0094.jpeg)\n\n\nThe diagram shows three concentric tracks on a hard disk. The innermost track has sectors numbered 0 to 15, the middle track has sectors 16 to 31, and the outermost track has sectors 32 to 35. A read/write head is shown on the inner track at sector 26. An arrow indicates the disk rotates counter-clockwise. The head moves to sector 22 on the outer track. The text 'Track skew: 2 blocks' is written below the diagram.\n\n\nDiagram illustrating track skew on a hard disk drive. It shows three concentric tracks with sectors numbered 0-35. The disk rotates counter-clockwise as indicated by an arrow and the text 'Rotates this way'. A read/write head is shown moving from sector 26 on the inner track to sector 22 on the outer track. The text 'Track skew: 2 blocks' is displayed below the diagram.\n\n\nFigure 37.4:\n   **Three Tracks: Track Skew Of 2**\n\n\nSectors are often skewed like this because when switching from one track to another, the disk needs time to reposition the head (even to neighboring tracks). Without such skew, the head would be moved to the next track but the desired next block would have already rotated under the head, and thus the drive would have to wait almost the entire rotational delay to access the next block.\n\n\nAnother reality is that outer tracks tend to have more sectors than inner tracks, which is a result of geometry; there is simply more room out there. These tracks are often referred to as\n   **multi-zoned**\n   disk drives, where the disk is organized into multiple zones, and where a zone is consecutive set of tracks on a surface. Each zone has the same number of sectors per track, and outer zones have more sectors than inner zones.\n\n\nFinally, an important part of any modern disk drive is its\n   **cache**\n   , for historical reasons sometimes called a\n   **track buffer**\n   . This cache is just some small amount of memory (usually around 8 or 16 MB) which the drive can use to hold data read from or written to the disk. For example, when reading a sector from the disk, the drive might decide to read in all of the sectors on that track and cache them in its memory; doing so allows the drive to quickly respond to any subsequent requests to the same track.\n\n\nOn writes, the drive has a choice: should it acknowledge the write has completed when it has put the data in its memory, or after the write has actually been written to disk? The former is called\n   **write back**\n   caching (or sometimes\n   **immediate reporting**\n   ), and the latter\n   **write through**\n   . Write back caching sometimes makes the drive appear “faster”, but can be dangerous; if the file system or applications require that data be written to disk in a certain order for correctness, write-back caching can lead to problems (read the chapter on file-system journaling for details).\n\n\n\n\n**ASIDE: DIMENSIONAL ANALYSIS**\n\n\nRemember in Chemistry class, how you solved virtually every problem by simply setting up the units such that they canceled out, and somehow the answers popped out as a result? That chemical magic is known by the highfalutin name of\n   **dimensional analysis**\n   and it turns out it is useful in computer systems analysis too.\n\n\nLet's do an example to see how dimensional analysis works and why it is useful. In this case, assume you have to figure out how long, in milliseconds, a single rotation of a disk takes. Unfortunately, you are given only the\n   **RPM**\n   of the disk, or\n   **rotations per minute**\n   . Let's assume we're talking about a 10K RPM disk (i.e., it rotates 10,000 times per minute). How do we set up the dimensional analysis so that we get time per rotation in milliseconds?\n\n\nTo do so, we start by putting the desired units on the left; in this case, we wish to obtain the time (in milliseconds) per rotation, so that is exactly what we write down:\n   \n    \\frac{Time (ms)}{1 Rotation}\n   \n   . We then write down everything we know, making sure to cancel units where possible. First, we obtain\n   \n    \\frac{1 minute}{10,000 Rotations}\n   \n   (keeping rotation on the bottom, as that's where it is on the left), then transform minutes into seconds with\n   \n    \\frac{60 seconds}{1 minute}\n   \n   , and then finally transform seconds in milliseconds with\n   \n    \\frac{1000 ms}{1 second}\n   \n   . The final result is the following (with units nicely canceled):\n\n\n\\frac{Time (ms)}{1 Rot.} = \\frac{1 \\cancel{minute}}{10,000 \\cancel{Rot.}} \\cdot \\frac{60 \\cancel{seconds}}{1 \\cancel{minute}} \\cdot \\frac{1000 ms}{1 \\cancel{second}} = \\frac{60,000 ms}{10,000 Rot.} = \\frac{6 ms}{Rotation}\n\n\nAs you can see from this example, dimensional analysis makes what seems intuitive into a simple and repeatable process. Beyond the RPM calculation above, it comes in handy with I/O analysis regularly. For example, you will often be given the transfer rate of a disk, e.g., 100 MB/second, and then asked: how long does it take to transfer a 512 KB block (in milliseconds)? With dimensional analysis, it's easy:\n\n\n\\frac{Time (ms)}{1 Request} = \\frac{512 \\cancel{KB}}{1 \\cancel{Request}} \\cdot \\frac{1 \\cancel{MB}}{1024 \\cancel{KB}} \\cdot \\frac{1 \\cancel{second}}{100 \\cancel{MB}} \\cdot \\frac{1000 ms}{1 \\cancel{second}} = \\frac{5 ms}{Request}"
        },
        {
          "name": "I/O Time: Doing The Math",
          "content": "Now that we have an abstract model of the disk, we can use a little analysis to better understand disk performance. In particular, we can now represent I/O time as the sum of three major components:\n\n\nT_{I/O} = T_{seek} + T_{rotation} + T_{transfer} \\quad (37.1)\n\n\nNote that the rate of I/O (\n   \n    R_{I/O}\n   \n   ), which is often more easily used for\n\n\n\n | Cheetah 15K.5 | Barracuda\nCapacity | 300 GB | 1 TB\nRPM | 15,000 | 7,200\nAverage Seek | 4 ms | 9 ms\nMax Transfer | 125 MB/s | 105 MB/s\nPlatters | 4 | 4\nCache | 16 MB | 16/32 MB\nConnects via | SCSI | SATA\n\n\nFigure 37.5: Disk Drive Specs: SCSI Versus SATA\n\n\ncomparison between drives (as we will do below), is easily computed from the time. Simply divide the size of the transfer by the time it took:\n\n\nR_{I/O} = \\frac{Size_{Transfer}}{T_{I/O}} \\quad (37.2)\n\n\nTo get a better feel for I/O time, let us perform the following calculation. Assume there are two workloads we are interested in. The first, known as the\n   **random**\n   workload, issues small (e.g., 4KB) reads to random locations on the disk. Random workloads are common in many important applications, including database management systems. The second, known as the\n   **sequential**\n   workload, simply reads a large number of sectors consecutively from the disk, without jumping around. Sequential access patterns are quite common and thus important as well.\n\n\nTo understand the difference in performance between random and sequential workloads, we need to make a few assumptions about the disk drive first. Let's look at a couple of modern disks from Seagate. The first, known as the Cheetah 15K.5 [S09b], is a high-performance SCSI drive. The second, the Barracuda [S09a], is a drive built for capacity. Details on both are found in Figure 37.5.\n\n\nAs you can see, the drives have quite different characteristics, and in many ways nicely summarize two important components of the disk drive market. The first is the “high performance” drive market, where drives are engineered to spin as fast as possible, deliver low seek times, and transfer data quickly. The second is the “capacity” market, where cost per byte is the most important aspect; thus, the drives are slower but pack as many bits as possible into the space available.\n\n\nFrom these numbers, we can start to calculate how well the drives would do under our two workloads outlined above. Let's start by looking at the random workload. Assuming each 4 KB read occurs at a random location on disk, we can calculate how long each such read would take. On the Cheetah:\n\n\nT_{seek} = 4 \\text{ ms}, T_{rotation} = 2 \\text{ ms}, T_{transfer} = 30 \\text{ microsecs} \\quad (37.3)\n\n\nThe average seek time (4 milliseconds) is just taken as the average time reported by the manufacturer; note that a full seek (from one end of the\n\n\n**TIP: USE DISKS SEQUENTIALLY**\nWhen at all possible, transfer data to and from disks in a sequential manner. If sequential is not possible, at least think about transferring data in large chunks: the bigger, the better. If I/O is done in little random pieces, I/O performance will suffer dramatically. Also, users will suffer. Also, you will suffer, knowing what suffering you have wrought with your careless random I/Os.\n\n\nsurface to the other) would likely take two or three times longer. The average rotational delay is calculated from the RPM directly. 15000 RPM is equal to 250 RPS (rotations per second); thus, each rotation takes 4 ms. On average, the disk will encounter a half rotation and thus 2 ms is the average time. Finally, the transfer time is just the size of the transfer over the peak transfer rate; here it is vanishingly small (30\n   *microseconds*\n   ; note that we need 1000 microseconds just to get 1 millisecond!).\n\n\nThus, from our equation above,\n   \n    T_{I/O}\n   \n   for the Cheetah roughly equals 6 ms. To compute the rate of I/O, we just divide the size of the transfer by the average time, and thus arrive at\n   \n    R_{I/O}\n   \n   for the Cheetah under the random workload of about 0.66 MB/s. The same calculation for the Barracuda yields a\n   \n    T_{I/O}\n   \n   of about 13.2 ms, more than twice as slow, and thus a rate of about 0.31 MB/s.\n\n\nNow let's look at the sequential workload. Here we can assume there is a single seek and rotation before a very long transfer. For simplicity, assume the size of the transfer is 100 MB. Thus,\n   \n    T_{I/O}\n   \n   for the Cheetah and Barracuda is about 800 ms and 950 ms, respectively. The rates of I/O are thus very nearly the peak transfer rates of 125 MB/s and 105 MB/s, respectively. Figure 37.6 summarizes these numbers.\n\n\n\n |  | Cheetah | Barracuda\nR_{I/O} | Random | 0.66 MB/s | 0.31 MB/s\nR_{I/O} | Sequential | 125 MB/s | 105 MB/s\n\n\nFigure 37.6:\n   **Disk Drive Performance: SCSI Versus SATA**\n\n\nThe figure shows us a number of important things. First, and most importantly, there is a huge gap in drive performance between random and sequential workloads, almost a factor of 200 or so for the Cheetah and more than a factor 300 difference for the Barracuda. And thus we arrive at the most obvious design tip in the history of computing.\n\n\nA second, more subtle point: there is a large difference in performance between high-end “performance” drives and low-end “capacity” drives. For this reason (and others), people are often willing to pay top dollar for the former while trying to get the latter as cheaply as possible.\n\n\n\n\n**ASIDE: COMPUTING THE “AVERAGE” SEEK**\n\n\nIn many books and papers, you will see average disk-seek time cited as being roughly one-third of the full seek time. Where does this come from?\n\n\nTurns out it arises from a simple calculation based on average seek distance, not time. Imagine the disk as a set of tracks, from 0 to\n   \n    N\n   \n   . The seek distance between any two tracks\n   \n    x\n   \n   and\n   \n    y\n   \n   is thus computed as the absolute value of the difference between them:\n   \n    |x - y|\n   \n   .\n\n\nTo compute the average seek distance, all you need to do is to first add up all possible seek distances:\n\n\n\\sum_{x=0}^{N} \\sum_{y=0}^{N} |x - y|. \\quad (37.4)\n\n\nThen, divide this by the number of different possible seeks:\n   \n    N^2\n   \n   . To compute the sum, we'll just use the integral form:\n\n\n\\int_{x=0}^{N} \\int_{y=0}^{N} |x - y| \\, dy \\, dx. \\quad (37.5)\n\n\nTo compute the inner integral, let's break out the absolute value:\n\n\n\\int_{y=0}^{x} (x - y) \\, dy + \\int_{y=x}^{N} (y - x) \\, dy. \\quad (37.6)\n\n\nSolving this leads to\n   \n    (xy - \\frac{1}{2}y^2)\\big|_0^x + (\\frac{1}{2}y^2 - xy)\\big|_x^N\n   \n   which can be simplified to\n   \n    (x^2 - Nx + \\frac{1}{2}N^2)\n   \n   . Now we have to compute the outer integral:\n\n\n\\int_{x=0}^{N} (x^2 - Nx + \\frac{1}{2}N^2) \\, dx, \\quad (37.7)\n\n\nwhich results in:\n\n\n\\left( \\frac{1}{3}x^3 - \\frac{N}{2}x^2 + \\frac{N^2}{2}x \\right) \\bigg|_0^N = \\frac{N^3}{3}. \\quad (37.8)\n\n\nRemember that we still have to divide by the total number of seeks (\n   \n    N^2\n   \n   ) to compute the average seek distance:\n   \n    (\\frac{N^3}{3})/(N^2) = \\frac{1}{3}N\n   \n   . Thus the average seek distance on a disk, over all possible seeks, is one-third the full distance. And now when you hear that an average seek is one-third of a full seek, you'll know where it came from.\n\n\n\n\n![Diagram illustrating the Shortest Seek Time First (SSTF) disk scheduling algorithm. The diagram shows a disk platter with four concentric tracks labeled 0, 1, 2, and 3. Sectors are numbered 1 through 36, distributed across these tracks. The head is currently positioned over sector 30 on track 1. An arrow labeled 'Rotates this way' points counter-clockwise. The head moves to sector 21 on track 2, then to sector 2 on track 3, and finally to sector 29 on track 1, completing the sequence of requests 21, 2, and 29.](images/image_0095.jpeg)\n\n\nDiagram illustrating the Shortest Seek Time First (SSTF) disk scheduling algorithm. The diagram shows a disk platter with four concentric tracks labeled 0, 1, 2, and 3. Sectors are numbered 1 through 36, distributed across these tracks. The head is currently positioned over sector 30 on track 1. An arrow labeled 'Rotates this way' points counter-clockwise. The head moves to sector 21 on track 2, then to sector 2 on track 3, and finally to sector 29 on track 1, completing the sequence of requests 21, 2, and 29.\n\n\nFigure 37.7:\n   **SSTF: Scheduling Requests 21 And 2**"
        },
        {
          "name": "Disk Scheduling",
          "content": "Because of the high cost of I/O, the OS has historically played a role in deciding the order of I/Os issued to the disk. More specifically, given a set of I/O requests, the\n   **disk scheduler**\n   examines the requests and decides which one to schedule next [SCO90, JW91].\n\n\nUnlike job scheduling, where the length of each job is usually unknown, with disk scheduling, we can make a good guess at how long a “job” (i.e., disk request) will take. By estimating the seek and possible rotational delay of a request, the disk scheduler can know how long each request will take, and thus (greedily) pick the one that will take the least time to service first. Thus, the disk scheduler will try to follow the\n   **principle of SJF (shortest job first)**\n   in its operation.\n\n\n\n\n**SSTF: Shortest Seek Time First**\n\n\nOne early disk scheduling approach is known as\n   **shortest-seek-time-first (SSTF)**\n   (also called\n   **shortest-seek-first**\n   or\n   **SSF**\n   ). SSTF orders the queue of I/O requests by track, picking requests on the nearest track to complete first. For example, assuming the current position of the head is over the inner track, and we have requests for sectors 21 (middle track) and 2 (outer track), we would then issue the request to 21 first, wait for it to complete, and then issue the request to 2 (Figure 37.7).\n\n\nSSTF works well in this example, seeking to the middle track first and then the outer track. However, SSTF is not a panacea, for the following reasons. First, the drive geometry is not available to the host OS; rather, it sees an array of blocks. Fortunately, this problem is rather easily fixed. Instead of SSTF, an OS can simply implement\n   **nearest-block-first (NBF)**\n   , which schedules the request with the nearest block address next.\n\n\nThe second problem is more fundamental:\n   **starvation**\n   . Imagine in our example above if there were a steady stream of requests to the inner track, where the head currently is positioned. Requests to any other tracks would then be ignored completely by a pure SSTF approach. And thus the crux of the problem:\n\n\n\n\n**CRUX: HOW TO HANDLE DISK STARVATION**\n\n\nHow can we implement SSTF-like scheduling but avoid starvation?\n\n\n\n\n**Elevator (a.k.a. SCAN or C-SCAN)**\n\n\nThe answer to this query was developed some time ago (see [CKR72] for example), and is relatively straightforward. The algorithm, originally called\n   **SCAN**\n   , simply moves back and forth across the disk servicing requests in order across the tracks. Let's call a single pass across the disk (from outer to inner tracks, or inner to outer) a\n   *sweep*\n   . Thus, if a request comes for a block on a track that has already been serviced on this sweep of the disk, it is not handled immediately, but rather queued until the next sweep (in the other direction).\n\n\nSCAN has a number of variants, all of which do about the same thing. For example, Coffman et al. introduced\n   **F-SCAN**\n   , which freezes the queue to be serviced when it is doing a sweep [CKR72]; this action places requests that come in during the sweep into a queue to be serviced later. Doing so avoids starvation of far-away requests, by delaying the servicing of late-arriving (but nearer by) requests.\n\n\n**C-SCAN**\n   is another common variant, short for\n   **Circular SCAN**\n   . Instead of sweeping in both directions across the disk, the algorithm only sweeps from outer-to-inner, and then resets at the outer track to begin again. Doing so is a bit more fair to inner and outer tracks, as pure back-and-forth SCAN favors the middle tracks, i.e., after servicing the outer track, SCAN passes through the middle twice before coming back to the outer track again.\n\n\nFor reasons that should now be clear, the SCAN algorithm (and its cousins) is sometimes referred to as the\n   **elevator**\n   algorithm, because it behaves like an elevator which is either going up or down and not just servicing requests to floors based on which floor is closer. Imagine how annoying it would be if you were going down from floor 10 to 1, and somebody got on at 3 and pressed 4, and the elevator went up to 4 because it was \"closer\" than 1! As you can see, the elevator algorithm, when used in real life, prevents fights from taking place on elevators. In disks, it just prevents starvation.\n\n\nUnfortunately, SCAN and its cousins do not represent the best scheduling technology. In particular, SCAN (or SSTF even) does not actually adhere as closely to the principle of SJF as they could. In particular, they ignore rotation. And thus, another crux:\n\n\n\n\n**CRUX: HOW TO ACCOUNT FOR DISK ROTATION COSTS**\n\n\nHow can we implement an algorithm that more closely approximates SJF by taking\n   *both*\n   seek and rotation into account?\n\n\n**SPTF: Shortest Positioning Time First**\nBefore discussing\n   **shortest positioning time first**\n   or\n   **SPTF**\n   scheduling (sometimes also called\n   **shortest access time first**\n   or\n   **SATF**\n   ), which is the solution to our problem, let us make sure we understand the problem in more detail. Figure 37.8 presents an example.\n\n\nIn the example, the head is currently positioned over sector 30 on the inner track. The scheduler thus has to decide: should it schedule sector 16 (on the middle track) or sector 8 (on the outer track) for its next request. So which should it service next?\n\n\nThe answer, of course, is “it depends”. In engineering, it turns out “it depends” is almost always the answer, reflecting that trade-offs are part of the life of the engineer; such maxims are also good in a pinch, e.g., when you don’t know an answer to your boss’s question, you might want to try this gem. However, it is almost always better to know\n   *why*\n   it depends, which is what we discuss here.\n\n\nWhat it depends on here is the relative time of seeking as compared to rotation. If, in our example, seek time is much higher than rotational delay, then SSTF (and variants) are just fine. However, imagine if seek is quite a bit faster than rotation. Then, in our example, it would make more sense to seek\n   *further*\n   to service request 8 on the outer track than it would to perform the shorter seek to the middle track to service 16, which has to rotate all the way around before passing under the disk head.\n\n\n\n\n![Diagram of a disk platter showing tracks and sectors. The platter has four concentric tracks. The innermost track has sectors 3, 4, 5, 6, 17, 18, 19, 7. The next track has sectors 14, 15, 16, 18, 28, 29, 31, 17. The next track has sectors 2, 13, 25, 24, 23, 35, 34, 12. The outermost track has sectors 9, 10, 11, 22, 21, 33, 32, 8. Sector 30 is highlighted in grey on the innermost track. A read/write head is shown on the innermost track, pointing to sector 30. An arrow at the top indicates counter-clockwise rotation. The center is labeled 'Spindle'.](images/image_0096.jpeg)\n\n\nThe diagram illustrates a disk platter with four concentric tracks. The innermost track contains sectors 3, 4, 5, 6, 17, 18, 19, and 7. The second track contains sectors 14, 15, 16, 18, 28, 29, 31, and 17. The third track contains sectors 2, 13, 25, 24, 23, 35, 34, and 12. The outermost track contains sectors 9, 10, 11, 22, 21, 33, 32, and 8. Sector 30 is highlighted in grey on the innermost track. A read/write head is shown on the innermost track, pointing to sector 30. An arrow at the top indicates counter-clockwise rotation. The center is labeled 'Spindle'.\n\n\nDiagram of a disk platter showing tracks and sectors. The platter has four concentric tracks. The innermost track has sectors 3, 4, 5, 6, 17, 18, 19, 7. The next track has sectors 14, 15, 16, 18, 28, 29, 31, 17. The next track has sectors 2, 13, 25, 24, 23, 35, 34, 12. The outermost track has sectors 9, 10, 11, 22, 21, 33, 32, 8. Sector 30 is highlighted in grey on the innermost track. A read/write head is shown on the innermost track, pointing to sector 30. An arrow at the top indicates counter-clockwise rotation. The center is labeled 'Spindle'.\n\n\nFigure 37.8: SSTF: Sometimes Not Good Enough\n\n\n**TIP: IT ALWAYS DEPENDS (LIVNY'S LAW)**\nAlmost any question can be answered with “it depends”, as our colleague Miron Livny always says. However, use with caution, as if you answer too many questions this way, people will stop asking you questions altogether. For example, somebody asks: “want to go to lunch?” You reply: “it depends, are\n   *you*\n   coming along?”\n\n\nOn modern drives, as we saw above, both seek and rotation are roughly equivalent (depending, of course, on the exact requests), and thus SPTF is useful and improves performance. However, it is even more difficult to implement in an OS, which generally does not have a good idea where track boundaries are or where the disk head currently is (in a rotational sense). Thus, SPTF is usually performed inside a drive, described below.\n\n\n\n\n**Other Scheduling Issues**\n\n\nThere are many other issues we do not discuss in this brief description of basic disk operation, scheduling, and related topics. One such issue is this:\n   *where*\n   is disk scheduling performed on modern systems? In older systems, the operating system did all the scheduling; after looking through the set of pending requests, the OS would pick the best one, and issue it to the disk. When that request completed, the next one would be chosen, and so forth. Disks were simpler then, and so was life.\n\n\nIn modern systems, disks can accommodate multiple outstanding requests, and have sophisticated internal schedulers themselves (which can implement SPTF accurately; inside the disk controller, all relevant details are available, including exact head position). Thus, the OS scheduler usually picks what it thinks the best few requests are (say 16) and issues them all to disk; the disk then uses its internal knowledge of head position and detailed track layout information to service said requests in the best possible (SPTF) order.\n\n\nAnother important related task performed by disk schedulers is\n   **I/O merging**\n   . For example, imagine a series of requests to read blocks 33, then 8, then 34, as in Figure 37.8. In this case, the scheduler should\n   **merge**\n   the requests for blocks 33 and 34 into a single two-block request; any re-ordering that the scheduler does is performed upon the merged requests. Merging is particularly important at the OS level, as it reduces the number of requests sent to the disk and thus lowers overheads.\n\n\nOne final problem that modern schedulers address is this: how long should the system wait before issuing an I/O to disk? One might naively think that the disk, once it has even a single I/O, should immediately issue the request to the drive; this approach is called\n   **work-conserving**\n   , as the disk will never be idle if there are requests to serve. However, research on\n   **anticipatory disk scheduling**\n   has shown that sometimes it is better to\n\n\nwait for a bit [ID01], in what is called a\n   **non-work-conserving**\n   approach. By waiting, a new and “better” request may arrive at the disk, and thus overall efficiency is increased. Of course, deciding when to wait, and for how long, can be tricky; see the research paper for details, or check out the Linux kernel implementation to see how such ideas are transitioned into practice (if you are the ambitious sort)."
        }
      ]
    },
    {
      "name": "Redundant Arrays of Inexpensive Disks(RAIDs)",
      "sections": [
        {
          "name": "Interface And RAID Internals",
          "content": "To a file system above, a RAID looks like a big, (hopefully) fast, and (hopefully) reliable disk. Just as with a single disk, it presents itself as a linear array of blocks, each of which can be read or written by the file system (or other client).\n\n\nWhen a file system issues a\n   *logical I/O*\n   request to the RAID, the RAID internally must calculate which disk (or disks) to access in order to complete the request, and then issue one or more\n   *physical I/Os*\n   to do so. The exact nature of these physical I/Os depends on the RAID level, as we will discuss in detail below. However, as a simple example, consider a RAID that keeps two copies of each block (each one on a separate disk); when writing to such a\n   **mirrored**\n   RAID system, the RAID will have to perform two physical I/Os for every one logical I/O it is issued.\n\n\nA RAID system is often built as a separate hardware box, with a standard connection (e.g., SCSI, or SATA) to a host. Internally, however, RAIDs are fairly complex, consisting of a microcontroller that runs firmware to direct the operation of the RAID, volatile memory such as DRAM to buffer data blocks as they are read and written, and in some cases,\n\n\nnon-volatile memory to buffer writes safely and perhaps even specialized logic to perform parity calculations (useful in some RAID levels, as we will also see below). At a high level, a RAID is very much a specialized computer system: it has a processor, memory, and disks; however, instead of running applications, it runs specialized software designed to operate the RAID."
        },
        {
          "name": "Fault Model",
          "content": "To understand RAID and compare different approaches, we must have a fault model in mind. RAIDs are designed to detect and recover from certain kinds of disk faults; thus, knowing exactly which faults to expect is critical in arriving upon a working design.\n\n\nThe first fault model we will assume is quite simple, and has been called the\n   **fail-stop**\n   fault model [S84]. In this model, a disk can be in exactly one of two states: working or failed. With a working disk, all blocks can be read or written. In contrast, when a disk has failed, we assume it is permanently lost.\n\n\nOne critical aspect of the fail-stop model is what it assumes about fault detection. Specifically, when a disk has failed, we assume that this is easily detected. For example, in a RAID array, we would assume that the RAID controller hardware (or software) can immediately observe when a disk has failed.\n\n\nThus, for now, we do not have to worry about more complex “silent” failures such as disk corruption. We also do not have to worry about a single block becoming inaccessible upon an otherwise working disk (sometimes called a latent sector error). We will consider these more complex (and unfortunately, more realistic) disk faults later."
        },
        {
          "name": "How To Evaluate A RAID",
          "content": "As we will soon see, there are a number of different approaches to building a RAID. Each of these approaches has different characteristics which are worth evaluating, in order to understand their strengths and weaknesses.\n\n\nSpecifically, we will evaluate each RAID design along three axes. The first axis is\n   **capacity**\n   ; given a set of\n   \n    N\n   \n   disks each with\n   \n    B\n   \n   blocks, how much useful capacity is available to clients of the RAID? Without redundancy, the answer is\n   \n    N \\cdot B\n   \n   ; in contrast, if we have a system that keeps two copies of each block (called\n   **mirroring**\n   ), we obtain a useful capacity of\n   \n    (N \\cdot B)/2\n   \n   . Different schemes (e.g., parity-based ones) tend to fall in between.\n\n\nThe second axis of evaluation is\n   **reliability**\n   . How many disk faults can the given design tolerate? In alignment with our fault model, we assume only that an entire disk can fail; in later chapters (i.e., on data integrity), we'll think about how to handle more complex failure modes.\n\n\nFinally, the third axis is\n   **performance**\n   . Performance is somewhat challenging to evaluate, because it depends heavily on the workload presented to the disk array. Thus, before evaluating performance, we will first present a set of typical workloads that one should consider.\n\n\nWe now consider three important RAID designs: RAID Level 0 (stripping), RAID Level 1 (mirroring), and RAID Levels 4/5 (parity-based redundancy). The naming of each of these designs as a “level” stems from the pioneering work of Patterson, Gibson, and Katz at Berkeley [P+88]."
        },
        {
          "name": "RAID Level 0: Striping",
          "content": "The first RAID level is actually not a RAID level at all, in that there is no redundancy. However, RAID level 0, or\n   **stripping**\n   as it is better known, serves as an excellent upper-bound on performance and capacity and thus is worth understanding.\n\n\nThe simplest form of stripping will\n   **stripe**\n   blocks across the disks of the system as follows (assume here a 4-disk array):\n\n\n\nDisk 0 | Disk 1 | Disk 2 | Disk 3\n0 | 1 | 2 | 3\n4 | 5 | 6 | 7\n8 | 9 | 10 | 11\n12 | 13 | 14 | 15\n\n\nFigure 38.1:\n   **RAID-0: Simple Striping**\n\n\nFrom Figure 38.1, you get the basic idea: spread the blocks of the array across the disks in a round-robin fashion. This approach is designed to extract the most parallelism from the array when requests are made for contiguous chunks of the array (as in a large, sequential read, for example). We call the blocks in the same row a\n   **stripe**\n   ; thus, blocks 0, 1, 2, and 3 are in the same stripe above.\n\n\nIn the example, we have made the simplifying assumption that only 1 block (each of say size 4KB) is placed on each disk before moving on to the next. However, this arrangement need not be the case. For example, we could arrange the blocks across disks as in Figure 38.2:\n\n\n\nDisk 0 | Disk 1 | Disk 2 | Disk 3 | \n0 | 2 | 4 | 6 | chunk size:\n1 | 3 | 5 | 7 | 2 blocks\n8 | 10 | 12 | 14 | \n9 | 11 | 13 | 15 | \n\n\nFigure 38.2:\n   **Striping With A Bigger Chunk Size**\n\n\nIn this example, we place two 4KB blocks on each disk before moving on to the next disk. Thus, the\n   **chunk size**\n   of this RAID array is 8KB, and a stripe thus consists of 4 chunks or 32KB of data.\n\n\n**ASIDE: THE RAID MAPPING PROBLEM**\nBefore studying the capacity, reliability, and performance characteristics of the RAID, we first present an aside on what we call the\n   **mapping problem**\n   . This problem arises in all RAID arrays; simply put, given a logical block to read or write, how does the RAID know exactly which physical disk and offset to access?\n\n\nFor these simple RAID levels, we do not need much sophistication in order to correctly map logical blocks onto their physical locations. Take the first striping example above (chunk size = 1 block = 4KB). In this case, given a logical block address\n   \n    A\n   \n   , the RAID can easily compute the desired disk and offset with two simple equations:\n\n\n\\begin{aligned}\\text{Disk} &= A \\% \\text{number\\_of\\_disks} \\\\ \\text{Offset} &= A / \\text{number\\_of\\_disks}\\end{aligned}\n\n\nNote that these are all integer operations (e.g.,\n   \n    4 / 3 = 1\n   \n   not\n   \n    1.3333\\dots\n   \n   ).\n\n\nLet's see how these equations work for a simple example. Imagine in the first RAID above that a request arrives for block 14. Given that there are 4 disks, this would mean that the disk we are interested in is\n   \n    (14 \\% 4 = 2)\n   \n   : disk 2. The exact block is calculated as\n   \n    (14 / 4 = 3)\n   \n   : block 3. Thus, block 14 should be found on the fourth block (block 3, starting at 0) of the third disk (disk 2, starting at 0), which is exactly where it is.\n\n\nYou can think about how these equations would be modified to support different chunk sizes. Try it! It's not too hard.\n\n\n\n\n**Chunk Sizes**\n\n\nChunk size mostly affects performance of the array. For example, a small chunk size implies that many files will get striped across many disks, thus increasing the parallelism of reads and writes to a single file; however, the positioning time to access blocks across multiple disks increases, because the positioning time for the entire request is determined by the maximum of the positioning times of the requests across all drives.\n\n\nA big chunk size, on the other hand, reduces such intra-file parallelism, and thus relies on multiple concurrent requests to achieve high throughput. However, large chunk sizes reduce positioning time; if, for example, a single file fits within a chunk and thus is placed on a single disk, the positioning time incurred while accessing it will just be the positioning time of a single disk.\n\n\nThus, determining the “best” chunk size is hard to do, as it requires a great deal of knowledge about the workload presented to the disk system [CL95]. For the rest of this discussion, we will assume that the array uses a chunk size of a single block (4KB). Most arrays use larger chunk sizes (e.g., 64 KB), but for the issues we discuss below, the exact chunk size does not matter; thus we use a single block for the sake of simplicity.\n\n\n\n\n**Back To RAID-0 Analysis**\n\n\nLet us now evaluate the capacity, reliability, and performance of striping. From the perspective of capacity, it is perfect: given\n   \n    N\n   \n   disks each of size\n   \n    B\n   \n   blocks, striping delivers\n   \n    N \\cdot B\n   \n   blocks of useful capacity. From the standpoint of reliability, striping is also perfect, but in the bad way: any disk failure will lead to data loss. Finally, performance is excellent: all disks are utilized, often in parallel, to service user I/O requests.\n\n\n\n\n**Evaluating RAID Performance**\n\n\nIn analyzing RAID performance, one can consider two different performance metrics. The first is\n   *single-request latency*\n   . Understanding the latency of a single I/O request to a RAID is useful as it reveals how much parallelism can exist during a single logical I/O operation. The second is\n   *steady-state throughput*\n   of the RAID, i.e., the total bandwidth of many concurrent requests. Because RAIDs are often used in high-performance environments, the steady-state bandwidth is critical, and thus will be the main focus of our analyses.\n\n\nTo understand throughput in more detail, we need to put forth some workloads of interest. We will assume, for this discussion, that there are two types of workloads:\n   **sequential**\n   and\n   **random**\n   . With a sequential workload, we assume that requests to the array come in large contiguous chunks; for example, a request (or series of requests) that accesses 1 MB of data, starting at block\n   \n    x\n   \n   and ending at block\n   \n    (x+1)\n   \n   MB, would be deemed sequential. Sequential workloads are common in many environments (think of searching through a large file for a keyword), and thus are considered important.\n\n\nFor random workloads, we assume that each request is rather small, and that each request is to a different random location on disk. For example, a random stream of requests may first access 4KB at logical address 10, then at logical address 550,000, then at 20,100, and so forth. Some important workloads, such as transactional workloads on a database management system (DBMS), exhibit this type of access pattern, and thus it is considered an important workload.\n\n\nOf course, real workloads are not so simple, and often have a mix of sequential and random-seeming components as well as behaviors in-between the two. For simplicity, we just consider these two possibilities.\n\n\nAs you can tell, sequential and random workloads will result in widely different performance characteristics from a disk. With sequential access, a disk operates in its most efficient mode, spending little time seeking and waiting for rotation and most of its time transferring data. With random access, just the opposite is true: most time is spent seeking and waiting for rotation and relatively little time is spent transferring data. To capture this difference in our analysis, we will assume that a disk can transfer data at\n   \n    S\n   \n   MB/s under a sequential workload, and\n   \n    R\n   \n   MB/s when under a random workload. In general,\n   \n    S\n   \n   is much greater than\n   \n    R\n   \n   (i.e.,\n   \n    S \\gg R\n   \n   ).\n\n\nTo make sure we understand this difference, let's do a simple exercise. Specifically, let's calculate\n   \n    S\n   \n   and\n   \n    R\n   \n   given the following disk characteristics. Assume a sequential transfer of size 10 MB on average, and a random transfer of 10 KB on average. Also, assume the following disk characteristics:\n\n\n\nAverage seek time | 7 ms\nAverage rotational delay | 3 ms\nTransfer rate of disk | 50 MB/s\n\n\nTo compute\n   \n    S\n   \n   , we need to first figure out how time is spent in a typical 10 MB transfer. First, we spend 7 ms seeking, and then 3 ms rotating. Finally, transfer begins; 10 MB @ 50 MB/s leads to 1/5th of a second, or 200 ms, spent in transfer. Thus, for each 10 MB request, we spend 210 ms completing the request. To compute\n   \n    S\n   \n   , we just need to divide:\n\n\nS = \\frac{\\text{Amount of Data}}{\\text{Time to access}} = \\frac{10 \\text{ MB}}{210 \\text{ ms}} = 47.62 \\text{ MB/s}\n\n\nAs we can see, because of the large time spent transferring data,\n   \n    S\n   \n   is very near the peak bandwidth of the disk (the seek and rotational costs have been amortized).\n\n\nWe can compute\n   \n    R\n   \n   similarly. Seek and rotation are the same; we then compute the time spent in transfer, which is 10 KB @ 50 MB/s, or 0.195 ms.\n\n\nR = \\frac{\\text{Amount of Data}}{\\text{Time to access}} = \\frac{10 \\text{ KB}}{10.195 \\text{ ms}} = 0.981 \\text{ MB/s}\n\n\nAs we can see,\n   \n    R\n   \n   is less than 1 MB/s, and\n   \n    S/R\n   \n   is almost 50.\n\n\n\n\n**Back To RAID-0 Analysis, Again**\n\n\nLet's now evaluate the performance of striping. As we said above, it is generally good. From a latency perspective, for example, the latency of a single-block request should be just about identical to that of a single disk; after all, RAID-0 will simply redirect that request to one of its disks.\n\n\nFrom the perspective of steady-state sequential throughput, we'd expect to get the full bandwidth of the system. Thus, throughput equals\n   \n    N\n   \n   (the number of disks) multiplied by\n   \n    S\n   \n   (the sequential bandwidth of a single disk). For a large number of random I/Os, we can again use all of the disks, and thus obtain\n   \n    N \\cdot R\n   \n   MB/s. As we will see below, these values are both the simplest to calculate and will serve as an upper bound in comparison with other RAID levels."
        },
        {
          "name": "RAID Level 1: Mirroring",
          "content": "Our first RAID level beyond striping is known as RAID level 1, or mirroring. With a mirrored system, we simply make more than one copy of each block in the system; each copy should be placed on a separate disk, of course. By doing so, we can tolerate disk failures.\n\n\nIn a typical mirrored system, we will assume that for each logical block, the RAID keeps two physical copies of it. Here is an example:\n\n\n\nDisk 0 | Disk 1 | Disk 2 | Disk 3\n0 | 0 | 1 | 1\n2 | 2 | 3 | 3\n4 | 4 | 5 | 5\n6 | 6 | 7 | 7\n\n\nFigure 38.3:\n   **Simple RAID-1: Mirroring**\n\n\nIn the example, disk 0 and disk 1 have identical contents, and disk 2 and disk 3 do as well; the data is striped across these mirror pairs. In fact, you may have noticed that there are a number of different ways to place block copies across the disks. The arrangement above is a common one and is sometimes called\n   **RAID-10**\n   (or\n   **RAID 1+0**\n   ,\n   **stripe of mirrors**\n   ) because it uses mirrored pairs (RAID-1) and then stripes (RAID-0) on top of them; another common arrangement is\n   **RAID-01**\n   (or\n   **RAID 0+1**\n   ,\n   **mirror of stripes**\n   ), which contains two large striping (RAID-0) arrays, and then mirrors (RAID-1) on top of them. For now, we will just talk about mirroring assuming the above layout.\n\n\nWhen reading a block from a mirrored array, the RAID has a choice: it can read either copy. For example, if a read to logical block 5 is issued to the RAID, it is free to read it from either disk 2 or disk 3. When writing a block, though, no such choice exists: the RAID must update\n   *both*\n   copies of the data, in order to preserve reliability. Do note, though, that these writes can take place in parallel; for example, a write to logical block 5 could proceed to disks 2 and 3 at the same time.\n\n\n\n\n**RAID-1 Analysis**\n\n\nLet us assess RAID-1. From a capacity standpoint, RAID-1 is expensive; with the mirroring level = 2, we only obtain half of our peak useful capacity. With\n   \n    N\n   \n   disks of\n   \n    B\n   \n   blocks, RAID-1 useful capacity is\n   \n    (N \\cdot B)/2\n   \n   .\n\n\nFrom a reliability standpoint, RAID-1 does well. It can tolerate the failure of any one disk. You may also notice RAID-1 can actually do better than this, with a little luck. Imagine, in the figure above, that disk 0 and disk 2 both failed. In such a situation, there is no data loss! More generally, a mirrored system (with mirroring level of 2) can tolerate 1 disk failure for certain, and up to\n   \n    N/2\n   \n   failures depending on which disks fail. In practice, we generally don't like to leave things like this to chance; thus most people consider mirroring to be good for handling a single failure.\n\n\nFinally, we analyze performance. From the perspective of the latency of a single read request, we can see it is the same as the latency on a single disk; all the RAID-1 does is direct the read to one of its copies. A write is a little different: it requires two physical writes to complete before it is done. These two writes happen in parallel, and thus the time will be roughly equivalent to the time of a single write; however, because the logical write must wait for both physical writes to complete, it suffers the\n\n\n\n\n**ASIDE: THE RAID CONSISTENT-UPDATE PROBLEM**\n\n\nBefore analyzing RAID-1, let us first discuss a problem that arises in any multi-disk RAID system, known as the\n   **consistent-update problem**\n   [DAA05]. The problem occurs on a write to any RAID that has to update multiple disks during a single logical operation. In this case, let us assume we are considering a mirrored disk array.\n\n\nImagine the write is issued to the RAID, and then the RAID decides that it must be written to two disks, disk 0 and disk 1. The RAID then issues the write to disk 0, but just before the RAID can issue the request to disk 1, a power loss (or system crash) occurs. In this unfortunate case, let us assume that the request to disk 0 completed (but clearly the request to disk 1 did not, as it was never issued).\n\n\nThe result of this untimely power loss is that the two copies of the block are now\n   **inconsistent**\n   ; the copy on disk 0 is the new version, and the copy on disk 1 is the old. What we would like to happen is for the state of both disks to change\n   **atomically**\n   , i.e., either both should end up as the new version or neither.\n\n\nThe general way to solve this problem is to use a\n   **write-ahead log**\n   of some kind to first record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it. By taking this approach, we can ensure that in the presence of a crash, the right thing will happen; by running a\n   **recovery**\n   procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 case) are out of sync.\n\n\nOne last note: because logging to disk on every write is prohibitively expensive, most RAID hardware includes a small amount of non-volatile RAM (e.g., battery-backed) where it performs this type of logging. Thus, consistent update is provided without the high cost of logging to disk.\n\n\nworst-case seek and rotational delay of the two requests, and thus (on average) will be slightly higher than a write to a single disk.\n\n\nTo analyze steady-state throughput, let us start with the sequential workload. When writing out to disk sequentially, each logical write must result in two physical writes; for example, when we write logical block 0 (in the figure above), the RAID internally would write it to both disk 0 and disk 1. Thus, we can conclude that the maximum bandwidth obtained during sequential writing to a mirrored array is\n   \n    (\\frac{N}{2} \\cdot S)\n   \n   , or half the peak bandwidth.\n\n\nUnfortunately, we obtain the exact same performance during a sequential read. One might think that a sequential read could do better, because it only needs to read one copy of the data, not both. However, let's use an example to illustrate why this doesn't help much. Imagine we need to read blocks 0, 1, 2, 3, 4, 5, 6, and 7. Let's say we issue the read of 0 to disk 0, the read of 1 to disk 2, the read of 2 to disk 1, and the read of 3 to disk 3. We continue by issuing reads to 4, 5, 6, and 7 to disks 0, 2, 1,\n\n\nand 3, respectively. One might naively think that because we are utilizing all disks, we are achieving the full bandwidth of the array.\n\n\nTo see that this is not (necessarily) the case, however, consider the requests a single disk receives (say disk 0). First, it gets a request for block 0; then, it gets a request for block 4 (skipping block 2). In fact, each disk receives a request for every other block. While it is rotating over the skipped block, it is not delivering useful bandwidth to the client. Thus, each disk will only deliver half its peak bandwidth. And thus, the sequential read will only obtain a bandwidth of\n   \n    (\\frac{N}{2} \\cdot S)\n   \n   MB/s.\n\n\nRandom reads are the best case for a mirrored RAID. In this case, we can distribute the reads across all the disks, and thus obtain the full possible bandwidth. Thus, for random reads, RAID-1 delivers\n   \n    N \\cdot R\n   \n   MB/s.\n\n\nFinally, random writes perform as you might expect:\n   \n    \\frac{N}{2} \\cdot R\n   \n   MB/s. Each logical write must turn into two physical writes, and thus while all the disks will be in use, the client will only perceive this as half the available bandwidth. Even though a write to logical block\n   \n    x\n   \n   turns into two parallel writes to two different physical disks, the bandwidth of many small requests only achieves half of what we saw with striping. As we will soon see, getting half the available bandwidth is actually pretty good!"
        },
        {
          "name": "RAID Level 4: Saving Space With Parity",
          "content": "We now present a different method of adding redundancy to a disk array known as\n   **parity**\n   . Parity-based approaches attempt to use less capacity and thus overcome the huge space penalty paid by mirrored systems. They do so at a cost, however: performance.\n\n\n\nDisk 0 | Disk 1 | Disk 2 | Disk 3 | Disk 4\n0 | 1 | 2 | 3 | P0\n4 | 5 | 6 | 7 | P1\n8 | 9 | 10 | 11 | P2\n12 | 13 | 14 | 15 | P3\n\n\nFigure 38.4:\n   **RAID-4 With Parity**\n\n\nHere is an example five-disk RAID-4 system (Figure 38.4). For each stripe of data, we have added a single\n   **parity**\n   block that stores the redundant information for that stripe of blocks. For example, parity block P1 has redundant information that it calculated from blocks 4, 5, 6, and 7.\n\n\nTo compute parity, we need to use a mathematical function that enables us to withstand the loss of any one block from our stripe. It turns out the simple function\n   **XOR**\n   does the trick quite nicely. For a given set of bits, the XOR of all of those bits returns a 0 if there are an even number of 1's in the bits, and a 1 if there are an odd number of 1's. For example:\n\n\nIn the first row (0,0,1,1), there are two 1's (C2, C3), and thus XOR of all of those values will be 0 (P); similarly, in the second row there is only\n\n\n\nC0 | C1 | C2 | C3 | P\n0 | 0 | 1 | 1 | \\text{XOR}(0,0,1,1) = 0\n0 | 1 | 0 | 0 | \\text{XOR}(0,1,0,0) = 1\n\n\none 1 (C1), and thus the XOR must be 1 (P). You can remember this in a simple way: that the number of 1s in any row, including the parity bit, must be an even (not odd) number; that is the\n   **invariant**\n   that the RAID must maintain in order for parity to be correct.\n\n\nFrom the example above, you might also be able to guess how parity information can be used to recover from a failure. Imagine the column-labeled C2 is lost. To figure out what values must have been in the column, we simply have to read in all the other values in that row (including the XOR'd parity bit) and\n   **reconstruct**\n   the right answer. Specifically, assume the first row's value in column C2 is lost (it is a 1); by reading the other values in that row (0 from C0, 0 from C1, 1 from C3, and 0 from the parity column P), we get the values 0, 0, 1, and 0. Because we know that XOR keeps an even number of 1's in each row, we know what the missing data must be: a 1. And that is how reconstruction works in a XOR-based parity scheme! Note also how we compute the reconstructed value: we just XOR the data bits and the parity bits together, in the same way that we calculated the parity in the first place.\n\n\nNow you might be wondering: we are talking about XORing all of these bits, and yet from above we know that the RAID places 4KB (or larger) blocks on each disk; how do we apply XOR to a bunch of blocks to compute the parity? It turns out this is easy as well. Simply perform a bitwise XOR across each bit of the data blocks; put the result of each bitwise XOR into the corresponding bit slot in the parity block. For example, if we had blocks of size 4 bits (yes, this is still quite a bit smaller than a 4KB block, but you get the picture), they might look something like this:\n\n\n\nBlock0 | Block1 | Block2 | Block3 | Parity\n00 | 10 | 11 | 10 | 11\n10 | 01 | 00 | 01 | 10\n\n\nAs you can see from the figure, the parity is computed for each bit of each block and the result placed in the parity block.\n\n\n\n\n**RAID-4 Analysis**\n\n\nLet us now analyze RAID-4. From a capacity standpoint, RAID-4 uses 1 disk for parity information for every group of disks it is protecting. Thus, our useful capacity for a RAID group is\n   \n    (N - 1) \\cdot B\n   \n   .\n\n\nReliability is also quite easy to understand: RAID-4 tolerates 1 disk failure and no more. If more than one disk is lost, there is simply no way to reconstruct the lost data.\n\n\n\nDisk 0 | Disk 1 | Disk 2 | Disk 3 | Disk 4\n0 | 1 | 2 | 3 | P0\n4 | 5 | 6 | 7 | P1\n8 | 9 | 10 | 11 | P2\n12 | 13 | 14 | 15 | P3\n\n\nFigure 38.5:\n   **Full-stripe Writes In RAID-4**\n\n\nFinally, there is performance. This time, let us start by analyzing steady-state throughput. Sequential read performance can utilize all of the disks except for the parity disk, and thus deliver a peak effective bandwidth of\n   \n    (N - 1) \\cdot S\n   \n   MB/s (an easy case).\n\n\nTo understand the performance of sequential writes, we must first understand how they are done. When writing a big chunk of data to disk, RAID-4 can perform a simple optimization known as a\n   **full-stripe write**\n   . For example, imagine the case where the blocks 0, 1, 2, and 3 have been sent to the RAID as part of a write request (Figure 38.5).\n\n\nIn this case, the RAID can simply calculate the new value of P0 (by performing an XOR across the blocks 0, 1, 2, and 3) and then write all of the blocks (including the parity block) to the five disks above in parallel (highlighted in gray in the figure). Thus, full-stripe writes are the most efficient way for RAID-4 to write to disk.\n\n\nOnce we understand the full-stripe write, calculating the performance of sequential writes on RAID-4 is easy; the effective bandwidth is also\n   \n    (N - 1) \\cdot S\n   \n   MB/s. Even though the parity disk is constantly in use during the operation, the client does not gain performance advantage from it.\n\n\nNow let us analyze the performance of random reads. As you can also see from the figure above, a set of 1-block random reads will be spread across the data disks of the system but not the parity disk. Thus, the effective performance is:\n   \n    (N - 1) \\cdot R\n   \n   MB/s.\n\n\nRandom writes, which we have saved for last, present the most interesting case for RAID-4. Imagine we wish to overwrite block 1 in the example above. We could just go ahead and overwrite it, but that would leave us with a problem: the parity block P0 would no longer accurately reflect the correct parity value of the stripe; in this example, P0 must also be updated. How can we update it both correctly and efficiently?\n\n\nIt turns out there are two methods. The first, known as\n   **additive parity**\n   , requires us to do the following. To compute the value of the new parity block, read in all of the other data blocks in the stripe in parallel (in the example, blocks 0, 2, and 3) and XOR those with the new block (1). The result is your new parity block. To complete the write, you can then write the new data and new parity to their respective disks, also in parallel.\n\n\nThe problem with this technique is that it scales with the number of disks, and thus in larger RAID-4s requires a high number of reads to compute parity. Thus, the\n   **subtractive parity**\n   method.\n\n\nFor example, imagine this string of bits (4 data bits, one parity):\n\n\n\nC0 | C1 | C2 | C3 | P\n0 | 0 | 1 | 1 | \\text{XOR}(0,0,1,1) = 0\n\n\nLet's imagine that we wish to overwrite bit C2 with a new value which we will call\n   \n    C_{2,new}\n   \n   . The subtractive method works in three steps. First, we read in the old data at C2 (\n   \n    C_{2,old} = 1\n   \n   ) and the old parity (\n   \n    P_{old} = 0\n   \n   ). Then, we compare the old data and the new data; if they are the same (e.g.,\n   \n    C_{2,new} = C_{2,old}\n   \n   ), then we know the parity bit will also remain the same (i.e.,\n   \n    P_{new} = P_{old}\n   \n   ). If, however, they are different, then we must flip the old parity bit to the opposite of its current state, that is, if (\n   \n    P_{old} == 1\n   \n   ),\n   \n    P_{new}\n   \n   will be set to 0; if (\n   \n    P_{old} == 0\n   \n   ),\n   \n    P_{new}\n   \n   will be set to 1. We can express this whole mess neatly with XOR (where\n   \n    \\oplus\n   \n   is the XOR operator):\n\n\nP_{new} = (C_{old} \\oplus C_{new}) \\oplus P_{old} \\quad (38.1)\n\n\nBecause we are dealing with blocks, not bits, we perform this calculation over all the bits in the block (e.g., 4096 bytes in each block multiplied by 8 bits per byte). Thus, in most cases, the new block will be different than the old block and thus the new parity block will too.\n\n\nYou should now be able to figure out when we would use the additive parity calculation and when we would use the subtractive method. Think about how many disks would need to be in the system so that the additive method performs fewer I/Os than the subtractive method; what is the cross-over point?\n\n\nFor this performance analysis, let us assume we are using the subtractive method. Thus, for each write, the RAID has to perform 4 physical I/Os (two reads and two writes). Now imagine there are lots of writes submitted to the RAID; how many can RAID-4 perform in parallel? To understand, let us again look at the RAID-4 layout (Figure 38.6).\n\n\n\nDisk 0 | Disk 1 | Disk 2 | Disk 3 | Disk 4\n0 | 1 | 2 | 3 | P0\n*4 | 5 | 6 | 7 | +P1\n8 | 9 | 10 | 11 | P2\n12 | *13 | 14 | 15 | +P3\n\n\nFigure 38.6:\n   **Example: Writes To 4, 13, And Respective Parity Blocks**\n\n\nNow imagine there were 2 small writes submitted to the RAID-4 at about the same time, to blocks 4 and 13 (marked with * in the diagram). The data for those disks is on disks 0 and 1, and thus the read and write to data could happen in parallel, which is good. The problem that arises is with the parity disk; both the requests have to read the related parity blocks for 4 and 13, parity blocks 1 and 3 (marked with +). Hopefully, the issue is now clear: the parity disk is a bottleneck under this type of workload; we sometimes thus call this the\n   **small-write problem**\n   for parity-based RAIDs. Thus, even though the data disks could be accessed in\n\n\nparallel, the parity disk prevents any parallelism from materializing; all writes to the system will be serialized because of the parity disk. Because the parity disk has to perform two I/Os (one read, one write) per logical I/O, we can compute the performance of small random writes in RAID-4 by computing the parity disk's performance on those two I/Os, and thus we achieve\n   \n    (R/2)\n   \n   MB/s. RAID-4 throughput under random small writes is terrible; it does not improve as you add disks to the system.\n\n\nWe conclude by analyzing I/O latency in RAID-4. As you now know, a single read (assuming no failure) is just mapped to a single disk, and thus its latency is equivalent to the latency of a single disk request. The latency of a single write requires two reads and then two writes; the reads can happen in parallel, as can the writes, and thus total latency is about twice that of a single disk (with some differences because we have to wait for both reads to complete and thus get the worst-case positioning time, but then the updates don't incur seek cost and thus may be a better-than-average positioning cost)."
        },
        {
          "name": "RAID Level 5: Rotating Parity",
          "content": "To address the small-write problem (at least, partially), Patterson, Gibson, and Katz introduced RAID-5. RAID-5 works almost identically to RAID-4, except that it\n   **rotates**\n   the parity block across drives (Figure 38.7).\n\n\n\nDisk 0 | Disk 1 | Disk 2 | Disk 3 | Disk 4\n0 | 1 | 2 | 3 | P0\n5 | 6 | 7 | P1 | 4\n10 | 11 | P2 | 8 | 9\n15 | P3 | 12 | 13 | 14\nP4 | 16 | 17 | 18 | 19\n\n\nFigure 38.7:\n   **RAID-5 With Rotated Parity**\n\n\nAs you can see, the parity block for each stripe is now rotated across the disks, in order to remove the parity-disk bottleneck for RAID-4.\n\n\n\n\n**RAID-5 Analysis**\n\n\nMuch of the analysis for RAID-5 is identical to RAID-4. For example, the effective capacity and failure tolerance of the two levels are identical. So are sequential read and write performance. The latency of a single request (whether a read or a write) is also the same as RAID-4.\n\n\nRandom read performance is a little better, because we can now utilize all disks. Finally, random write performance improves noticeably over RAID-4, as it allows for parallelism across requests. Imagine a write to block 4 and a write to block 10; this will turn into requests to disk 1 and disk 4 (for block 1 and its parity) and requests to disk 0 and disk 2 (for\n\n\n\n | RAID-0 | RAID-1 | RAID-4 | RAID-5\nCapacity | N \\cdot B | (N \\cdot B)/2 | (N-1) \\cdot B | (N-1) \\cdot B\nReliability | 0 | 1 (for sure)\n      \n\n       \\frac{N}{2}\n      \n      (if lucky) | 1 | 1\nThroughput |  |  |  | \nSequential Read | N \\cdot S | (N/2) \\cdot S^1 | (N-1) \\cdot S | (N-1) \\cdot S\nSequential Write | N \\cdot S | (N/2) \\cdot S^1 | (N-1) \\cdot S | (N-1) \\cdot S\nRandom Read | N \\cdot R | N \\cdot R | (N-1) \\cdot R | N \\cdot R\nRandom Write | N \\cdot R | (N/2) \\cdot R | \\frac{1}{2} \\cdot R | \\frac{N}{4} R\nLatency |  |  |  | \nRead | T | T | T | T\nWrite | T | T | 2T | 2T\n\n\n**RAID Capacity, Reliability, and Performance**\nblock 10 and its parity). Thus, they can proceed in parallel. In fact, we can generally assume that given a large number of random requests, we will be able to keep all the disks about evenly busy. If that is the case, then our total bandwidth for small writes will be\n   \n    \\frac{N}{4} \\cdot R\n   \n   MB/s. The factor of four loss is due to the fact that each RAID-5 write still generates 4 total I/O operations, which is simply the cost of using parity-based RAID.\n\n\nBecause RAID-5 is basically identical to RAID-4 except in the few cases where it is better, it has almost completely replaced RAID-4 in the marketplace. The only place where it has not is in systems that know they will never perform anything other than a large write, thus avoiding the small-write problem altogether [HLM94]; in those cases, RAID-4 is sometimes used as it is slightly simpler to build."
        },
        {
          "name": "RAID Comparison: A Summary",
          "content": "We now summarize our simplified comparison of RAID levels in Figure 38.8. Note that we have omitted a number of details to simplify our analysis. For example, when writing in a mirrored system, the average seek time is a little higher than when writing to just a single disk, because the seek time is the max of two seeks (one on each disk). Thus, random write performance to two disks will generally be a little less than random write performance of a single disk. Also, when updating the parity disk in RAID-4/5, the first read of the old parity will likely cause a full seek and rotation, but the second write of the parity will only result in rotation. Finally, sequential I/O to mirrored RAIDs pay a\n   \n    2\\times\n   \n   performance penalty as compared to other approaches\n   \n    1\n   \n   .\n\n\n1\n   \n   The\n   \n    1/2\n   \n   penalty assumes a naive read/write pattern for mirroring; a more sophisticated approach that issued large I/O requests to differing parts of each mirror could potentially achieve full bandwidth. Think about this to see if you can figure out why.\n\n\nHowever, the comparison in Figure 38.8 does capture the essential differences, and is useful for understanding tradeoffs across RAID levels. For the latency analysis, we simply use\n   \n    T\n   \n   to represent the time that a request to a single disk would take.\n\n\nTo conclude, if you strictly want performance and do not care about reliability, striping is obviously best. If, however, you want random I/O performance and reliability, mirroring is the best; the cost you pay is in lost capacity. If capacity and reliability are your main goals, then RAID-5 is the winner; the cost you pay is in small-write performance. Finally, if you are always doing sequential I/O and want to maximize capacity, RAID-5 also makes the most sense."
        },
        {
          "name": "Other Interesting RAID Issues",
          "content": "There are a number of other interesting ideas that one could (and perhaps should) discuss when thinking about RAID. Here are some things we might eventually write about.\n\n\nFor example, there are many other RAID designs, including Levels 2 and 3 from the original taxonomy, and Level 6 to tolerate multiple disk faults [C+04]. There is also what the RAID does when a disk fails; sometimes it has a\n   **hot spare**\n   sitting around to fill in for the failed disk. What happens to performance under failure, and performance during reconstruction of the failed disk? There are also more realistic fault models, to take into account\n   **latent sector errors**\n   or\n   **block corruption**\n   [B+08], and lots of techniques to handle such faults (see the data integrity chapter for details). Finally, you can even build RAID as a software layer: such\n   **software RAID**\n   systems are cheaper but have other problems, including the consistent-update problem [DAA05]."
        }
      ]
    },
    {
      "name": "Interlude: Files and Directories",
      "sections": [
        {
          "name": "Files And Directories",
          "content": "Two key abstractions have developed over time in the virtualization of storage. The first is the\n   **file**\n   . A file is simply a linear array of bytes, each of which you can read or write. Each file has some kind of\n   **low-level name**\n   , usually a number of some kind; often, the user is not aware of this\n\n\n\n\n![A diagram of a directory tree starting from the root directory '/'. The root directory has two children: 'foo' and 'bar'. The 'foo' directory has one child: 'bar.txt'. The 'bar' directory has two children: 'bar' and 'foo'. The second 'bar' directory has one child: 'bar.txt'. The 'foo' directory under 'bar' has one child: 'bar.txt'.](images/image_0097.jpeg)\n\n\ngraph TD\n    root((/)) --- foo1((foo))\n    root --- bar1((bar))\n    foo1 --- bar_txt1((bar.txt))\n    bar1 --- bar2((bar))\n    bar1 --- foo2((foo))\n    bar2 --- bar_txt2((bar.txt))\n    foo2 --- bar_txt3((bar.txt))\nA diagram of a directory tree starting from the root directory '/'. The root directory has two children: 'foo' and 'bar'. The 'foo' directory has one child: 'bar.txt'. The 'bar' directory has two children: 'bar' and 'foo'. The second 'bar' directory has one child: 'bar.txt'. The 'foo' directory under 'bar' has one child: 'bar.txt'.\n\n\nFigure 39.1: An Example Directory Tree\n\n\nname (as we will see). For historical reasons, the low-level name of a file is often referred to as its\n   **inode number**\n   (\n   **i-number**\n   ). We'll be learning a lot more about inodes in future chapters; for now, just assume that each file has an inode number associated with it.\n\n\nIn most systems, the OS does not know much about the structure of the file (e.g., whether it is a picture, or a text file, or C code); rather, the responsibility of the file system is simply to store such data persistently on disk and make sure that when you request the data again, you get what you put there in the first place. Doing so is not as simple as it seems!\n\n\nThe second abstraction is that of a\n   **directory**\n   . A directory, like a file, also has a low-level name (i.e., an inode number), but its contents are quite specific: it contains a list of (user-readable name, low-level name) pairs. For example, let's say there is a file with the low-level name \"10\", and it is referred to by the user-readable name of \"foo\". The directory that \"foo\" resides in thus would have an entry (\"foo\", \"10\") that maps the user-readable name to the low-level name. Each entry in a directory refers to either files or other directories. By placing directories within other directories, users are able to build an arbitrary\n   **directory tree**\n   (or\n   **directory hierarchy**\n   ), under which all files and directories are stored.\n\n\nThe directory hierarchy starts at a\n   **root directory**\n   (in UNIX-based systems, the root directory is simply referred to as\n   \n    /\n   \n   ) and uses some kind of\n   **separator**\n   to name subsequent\n   **sub-directories**\n   until the desired file or directory is named. For example, if a user created a directory\n   \n    foo\n   \n   in the root directory\n   \n    /\n   \n   , and then created a file\n   \n    bar.txt\n   \n   in the directory\n   \n    foo\n   \n   , we could refer to the file by its\n   **absolute pathname**\n   , which in this case would be\n   \n    /foo/bar.txt\n   \n   . See Figure 39.1 for a more complex directory tree; valid directories in the example are\n   \n    /\n   \n   ,\n   \n    /foo\n   \n   ,\n   \n    /bar\n   \n   ,\n   \n    /bar/bar\n   \n   ,\n   \n    /bar/foo\n   \n   and valid files are\n   \n    /foo/bar.txt\n   \n   and\n   \n    /bar/foo/bar.txt\n   \n   .\n\n\n**TIP: THINK CAREFULLY ABOUT NAMING**\nNaming is an important aspect of computer systems [SK09]. In UNIX systems, virtually everything that you can think of is named through the file system. Beyond just files, devices, pipes, and even processes [K84] can be found in what looks like a plain old file system. This uniformity of naming eases your conceptual model of the system, and makes the system simpler and more modular. Thus, whenever creating a system or interface, think carefully about what names you are using.\n\n\nDirectories and files can have the same name as long as they are in different locations in the file-system tree (e.g., there are two files named\n   \n    bar.txt\n   \n   in the figure,\n   \n    /foo/bar.txt\n   \n   and\n   \n    /bar/foo/bar.txt\n   \n   ).\n\n\nYou may also notice that the file name in this example often has two parts:\n   \n    bar\n   \n   and\n   \n    txt\n   \n   , separated by a period. The first part is an arbitrary name, whereas the second part of the file name is usually used to indicate the\n   **type**\n   of the file, e.g., whether it is C code (e.g.,\n   \n    .c\n   \n   ), or an image (e.g.,\n   \n    .jpg\n   \n   ), or a music file (e.g.,\n   \n    .mp3\n   \n   ). However, this is usually just a\n   **convention**\n   : there is usually no enforcement that the data contained in a file named\n   \n    main.c\n   \n   is indeed C source code.\n\n\nThus, we can see one great thing provided by the file system: a convenient way to\n   **name**\n   all the files we are interested in. Names are important in systems as the first step to accessing any resource is being able to name it. In UNIX systems, the file system thus provides a unified way to access files on disk, USB stick, CD-ROM, many other devices, and in fact many other things, all located under the single directory tree."
        },
        {
          "name": "The File System Interface",
          "content": "Let's now discuss the file system interface in more detail. We'll start with the basics of creating, accessing, and deleting files. You may think this is straightforward, but along the way we'll discover the mysterious call that is used to remove files, known as\n   \n    unlink()\n   \n   . Hopefully, by the end of this chapter, this mystery won't be so mysterious to you!"
        },
        {
          "name": "Creating Files",
          "content": "We'll start with the most basic of operations: creating a file. This can be accomplished with the\n   \n    open\n   \n   system call; by calling\n   \n    open()\n   \n   and passing it the\n   \n    O_CREAT\n   \n   flag, a program can create a new file. Here is some example code to create a file called \"foo\" in the current working directory:\n\n\nint fd = open(\"foo\", O_CREAT|O_WRONLY|O_TRUNC,\n              S_IRUSR|S_IWUSR);\n\n   CREAT()\n  \nThe older way of creating a file is to call\n   \n    creat()\n   \n   , as follows:\n\n\n// option: add second flag to set permissions\nint fd = creat(\"foo\");\nYou can think of\n   \n    creat()\n   \n   as\n   \n    open()\n   \n   with the following flags:\n   \n    O_CREAT\n   \n   |\n   \n    O_WRONLY\n   \n   |\n   \n    O_TRUNC\n   \n   . Because\n   \n    open()\n   \n   can create a file, the usage of\n   \n    creat()\n   \n   has somewhat fallen out of favor (indeed, it could just be implemented as a library call to\n   \n    open()\n   \n   ); however, it does hold a special place in UNIX lore. Specifically, when Ken Thompson was asked what he would do differently if he were redesigning UNIX, he replied: “I’d spell\n   \n    creat\n   \n   with an e.”\n\n\nThe routine\n   \n    open()\n   \n   takes a number of different flags. In this example, the second parameter creates the file (\n   \n    O_CREAT\n   \n   ) if it does not exist, ensures that the file can only be written to (\n   \n    O_WRONLY\n   \n   ), and, if the file already exists, truncates it to a size of zero bytes thus removing any existing content (\n   \n    O_TRUNC\n   \n   ). The third parameter specifies permissions, in this case making the file readable and writable by the owner.\n\n\nOne important aspect of\n   \n    open()\n   \n   is what it returns: a\n   **file descriptor**\n   . A file descriptor is just an integer, private per process, and is used in UNIX systems to access files; thus, once a file is opened, you use the file descriptor to read or write the file, assuming you have permission to do so. In this way, a file descriptor is a\n   **capability**\n   [L84], i.e., an opaque handle that gives you the power to perform certain operations. Another way to think of a file descriptor is as a pointer to an object of type\n   \n    file\n   \n   ; once you have such an object, you can call other “methods” to access the file, like\n   \n    read()\n   \n   and\n   \n    write()\n   \n   (we’ll see how to do so below).\n\n\nAs stated above, file descriptors are managed by the operating system on a per-process basis. This means some kind of simple structure (e.g., an array) is kept in the\n   \n    proc\n   \n   structure on UNIX systems. Here is the relevant piece from the\n   \n    xv6\n   \n   kernel [CK+08]:\n\n\nstruct proc {\n    ...\n    struct file *ofile[NOFILE]; // Open files\n    ...\n};\nA simple array (with a maximum of\n   \n    NOFILE\n   \n   open files), indexed by the file descriptor, tracks which files are opened on a per-process basis. Each entry of the array is actually just a pointer to a\n   \n    struct file\n   \n   , which will be used to track information about the file being read or written; we’ll discuss this further below.\n\n\n**TIP: USE STRACE (AND SIMILAR TOOLS)**\nThe\n   \n    strace\n   \n   tool provides an awesome way to see what programs are up to. By running it, you can trace which system calls a program makes, see the arguments and return codes, and generally get a very good idea of what is going on.\n\n\nThe tool also takes some arguments which can be quite useful. For example,\n   \n    -f\n   \n   follows any fork'd children too;\n   \n    -t\n   \n   reports the time of day at each call;\n   \n    -e trace=open,close,read,write\n   \n   only traces calls to those system calls and ignores all others. There are many other flags; read the man pages and find out how to harness this wonderful tool."
        },
        {
          "name": "Reading And Writing Files",
          "content": "Once we have some files, of course we might like to read or write them. Let's start by reading an existing file. If we were typing at a command line, we might just use the program\n   \n    cat\n   \n   to dump the contents of the file to the screen.\n\n\nprompt> echo hello > foo\nprompt> cat foo\nhello\nprompt>\nIn this code snippet, we redirect the output of the program\n   \n    echo\n   \n   to the file\n   \n    foo\n   \n   , which then contains the word “hello” in it. We then use\n   \n    cat\n   \n   to see the contents of the file. But how does the\n   \n    cat\n   \n   program access the file\n   \n    foo\n   \n   ?\n\n\nTo find this out, we'll use an incredibly useful tool to trace the system calls made by a program. On Linux, the tool is called\n   \n    strace\n   \n   ; other systems have similar tools (see\n   \n    dtruss\n   \n   on a Mac, or\n   \n    truss\n   \n   on some older UNIX variants). What\n   \n    strace\n   \n   does is trace every system call made by a program while it runs, and dump the trace to the screen for you to see.\n\n\nHere is an example of using\n   \n    strace\n   \n   to figure out what\n   \n    cat\n   \n   is doing (some calls removed for readability):\n\n\nprompt> strace cat foo\n...\nopen(\"foo\", O_RDONLY|O_LARGEFILE)      = 3\nread(3, \"hello\\n\", 4096)               = 6\nwrite(1, \"hello\\n\", 6)                 = 6\nhello\nread(3, \"\", 4096)                      = 0\nclose(3)                                = 0\n...\nprompt>\nThe first thing that\n   \n    cat\n   \n   does is open the file for reading. A couple of things we should note about this; first, that the file is only opened for reading (not writing), as indicated by the\n   \n    O_RDONLY\n   \n   flag; second, that the 64-bit offset is used (\n   \n    O_LARGEFILE\n   \n   ); third, that the call to\n   \n    open()\n   \n   succeeds and returns a file descriptor, which has the value of 3.\n\n\nWhy does the first call to\n   \n    open()\n   \n   return 3, not 0 or perhaps 1 as you might expect? As it turns out, each running process already has three files open, standard input (which the process can read to receive input), standard output (which the process can write to in order to dump information to the screen), and standard error (which the process can write error messages to). These are represented by file descriptors 0, 1, and 2, respectively. Thus, when you first open another file (as\n   \n    cat\n   \n   does above), it will almost certainly be file descriptor 3.\n\n\nAfter the open succeeds,\n   \n    cat\n   \n   uses the\n   \n    read()\n   \n   system call to repeatedly read some bytes from a file. The first argument to\n   \n    read()\n   \n   is the file descriptor, thus telling the file system which file to read; a process can of course have multiple files open at once, and thus the descriptor enables the operating system to know which file a particular read refers to. The second argument points to a buffer where the result of the\n   \n    read()\n   \n   will be placed; in the system-call trace above,\n   \n    strace\n   \n   shows the results of the read in this spot (\"hello\"). The third argument is the size of the buffer, which in this case is 4 KB. The call to\n   \n    read()\n   \n   returns successfully as well, here returning the number of bytes it read (6, which includes 5 for the letters in the word \"hello\" and one for an end-of-line marker).\n\n\nAt this point, you see another interesting result of the\n   \n    strace\n   \n   : a single call to the\n   \n    write()\n   \n   system call, to the file descriptor 1. As we mentioned above, this descriptor is known as the standard output, and thus is used to write the word \"hello\" to the screen as the program\n   \n    cat\n   \n   is meant to do. But does it call\n   \n    write()\n   \n   directly? Maybe (if it is highly optimized). But if not, what\n   \n    cat\n   \n   might do is call the library routine\n   \n    printf()\n   \n   ; internally,\n   \n    printf()\n   \n   figures out all the formatting details passed to it, and eventually writes to standard output to print the results to the screen.\n\n\nThe\n   \n    cat\n   \n   program then tries to read more from the file, but since there are no bytes left in the file, the\n   \n    read()\n   \n   returns 0 and the program knows that this means it has read the entire file. Thus, the program calls\n   \n    close()\n   \n   to indicate that it is done with the file \"foo\", passing in the corresponding file descriptor. The file is thus closed, and the reading of it thus complete.\n\n\nWriting a file is accomplished via a similar set of steps. First, a file is opened for writing, then the\n   \n    write()\n   \n   system call is called, perhaps repeatedly for larger files, and then\n   \n    close()\n   \n   . Use\n   \n    strace\n   \n   to trace writes to a file, perhaps of a program you wrote yourself, or by tracing the\n   \n    dd\n   \n   utility, e.g.,\n   \n    dd if=foo of=bar\n   \n   .\n\n\n\n\n**ASIDE: DATA STRUCTURE — THE OPEN FILE TABLE**\n\n\nEach process maintains an array of file descriptors, each of which refers to an entry in the system-wide\n   **open file table**\n   . Each entry in this table tracks which underlying file the descriptor refers to, the current offset, and other relevant details such as whether the file is readable or writable."
        },
        {
          "name": "Reading And Writing, But Not Sequentially",
          "content": "Thus far, we've discussed how to read and write files, but all access has been\n   **sequential**\n   ; that is, we have either read a file from the beginning to the end, or written a file out from beginning to end.\n\n\nSometimes, however, it is useful to be able to read or write to a specific offset within a file; for example, if you build an index over a text document, and use it to look up a specific word, you may end up reading from some\n   **random**\n   offsets within the document. To do so, we will use the\n   \n    lseek()\n   \n   system call. Here is the function prototype:\n\n\noff_t lseek(int fildes, off_t offset, int whence);\nThe first argument is familiar (a file descriptor). The second argument is the\n   \n    offset\n   \n   , which positions the\n   **file offset**\n   to a particular location within the file. The third argument, called\n   \n    whence\n   \n   for historical reasons, determines exactly how the seek is performed. From the man page:\n\n\nIf\n   \n    whence\n   \n   is\n   \n    SEEK_SET\n   \n   , the offset is set to\n   \n    offset\n   \n   bytes.\n   \n\n   If\n   \n    whence\n   \n   is\n   \n    SEEK_CUR\n   \n   , the offset is set to its current location plus\n   \n    offset\n   \n   bytes.\n   \n\n   If\n   \n    whence\n   \n   is\n   \n    SEEK_END\n   \n   , the offset is set to the size of the file plus\n   \n    offset\n   \n   bytes.\n\n\nAs you can tell from this description, for each file a process opens, the OS tracks a “current” offset, which determines where the next read or write will begin reading from or writing to within the file. Thus, part of the abstraction of an open file is that it has a current offset, which is updated in one of two ways. The first is when a read or write of\n   \n    N\n   \n   bytes takes place,\n   \n    N\n   \n   is added to the current offset; thus each read or write\n   *implicitly*\n   updates the offset. The second is\n   *explicitly*\n   with\n   \n    lseek\n   \n   , which changes the offset as specified above.\n\n\nThe offset, as you might have guessed, is kept in that\n   \n    struct file\n   \n   we saw earlier, as referenced from the\n   \n    struct proc\n   \n   . Here is a (simplified) xv6 definition of the structure:\n\n\nstruct file {\n    int ref;\n    char readable;\n    char writable;\n    struct inode *ip;\n    uint off;\n};\n\n   lseek()\n  \nThe poorly-named system call\n   \n    lseek()\n   \n   confuses many a student trying to understand disks and how the file systems atop them work. Do not confuse the two! The\n   \n    lseek()\n   \n   call simply changes a variable in OS memory that tracks, for a particular process, at which offset its next read or write will start. A disk seek occurs when a read or write issued to the disk is not on the same track as the last read or write, and thus necessitates a head movement. Making this even more confusing is the fact that calling\n   \n    lseek()\n   \n   to read or write from/to random parts of a file, and then reading/writing to those random parts, will indeed lead to more disk seeks. Thus, calling\n   \n    lseek()\n   \n   can lead to a seek in an upcoming read or write, but absolutely does not cause any disk I/O to occur itself.\n\n\nAs you can see in the structure, the OS can use this to determine whether the opened file is readable or writable (or both), which underlying file it refers to (as pointed to by the\n   \n    struct inode\n   \n   pointer\n   \n    ip\n   \n   ), and the current offset (\n   \n    off\n   \n   ). There is also a reference count (\n   \n    ref\n   \n   ), which we will discuss further below.\n\n\nThese file structures represent all of the currently opened files in the system; together, they are sometimes referred to as the\n   **open file table**\n   . The xv6 kernel just keeps these as an array, with one lock for the entire table:\n\n\nstruct {\n    struct spinlock lock;\n    struct file file[NFILE];\n} ftable;\nLet's make this a bit clearer with a few examples. First, let's track a process that opens a file (of size 300 bytes) and reads it by calling the\n   \n    read()\n   \n   system call repeatedly, each time reading 100 bytes. Here is a trace of the relevant system calls, along with the values returned by each system call, and the value of the current offset in the open file table for this file access:\n\n\n\nSystem Calls | Return Code | Current Offset\nfd = open(\"file\", O_RDONLY); | 3 | 0\nread(fd, buffer, 100); | 100 | 100\nread(fd, buffer, 100); | 100 | 200\nread(fd, buffer, 100); | 100 | 300\nread(fd, buffer, 100); | 0 | 300\nclose(fd); | 0 | –\n\n\nThere are a couple of items of interest to note from the trace. First, you can see how the current offset gets initialized to zero when the file is\n\n\nopened. Next, you can see how it is incremented with each\n   \n    read()\n   \n   by the process; this makes it easy for a process to just keep calling\n   \n    read()\n   \n   to get the next chunk of the file. Finally, you can see how at the end, an attempted\n   \n    read()\n   \n   past the end of the file returns zero, thus indicating to the process that it has read the file in its entirety.\n\n\nSecond, let's trace a process that opens the\n   *same*\n   file twice and issues a read to each of them.\n\n\n\nSystem Calls | Return Code | OFT[10]\n      \n      Current Offset | OFT[11]\n      \n      Current Offset\nfd1 = open(\"file\", O_RDONLY); | 3 | 0 | —\nfd2 = open(\"file\", O_RDONLY); | 4 | 0 | 0\nread(fd1, buffer1, 100); | 100 | 100 | 0\nread(fd2, buffer2, 100); | 100 | 100 | 100\nclose(fd1); | 0 | — | 100\nclose(fd2); | 0 | — | —\n\n\nIn this example, two file descriptors are allocated (3 and 4), and each refers to a\n   *different*\n   entry in the open file table (in this example, entries 10 and 11, as shown in the table heading; OFT stands for Open File Table). If you trace through what happens, you can see how each current offset is updated independently.\n\n\nIn one final example, a process uses\n   \n    lseek()\n   \n   to reposition the current offset before reading; in this case, only a single open file table entry is needed (as with the first example).\n\n\n\nSystem Calls | Return Code | Current Offset\nfd = open(\"file\", O_RDONLY); | 3 | 0\nlseek(fd, 200, SEEK_SET); | 200 | 200\nread(fd, buffer, 50); | 50 | 250\nclose(fd); | 0 | —\n\n\nHere, the\n   \n    lseek()\n   \n   call first sets the current offset to 200. The subsequent\n   \n    read()\n   \n   then reads the next 50 bytes, and updates the current offset accordingly."
        },
        {
          "name": "Shared File Table Entries: fork() And dup()",
          "content": "In many cases (as in the examples shown above), the mapping of file descriptor to an entry in the open file table is a one-to-one mapping. For example, when a process runs, it might decide to open a file, read it, and then close it; in this example, the file will have a unique entry in the open file table. Even if some other process reads the same file at the same time, each will have its own entry in the open file table. In this way, each logical\n\n\nint main(int argc, char *argv[]) {\n    int fd = open(\"file.txt\", O_RDONLY);\n    assert(fd >= 0);\n    int rc = fork();\n    if (rc == 0) {\n        rc = lseek(fd, 10, SEEK_SET);\n        printf(\"child: offset %d\\n\", rc);\n    } else if (rc > 0) {\n        (void) wait(NULL);\n        printf(\"parent: offset %d\\n\",\n            (int) lseek(fd, 0, SEEK_CUR));\n    }\n    return 0;\n}\nFigure 39.2:\n   **Shared Parent/Child File Table Entries (\n    \n     fork-seek.c\n    \n    )**\n\n\nreading or writing of a file is independent, and each has its own current offset while it accesses the given file.\n\n\nHowever, there are a few interesting cases where an entry in the open file table is\n   *shared*\n   . One of those cases occurs when a parent process creates a child process with\n   \n    fork()\n   \n   . Figure 39.2 shows a small code snippet in which a parent creates a child and then waits for it to complete. The child adjusts the current offset via a call to\n   \n    lseek()\n   \n   and then exits. Finally the parent, after waiting for the child, checks the current offset and prints out its value.\n\n\nWhen we run this program, we see the following output:\n\n\nprompt> ./fork-seek\nchild: offset 10\nparent: offset 10\nprompt>\nFigure 39.3 shows the relationships that connect each process's private descriptor array, the shared open file table entry, and the reference from it to the underlying file-system inode. Note that we finally make use of the\n   **reference count**\n   here. When a file table entry is shared, its reference count is incremented; only when both processes close the file (or exit) will the entry be removed.\n\n\nSharing open file table entries across parent and child is occasionally useful. For example, if you create a number of processes that are cooperatively working on a task, they can write to the same output file without any extra coordination. For more on what is shared by processes when\n   \n    fork()\n   \n   is called, please see the man pages.\n\n\n\n\n![Diagram illustrating processes sharing an open file table entry. A Parent process and a Child process both have file descriptors pointing to the same entry in the Open File Table. The entry shows refcnt: 2, off: 10, and inode: 1000 (file.txt).](images/image_0098.jpeg)\n\n\nThe diagram shows two processes, 'Parent' and 'Child', each with a 'File Descriptors' box. Both processes have a file descriptor labeled '3:' which points to the same entry in the 'Open File Table'. The 'Open File Table' entry contains 'refcnt: 2', 'off: 10', and 'inode:'. The 'inode:' field points to a box labeled 'Inode #1000 (file.txt)'.\n\n\nDiagram illustrating processes sharing an open file table entry. A Parent process and a Child process both have file descriptors pointing to the same entry in the Open File Table. The entry shows refcnt: 2, off: 10, and inode: 1000 (file.txt).\n\n\nFigure 39.3:\n   **Processes Sharing An Open File Table Entry**\n\n\nOne other interesting, and perhaps more useful, case of sharing occurs with the\n   \n    dup()\n   \n   system call (and its cousins,\n   \n    dup2()\n   \n   and\n   \n    dup3()\n   \n   ).\n\n\nThe\n   \n    dup()\n   \n   call allows a process to create a new file descriptor that refers to the same underlying open file as an existing descriptor. Figure 39.4 shows a small code snippet that shows how\n   \n    dup()\n   \n   can be used.\n\n\nThe\n   \n    dup()\n   \n   call (and, in particular,\n   \n    dup2()\n   \n   ) is useful when writing a UNIX shell and performing operations like output redirection; spend some time and think about why! And now, you are thinking: why didn't they tell me this when I was doing the shell project? Oh well, you can't get everything in the right order, even in an incredible book about operating systems. Sorry!\n\n\nint main(int argc, char *argv[]) {\n    int fd = open(\"README\", O_RDONLY);\n    assert(fd >= 0);\n    int fd2 = dup(fd);\n    // now fd and fd2 can be used interchangeably\n    return 0;\n}\nFigure 39.4:\n   **Shared File Table Entry With\n    \n     dup()\n    \n    (\n    \n     dup.c\n    \n    )**"
        },
        {
          "name": "Writing Immediately With fsync()",
          "content": "Most times when a program calls\n   \n    write()\n   \n   , it is just telling the file system: please write this data to persistent storage, at some point in the future. The file system, for performance reasons, will\n   **buffer**\n   such writes in memory for some time (say 5 seconds, or 30); at that later point in time, the write(s) will actually be issued to the storage device. From the perspective of the calling application, writes seem to complete quickly, and only in rare cases (e.g., the machine crashes after the\n   \n    write()\n   \n   call but before the write to disk) will data be lost.\n\n\nHowever, some applications require something more than this eventual guarantee. For example, in a database management system (DBMS), development of a correct recovery protocol requires the ability to force writes to disk from time to time.\n\n\nTo support these types of applications, most file systems provide some additional control APIs. In the UNIX world, the interface provided to applications is known as\n   \n    fsync(int fd)\n   \n   . When a process calls\n   \n    fsync()\n   \n   for a particular file descriptor, the file system responds by forcing all\n   **dirty**\n   (i.e., not yet written) data to disk, for the file referred to by the specified file descriptor. The\n   \n    fsync()\n   \n   routine returns once all of these writes are complete.\n\n\nHere is a simple example of how to use\n   \n    fsync()\n   \n   . The code opens the file\n   \n    foo\n   \n   , writes a single chunk of data to it, and then calls\n   \n    fsync()\n   \n   to ensure the writes are forced immediately to disk. Once the\n   \n    fsync()\n   \n   returns, the application can safely move on, knowing that the data has been persisted (if\n   \n    fsync()\n   \n   is correctly implemented, that is).\n\n\nint fd = open(\"foo\", O_CREAT|O_WRONLY|O_TRUNC,\n              S_IRUSR|S_IWUSR);\nassert(fd > -1);\nint rc = write(fd, buffer, size);\nassert(rc == size);\nrc = fsync(fd);\nassert(rc == 0);\nInterestingly, this sequence does not guarantee everything that you might expect; in some cases, you also need to\n   \n    fsync()\n   \n   the directory that contains the file\n   \n    foo\n   \n   . Adding this step ensures not only that the file itself is on disk, but that the file, if newly created, also is durably a part of the directory. Not surprisingly, this type of detail is often overlooked, leading to many application-level bugs [P+13,P+14]."
        },
        {
          "name": "Renaming Files",
          "content": "Once we have a file, it is sometimes useful to be able to give a file a different name. When typing at the command line, this is accomplished with\n   \n    mv\n   \n   command; in this example, the file\n   \n    foo\n   \n   is renamed\n   \n    bar\n   \n   :\n\n\nASIDE:\n   \n    MMAP()\n   \n   AND PERSISTENT MEMORY\n   \n\n   (Guest Aside by Terence Kelly)\n\n\n**Memory mapping**\n   is an alternative way to access persistent data in files. The\n   \n    mmap()\n   \n   system call creates a correspondence between byte offsets in a file and virtual addresses in the calling process; the former is called the\n   **backing file**\n   and the latter its\n   **in-memory image**\n   . The process can then access the backing file using CPU instructions (i.e., loads and stores) to the in-memory image.\n\n\nBy combining the persistence of files with the access semantics of memory, file-backed memory mappings support a software abstraction called\n   **persistent memory**\n   . The persistent memory style of programming can streamline applications by eliminating translation between different data formats for memory and storage [K19].\n\n\n1 p = mmap(NULL, file_size, PROT_READ|PROT_WRITE,\n2           MAP_SHARED, fd, 0);\n3 assert(p != MAP_FAILED);\n4 for (int i = 1; i < argc; i++)\n5   if (strcmp(argv[i], \"pop\") == 0) // pop\n6     if (p->n > 0) // stack not empty\n7       printf(\"%d\\n\", p->stack[--p->n]);\n8   } else { // push\n9     if (sizeof(pstack_t) + (1 + p->n) * sizeof(int)\n10        <= file_size) // stack not full\n11       p->stack[p->n++] = atoi(argv[i]);\n12 }\nThe program\n   \n    pstack.c\n   \n   (included on the OSTEP code github repo, with a snippet shown above) stores a persistent stack in file\n   \n    ps.img\n   \n   , which begins life as a bag of zeros, e.g., created on the command line via the\n   \n    truncate\n   \n   or\n   \n    dd\n   \n   utility. The file contains a count of the size of the stack and an array of integers holding stack contents. After\n   \n    mmap()\n   \n   -ing the backing file we can access the stack using C pointers to the in-memory image, e.g.,\n   \n    p->n\n   \n   accesses the number of items on the stack, and\n   \n    p->stack\n   \n   the array of integers. Because the stack is persistent, data push’d by one invocation of\n   \n    pstack\n   \n   can be pop’d by the next.\n\n\nA crash, e.g., between the increment and the assignment of the\n   \n    push\n   \n   , could leave our persistent stack in an inconsistent state. Applications prevent such damage by using mechanisms that update persistent memory atomically with respect to failure [K20].\n\n\nprompt> mv foo bar\nUsing\n   \n    strace\n   \n   , we can see that\n   \n    mv\n   \n   uses the system call\n   \n    rename(char *old, char *new)\n   \n   , which takes precisely two arguments: the original name of the file (\n   \n    old\n   \n   ) and the new name (\n   \n    new\n   \n   ).\n\n\nOne interesting guarantee provided by the\n   \n    rename()\n   \n   call is that it is (usually) implemented as an\n   **atomic**\n   call with respect to system crashes; if the system crashes during the renaming, the file will either be named the old name or the new name, and no odd in-between state can arise. Thus,\n   \n    rename()\n   \n   is critical for supporting certain kinds of applications that require an atomic update to file state.\n\n\nLet's be a little more specific here. Imagine that you are using a file editor (e.g.,\n   \n    emacs\n   \n   ), and you insert a line into the middle of a file. The file's name, for the example, is\n   \n    foo.txt\n   \n   . The way the editor might update the file to guarantee that the new file has the original contents plus the line inserted is as follows (ignoring error-checking for simplicity):\n\n\nint fd = open(\"foo.txt.tmp\", O_WRONLY|O_CREAT|O_TRUNC,\n              S_IRUSR|S_IWUSR);\nwrite(fd, buffer, size); // write out new version of file\nfsync(fd);\nclose(fd);\nrename(\"foo.txt.tmp\", \"foo.txt\");\nWhat the editor does in this example is simple: write out the new version of the file under a temporary name (\n   \n    foo.txt.tmp\n   \n   ), force it to disk with\n   \n    fsync()\n   \n   , and then, when the application is certain the new file metadata and contents are on the disk, rename the temporary file to the original file's name. This last step atomically swaps the new file into place, while concurrently deleting the old version of the file, and thus an atomic file update is achieved."
        },
        {
          "name": "Getting Information About Files",
          "content": "Beyond file access, we expect the file system to keep a fair amount of information about each file it is storing. We generally call such data about files\n   **metadata**\n   . To see the metadata for a certain file, we can use the\n   \n    stat()\n   \n   or\n   \n    fstat()\n   \n   system calls. These calls take a pathname (or file descriptor) to a file and fill in a\n   \n    stat\n   \n   structure as seen in Figure 39.5.\n\n\nYou can see that there is a lot of information kept about each file, including its size (in bytes), its low-level name (i.e., inode number), some ownership information, and some information about when the file was accessed or modified, among other things. To see this information, you can use the command line tool\n   \n    stat\n   \n   . In this example, we first create a file (called\n   \n    file\n   \n   ) and then use the\n   \n    stat\n   \n   command line tool to learn some things about the file.\n\n\nstruct stat {\n    dev_t      st_dev;           // ID of device containing file\n    ino_t      st_ino;           // inode number\n    mode_t     st_mode;          // protection\n    nlink_t    st_nlink;         // number of hard links\n    uid_t      st_uid;           // user ID of owner\n    gid_t      st_gid;           // group ID of owner\n    dev_t      st_rdev;          // device ID (if special file)\n    off_t      st_size;          // total size, in bytes\n    blksize_t  st_blksize;       // blocksize for filesystem I/O\n    blkcnt_t   st_blocks;        // number of blocks allocated\n    time_t     st_atime;         // time of last access\n    time_t     st_mtime;         // time of last modification\n    time_t     st_ctime;         // time of last status change\n};\nFigure 39.5:\n   **The\n    \n     stat\n    \n    structure.**\n\n\nHere is the output on Linux:\n\n\nprompt> echo hello > file\nprompt> stat file\n  File: `file'\n  Size: 6 Blocks: 8 IO Block: 4096   regular file\nDevice: 811h/2065d Inode: 67158084   Links: 1\nAccess: (0640/-rw-r-----)  Uid: (30686/remzi)\n        Gid: (30686/remzi)\nAccess: 2011-05-03 15:50:20.157594748 -0500\nModify: 2011-05-03 15:50:20.157594748 -0500\nChange: 2011-05-03 15:50:20.157594748 -0500\nEach file system usually keeps this type of information in a structure called an\n   **inode**\n\n    1\n   \n   . We'll be learning a lot more about inodes when we talk about file system implementation. For now, you should just think of an inode as a persistent data structure kept by the file system that has information like we see above inside of it. All inodes reside on disk; a copy of active ones are usually cached in memory to speed up access."
        },
        {
          "name": "Removing Files",
          "content": "At this point, we know how to create files and access them, either sequentially or not. But how do you delete files? If you've used UNIX, you probably think you know: just run the program\n   \n    rm\n   \n   . But what system call does\n   \n    rm\n   \n   use to remove a file?\n\n\n1\n   \n   Some file systems call these structures similar, but slightly different, names, such as\n   \n    dnodes\n   \n   ; the basic idea is similar however.\n\n\nLet's use our old friend\n   \n    strace\n   \n   again to find out. Here we remove that pesky file\n   \n    foo\n   \n   :\n\n\nprompt> strace rm foo\n...\nunlink(\"foo\")\n                = 0\n...\nWe've removed a bunch of unrelated cruft from the traced output, leaving just a single call to the mysteriously-named system call\n   \n    unlink()\n   \n   . As you can see,\n   \n    unlink()\n   \n   just takes the name of the file to be removed, and returns zero upon success. But this leads us to a great puzzle: why is this system call named\n   \n    unlink\n   \n   ? Why not just\n   \n    remove\n   \n   or\n   \n    delete\n   \n   ? To understand the answer to this puzzle, we must first understand more than just files, but also directories."
        },
        {
          "name": "Making Directories",
          "content": "Beyond files, a set of directory-related system calls enable you to make, read, and delete directories. Note you can never write to a directory directly. Because the format of the directory is considered file system metadata, the file system considers itself responsible for the integrity of directory data; thus, you can only update a directory indirectly by, for example, creating files, directories, or other object types within it. In this way, the file system makes sure that directory contents are as expected.\n\n\nTo create a directory, a single system call,\n   \n    mkdir()\n   \n   , is available. The eponymous\n   \n    mkdir\n   \n   program can be used to create such a directory. Let's take a look at what happens when we run the\n   \n    mkdir\n   \n   program to make a simple directory called\n   \n    foo\n   \n   :\n\n\nprompt> strace mkdir foo\n...\nmkdir(\"foo\", 0777)\n                = 0\n...\nprompt>\nWhen such a directory is created, it is considered \"empty\", although it does have a bare minimum of contents. Specifically, an empty directory has two entries: one entry that refers to itself, and one entry that refers to its parent. The former is referred to as the \".\" (dot) directory, and the latter as \"..\" (dot-dot). You can see these directories by passing a flag (-a) to the program\n   \n    ls\n   \n   :\n\n\nprompt> ls -a\n./  ../\nprompt> ls -al\ntotal 8\ndrwxr-x--- 2 remzi remzi   6 Apr 30 16:17 ./\ndrwxr-x--- 26 remzi remzi 4096 Apr 30 16:17 ../\n**TIP: BE WARY OF POWERFUL COMMANDS**\nThe program\n   \n    rm\n   \n   provides us with a great example of powerful commands, and how sometimes too much power can be a bad thing. For example, to remove a bunch of files at once, you can type something like:\n\n\nprompt> rm *\nwhere the\n   \n    *\n   \n   will match all files in the current directory. But sometimes you want to also delete the directories too, and in fact all of their contents. You can do this by telling\n   \n    rm\n   \n   to recursively descend into each directory, and remove its contents too:\n\n\nprompt> rm -rf *\nWhere you get into trouble with this small string of characters is when you issue the command, accidentally, from the root directory of a file system, thus removing every file and directory from it. Oops!\n\n\nThus, remember the double-edged sword of powerful commands; while they give you the ability to do a lot of work with a small number of keystrokes, they also can quickly and readily do a great deal of harm."
        },
        {
          "name": "Reading Directories",
          "content": "Now that we've created a directory, we might wish to read one too. Indeed, that is exactly what the program\n   \n    ls\n   \n   does. Let's write our own little tool like\n   \n    ls\n   \n   and see how it is done.\n\n\nInstead of just opening a directory as if it were a file, we instead use a new set of calls. Below is an example program that prints the contents of a directory. The program uses three calls,\n   \n    opendir()\n   \n   ,\n   \n    readdir()\n   \n   , and\n   \n    closedir()\n   \n   , to get the job done, and you can see how simple the interface is; we just use a simple loop to read one directory entry at a time, and print out the name and inode number of each file in the directory.\n\n\nint main(int argc, char *argv[]) {\n    DIR *dp = opendir(\".\");\n    assert(dp != NULL);\n    struct dirent *d;\n    while ((d = readdir(dp)) != NULL) {\n        printf(\"%lu %s\\n\", (unsigned long) d->d_ino,\n                d->d_name);\n    }\n    closedir(dp);\n    return 0;\n}\nThe declaration below shows the information available within each directory entry in the\n   \n    struct dirent\n   \n   data structure:\n\n\nstruct dirent {\n    char        d_name[256];    // filename\n    ino_t       d_ino;          // inode number\n    off_t       d_off;          // offset to the next dirent\n    unsigned short d_reclen;    // length of this record\n    unsigned char d_type;       // type of file\n};\nBecause directories are light on information (basically, just mapping the name to the inode number, along with a few other details), a program may want to call\n   \n    stat()\n   \n   on each file to get more information on each, such as its length or other detailed information. Indeed, this is exactly what\n   \n    ls\n   \n   does when you pass it the\n   \n    -l\n   \n   flag; try\n   \n    strace\n   \n   on\n   \n    ls\n   \n   with and without that flag to see for yourself."
        },
        {
          "name": "Deleting Directories",
          "content": "Finally, you can delete a directory with a call to\n   \n    rmdir()\n   \n   (which is used by the program of the same name,\n   \n    rmdir\n   \n   ). Unlike file deletion, however, removing directories is more dangerous, as you could potentially delete a large amount of data with a single command. Thus,\n   \n    rmdir()\n   \n   has the requirement that the directory be empty (i.e., only has “.” and “..” entries) before it is deleted. If you try to delete a non-empty directory, the call to\n   \n    rmdir()\n   \n   simply will fail."
        },
        {
          "name": "Hard Links",
          "content": "We now come back to the mystery of why removing a file is performed via\n   \n    unlink()\n   \n   , by understanding a new way to make an entry in the file system tree, through a system call known as\n   \n    link()\n   \n   . The\n   \n    link()\n   \n   system call takes two arguments, an old pathname and a new one; when you “link” a new file name to an old one, you essentially create another way to refer to the same file. The command-line program\n   \n    ln\n   \n   is used to do this, as we see in this example:\n\n\nprompt> echo hello > file\nprompt> cat file\nhello\nprompt> ln file file2\nprompt> cat file2\nhello\nHere we created a file with the word “hello” in it, and called the file\n   \n    file\n   \n\n    2\n   \n   . We then create a hard link to that file using the\n   \n    ln\n   \n   program. After this, we can examine the file by either opening\n   \n    file\n   \n   or\n   \n    file2\n   \n   .\n\n\nThe way\n   \n    link()\n   \n   works is that it simply creates another name in the directory you are creating the link to, and refers it to the\n   *same*\n   inode number (i.e., low-level name) of the original file. The file is not copied in any way; rather, you now just have two human-readable names (\n   \n    file\n   \n   and\n   \n    file2\n   \n   ) that both refer to the same file. We can even see this in the directory itself, by printing out the inode number of each file:\n\n\nprompt> ls -i file file2\n67158084 file\n67158084 file2\nprompt>\nBy passing the\n   \n    -i\n   \n   flag to\n   \n    ls\n   \n   , it prints out the inode number of each file (as well as the file name). And thus you can see what\n   \n    link\n   \n   really has done: just make a new reference to the same exact inode number (67158084 in this example).\n\n\nBy now you might be starting to see why\n   \n    unlink()\n   \n   is called\n   \n    unlink()\n   \n   . When you create a file, you are really doing\n   *two*\n   things. First, you are making a structure (the inode) that will track virtually all relevant information about the file, including its size, where its blocks are on disk, and so forth. Second, you are\n   *linking*\n   a human-readable name to that file, and putting that link into a directory.\n\n\nAfter creating a hard link to a file, the file system perceives no difference between the original file name (\n   \n    file\n   \n   ) and the newly created file name (\n   \n    file2\n   \n   ); indeed, they are both just links to the underlying metadata about the file, which is found in inode number 67158084.\n\n\nThus, to remove a file from the file system, we call\n   \n    unlink()\n   \n   . In the example above, we could for example remove the file named\n   \n    file\n   \n   , and still access the file without difficulty:\n\n\nprompt> rm file\nremoved `file'\nprompt> cat file2\nhello\nThe reason this works is because when the file system unlinks\n   \n    file\n   \n   , it checks a\n   **reference count**\n   within the inode number. This reference count (sometimes called the\n   **link count**\n   ) allows the file system to track how many different file names have been linked to this particular inode. When\n   \n    unlink()\n   \n   is called, it removes the “link” between the human-readable\n\n\n2\n   \n   Note again how creative the authors of this book are. We also used to have a cat named “Cat” (true story). However, she died, and we now have a hamster named “Hammy.” Update: Hammy is now dead too. The pet bodies are piling up.\n\n\nname (the file that is being deleted) to the given inode number, and decrements the reference count; only when the reference count reaches zero does the file system also free the inode and related data blocks, and thus truly “delete” the file.\n\n\nYou can see the reference count of a file using\n   \n    stat()\n   \n   of course. Let’s see what it is when we create and delete hard links to a file. In this example, we’ll create three links to the same file, and then delete them. Watch the link count!\n\n\nprompt> echo hello > file\nprompt> stat file\n... Inode: 67158084      Links: 1 ...\nprompt> ln file file2\nprompt> stat file\n... Inode: 67158084      Links: 2 ...\nprompt> stat file2\n... Inode: 67158084      Links: 2 ...\nprompt> ln file2 file3\nprompt> stat file\n... Inode: 67158084      Links: 3 ...\nprompt> rm file\nprompt> stat file2\n... Inode: 67158084      Links: 2 ...\nprompt> rm file2\nprompt> stat file3\n... Inode: 67158084      Links: 1 ...\nprompt> rm file3"
        },
        {
          "name": "Symbolic Links",
          "content": "There is one other type of link that is really useful, and it is called a\n   **symbolic link**\n   or sometimes a\n   **soft link**\n   . Hard links are somewhat limited: you can’t create one to a directory (for fear that you will create a cycle in the directory tree); you can’t hard link to files in other disk partitions (because inode numbers are only unique within a particular file system, not across file systems); etc. Thus, a new type of link called the symbolic link was created [MJLF84].\n\n\nTo create such a link, you can use the same program\n   \n    ln\n   \n   , but with the\n   \n    -s\n   \n   flag. Here is an example:\n\n\nprompt> echo hello > file\nprompt> ln -s file file2\nprompt> cat file2\nhello\nAs you can see, creating a soft link looks much the same, and the original file can now be accessed through the file name\n   \n    file\n   \n   as well as the symbolic link name\n   \n    file2\n   \n   .\n\n\nHowever, beyond this surface similarity, symbolic links are actually quite different from hard links. The first difference is that a symbolic link is actually a file itself, of a different type. We've already talked about regular files and directories; symbolic links are a third type the file system knows about. A\n   \n    stat\n   \n   on the symlink reveals all:\n\n\nprompt> stat file\n... regular file ...\nprompt> stat file2\n... symbolic link ...\nRunning\n   \n    ls\n   \n   also reveals this fact. If you look closely at the first character of the long-form of the output from\n   \n    ls\n   \n   , you can see that the first character in the left-most column is a\n   \n    -\n   \n   for regular files, a\n   \n    d\n   \n   for directories, and an\n   \n    l\n   \n   for soft links. You can also see the size of the symbolic link (4 bytes in this case) and what the link points to (the file named\n   \n    file\n   \n   ).\n\n\nprompt> ls -al\ndrwxr-x---  2 remzi remzi   29 May  3 19:10 ./\ndrwxr-x--- 27 remzi remzi 4096 May  3 15:14 ../\n-rw-r-----  1 remzi remzi    6 May  3 19:10 file\nlrwxrwxrwx  1 remzi remzi    4 May  3 19:10 file2 -> file\nThe reason that\n   \n    file2\n   \n   is 4 bytes is because the way a symbolic link is formed is by holding the pathname of the linked-to file as the data of the link file. Because we've linked to a file named\n   \n    file\n   \n   , our link file\n   \n    file2\n   \n   is small (4 bytes). If we link to a longer pathname, our link file would be bigger:\n\n\nprompt> echo hello > alongerfilename\nprompt> ln -s alongerfilename file3\nprompt> ls -al alongerfilename file3\n-rw-r-----  1 remzi remzi    6 May  3 19:17 alongerfilename\nlrwxrwxrwx  1 remzi remzi   15 May  3 19:17 file3 ->\n                                alongerfilename\nFinally, because of the way symbolic links are created, they leave the possibility for what is known as a\n   **dangling reference**\n   :\n\n\nprompt> echo hello > file\nprompt> ln -s file file2\nprompt> cat file2\nhello\nprompt> rm file\nprompt> cat file2\ncat: file2: No such file or directory\nAs you can see in this example, quite unlike hard links, removing the original file named\n   \n    file\n   \n   causes the link to point to a pathname that no longer exists."
        },
        {
          "name": "Permission Bits And Access Control Lists",
          "content": "The abstraction of a process provided two central virtualizations: of the CPU and of memory. Each of these gave the illusion to a process that it had its own\n   *private*\n   CPU and its own\n   *private*\n   memory; in reality, the OS underneath used various techniques to share limited physical resources among competing entities in a safe and secure manner.\n\n\nThe file system also presents a virtual view of a disk, transforming it from a bunch of raw blocks into much more user-friendly files and directories, as described within this chapter. However, the abstraction is notably different from that of the CPU and memory, in that files are commonly\n   *shared*\n   among different users and processes and are not (always) private. Thus, a more comprehensive set of mechanisms for enabling various degrees of sharing are usually present within file systems.\n\n\nThe first form of such mechanisms is the classic UNIX\n   **permission bits**\n   . To see permissions for a file\n   \n    foo.txt\n   \n   , just type:\n\n\nprompt> ls -l foo.txt\n-rw-r--r-- 1 remzi wheel 0 Aug 24 16:29 foo.txt\nWe'll just pay attention to the first part of this output, namely the\n   \n    -rw-r--r--\n   \n   . The first character here just shows the type of the file:\n   \n    -\n   \n   for a regular file (which\n   \n    foo.txt\n   \n   is),\n   \n    d\n   \n   for a directory,\n   \n    l\n   \n   for a symbolic link, and so forth; this is (mostly) not related to permissions, so we'll ignore it for now.\n\n\nWe are interested in the permission bits, which are represented by the next nine characters (\n   \n    rw-r--r--\n   \n   ). These bits determine, for each regular file, directory, and other entities, exactly who can access it and how.\n\n\nThe permissions consist of three groupings: what the\n   **owner**\n   of the file can do to it, what someone in a\n   **group**\n   can do to the file, and finally, what anyone (sometimes referred to as\n   **other**\n   ) can do. The abilities the owner, group member, or others can have include the ability to read the file, write it, or execute it.\n\n\nIn the example above, the first three characters of the output of\n   \n    ls\n   \n   show that the file is both readable and writable by the owner (\n   \n    rw-\n   \n   ), and only readable by members of the group\n   \n    wheel\n   \n   and also by anyone else in the system (\n   \n    r--\n   \n   followed by\n   \n    r--\n   \n   ).\n\n\nThe owner of the file can readily change these permissions, for example by using the\n   \n    chmod\n   \n   command (to change the\n   **file mode**\n   ). To remove the ability for anyone except the owner to access the file, you could type:\n\n\nprompt> chmod 600 foo.txt\n\n\n**ASIDE: SUPERUSER FOR FILE SYSTEMS**\n\n\nWhich user is allowed to do privileged operations to help administer the file system? For example, if an inactive user's files need to be deleted to save space, who has the rights to do so?\n\n\nOn local file systems, the common default is for there to be some kind of\n   **superuser**\n   (i.e.,\n   **root**\n   ) who can access all files regardless of privileges. In a distributed file system such as AFS (which has access control lists), a group called\n   \n    system:administrators\n   \n   contains users that are trusted to do so. In both cases, these trusted users represent an inherent security risk; if an attacker is able to somehow impersonate such a user, the attacker can access all the information in the system, thus violating expected privacy and protection guarantees.\n\n\nThis command enables the readable bit (4) and writable bit (2) for the owner (OR'ing them together yields the 6 above), but set the group and other permission bits to 0 and 0, respectively, thus setting the permissions to\n   \n    rw-----\n   \n   .\n\n\nThe execute bit is particularly interesting. For regular files, its presence determines whether a program can be run or not. For example, if we have a simple shell script called\n   \n    hello.csh\n   \n   , we may wish to run it by typing:\n\n\nprompt> ./hello.csh\nhello, from shell world.\nHowever, if we don't set the execute bit properly for this file, the following happens:\n\n\nprompt> chmod 600 hello.csh\nprompt> ./hello.csh\n./hello.csh: Permission denied.\nFor directories, the execute bit behaves a bit differently. Specifically, it enables a user (or group, or everyone) to do things like change directories (i.e.,\n   \n    cd\n   \n   ) into the given directory, and, in combination with the writable bit, create files therein. The best way to learn more about this: play around with it yourself! Don't worry, you (probably) won't mess anything up too badly.\n\n\nBeyond permissions bits, some file systems, such as the distributed file system known as AFS (discussed in a later chapter), include more sophisticated controls. AFS, for example, does this in the form of an\n   **access control list (ACL)**\n   per directory. Access control lists are a more general and powerful way to represent exactly who can access a given resource. In a file system, this enables a user to create a very specific list of who can and cannot read a set of files, in contrast to the somewhat limited owner/group/everyone model of permissions bits described above.\n\n\nFor example, here are the access controls for a private directory in one author’s AFS account, as shown by the\n   \n    fs listacl\n   \n   command:\n\n\nprompt> fs listacl private\nAccess list for private is\nNormal rights:\n  system:administrators rldwka\n  remzi rldwka\nThe listing shows that both the system administrators and the user\n   \n    remzi\n   \n   can lookup, insert, delete, and administer files in this directory, as well as read, write, and lock those files.\n\n\nTo allow someone (in this case, the other author) to access this directory, user\n   \n    remzi\n   \n   can just type the following command.\n\n\nprompt> fs setacl private/ andrea rl\nThere goes\n   \n    remzi\n   \n   ’s privacy! But now you have learned an even more important lesson: there can be no secrets in a good marriage, even within the file system\n   \n    3\n   \n   ."
        },
        {
          "name": "Making And Mounting A File System",
          "content": "We’ve now toured the basic interfaces to access files, directories, and certain special types of links. But there is one more topic we should discuss: how to assemble a full directory tree from many underlying file systems. This task is accomplished via first making file systems, and then mounting them to make their contents accessible.\n\n\nTo make a file system, most file systems provide a tool, usually referred to as\n   \n    mkfs\n   \n   (pronounced “make fs”), that performs exactly this task. The idea is as follows: give the tool, as input, a device (such as a disk partition, e.g.,\n   \n    /dev/sda1\n   \n   ) and a file system type (e.g.,\n   \n    ext3\n   \n   ), and it simply writes an empty file system, starting with a root directory, onto that disk partition. And\n   \n    mkfs\n   \n   said, let there be a file system!\n\n\nHowever, once such a file system is created, it needs to be made accessible within the uniform file-system tree. This task is achieved via the\n   \n    mount\n   \n   program (which makes the underlying system call\n   \n    mount()\n   \n   to do the real work). What\n   \n    mount\n   \n   does, quite simply is take an existing directory as a target\n   **mount point**\n   and essentially paste a new file system onto the directory tree at that point.\n\n\nAn example here might be useful. Imagine we have an unmounted\n   \n    ext3\n   \n   file system, stored in device partition\n   \n    /dev/sda1\n   \n   , that has the following contents: a root directory which contains two sub-directories,\n   \n    a\n   \n   and\n   \n    b\n   \n   , each of which in turn holds a single file named\n   \n    foo\n   \n   . Let’s say we wish to mount this file system at the mount point\n   \n    /home/users\n   \n   . We would type something like this:\n\n\n3\n   \n   Married happily since 1996, if you were wondering. We know, you weren’t.\n\n\n\n\n**TIP: BE WARY OF TOCTTOU**\n\n\nIn 1974, McPherrin noticed a problem in computer systems. Specifically, McPherrin noted that \"... if there exists a time interval between a validity-check and the operation connected with that validity-check, [and,] through multitasking, the validity-check variables can deliberately be changed during this time interval, resulting in an invalid operation being performed by the control program.\" We today call this the\n   **Time Of Check To Time Of Use (TOCTTOU)**\n   problem, and alas, it still can occur.\n\n\nA simple example, as described by Bishop and Dilger [BD96], shows how a user can trick a more trusted service and thus cause trouble. Imagine, for example, that a mail service runs as root (and thus has privilege to access all files on a system). This service appends an incoming message to a user's inbox file as follows. First, it calls\n   \n    lstat()\n   \n   to get information about the file, specifically ensuring that it is actually just a regular file owned by the target user, and not a link to another file that the mail server should not be updating. Then, after the check succeeds, the server updates the file with the new message.\n\n\nUnfortunately, the gap between the check and the update leads to a problem: the attacker (in this case, the user who is receiving the mail, and thus has permissions to access the inbox) switches the inbox file (via a call to\n   \n    rename()\n   \n   ) to point to a sensitive file such as\n   \n    /etc/passwd\n   \n   (which holds information about users and their passwords). If this switch happens at just the right time (between the check and the access), the server will blithely update the sensitive file with the contents of the mail. The attacker can now write to the sensitive file by sending an email, an escalation in privilege; by updating\n   \n    /etc/passwd\n   \n   , the attacker can add an account with root privileges and thus gain control of the system.\n\n\nThere are not any simple and great solutions to the TOCTTOU problem [T+08]. One approach is to reduce the number of services that need root privileges to run, which helps. The\n   \n    O_NOFOLLOW\n   \n   flag makes it so that\n   \n    open()\n   \n   will fail if the target is a symbolic link, thus avoiding attacks that require said links. More radical approaches, such as using a\n   **transactional file system**\n   [H+18], would solve the problem, but there aren't many transactional file systems in wide deployment. Thus, the usual (lame) advice: be careful when you write code that runs with high privileges!\n\n\nprompt> mount -t ext3 /dev/sda1 /home/users\nIf successful, the mount would thus make this new file system available. However, note how the new file system is now accessed. To look at the contents of the root directory, we would use\n   \n    ls\n   \n   like this:\n\n\nprompt> ls /home/users/\na b\nAs you can see, the pathname\n   \n    /home/users/\n   \n   now refers to the root of the newly-mounted directory. Similarly, we could access directories\n   \n    a\n   \n   and\n   \n    b\n   \n   with the pathnames\n   \n    /home/users/a\n   \n   and\n   \n    /home/users/b\n   \n   . Finally, the files named\n   \n    foo\n   \n   could be accessed via\n   \n    /home/users/a/foo\n   \n   and\n   \n    /home/users/b/foo\n   \n   . And thus the beauty of\n   \n    mount\n   \n   : instead of having a number of separate file systems,\n   \n    mount\n   \n   unifies all file systems into one tree, making naming uniform and convenient.\n\n\nTo see what is mounted on your system, and at which points, simply run the\n   \n    mount\n   \n   program. You'll see something like this:\n\n\n/dev/sda1 on / type ext3 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\n/dev/sda5 on /tmp type ext3 (rw)\n/dev/sda7 on /var/vice/cache type ext3 (rw)\ntmpfs on /dev/shm type tmpfs (rw)\nAFS on /afs type afs (rw)\nThis crazy mix shows that a whole number of different file systems, including\n   \n    ext3\n   \n   (a standard disk-based file system), the\n   \n    proc\n   \n   file system (a file system for accessing information about current processes),\n   \n    tmpfs\n   \n   (a file system just for temporary files), and\n   \n    AFS\n   \n   (a distributed file system) are all glued together onto this one machine's file-system tree."
        }
      ]
    },
    {
      "name": "File System Implementation",
      "sections": [
        {
          "name": "Overall Organization",
          "content": "We now develop the overall on-disk organization of the data structures of the\n   \n    vsfs\n   \n   file system. The first thing we'll need to do is divide the disk into\n   **blocks**\n   ; simple file systems use just one block size, and that's exactly what we'll do here. Let's choose a commonly-used size of 4 KB.\n\n\nThus, our view of the disk partition where we're building our file system is simple: a series of blocks, each of size 4 KB. The blocks are addressed from 0 to\n   \n    N - 1\n   \n   , in a partition of size\n   \n    N\n   \n   4-KB blocks. Assume we have a really small disk, with just 64 blocks:\n\n\n\n\n![Diagram showing a disk partition of 64 blocks, numbered 0 to 63. The blocks are arranged in two rows of 32 blocks each. The first row contains blocks 0 through 31, with labels 0, 7, 8, 15, 16, 23, 24, and 31. The second row contains blocks 32 through 63, with labels 32, 39, 40, 47, 48, 55, 56, and 63.](images/image_0099.jpeg)\n\n\n0 | 7 | 8 | 15 | 16 | 23 | 24 | 31\n32 | 39 | 40 | 47 | 48 | 55 | 56 | 63\n\n\nDiagram showing a disk partition of 64 blocks, numbered 0 to 63. The blocks are arranged in two rows of 32 blocks each. The first row contains blocks 0 through 31, with labels 0, 7, 8, 15, 16, 23, 24, and 31. The second row contains blocks 32 through 63, with labels 32, 39, 40, 47, 48, 55, 56, and 63.\n\n\nLet's now think about what we need to store in these blocks to build a file system. Of course, the first thing that comes to mind is user data. In fact, most of the space in any file system is (and should be) user data. Let's call the region of the disk we use for user data the\n   **data region**\n   , and,\n\n\nagain for simplicity, reserve a fixed portion of the disk for these blocks, say the last 56 of 64 blocks on the disk:\n\n\n\n\n![Diagram of a disk partition with 64 blocks. The first 8 blocks (0-7) are reserved for the inode table. The remaining 56 blocks (8-63) are divided into two Data Regions: blocks 8-23 and blocks 24-63.](images/image_0100.jpeg)\n\n\nThe diagram shows a horizontal row of 64 blocks, numbered 0 to 63. A bracket labeled \"Inodes\" spans blocks 0 through 7. A bracket labeled \"Data Region\" spans blocks 8 through 23. Another bracket labeled \"Data Region\" spans blocks 24 through 63.\n\n\nDiagram of a disk partition with 64 blocks. The first 8 blocks (0-7) are reserved for the inode table. The remaining 56 blocks (8-63) are divided into two Data Regions: blocks 8-23 and blocks 24-63.\n\n\nAs we learned about (a little) last chapter, the file system has to track information about each file. This information is a key piece of\n   **metadata**\n   , and tracks things like which data blocks (in the data region) comprise a file, the size of the file, its owner and access rights, access and modify times, and other similar kinds of information. To store this information, file systems usually have a structure called an\n   **inode**\n   (we'll read more about inodes below).\n\n\nTo accommodate inodes, we'll need to reserve some space on the disk for them as well. Let's call this portion of the disk the\n   **inode table**\n   , which simply holds an array of on-disk inodes. Thus, our on-disk image now looks like this picture, assuming that we use 5 of our 64 blocks for inodes (denoted by I's in the diagram):\n\n\n\n\n![Diagram of a disk partition with 64 blocks. The first 5 blocks (0-4) are reserved for the inode table, containing 'I' in each block. The remaining 59 blocks (5-63) are divided into two Data Regions: blocks 5-23 and blocks 24-63.](images/image_0101.jpeg)\n\n\nThe diagram shows a horizontal row of 64 blocks, numbered 0 to 63. A bracket labeled \"Inodes\" spans blocks 0 through 4, with each block containing the letter 'I'. A bracket labeled \"Data Region\" spans blocks 5 through 23. Another bracket labeled \"Data Region\" spans blocks 24 through 63.\n\n\nDiagram of a disk partition with 64 blocks. The first 5 blocks (0-4) are reserved for the inode table, containing 'I' in each block. The remaining 59 blocks (5-63) are divided into two Data Regions: blocks 5-23 and blocks 24-63.\n\n\nWe should note here that inodes are typically not that big, for example 128 or 256 bytes. Assuming 256 bytes per inode, a 4-KB block can hold 16 inodes, and our file system above contains 80 total inodes. In our simple file system, built on a tiny 64-block partition, this number represents the maximum number of files we can have in our file system; however, do note that the same file system, built on a larger disk, could simply allocate a larger inode table and thus accommodate more files.\n\n\nOur file system thus far has data blocks (D), and inodes (I), but a few things are still missing. One primary component that is still needed, as you might have guessed, is some way to track whether inodes or data blocks are free or allocated. Such\n   **allocation structures**\n   are thus a requisite element in any file system.\n\n\nMany allocation-tracking methods are possible, of course. For example, we could use a\n   **free list**\n   that points to the first free block, which then points to the next free block, and so forth. We instead choose a simple and popular structure known as a\n   **bitmap**\n   , one for the data region (the\n   **data bitmap**\n   ), and one for the inode table (the\n   **inode bitmap**\n   ). A bitmap is a\n\n\nsimple structure: each bit is used to indicate whether the corresponding object/block is free (0) or in-use (1). And thus our new on-disk layout, with an inode bitmap (i) and a data bitmap (d):\n\n\n\n\n![Diagram of the on-disk layout without a superblock. It shows a sequence of 64 blocks (0-63). Blocks 0-7 are the Inodes region, with block 0 containing 'i' and block 1 containing 'd'. Blocks 8-63 are the Data Region, divided into four 16-block segments (8-15, 16-23, 24-31, 32-39, 40-47, 48-55, 56-63). Each segment contains a block with 'i' and a block with 'd'.](images/image_0102.jpeg)\n\n\nDiagram of the on-disk layout without a superblock. It shows a sequence of 64 blocks (0-63). Blocks 0-7 are the Inodes region, with block 0 containing 'i' and block 1 containing 'd'. Blocks 8-63 are the Data Region, divided into four 16-block segments (8-15, 16-23, 24-31, 32-39, 40-47, 48-55, 56-63). Each segment contains a block with 'i' and a block with 'd'.\n\n\nYou may notice that it is a bit of overkill to use an entire 4-KB block for these bitmaps; such a bitmap can track whether 32K objects are allocated, and yet we only have 80 inodes and 56 data blocks. However, we just use an entire 4-KB block for each of these bitmaps for simplicity.\n\n\nThe careful reader (i.e., the reader who is still awake) may have noticed there is one block left in the design of the on-disk structure of our very simple file system. We reserve this for the\n   **superblock**\n   , denoted by an S in the diagram below. The superblock contains information about this particular file system, including, for example, how many inodes and data blocks are in the file system (80 and 56, respectively in this instance), where the inode table begins (block 3), and so forth. It will likely also include a magic number of some kind to identify the file system type (in this case, vsfs).\n\n\n\n\n![Diagram of the on-disk layout with a superblock. Block 0 is the Superblock, containing 'S', 'i', and 'd'. The rest of the layout is identical to the previous diagram, with Inodes (blocks 1-7) and Data Region (blocks 8-63).](images/image_0103.jpeg)\n\n\nDiagram of the on-disk layout with a superblock. Block 0 is the Superblock, containing 'S', 'i', and 'd'. The rest of the layout is identical to the previous diagram, with Inodes (blocks 1-7) and Data Region (blocks 8-63).\n\n\nThus, when mounting a file system, the operating system will read the superblock first, to initialize various parameters, and then attach the volume to the file-system tree. When files within the volume are accessed, the system will thus know exactly where to look for the needed on-disk structures."
        },
        {
          "name": "File Organization: The Inode",
          "content": "One of the most important on-disk structures of a file system is the\n   **inode**\n   ; virtually all file systems have a structure similar to this. The name inode is short for\n   **index node**\n   , the historical name given to it in UNIX [RT74] and possibly earlier systems, used because these nodes were originally arranged in an array, and the array\n   *indexed*\n   into when accessing a particular inode.\n\n\n\n\n**ASIDE: DATA STRUCTURE — THE INODE**\n\n\nThe\n   **inode**\n   is the generic name that is used in many file systems to describe the structure that holds the metadata for a given file, such as its length, permissions, and the location of its constituent blocks. The name goes back at least as far as UNIX (and probably further back to Multics if not earlier systems); it is short for\n   **index node**\n   , as the inode number is used to index into an array of on-disk inodes in order to find the inode of that number. As we'll see, design of the inode is one key part of file system design. Most modern systems have some kind of structure like this for every file they track, but perhaps call them different things (such as dnodes, fnodes, etc.).\n\n\nEach inode is implicitly referred to by a number (called the\n   **i-number**\n   ), which we've earlier called the\n   **low-level name**\n   of the file. In vsfs (and other simple file systems), given an i-number, you should directly be able to calculate where on the disk the corresponding inode is located. For example, take the inode table of vsfs as above: 20KB in size (five 4KB blocks) and thus consisting of 80 inodes (assuming each inode is 256 bytes); further assume that the inode region starts at 12KB (i.e. the superblock starts at 0KB, the inode bitmap is at address 4KB, the data bitmap at 8KB, and thus the inode table comes right after). In vsfs, we thus have the following layout for the beginning of the file system partition (in closeup view):\n\n\n\n\n![](images/image_0104.jpeg)\n\n\n**The Inode Table (Closeup)**\n\n\n\n |  |  | iblock 0 | iblock 1 | iblock 2 | iblock 3 | iblock 4\nSuper | i-bmap | d-bmap | 0 | 1 | 2 | 3 | 16\n17 | 18 | 19 | 32 | 33 | 34\n35 | 48 | 49 | 50 | 51 | 64\n65 | 66 | 67 | 68 | 69 | 70\n |  |  | 4 | 5 | 6 | 7 | 20\n |  |  | 21 | 22 | 23 | 36 | 37\n |  |  | 38 | 39 | 52 | 53 | 54\n |  |  | 55 | 56 | 57 | 58 | 59\n |  |  | 72 | 73 | 74 | 75 | 76\n |  |  | 77 | 78 | 79 |  | \n\n\n0KB   4KB   8KB   12KB   16KB   20KB   24KB   28KB   32KB\n\n\nTo read inode number 32, the file system would first calculate the offset into the inode region (\n   \n    32 \\cdot \\text{sizeof(inode)}\n   \n   or 8192), add it to the start address of the inode table on disk (\n   \n    \\text{inodeStartAddr} = 12\\text{KB}\n   \n   ), and thus arrive upon the correct byte address of the desired block of inodes:\n   \n    20\\text{KB}\n   \n   . Recall that disks are not byte addressable, but rather consist of a large number of addressable sectors, usually 512 bytes. Thus, to fetch the block of inodes that contains inode 32, the file system would issue a read to sector\n   \n    \\frac{20 \\times 1024}{512}\n   \n   , or 40, to fetch the desired inode block. More generally, the sector address\n   \n    sector\n   \n   of the inode block can be calculated as follows:\n\n\nblk    = (inumber * sizeof(inode_t)) / blockSize;\nsector = ((blk * blockSize) + inodeStartAddr) / sectorSize;\nInside each inode is virtually all of the information you need about a file: its\n   *type*\n   (e.g., regular file, directory, etc.), its\n   *size*\n   , the number of\n   *blocks*\n   allocated to it,\n   *protection information*\n   (such as who owns the file, as well\n\n\n\nSize | Name | What is this inode field for?\n2 | mode | can this file be read/written/executed?\n2 | uid | who owns this file?\n4 | size | how many bytes are in this file?\n4 | time | what time was this file last accessed?\n4 | ctime | what time was this file created?\n4 | mtime | what time was this file last modified?\n4 | dttime | what time was this inode deleted?\n2 | gid | which group does this file belong to?\n2 | links_count | how many hard links are there to this file?\n4 | blocks | how many blocks have been allocated to this file?\n4 | flags | how should ext2 use this inode?\n4 | osd1 | an OS-dependent field\n60 | block | a set of disk pointers (15 total)\n4 | generation | file version (used by NFS)\n4 | file_acl | a new permissions model beyond mode bits\n4 | dir_acl | called access control lists\n\n\nFigure 40.1: Simplified Ext2 Inode\n\n\nas who can access it), some\n   *time*\n   information, including when the file was created, modified, or last accessed, as well as information about where its data blocks reside on disk (e.g., pointers of some kind). We refer to all such information about a file as\n   **metadata**\n   ; in fact, any information inside the file system that isn't pure user data is often referred to as such. An example inode from ext2 [P09] is shown in Figure 40.1\n   \n    1\n   \n   .\n\n\nOne of the most important decisions in the design of the inode is how it refers to where data blocks are. One simple approach would be to have one or more\n   **direct pointers**\n   (disk addresses) inside the inode; each pointer refers to one disk block that belongs to the file. Such an approach is limited: for example, if you want to have a file that is really big (e.g., bigger than the block size multiplied by the number of direct pointers in the inode), you are out of luck.\n\n\n\n\n**The Multi-Level Index**\n\n\nTo support bigger files, file system designers have had to introduce different structures within inodes. One common idea is to have a special pointer known as an\n   **indirect pointer**\n   . Instead of pointing to a block that contains user data, it points to a block that contains more pointers, each of which point to user data. Thus, an inode may have some fixed number of direct pointers (e.g., 12), and a single indirect pointer. If a file grows large enough, an indirect block is allocated (from the data-block region of the disk), and the inode's slot for an indirect pointer is set to point to it. Assuming 4-KB blocks and 4-byte disk addresses, that adds another 1024 pointers; the file can grow to be\n   \n    (12 + 1024) \\cdot 4K\n   \n   or 4144KB.\n\n\n1\n   \n   Type info is kept in the directory entry, and thus is not found in the inode itself.\n\n\n**TIP: CONSIDER EXTENT-BASED APPROACHES**\nA different approach is to use\n   **extents**\n   instead of pointers. An extent is simply a disk pointer plus a length (in blocks); thus, instead of requiring a pointer for every block of a file, all one needs is a pointer and a length to specify the on-disk location of a file. Just a single extent is limiting, as one may have trouble finding a contiguous chunk of on-disk free space when allocating a file. Thus, extent-based file systems often allow for more than one extent, thus giving more freedom to the file system during file allocation.\n\n\nIn comparing the two approaches, pointer-based approaches are the most flexible but use a large amount of metadata per file (particularly for large files). Extent-based approaches are less flexible but more compact; in particular, they work well when there is enough free space on the disk and files can be laid out contiguously (which is the goal for virtually any file allocation policy anyhow).\n\n\nNot surprisingly, in such an approach, you might want to support even larger files. To do so, just add another pointer to the inode: the\n   **double indirect pointer**\n   . This pointer refers to a block that contains pointers to indirect blocks, each of which contain pointers to data blocks. A double indirect block thus adds the possibility to grow files with an additional\n   \n    1024 \\cdot 1024\n   \n   or 1-million 4KB blocks, in other words supporting files that are over 4GB in size. You may want even more, though, and we bet you know where this is headed: the\n   **triple indirect pointer**\n   .\n\n\nOverall, this imbalanced tree is referred to as the\n   **multi-level index**\n   approach to pointing to file blocks. Let's examine an example with twelve direct pointers, as well as both a single and a double indirect block. Assuming a block size of 4 KB, and 4-byte pointers, this structure can accommodate a file of just over 4 GB in size (i.e.,\n   \n    (12 + 1024 + 1024^2) \\times 4 \\text{ KB}\n   \n   ). Can you figure out how big of a file can be handled with the addition of a triple-indirect block? (hint: pretty big)\n\n\nMany file systems use a multi-level index, including commonly-used file systems such as Linux ext2 [P09] and ext3, NetApp's WAFL, as well as the original UNIX file system. Other file systems, including SGI XFS and Linux ext4, use\n   **extents**\n   instead of simple pointers; see the earlier aside for details on how extent-based schemes work (they are akin to segments in the discussion of virtual memory).\n\n\nYou might be wondering: why use an imbalanced tree like this? Why not a different approach? Well, as it turns out, many researchers have studied file systems and how they are used, and virtually every time they find certain “truths” that hold across the decades. One such finding is that\n   *most files are small*\n   . This imbalanced design reflects such a reality; if most files are indeed small, it makes sense to optimize for this case. Thus, with a small number of direct pointers (12 is a typical number), an inode\n\n\n\nMost files are small | ~2K is the most common size\nAverage file size is growing | Almost 200K is the average\nMost bytes are stored in large files | A few big files use most of space\nFile systems contain lots of files | Almost 100K on average\nFile systems are roughly half full | Even as disks grow, file systems remain ~50% full\nDirectories are typically small | Many have few entries; most have 20 or fewer\n\n\nFigure 40.2: File System Measurement Summary\n\n\ncan directly point to 48 KB of data, needing one (or more) indirect blocks for larger files. See Agrawal et. al [A+07] for a recent-ish study; Figure 40.2 summarizes those results.\n\n\nOf course, in the space of inode design, many other possibilities exist; after all, the inode is just a data structure, and any data structure that stores the relevant information, and can query it effectively, is sufficient. As file system software is readily changed, you should be willing to explore different designs should workloads or technologies change."
        },
        {
          "name": "Directory Organization",
          "content": "In vsfs (as in many file systems), directories have a simple organization; a directory basically just contains a list of (entry name, inode number) pairs. For each file or directory in a given directory, there is a string and a number in the data block(s) of the directory. For each string, there may also be a length (assuming variable-sized names).\n\n\nFor example, assume a directory\n   \n    dir\n   \n   (inode number 5) has three files in it (\n   \n    foo\n   \n   ,\n   \n    bar\n   \n   , and\n   \n    foobar_is_a_pretty_longname\n   \n   ), with inode numbers 12, 13, and 24 respectively. The on-disk data for\n   \n    dir\n   \n   might look like:\n\n\n\ninum | reclen | strlen | name\n5 | 12 | 2 | .\n2 | 12 | 3 | ..\n12 | 12 | 4 | foo\n13 | 12 | 4 | bar\n24 | 36 | 28 | foobar_is_a_pretty_longname\n\n\nIn this example, each entry has an inode number, record length (the total bytes for the name plus any left over space), string length (the actual length of the name), and finally the name of the entry. Note that each directory has two extra entries,\n   \n    .\n   \n   “dot” and\n   \n    ..\n   \n   “dot-dot”; the dot directory is just the current directory (in this example,\n   \n    dir\n   \n   ), whereas dot-dot is the parent directory (in this case, the root).\n\n\nDeleting a file (e.g., calling\n   \n    unlink()\n   \n   ) can leave an empty space in the middle of the directory, and hence there should be some way to mark that as well (e.g., with a reserved inode number such as zero). Such a delete is one reason the record length is used: a new entry may reuse an old, bigger entry and thus have extra space within.\n\n\n\n\n**ASIDE: LINKED-BASED APPROACHES**\n\n\nAnother simpler approach in designing inodes is to use a\n   **linked list**\n   . Thus, inside an inode, instead of having multiple pointers, you just need one, to point to the first block of the file. To handle larger files, add another pointer at the end of that data block, and so on, and thus you can support large files.\n\n\nAs you might have guessed, linked file allocation performs poorly for some workloads; think about reading the last block of a file, for example, or just doing random access. Thus, to make linked allocation work better, some systems will keep an in-memory table of link information, instead of storing the next pointers with the data blocks themselves. The table is indexed by the address of a data block\n   \n    D\n   \n   ; the content of an entry is simply\n   \n    D\n   \n   's next pointer, i.e., the address of the next block in a file which follows\n   \n    D\n   \n   . A null-value could be there too (indicating an end-of-file), or some other marker to indicate that a particular block is free. Having such a table of next pointers makes it so that a linked allocation scheme can effectively do random file accesses, simply by first scanning through the (in memory) table to find the desired block, and then accessing (on disk) it directly.\n\n\nDoes such a table sound familiar? What we have described is the basic structure of what is known as the\n   **file allocation table**\n   , or\n   **FAT**\n   file system. Yes, this classic old Windows file system, before NTFS [C94], is based on a simple linked-based allocation scheme. There are other differences from a standard UNIX file system too; for example, there are no inodes per se, but rather directory entries which store metadata about a file and refer directly to the first block of said file, which makes creating hard links impossible. See Brouwer [B02] for more of the inelegant details.\n\n\nYou might be wondering where exactly directories are stored. Often, file systems treat directories as a special type of file. Thus, a directory has an inode, somewhere in the inode table (with the type field of the inode marked as “directory” instead of “regular file”). The directory has data blocks pointed to by the inode (and perhaps, indirect blocks); these data blocks live in the data block region of our simple file system. Our on-disk structure thus remains unchanged.\n\n\nWe should also note again that this simple linear list of directory entries is not the only way to store such information. As before, any data structure is possible. For example, XFS [S+96] stores directories in B-tree form, making file create operations (which have to ensure that a file name has not been used before creating it) faster than systems with simple lists that must be scanned in their entirety.\n\n\n**ASIDE: FREE SPACE MANAGEMENT**\nThere are many ways to manage free space; bitmaps are just one way. Some early file systems used\n   **free lists**\n   , where a single pointer in the super block was kept to point to the first free block; inside that block the next free pointer was kept, thus forming a list through the free blocks of the system. When a block was needed, the head block was used and the list updated accordingly.\n\n\nModern file systems use more sophisticated data structures. For example, SGI's XFS [S+96] uses some form of a\n   **B-tree**\n   to compactly represent which chunks of the disk are free. As with any data structure, different time-space trade-offs are possible."
        },
        {
          "name": "Free Space Management",
          "content": "A file system must track which inodes and data blocks are free, and which are not, so that when a new file or directory is allocated, it can find space for it. Thus\n   **free space management**\n   is important for all file systems. In vsfs, we have two simple bitmaps for this task.\n\n\nFor example, when we create a file, we will have to allocate an inode for that file. The file system will thus search through the bitmap for an inode that is free, and allocate it to the file; the file system will have to mark the inode as used (with a 1) and eventually update the on-disk bitmap with the correct information. A similar set of activities take place when a data block is allocated.\n\n\nSome other considerations might also come into play when allocating data blocks for a new file. For example, some Linux file systems, such as ext2 and ext3, will look for a sequence of blocks (say 8) that are free when a new file is created and needs data blocks; by finding such a sequence of free blocks, and then allocating them to the newly-created file, the file system guarantees that a portion of the file will be contiguous on the disk, thus improving performance. Such a\n   **pre-allocation**\n   policy is thus a commonly-used heuristic when allocating space for data blocks."
        },
        {
          "name": "Access Paths: Reading and Writing",
          "content": "Now that we have some idea of how files and directories are stored on disk, we should be able to follow the flow of operation during the activity of reading or writing a file. Understanding what happens on this\n   **access path**\n   is thus the second key in developing an understanding of how a file system works; pay attention!\n\n\nFor the following examples, let us assume that the file system has been mounted and thus that the superblock is already in memory. Everything else (i.e., inodes, directories) is still on the disk.\n\n\n\n | data\n      \n      bitmap | inode\n      \n      bitmap | root\n      \n      inode | foo\n      \n      inode | bar\n      \n      inode | root\n      \n      data | foo\n      \n      data | bar\n      \n      data\n      \n      [0] | bar\n      \n      data\n      \n      [1] | bar\n      \n      data\n      \n      [2]\nopen(bar) |  |  | read | read |  | read |  | read |  | \nread() |  |  |  |  | read |  |  | read |  | \nread() |  |  |  |  | write |  |  |  | read | \nread() |  |  |  |  | read |  |  |  |  | read\n |  |  |  |  | write |  |  |  |  | \n\n\nFigure 40.3: File Read Timeline (Time Increasing Downward)\n\n\n\n\n**Reading A File From Disk**\n\n\nIn this simple example, let us first assume that you want to simply open a file (e.g.,\n   \n    /foo/bar\n   \n   ), read it, and then close it. For this simple example, let's assume the file is just 12KB in size (i.e., 3 blocks).\n\n\nWhen you issue an\n   \n    open(\"/foo/bar\", O_RDONLY)\n   \n   call, the file system first needs to find the inode for the file\n   \n    bar\n   \n   , to obtain some basic information about the file (permissions information, file size, etc.). To do so, the file system must be able to find the inode, but all it has right now is the full pathname. The file system must\n   **traverse**\n   the pathname and thus locate the desired inode.\n\n\nAll traversals begin at the root of the file system, in the\n   **root directory**\n   which is simply called\n   \n    /\n   \n   . Thus, the first thing the FS will read from disk is the inode of the root directory. But where is this inode? To find an inode, we must know its i-number. Usually, we find the i-number of a file or directory in its parent directory; the root has no parent (by definition). Thus, the root inode number must be “well known”; the FS must know what it is when the file system is mounted. In most UNIX file systems, the root inode number is 2. Thus, to begin the process, the FS reads in the block that contains inode number 2 (the first inode block).\n\n\nOnce the inode is read in, the FS can look inside of it to find pointers to data blocks, which contain the contents of the root directory. The FS will thus use these on-disk pointers to read through the directory, in this case looking for an entry for\n   \n    foo\n   \n   . By reading in one or more directory data blocks, it will find the entry for\n   \n    foo\n   \n   ; once found, the FS will also have found the inode number of\n   \n    foo\n   \n   (say it is 44) which it will need next.\n\n\nThe next step is to recursively traverse the pathname until the desired inode is found. In this example, the FS reads the block containing the\n\n\n**ASIDE: READS DON'T ACCESS ALLOCATION STRUCTURES**\nWe've seen many students get confused by allocation structures such as bitmaps. In particular, many often think that when you are simply reading a file, and not allocating any new blocks, that the bitmap will still be consulted. This is not true! Allocation structures, such as bitmaps, are only accessed when allocation is needed. The inodes, directories, and indirect blocks have all the information they need to complete a read request; there is no need to make sure a block is allocated when the inode already points to it.\n\n\ninode of\n   \n    foo\n   \n   and then its directory data, finally finding the inode number of\n   \n    bar\n   \n   . The final step of\n   \n    open()\n   \n   is to read\n   \n    bar\n   \n   's inode into memory; the FS then does a final permissions check, allocates a file descriptor for this process in the per-process open-file table, and returns it to the user.\n\n\nOnce\n   \n    open()\n   \n   , the program can then issue a\n   \n    read()\n   \n   system call to read from the file. The first read (at offset 0 unless\n   \n    lseek()\n   \n   has been called) will thus read in the first block of the file, consulting the inode to find the location of such a block; it may also update the inode with a new last-accessed time. The read will further update the in-memory open file table for this file descriptor, updating the file offset such that the next read will read the second file block, etc.\n\n\nAt some point, the file will be closed. There is much less work to be done here; clearly, the file descriptor should be deallocated, but for now, that is all the FS really needs to do. No disk I/Os take place.\n\n\nA depiction of this entire process is found in Figure 40.3 (page 11); time increases downward in the figure. In the figure, the\n   \n    open\n   \n   causes numerous reads to take place in order to finally locate the inode of the file. Afterwards, reading each block requires the file system to first consult the inode, then read the block, and then update the inode's last-accessed-time field with a write. Spend some time and understand what is going on.\n\n\nAlso note that the amount of I/O generated by the\n   \n    open\n   \n   is proportional to the length of the pathname. For each additional directory in the path, we have to read its inode as well as its data. Making this worse would be the presence of large directories; here, we only have to read one block to get the contents of a directory, whereas with a large directory, we might have to read many data blocks to find the desired entry. Yes, life can get pretty bad when reading a file; as you're about to find out, writing out a file (and especially, creating a new one) is even worse.\n\n\n\n\n**Writing A File To Disk**\n\n\nWriting to a file is a similar process. First, the file must be opened (as above). Then, the application can issue\n   \n    write()\n   \n   calls to update the file with new contents. Finally, the file is closed.\n\n\nUnlike reading, writing to the file may also\n   **allocate**\n   a block (unless the block is being overwritten, for example). When writing out a new file, each write not only has to write data to disk but has to first decide\n\n\n\n | data\n      \n      bitmap | inode\n      \n      bitmap | root\n      \n      inode | foo\n      \n      inode | bar\n      \n      inode | root\n      \n      data | foo\n      \n      data | bar\n      \n      data\n      \n      [0] | bar\n      \n      data\n      \n      [1] | bar\n      \n      data\n      \n      [2]\ncreate\n      \n      (/foo/bar) |  | read\n      \n      write | read | read |  | read |  | read | write | \n |  |  |  | write | read\n      \n      write |  |  |  |  | \nwrite() | read\n      \n      write |  |  |  | read |  |  |  | write | \n |  |  |  |  | write |  |  |  |  | \nwrite() | read\n      \n      write |  |  |  | read |  |  |  | write | \n |  |  |  |  | write |  |  |  |  | \nwrite() | read\n      \n      write |  |  |  | read |  |  |  |  | write\n |  |  |  |  | write |  |  |  |  | \n\n\n**File Creation Timeline (Time Increasing Downward)**\nwhich block to allocate to the file and thus update other structures of the disk accordingly (e.g., the data bitmap and inode). Thus, each write to a file logically generates five I/Os: one to read the data bitmap (which is then updated to mark the newly-allocated block as used), one to write the bitmap (to reflect its new state to disk), two more to read and then write the inode (which is updated with the new block's location), and finally one to write the actual block itself.\n\n\nThe amount of write traffic is even worse when one considers a simple and common operation such as file creation. To create a file, the file system must not only allocate an inode, but also allocate space within the directory containing the new file. The total amount of I/O traffic to do so is quite high: one read to the inode bitmap (to find a free inode), one write to the inode bitmap (to mark it allocated), one write to the new inode itself (to initialize it), one to the data of the directory (to link the high-level name of the file to its inode number), and one read and write to the directory inode to update it. If the directory needs to grow to accommodate the new entry, additional I/Os (i.e., to the data bitmap, and the new directory block) will be needed too. All that just to create a file!\n\n\nLet's look at a specific example, where the file\n   \n    /foo/bar\n   \n   is created, and three blocks are written to it. Figure 40.4 (page 13) shows what happens during the\n   \n    open()\n   \n   (which creates the file) and during each of three 4KB writes.\n\n\nIn the figure, reads and writes to the disk are grouped under which system call caused them to occur, and the rough ordering they might take place in goes from top to bottom of the figure. You can see how much work it is to create the file: 10 I/Os in this case, to walk the pathname and then finally create the file. You can also see that each allocating write costs 5 I/Os: a pair to read and update the inode, another pair to read and update the data bitmap, and then finally the write of the data itself. How can a file system accomplish any of this with reasonable efficiency?\n\n\n\n\n**THE CRUX: HOW TO REDUCE FILE SYSTEM I/O COSTS**\n\n\nEven the simplest of operations like opening, reading, or writing a file incurs a huge number of I/O operations, scattered over the disk. What can a file system do to reduce the high costs of doing so many I/Os?"
        },
        {
          "name": "Caching and Buffering",
          "content": "As the examples above show, reading and writing files can be expensive, incurring many I/Os to the (slow) disk. To remedy what would clearly be a huge performance problem, most file systems aggressively use system memory (DRAM) to cache important blocks.\n\n\nImagine the open example above: without caching, every file open would require at least two reads for every level in the directory hierarchy (one to read the inode of the directory in question, and at least one to read its data). With a long pathname (e.g.,\n   \n    /1/2/3/ ... /100/file.txt\n   \n   ), the file system would literally perform hundreds of reads just to open the file!\n\n\nEarly file systems thus introduced a\n   **fixed-size cache**\n   to hold popular blocks. As in our discussion of virtual memory, strategies such as\n   **LRU**\n   and different variants would decide which blocks to keep in cache. This fixed-size cache would usually be allocated at boot time to be roughly 10% of total memory.\n\n\nThis\n   **static partitioning**\n   of memory, however, can be wasteful; what if the file system doesn't need 10% of memory at a given point in time? With the fixed-size approach described above, unused pages in the file cache cannot be re-purposed for some other use, and thus go to waste.\n\n\nModern systems, in contrast, employ a\n   **dynamic partitioning**\n   approach. Specifically, many modern operating systems integrate virtual memory pages and file system pages into a\n   **unified page cache**\n   [S00]. In this way, memory can be allocated more flexibly across virtual memory and file system, depending on which needs more memory at a given time.\n\n\nNow imagine the file open example with caching. The first open may generate a lot of I/O traffic to read in directory inode and data, but sub-\n\n\n**TIP: UNDERSTAND STATIC VS. DYNAMIC PARTITIONING**\nWhen dividing a resource among different clients/users, you can use either\n   **static partitioning**\n   or\n   **dynamic partitioning**\n   . The static approach simply divides the resource into fixed proportions once; for example, if there are two possible users of memory, you can give some fixed fraction of memory to one user, and the rest to the other. The dynamic approach is more flexible, giving out differing amounts of the resource over time; for example, one user may get a higher percentage of disk bandwidth for a period of time, but then later, the system may switch and decide to give a different user a larger fraction of available disk bandwidth.\n\n\nEach approach has its advantages. Static partitioning ensures each user receives some share of the resource, usually delivers more predictable performance, and is often easier to implement. Dynamic partitioning can achieve better utilization (by letting resource-hungry users consume otherwise idle resources), but can be more complex to implement, and can lead to worse performance for users whose idle resources get consumed by others and then take a long time to reclaim when needed. As is often the case, there is no best method; rather, you should think about the problem at hand and decide which approach is most suitable. Indeed, shouldn't you always be doing that?\n\n\nsequent file opens of that same file (or files in the same directory) will mostly hit in the cache and thus no I/O is needed.\n\n\nLet us also consider the effect of caching on writes. Whereas read I/O can be avoided altogether with a sufficiently large cache, write traffic has to go to disk in order to become persistent. Thus, a cache does not serve as the same kind of filter on write traffic that it does for reads. That said,\n   **write buffering**\n   (as it is sometimes called) certainly has a number of performance benefits. First, by delaying writes, the file system can\n   **batch**\n   some updates into a smaller set of I/Os; for example, if an inode bitmap is updated when one file is created and then updated moments later as another file is created, the file system saves an I/O by delaying the write after the first update. Second, by buffering a number of writes in memory, the system can then\n   **schedule**\n   the subsequent I/Os and thus increase performance. Finally, some writes are avoided altogether by delaying them; for example, if an application creates a file and then deletes it, delaying the writes to reflect the file creation to disk\n   **avoids**\n   them entirely. In this case, laziness (in writing blocks to disk) is a virtue.\n\n\nFor the reasons above, most modern file systems buffer writes in memory for anywhere between five and thirty seconds, representing yet another trade-off: if the system crashes before the updates have been propagated to disk, the updates are lost; however, by keeping writes in memory longer, performance can be improved by batching, scheduling, and even avoiding writes.\n\n\n**TIP: UNDERSTAND THE DURABILITY/PERFORMANCE TRADE-OFF**\nStorage systems often present a durability/performance trade-off to users. If the user wishes data that is written to be immediately durable, the system must go through the full effort of committing the newly-written data to disk, and thus the write is slow (but safe). However, if the user can tolerate the loss of a little data, the system can buffer writes in memory for some time and write them later to the disk (in the background). Doing so makes writes appear to complete quickly, thus improving perceived performance; however, if a crash occurs, writes not yet committed to disk will be lost, and hence the trade-off. To understand how to make this trade-off properly, it is best to understand what the application using the storage system requires; for example, while it may be tolerable to lose the last few images downloaded by your web browser, losing part of a database transaction that is adding money to your bank account may be less tolerable. Unless you're rich, of course; in that case, why do you care so much about hoarding every last penny?\n\n\nSome applications (such as databases) don't enjoy this trade-off. Thus, to avoid unexpected data loss due to write buffering, they simply force writes to disk, by calling\n   \n    fsync()\n   \n   , by using\n   **direct I/O**\n   interfaces that work around the cache, or by using the\n   **raw disk**\n   interface and avoiding the file system altogether\n   \n    2\n   \n   . While most applications live with the trade-offs made by the file system, there are enough controls in place to get the system to do what you want it to, should the default not be satisfying."
        }
      ]
    },
    {
      "name": "Locality and The Fast File System",
      "sections": [
        {
          "name": "The Problem: Poor Performance",
          "content": "The problem: performance was terrible. As measured by Kirk McKusick and his colleagues at Berkeley [MJLF84], performance started off bad and got worse over time, to the point where the file system was delivering only 2% of overall disk bandwidth!\n\n\nThe main issue was that the old UNIX file system treated the disk like it was a random-access memory; data was spread all over the place without regard to the fact that the medium holding the data was a disk, and thus had real and expensive positioning costs. For example, the data blocks of a file were often very far away from its inode, thus inducing an expensive seek whenever one first read the inode and then the data blocks of a file (a pretty common operation).\n\n\nWorse, the file system would end up getting quite\n   **fragmented**\n   , as the free space was not carefully managed. The free list would end up pointing to a bunch of blocks spread across the disk, and as files got allocated, they would simply take the next free block. The result was that a logically contiguous file would be accessed by going back and forth across the disk, thus reducing performance dramatically.\n\n\nFor example, imagine the following data block region, which contains four files (A, B, C, and D), each of size 2 blocks:\n\n\n\nA1 | A2 | B1 | B2 | C1 | C2 | D1 | D2\n\n\nIf B and D are deleted, the resulting layout is:\n\n\n\nA1 | A2 |  |  | C1 | C2 |  | \n\n\nAs you can see, the free space is fragmented into two chunks of two blocks, instead of one nice contiguous chunk of four. Let's say you now wish to allocate a file E, of size four blocks:\n\n\n\nA1 | A2 | E1 | E2 | C1 | C2 | E3 | E4\n\n\nYou can see what happens: E gets spread across the disk, and as a result, when accessing E, you don't get peak (sequential) performance from the disk. Rather, you first read E1 and E2, then seek, then read E3 and E4. This fragmentation problem happened all the time in the old UNIX file system, and it hurt performance. A side note: this problem is exactly what disk\n   **defragmentation**\n   tools help with; they reorganize on-disk data to place files contiguously and make free space for one or a few contiguous regions, moving data around and then rewriting inodes and such to reflect the changes.\n\n\nOne other problem: the original block size was too small (512 bytes). Thus, transferring data from the disk was inherently inefficient. Smaller blocks were good because they minimized\n   **internal fragmentation**\n   (waste within the block), but bad for transfer as each block might require a positioning overhead to reach it. Thus, the problem:\n\n\n\n\n**THE CRUX:**\n\n\n\n\n**HOW TO ORGANIZE ON-DISK DATA TO IMPROVE PERFORMANCE**\n\n\nHow can we organize file system data structures so as to improve performance? What types of allocation policies do we need on top of those data structures? How do we make the file system “disk aware”?"
        },
        {
          "name": "FFS: Disk Awareness Is The Solution",
          "content": "A group at Berkeley decided to build a better, faster file system, which they cleverly called the\n   **Fast File System (FFS)**\n   . The idea was to design the file system structures and allocation policies to be “disk aware” and thus improve performance, which is exactly what they did. FFS thus ushered in a new era of file system research; by keeping the same\n   *interface*\n   to the file system (the same APIs, including\n   \n    open()\n   \n   ,\n   \n    read()\n   \n   ,\n   \n    write()\n   \n   ,\n   \n    close()\n   \n   , and other file system calls) but changing the internal\n   *implementation*\n   , the authors paved the path for new file system construction, work that continues today. Virtually all modern file systems adhere to the existing interface (and thus preserve compatibility with applications) while changing their internals for performance, reliability, or other reasons."
        },
        {
          "name": "Organizing Structure: The Cylinder Group",
          "content": "The first step was to change the on-disk structures. FFS divides the disk into a number of\n   **cylinder groups**\n   . A single\n   **cylinder**\n   is a set of tracks on different surfaces of a hard drive that are the same distance from the center of the drive; it is called a cylinder because of its clear resemblance to the so-called geometrical shape. FFS aggregates\n   \n    N\n   \n   consecutive cylinders into a group, and thus the entire disk can thus be viewed as a collection of cylinder groups. Here is a simple example, showing the four outer most tracks of a drive with six platters, and a cylinder group that consists of three cylinders:\n\n\n\n\n![Diagram of a disk platter showing tracks and cylinder groups.](images/image_0105.jpeg)\n\n\nThe diagram illustrates a disk platter with six platters (surfaces). It shows four outer tracks. A single track is highlighted in dark gray. A cylinder is defined as tracks at the same distance from the center across different surfaces. A cylinder group is a set of\n    \n     N\n    \n    consecutive cylinders. In this example, a cylinder group of three cylinders is shown, with the first cylinder not including the black track.\n\n\nDiagram of a disk platter showing tracks and cylinder groups.\n\n\nNote that modern drives do not export enough information for the file system to truly understand whether a particular cylinder is in use; as discussed previously [AD14a], disks export a logical address space of blocks and hide details of their geometry from clients. Thus, modern file\n\n\nsystems (such as Linux ext2, ext3, and ext4) instead organize the drive into\n   **block groups**\n   , each of which is just a consecutive portion of the disk's address space. The picture below illustrates an example where every 8 blocks are organized into a different block group (note that real groups would consist of many more blocks):\n\n\n\n\n![Diagram showing disk blocks organized into three groups: Group 0, Group 1, and Group 2. Each group contains 8 blocks represented by small rectangles in a row.](images/image_0106.jpeg)\n\n\nThe diagram shows a horizontal row of 24 small rectangular blocks representing disk blocks. These blocks are divided into three groups by vertical dashed lines. The first group, labeled 'Group 0', contains the first 8 blocks. The second group, labeled 'Group 1', contains the next 8 blocks. The third group, labeled 'Group 2', contains the final 8 blocks. Above the first block of each group, there are vertical dotted lines extending upwards.\n\n\nDiagram showing disk blocks organized into three groups: Group 0, Group 1, and Group 2. Each group contains 8 blocks represented by small rectangles in a row.\n\n\nWhether you call them cylinder groups or block groups, these groups are the central mechanism that FFS uses to improve performance. Critically, by placing two files within the same group, FFS can ensure that accessing one after the other will not result in long seeks across the disk.\n\n\nTo use these groups to store files and directories, FFS needs to have the ability to place files and directories into a group, and track all necessary information about them therein. To do so, FFS includes all the structures you might expect a file system to have within each group, e.g., space for inodes, data blocks, and some structures to track whether each of those are allocated or free. Here is a depiction of what FFS keeps within a single cylinder group:\n\n\n\n\n![Diagram of a cylinder group structure showing four regions: Super block (S), Inode bitmap (ib db), Inodes, and Data.](images/image_0107.jpeg)\n\n\nThe diagram shows a horizontal rectangular box divided into four vertical sections. The first section is labeled 'S' and is further divided into two sub-sections labeled 'ib' and 'db'. The second section is labeled 'Inodes'. The third section is labeled 'Data'. The fourth section is empty.\n\n\nDiagram of a cylinder group structure showing four regions: Super block (S), Inode bitmap (ib db), Inodes, and Data.\n\n\nLet's now examine the components of this single cylinder group in more detail. FFS keeps a copy of the\n   **super block**\n   (S) in each group for reliability reasons. The super block is needed to mount the file system; by keeping multiple copies, if one copy becomes corrupt, you can still mount and access the file system by using a working replica.\n\n\nWithin each group, FFS needs to track whether the inodes and data blocks of the group are allocated. A per-group\n   **inode bitmap**\n   (ib) and\n   **data bitmap**\n   (db) serve this role for inodes and data blocks in each group. Bitmaps are an excellent way to manage free space in a file system because it is easy to find a large chunk of free space and allocate it to a file, perhaps avoiding some of the fragmentation problems of the free list in the old file system.\n\n\nFinally, the\n   **inode**\n   and\n   **data block**\n   regions are just like those in the previous very-simple file system (VSFS). Most of each cylinder group, as usual, is comprised of data blocks.\n\n\n\n\n**ASIDE: FFS FILE CREATION**\n\n\nAs an example, think about what data structures must be updated when a file is created; assume, for this example, that the user creates a new file\n   \n    /foo/bar.txt\n   \n   and that the file is one block long (4KB). The file is new, and thus needs a new inode; thus, both the inode bitmap and the newly-allocated inode will be written to disk. The file also has data in it and thus it too must be allocated; the data bitmap and a data block will thus (eventually) be written to disk. Hence, at least four writes to the current cylinder group will take place (recall that these writes may be buffered in memory for a while before they take place). But this is not all! In particular, when creating a new file, you must also place the file in the file-system hierarchy, i.e., the directory must be updated. Specifically, the parent directory\n   \n    foo\n   \n   must be updated to add the entry for\n   \n    bar.txt\n   \n   ; this update may fit in an existing data block of\n   \n    foo\n   \n   or require a new block to be allocated (with associated data bitmap). The inode of\n   \n    foo\n   \n   must also be updated, both to reflect the new length of the directory as well as to update time fields (such as last-modified-time). Overall, it is a lot of work just to create a new file! Perhaps next time you do so, you should be more thankful, or at least surprised that it all works so well."
        },
        {
          "name": "Policies: How To Allocate Files and Directories",
          "content": "With this group structure in place, FFS now has to decide how to place files and directories and associated metadata on disk to improve performance. The basic mantra is simple:\n   *keep related stuff together*\n   (and its corollary,\n   *keep unrelated stuff far apart*\n   ).\n\n\nThus, to obey the mantra, FFS has to decide what is “related” and place it within the same block group; conversely, unrelated items should be placed into different block groups. To achieve this end, FFS makes use of a few simple placement heuristics.\n\n\nThe first is the placement of directories. FFS employs a simple approach: find the cylinder group with a low number of allocated directories (to balance directories across groups) and a high number of free inodes (to subsequently be able to allocate a bunch of files), and put the directory data and inode in that group. Of course, other heuristics could be used here (e.g., taking into account the number of free data blocks).\n\n\nFor files, FFS does two things. First, it makes sure (in the general case) to allocate the data blocks of a file in the same group as its inode, thus preventing long seeks between inode and data (as in the old file system). Second, it places all files that are in the same directory in the cylinder group of the directory they are in. Thus, if a user creates four files,\n   \n    /a/b\n   \n   ,\n   \n    /a/c\n   \n   ,\n   \n    /a/d\n   \n   , and\n   \n    b/f\n   \n   , FFS would try to place the first three near one another (same group) and the fourth far away (in some other group).\n\n\nLet’s look at an example of such an allocation. In the example, assume that there are only 10 inodes and 10 data blocks in each group (both\n\n\nunrealistically small numbers), and that the three directories (the root directory\n   \n    /\n   \n   ,\n   \n    /a\n   \n   , and\n   \n    /b\n   \n   ) and four files (\n   \n    /a/c\n   \n   ,\n   \n    /a/d\n   \n   ,\n   \n    /a/e\n   \n   ,\n   \n    /b/f\n   \n   ) are placed within them per the FFS policies. Assume the regular files are each two blocks in size, and that the directories have just a single block of data. For this figure, we use the obvious symbols for each file or directory (i.e.,\n   \n    /\n   \n   for the root directory,\n   \n    a\n   \n   for\n   \n    /a\n   \n   ,\n   \n    f\n   \n   for\n   \n    /b/f\n   \n   , and so forth).\n\n\n\ngroup | inodes | data\n0 | /----- | /-----\n1 | acde----- | accddee---\n2 | bf----- | bff-----\n3 | ----- | -----\n4 | ----- | -----\n5 | ----- | -----\n6 | ----- | -----\n7 | ----- | -----\n\n\nNote that the FFS policy does two positive things: the data blocks of each file are near each file's inode, and files in the same directory are near one another (namely,\n   \n    /a/c\n   \n   ,\n   \n    /a/d\n   \n   , and\n   \n    /a/e\n   \n   are all in Group 1, and directory\n   \n    /b\n   \n   and its file\n   \n    /b/f\n   \n   are near one another in Group 2).\n\n\nIn contrast, let's now look at an inode allocation policy that simply spreads inodes across groups, trying to ensure that no group's inode table fills up quickly. The final allocation might thus look something like this:\n\n\n\ngroup | inodes | data\n0 | /----- | /-----\n1 | a----- | a-----\n2 | b----- | b-----\n3 | c----- | cc-----\n4 | d----- | dd-----\n5 | e----- | ee-----\n6 | f----- | ff-----\n7 | ----- | -----\n\n\nAs you can see from the figure, while this policy does indeed keep file (and directory) data near its respective inode, files within a directory are arbitrarily spread around the disk, and thus name-based locality is not preserved. Access to files\n   \n    /a/c\n   \n   ,\n   \n    /a/d\n   \n   , and\n   \n    /a/e\n   \n   now spans three groups instead of one as per the FFS approach.\n\n\nThe FFS policy heuristics are not based on extensive studies of file-system traffic or anything particularly nuanced; rather, they are based on good old-fashioned\n   **common sense**\n   (isn't that what CS stands for after all?)\n   \n    1\n   \n   . Files in a directory\n   *are*\n   often accessed together: imagine compiling a bunch of files and then linking them into a single executable. Be-\n\n\n1\n   \n   Some people refer to common sense as\n   **horse sense**\n   , especially people who work regularly with horses. However, we have a feeling that this idiom may be lost as the “mechanized horse”, a.k.a. the car, gains in popularity. What will they invent next? A flying machine??!!\n\n\n\n\n![Figure 41.1: FFS Locality For SEER Traces. A line graph showing Cumulative Frequency (0% to 100%) versus Path Difference (0 to 10). The 'Trace' series (solid line with dots) shows higher locality, reaching 100% frequency at a path difference of 10. The 'Random' series (dashed line with crosses) shows lower locality, reaching approximately 95% frequency at a path difference of 10.](images/image_0108.jpeg)\n\n\nPath Difference | Trace (Cumulative Frequency) | Random (Cumulative Frequency)\n0 | 0% | 0%\n1 | 35% | 0%\n2 | 62% | 25%\n3 | 75% | 35%\n4 | 80% | 48%\n5 | 85% | 62%\n6 | 90% | 75%\n7 | 93% | 82%\n8 | 95% | 88%\n9 | 97% | 92%\n10 | 100% | 95%\n\n\nFigure 41.1: FFS Locality For SEER Traces. A line graph showing Cumulative Frequency (0% to 100%) versus Path Difference (0 to 10). The 'Trace' series (solid line with dots) shows higher locality, reaching 100% frequency at a path difference of 10. The 'Random' series (dashed line with crosses) shows lower locality, reaching approximately 95% frequency at a path difference of 10.\n\n\nFigure 41.1:\n   **FFS Locality For SEER Traces**\n\n\ncause such namespace-based locality exists, FFS will often improve performance, making sure that seeks between related files are nice and short."
        },
        {
          "name": "Measuring File Locality",
          "content": "To understand better whether these heuristics make sense, let's analyze some traces of file system access and see if indeed there is namespace locality. For some reason, there doesn't seem to be a good study of this topic in the literature.\n\n\nSpecifically, we'll use the SEER traces [K94] and analyze how \"far away\" file accesses were from one another in the directory tree. For example, if file\n   \n    f\n   \n   is opened, and then re-opened next in the trace (before any other files are opened), the distance between these two opens in the directory tree is zero (as they are the same file). If a file\n   \n    f\n   \n   in directory\n   \n    dir\n   \n   (i.e.,\n   \n    dir/f\n   \n   ) is opened, and followed by an open of file\n   \n    g\n   \n   in the same directory (i.e.,\n   \n    dir/g\n   \n   ), the distance between the two file accesses is one, as they share the same directory but are not the same file. Our distance metric, in other words, measures how far up the directory tree you have to travel to find the\n   *common ancestor*\n   of two files; the closer they are in the tree, the lower the metric.\n\n\nFigure 41.1 shows the locality observed in the SEER traces over all workstations in the SEER cluster over the entirety of all traces. The graph plots the difference metric along the x-axis, and shows the cumulative percentage of file opens that were of that difference along the y-axis. Specifically, for the SEER traces (marked \"Trace\" in the graph), you can see that about 7% of file accesses were to the file that was opened previ-\n\n\nously, and that nearly 40% of file accesses were to either the same file or to one in the same directory (i.e., a difference of zero or one). Thus, the FFS locality assumption seems to make sense (at least for these traces).\n\n\nInterestingly, another 25% or so of file accesses were to files that had a distance of two. This type of locality occurs when the user has structured a set of related directories in a multi-level fashion and consistently jumps between them. For example, if a user has a\n   \n    src\n   \n   directory and builds object files (\n   \n    .o\n   \n   files) into an\n   \n    obj\n   \n   directory, and both of these directories are sub-directories of a main\n   \n    proj\n   \n   directory, a common access pattern will be\n   \n    proj/src/foo.c\n   \n   followed by\n   \n    proj/obj/foo.o\n   \n   . The distance between these two accesses is two, as\n   \n    proj\n   \n   is the common ancestor. FFS does\n   *not*\n   capture this type of locality in its policies, and thus more seeking will occur between such accesses.\n\n\nFor comparison, the graph also shows locality for a “Random” trace. The random trace was generated by selecting files from within an existing SEER trace in random order, and calculating the distance metric between these randomly-ordered accesses. As you can see, there is less namespace locality in the random traces, as expected. However, because eventually every file shares a common ancestor (e.g., the root), there is some locality, and thus random is useful as a comparison point."
        },
        {
          "name": "The Large-File Exception",
          "content": "In FFS, there is one important exception to the general policy of file placement, and it arises for large files. Without a different rule, a large file would entirely fill the block group it is first placed within (and maybe others). Filling a block group in this manner is undesirable, as it prevents subsequent “related” files from being placed within this block group, and thus may hurt file-access locality.\n\n\nThus, for large files, FFS does the following. After some number of blocks are allocated into the first block group (e.g., 12 blocks, or the number of direct pointers available within an inode), FFS places the next “large” chunk of the file (e.g., those pointed to by the first indirect block) in another block group (perhaps chosen for its low utilization). Then, the next chunk of the file is placed in yet another different block group, and so on.\n\n\nLet’s look at some diagrams to understand this policy better. Without the large-file exception, a single large file would place all of its blocks into one part of the disk. We investigate a small example of a file (\n   \n    /a\n   \n   ) with 30 blocks in an FFS configured with 10 inodes and 40 data blocks per group. Here is the depiction of FFS without the large-file exception:\n\n\n\ngroup | inodes | data\n0 | /a----- | /aaaaaaaaaa aaaaaaaaaa aaaaaaaaaa a-----\n1 | ----- | -----\n2 | ----- | -----\n\n\nAs you can see in the picture,\n   \n    /a\n   \n   fills up most of the data blocks in Group 0, whereas other groups remain empty. If some other files are now created in the root directory (\n   \n    /\n   \n   ), there is not much room for their data in the group.\n\n\nWith the large-file exception (here set to five blocks in each chunk), FFS instead spreads the file spread across groups, and the resulting utilization within any one group is not too high:\n\n\n\ngroup | inodes | data\n0 | /a----- | /aaaaa-----\n1 | ----- | aaaaa-----\n2 | ----- | aaaaa-----\n3 | ----- | aaaaa-----\n4 | ----- | aaaaa-----\n5 | ----- | aaaaa-----\n6 | ----- | -----\n\n\nThe astute reader (that's you) will note that spreading blocks of a file across the disk will hurt performance, particularly in the relatively common case of sequential file access (e.g., when a user or application reads chunks 0 through 29 in order). And you are right, oh astute reader of ours! But you can address this problem by choosing chunk size carefully.\n\n\nSpecifically, if the chunk size is large enough, the file system will spend most of its time transferring data from disk and just a (relatively) little time seeking between chunks of the block. This process of reducing an overhead by doing more work per overhead paid is called\n   **amortization**\n   and is a common technique in computer systems.\n\n\nLet's do an example: assume that the average positioning time (i.e., seek and rotation) for a disk is 10 ms. Assume further that the disk transfers data at 40 MB/s. If your goal was to spend half our time seeking between chunks and half our time transferring data (and thus achieve 50% of peak disk performance), you would thus need to spend 10 ms transferring data for every 10 ms positioning. So the question becomes: how big does a chunk have to be in order to spend 10 ms in transfer? Easy, just use our old friend, math, in particular the dimensional analysis mentioned in the chapter on disks [AD14a]:\n\n\n\\frac{40 \\text{ MB}}{\\text{sec}} \\cdot \\frac{1024 \\text{ KB}}{1 \\text{ MB}} \\cdot \\frac{1 \\text{ sec}}{1000 \\text{ ms}} \\cdot 10 \\text{ ms} = 409.6 \\text{ KB} \\quad (41.1)\n\n\nBasically, what this equation says is this: if you transfer data at 40 MB/s, you need to transfer only 409.6KB every time you seek in order to spend half your time seeking and half your time transferring. Similarly, you can compute the size of the chunk you would need to achieve 90% of peak bandwidth (turns out it is about 3.6MB), or even 99% of peak bandwidth (39.6MB). As you can see, the closer you want to get to peak, the bigger these chunks get (see Figure 41.2 for a plot of these values).\n\n\nFFS did not use this type of calculation in order to spread large files across groups, however. Instead, it took a simple approach, based on the\n\n\n\n\n![A line graph titled 'The Challenges of Amortization' showing the relationship between Log(Chunk Size Needed) and Percent Bandwidth (Desired). The y-axis is logarithmic, with labels 1K, 32K, 1M, and 10M. The x-axis is linear, with labels 0%, 25%, 50%, 75%, and 100%. An orange curve starts at approximately (0%, 1K) and increases rapidly, passing through (25%, 32K), (50%, 409.6K), and (90%, 3.69M). Dashed lines connect these points to their respective values on the axes.](images/image_0109.jpeg)\n\n\nPercent Bandwidth (Desired) | Log(Chunk Size Needed)\n0% | 1K\n25% | 32K\n50% | 50%, 409.6K\n90% | 90%, 3.69M\n\n\nA line graph titled 'The Challenges of Amortization' showing the relationship between Log(Chunk Size Needed) and Percent Bandwidth (Desired). The y-axis is logarithmic, with labels 1K, 32K, 1M, and 10M. The x-axis is linear, with labels 0%, 25%, 50%, 75%, and 100%. An orange curve starts at approximately (0%, 1K) and increases rapidly, passing through (25%, 32K), (50%, 409.6K), and (90%, 3.69M). Dashed lines connect these points to their respective values on the axes.\n\n\nFigure 41.2:\n   **Amortization: How Big Do Chunks Have To Be?**\n\n\nstructure of the inode itself. The first twelve direct blocks were placed in the same group as the inode; each subsequent indirect block, and all the blocks it pointed to, was placed in a different group. With a block size of 4KB, and 32-bit disk addresses, this strategy implies that every 1024 blocks of the file (4MB) were placed in separate groups, the lone exception being the first 48KB of the file as pointed to by direct pointers.\n\n\nNote that the trend in disk drives is that transfer rate improves fairly rapidly, as disk manufacturers are good at cramming more bits into the same surface, but the mechanical aspects of drives related to seeks (disk arm speed and the rate of rotation) improve rather slowly [P98]. The implication is that over time, mechanical costs become relatively more expensive, and thus, to amortize said costs, you have to transfer more data between seeks."
        },
        {
          "name": "A Few Other Things About FFS",
          "content": "FFS introduced a few other innovations too. In particular, the designers were extremely worried about accommodating small files; as it turned out, many files were 2KB or so in size back then, and using 4KB blocks, while good for transferring data, was not so good for space efficiency. This\n   **internal fragmentation**\n   could thus lead to roughly half the disk being wasted for a typical file system.\n\n\nThe solution the FFS designers hit upon was simple and solved the problem. They decided to introduce\n   **sub-blocks**\n   , which were 512-byte little blocks that the file system could allocate to files. Thus, if you created a small file (say 1KB in size), it would occupy two sub-blocks and thus not\n\n\n\n\n![Figure 41.3: FFS: Standard Versus Parameterized Placement. Two disk diagrams showing sector layouts. The left diagram shows standard placement with sectors 0-11 in order. The right diagram shows parameterized placement with sectors 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11 in order.](images/image_0110.jpeg)\n\n\nThe diagram consists of two circular disk representations. Each disk has a central spindle and 12 numbered sectors from 0 to 11 arranged in a circle. The left disk shows sectors in sequential order: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11. The right disk shows sectors in a staggered order: 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11.\n\n\nFigure 41.3: FFS: Standard Versus Parameterized Placement. Two disk diagrams showing sector layouts. The left diagram shows standard placement with sectors 0-11 in order. The right diagram shows parameterized placement with sectors 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11 in order.\n\n\nFigure 41.3:\n   **FFS: Standard Versus Parameterized Placement**\n\n\nwaste an entire 4KB block. As the file grew, the file system will continue allocating 512-byte blocks to it until it acquires a full 4KB of data. At that point, FFS will find a 4KB block,\n   *copy*\n   the sub-blocks into it, and free the sub-blocks for future use.\n\n\nYou might observe that this process is inefficient, requiring a lot of extra work for the file system (in particular, a lot of extra I/O to perform the copy). And you'd be right again! Thus, FFS generally avoided this pessimism behavior by modifying the\n   \n    libc\n   \n   library; the library would buffer writes and then issue them in 4KB chunks to the file system, thus avoiding the sub-block specialization entirely in most cases.\n\n\nA second neat thing that FFS introduced was a disk layout that was optimized for performance. In those times (before SCSI and other more modern device interfaces), disks were much less sophisticated and required the host CPU to control their operation in a more hands-on way. A problem arose in FFS when a file was placed on consecutive sectors of the disk, as on the left in Figure 41.3.\n\n\nIn particular, the problem arose during sequential reads. FFS would first issue a read to block 0; by the time the read was complete, and FFS issued a read to block 1, it was too late: block 1 had rotated under the head and now the read to block 1 would incur a full rotation.\n\n\nFFS solved this problem with a different layout, as you can see on the right in Figure 41.3. By skipping over every other block (in the example), FFS has enough time to request the next block before it went past the disk head. In fact, FFS was smart enough to figure out for a particular disk\n   *how many*\n   blocks it should skip in doing layout in order to avoid the extra rotations; this technique was called\n   **parameterization**\n   , as FFS would figure out the specific performance parameters of the disk and use those to decide on the exact staggered layout scheme.\n\n\nYou might be thinking: this scheme isn't so great after all. In fact, you will only get 50% of peak bandwidth with this type of layout, because you have to go around each track twice just to read each block once. Fortunately, modern disks are much smarter: they internally read the entire track in and buffer it in an internal disk cache (often called a\n   **track buffer**\n   for this very reason). Then, on subsequent reads to the track, the disk will\n\n\n**TIP: MAKE THE SYSTEM USABLE**\nProbably the most basic lesson from FFS is that not only did it introduce the conceptually good idea of disk-aware layout, but it also added a number of features that simply made the system more usable. Long file names, symbolic links, and a rename operation that worked atomically all improved the utility of a system; while hard to write a research paper about (imagine trying to read a 14-pager about “The Symbolic Link: Hard Link’s Long Lost Cousin”), such small features made FFS more useful and thus likely increased its chances for adoption. Making a system usable is often as or more important than its deep technical innovations.\n\n\njust return the desired data from its cache. File systems thus no longer have to worry about these incredibly low-level details. Abstraction and higher-level interfaces can be a good thing, when designed properly.\n\n\nSome other usability improvements were added as well. FFS was one of the first file systems to allow for\n   **long file names**\n   , thus enabling more expressive names in the file system instead of the traditional fixed-size approach (e.g., 8 characters). Further, a new concept was introduced called a\n   **symbolic link**\n   . As discussed in a previous chapter [AD14b], hard links are limited in that they both could not point to directories (for fear of introducing loops in the file system hierarchy) and that they can only point to files within the same volume (i.e., the inode number must still be meaningful). Symbolic links allow the user to create an “alias” to any other file or directory on a system and thus are much more flexible. FFS also introduced an atomic\n   \n    rename()\n   \n   operation for renaming files. Usability improvements, beyond the basic technology, also likely gained FFS a stronger user base."
        }
      ]
    },
    {
      "name": "Crash Consistency: FSCK and Journaling",
      "sections": [
        {
          "name": "A Detailed Example",
          "content": "To kick off our investigation of journaling, let's look at an example. We'll need to use a\n   **workload**\n   that updates on-disk structures in some way. Assume here that the workload is simple: the append of a single data block to an existing file. The append is accomplished by opening the file, calling\n   \n    lseek()\n   \n   to move the file offset to the end of the file, and then issuing a single 4KB write to the file before closing it.\n\n\nLet's also assume we are using standard simple file system structures on the disk, similar to file systems we have seen before. This tiny example includes an\n   **inode bitmap**\n   (with just 8 bits, one per inode), a\n   **data bitmap**\n   (also 8 bits, one per data block), inodes (8 total, numbered 0 to 7), and spread across four blocks), and data blocks (8 total, numbered 0 to 7). Here is a diagram of this file system:\n\n\n\n\n![Diagram of a simplified file system structure showing Bitmaps, Inodes, and Data Blocks.](images/image_0111.jpeg)\n\n\nThe diagram illustrates a simplified file system layout across three main sections: Bitmaps, Inodes, and Data Blocks.\n\n\n  * **Bitmaps:**\n     Located at the top left, it consists of two rows of 8 bits each. The first row is labeled 'Inode Data' and the second 'Inodes'. The second row has the 2nd bit set to 1, indicating inode 2 is allocated.\n  * **Inodes:**\n     A row of 8 blocks, numbered 0 through 7. The 2nd block (inode 2) contains the text 'I[v1]', representing the first version of the inode.\n  * **Data Blocks:**\n     A row of 8 blocks, numbered 0 through 7. The 4th block (data block 4) contains the text 'Da', representing the data block.\n\n\nDiagram of a simplified file system structure showing Bitmaps, Inodes, and Data Blocks.\n\n\nIf you look at the structures in the picture, you can see that a single inode is allocated (inode number 2), which is marked in the inode bitmap, and a single allocated data block (data block 4), also marked in the data bitmap. The inode is denoted\n   \n    I[v1]\n   \n   , as it is the first version of this inode; it will soon be updated (due to the workload described above).\n\n\nLet's peek inside this simplified inode too. Inside of\n   \n    I[v1]\n   \n   , we see:\n\n\nowner       : remzi\npermissions : read-write\nsize       : 1\npointer    : 4\npointer    : null\npointer    : null\npointer    : null\nIn this simplified inode, the\n   *size*\n   of the file is 1 (it has one block allocated), the first direct pointer points to block 4 (the first data block of\n\n\nthe file,\n   \n    D_a\n   \n   ), and all three other direct pointers are set to\n   \n    null\n   \n   (indicating that they are not used). Of course, real inodes have many more fields; see previous chapters for more information.\n\n\nWhen we append to the file, we are adding a new data block to it, and thus must update three on-disk structures: the inode (which must point to the new block and record the new larger size due to the append), the new data block\n   \n    D_b\n   \n   , and a new version of the data bitmap (call it\n   \n    B[v2]\n   \n   ) to indicate that the new data block has been allocated.\n\n\nThus, in the memory of the system, we have three blocks which we must write to disk. The updated inode (inode version 2, or\n   \n    I[v2]\n   \n   for short) now looks like this:\n\n\nowner       : remzi\npermissions : read-write\nsize        : 2\npointer     : 4\npointer     : 5\npointer     : null\npointer     : null\nThe updated data bitmap (\n   \n    B[v2]\n   \n   ) now looks like this: 00001100. Finally, there is the data block (\n   \n    D_b\n   \n   ), which is just filled with whatever it is users put into files. Stolen music, perhaps?\n\n\nWhat we would like is for the final on-disk image of the file system to look like this:\n\n\n\n\n![Diagram of the file system layout on disk. It shows three main sections: Bitmaps, Inodes, and Data Blocks. The Bitmaps section has 8 bits. The Inodes section has 8 slots, with slot 2 containing 'I[v2]'. The Data Blocks section has 8 slots, with slots 4 and 5 containing 'Da' and 'Db' respectively. Below the Inodes and Data Blocks sections are indices 0 through 7.](images/image_0112.jpeg)\n\n\nThe diagram illustrates the on-disk layout of a file system. It is divided into three main sections:\n    **Bitmaps**\n    ,\n    **Inodes**\n    , and\n    **Data Blocks**\n    . The\n    **Bitmaps**\n    section is represented by a vertical bar with 8 segments. The\n    **Inodes**\n    section has 8 slots, with slot 2 containing the label\n    \n     I[v2]\n    \n    . The\n    **Data Blocks**\n    section has 8 slots, with slots 4 and 5 containing the labels\n    \n     D_a\n    \n    and\n    \n     D_b\n    \n    respectively. Below the\n    **Inodes**\n    and\n    **Data Blocks**\n    sections are indices 0 through 7.\n\n\nDiagram of the file system layout on disk. It shows three main sections: Bitmaps, Inodes, and Data Blocks. The Bitmaps section has 8 bits. The Inodes section has 8 slots, with slot 2 containing 'I[v2]'. The Data Blocks section has 8 slots, with slots 4 and 5 containing 'Da' and 'Db' respectively. Below the Inodes and Data Blocks sections are indices 0 through 7.\n\n\nTo achieve this transition, the file system must perform three separate writes to the disk, one each for the inode (\n   \n    I[v2]\n   \n   ), bitmap (\n   \n    B[v2]\n   \n   ), and data block (\n   \n    D_b\n   \n   ). Note that these writes usually don't happen immediately when the user issues a\n   \n    write()\n   \n   system call; rather, the dirty inode, bitmap, and new data will sit in main memory (in the\n   **page cache**\n   or\n   **buffer cache**\n   ) for some time first; then, when the file system finally decides to write them to disk (after say 5 seconds or 30 seconds), the file system will issue the requisite write requests to the disk. Unfortunately, a crash may occur and thus interfere with these updates to the disk. In particular, if a crash happens after one or two of these writes have taken place, but not all three, the file system could be left in a funny state.\n\n\n\n\n**Crash Scenarios**\n\n\nTo understand the problem better, let's look at some example crash scenarios. Imagine only a single write succeeds; there are thus three possible outcomes, which we list here:\n\n\n  * •\n    **Just the data block (Db) is written to disk.**\n    In this case, the data is on disk, but there is no inode that points to it and no bitmap that even says the block is allocated. Thus, it is as if the write never occurred. This case is not a problem at all, from the perspective of file-system crash consistency\n    \n     1\n    \n    .\n  * •\n    **Just the updated inode (I[v2]) is written to disk.**\n    In this case, the inode points to the disk address (5) where Db was about to be written, but Db has not yet been written there. Thus, if we trust that pointer, we will read\n    **garbage**\n    data from the disk (the old contents of disk address 5).\n\n\nFurther, we have a new problem, which we call a\n   **file-system inconsistency**\n   . The on-disk bitmap is telling us that data block 5 has not been allocated, but the inode is saying that it has. The disagreement between the bitmap and the inode is an inconsistency in the data structures of the file system; to use the file system, we must somehow resolve this problem (more on that below).\n\n\n  * •\n    **Just the updated bitmap (B[v2]) is written to disk.**\n    In this case, the bitmap indicates that block 5 is allocated, but there is no inode that points to it. Thus the file system is inconsistent again; if left unresolved, this write would result in a\n    **space leak**\n    , as block 5 would never be used by the file system.\n\n\nThere are also three more crash scenarios in this attempt to write three blocks to disk. In these cases, two writes succeed and the last one fails:\n\n\n  * •\n    **The inode (I[v2]) and bitmap (B[v2]) are written to disk, but not data (Db).**\n    In this case, the file system metadata is completely consistent: the inode has a pointer to block 5, the bitmap indicates that 5 is in use, and thus everything looks OK from the perspective of the file system’s metadata. But there is one problem: 5 has garbage in it again.\n  * •\n    **The inode (I[v2]) and the data block (Db) are written, but not the bitmap (B[v2]).**\n    In this case, we have the inode pointing to the correct data on disk, but again have an inconsistency between the inode and the old version of the bitmap (B1). Thus, we once again need to resolve the problem before using the file system.\n  * •\n    **The bitmap (B[v2]) and data block (Db) are written, but not the inode (I[v2]).**\n    In this case, we again have an inconsistency between the inode and the data bitmap. However, even though the block was written and the bitmap indicates its usage, we have no idea which file it belongs to, as no inode points to the file.\n\n\n1\n   \n   However, it might be a problem for the user, who just lost some data!\n\n\n\n\n**The Crash Consistency Problem**\n\n\nHopefully, from these crash scenarios, you can see the many problems that can occur to our on-disk file system image because of crashes: we can have inconsistency in file system data structures; we can have space leaks; we can return garbage data to a user; and so forth. What we'd like to do ideally is move the file system from one consistent state (e.g., before the file got appended to) to another\n   **atomically**\n   (e.g., after the inode, bitmap, and new data block have been written to disk). Unfortunately, we can't do this easily because the disk only commits one write at a time, and crashes or power loss may occur between these updates. We call this general problem the\n   **crash-consistency problem**\n   (we could also call it the\n   **consistent-update problem**\n   )."
        },
        {
          "name": "Solution #1: The File System Checker",
          "content": "Early file systems took a simple approach to crash consistency. Basically, they decided to let inconsistencies happen and then fix them later (when rebooting). A classic example of this lazy approach is found in a tool that does this:\n   \n    fsck\n   \n\n    2\n   \n   .\n   \n    fsck\n   \n   is a UNIX tool for finding such inconsistencies and repairing them [MK96]; similar tools to check and repair a disk partition exist on different systems. Note that such an approach can't fix all problems; consider, for example, the case above where the file system looks consistent but the inode points to garbage data. The only real goal is to make sure the file system metadata is internally consistent.\n\n\nThe tool\n   \n    fsck\n   \n   operates in a number of phases, as summarized in McKusick and Kowalski's paper [MK96]. It is run\n   *before*\n   the file system is mounted and made available (\n   \n    fsck\n   \n   assumes that no other file-system activity is on-going while it runs); once finished, the on-disk file system should be consistent and thus can be made accessible to users.\n\n\nHere is a basic summary of what\n   \n    fsck\n   \n   does:\n\n\n  * •\n    **Superblock:**\n\n     fsck\n    \n    first checks if the superblock looks reasonable, mostly doing sanity checks such as making sure the file system size is greater than the number of blocks that have been allocated. Usually the goal of these sanity checks is to find a suspect (corrupt) superblock; in this case, the system (or administrator) may decide to use an alternate copy of the superblock.\n  * •\n    **Free blocks:**\n    Next,\n    \n     fsck\n    \n    scans the inodes, indirect blocks, double indirect blocks, etc., to build an understanding of which blocks are currently allocated within the file system. It uses this knowledge to produce a correct version of the allocation bitmaps; thus, if there is any inconsistency between bitmaps and inodes, it is resolved by trusting the information within the inodes. The same type of check is performed for all the inodes, making sure that all inodes that look like they are in use are marked as such in the inode bitmaps.\n\n\n2\n   \n   Pronounced either \"eff-ess-see-kay\", \"eff-ess-check\", or, if you don't like the tool, \"eff-suck\". Yes, serious professional people use this term.\n\n\n  * •\n    **Inode state:**\n    Each inode is checked for corruption or other problems. For example,\n    \n     fsck\n    \n    makes sure that each allocated inode has a valid type field (e.g., regular file, directory, symbolic link, etc.). If there are problems with the inode fields that are not easily fixed, the inode is considered suspect and cleared by\n    \n     fsck\n    \n    ; the inode bitmap is correspondingly updated.\n  * •\n    **Inode links:**\n\n     fsck\n    \n    also verifies the link count of each allocated inode. As you may recall, the link count indicates the number of different directories that contain a reference (i.e., a link) to this particular file. To verify the link count,\n    \n     fsck\n    \n    scans through the entire directory tree, starting at the root directory, and builds its own link counts for every file and directory in the file system. If there is a mismatch between the newly-calculated count and that found within an inode, corrective action must be taken, usually by fixing the count within the inode. If an allocated inode is discovered but no directory refers to it, it is moved to the\n    \n     lost+found\n    \n    directory.\n  * •\n    **Duplicates:**\n\n     fsck\n    \n    also checks for duplicate pointers, i.e., cases where two different inodes refer to the same block. If one inode is obviously bad, it may be cleared. Alternately, the pointed-to block could be copied, thus giving each inode its own copy as desired.\n  * •\n    **Bad blocks:**\n    A check for bad block pointers is also performed while scanning through the list of all pointers. A pointer is considered “bad” if it obviously points to something outside its valid range, e.g., it has an address that refers to a block greater than the partition size. In this case,\n    \n     fsck\n    \n    can’t do anything too intelligent; it just removes (clears) the pointer from the inode or indirect block.\n  * •\n    **Directory checks:**\n\n     fsck\n    \n    does not understand the contents of user files; however, directories hold specifically formatted information created by the file system itself. Thus,\n    \n     fsck\n    \n    performs additional integrity checks on the contents of each directory, making sure that “.” and “..” are the first entries, that each inode referred to in a directory entry is allocated, and ensuring that no directory is linked to more than once in the entire hierarchy.\n\n\nAs you can see, building a working\n   \n    fsck\n   \n   requires intricate knowledge of the file system; making sure such a piece of code works correctly in all cases can be challenging [G+08]. However,\n   \n    fsck\n   \n   (and similar approaches) have a bigger and perhaps more fundamental problem: they are\n   *too slow*\n   . With a very large disk volume, scanning the entire disk to find all the allocated blocks and read the entire directory tree may take many minutes or hours. Performance of\n   \n    fsck\n   \n   , as disks grew in capacity and RAIDs grew in popularity, became prohibitive (despite recent advances [M+13]).\n\n\nAt a higher level, the basic premise of\n   \n    fsck\n   \n   seems just a tad irrational. Consider our example above, where just three blocks are written to the disk; it is incredibly expensive to scan the entire disk to fix problems that occurred during an update of just three blocks. This situation is akin to dropping your keys on the floor in your bedroom, and then com-\n\n\nmencing a\n   *search-the-entire-house-for-keys*\n   recovery algorithm, starting in the basement and working your way through every room. It works but is wasteful. Thus, as disks (and RAIDs) grew, researchers and practitioners started to look for other solutions."
        },
        {
          "name": "Solution #2:Journaling (or Write-Ahead Logging)",
          "content": "Probably the most popular solution to the consistent update problem is to steal an idea from the world of database management systems. That idea, known as\n   **write-ahead logging**\n   , was invented to address exactly this type of problem. In file systems, we usually call write-ahead logging\n   **journaling**\n   for historical reasons. The first file system to do this was Cedar [H87], though many modern file systems use the idea, including Linux ext3 and ext4, reiserfs, IBM's JFS, SGI's XFS, and Windows NTFS.\n\n\nThe basic idea is as follows. When updating the disk, before overwriting the structures in place, first write down a little note (somewhere else on the disk, in a well-known location) describing what you are about to do. Writing this note is the “write ahead” part, and we write it to a structure that we organize as a “log”; hence, write-ahead logging.\n\n\nBy writing the note to disk, you are guaranteeing that if a crash takes places during the update (overwrite) of the structures you are updating, you can go back and look at the note you made and try again; thus, you will know exactly what to fix (and how to fix it) after a crash, instead of having to scan the entire disk. By design, journaling thus adds a bit of work during updates to greatly reduce the amount of work required during recovery.\n\n\nWe'll now describe how\n   **Linux ext3**\n   , a popular journaling file system, incorporates journaling into the file system. Most of the on-disk structures are identical to\n   **Linux ext2**\n   , e.g., the disk is divided into block groups, and each block group contains an inode bitmap, data bitmap, inodes, and data blocks. The new key structure is the journal itself, which occupies some small amount of space within the partition or on another device. Thus, an ext2 file system (without journaling) looks like this:\n\n\n\n\n![](images/image_0113.jpeg)\n\n\nSuper | Group 0 | Group 1 | ... | Group N\n\n\nAssuming the journal is placed within the same file system image (though sometimes it is placed on a separate device, or as a file within the file system), an ext3 file system with a journal looks like this:\n\n\n\n\n![](images/image_0114.jpeg)\n\n\nSuper | Journal | Group 0 | Group 1 | ... | Group N\n\n\nThe real difference is just the presence of the journal, and of course, how it is used.\n\n\n\n\n**Data Journaling**\n\n\nLet's look at a simple example to understand how\n   **data journaling**\n   works. Data journaling is available as a mode with the Linux ext3 file system, from which much of this discussion is based.\n\n\nSay we have our canonical update again, where we wish to write the inode (\n   \n    I[v2]\n   \n   ), bitmap (\n   \n    B[v2]\n   \n   ), and data block (\n   \n    Db\n   \n   ) to disk again. Before writing them to their final disk locations, we are now first going to write them to the log (a.k.a. journal). This is what this will look like in the log:\n\n\n\n\n![Diagram of a journal log showing five blocks: TxB, I[v2], B[v2], Db, and TxE. An arrow points to the right from the TxE block.](images/image_0115.jpeg)\n\n\nJournal | TxB | I[v2] | B[v2] | Db | TxE | →\n\n\nDiagram of a journal log showing five blocks: TxB, I[v2], B[v2], Db, and TxE. An arrow points to the right from the TxE block.\n\n\nYou can see we have written five blocks here. The transaction begin (\n   \n    TxB\n   \n   ) tells us about this update, including information about the pending update to the file system (e.g., the final addresses of the blocks\n   \n    I[v2]\n   \n   ,\n   \n    B[v2]\n   \n   , and\n   \n    Db\n   \n   ), and some kind of\n   **transaction identifier (TID)**\n   . The middle three blocks just contain the exact contents of the blocks themselves; this is known as\n   **physical logging**\n   as we are putting the exact physical contents of the update in the journal (an alternate idea,\n   **logical logging**\n   , puts a more compact logical representation of the update in the journal, e.g., “this update wishes to append data block\n   \n    Db\n   \n   to file\n   \n    X\n   \n   ”, which is a little more complex but can save space in the log and perhaps improve performance). The final block (\n   \n    TxE\n   \n   ) is a marker of the end of this transaction, and will also contain the TID.\n\n\nOnce this transaction is safely on disk, we are ready to overwrite the old structures in the file system; this process is called\n   **checkpointing**\n   . Thus, to\n   **checkpoint**\n   the file system (i.e., bring it up to date with the pending update in the journal), we issue the writes\n   \n    I[v2]\n   \n   ,\n   \n    B[v2]\n   \n   , and\n   \n    Db\n   \n   to their disk locations as seen above; if these writes complete successfully, we have successfully checkpointed the file system and are basically done. Thus, our initial sequence of operations:\n\n\n  * 1.\n    **Journal write:**\n    Write the transaction, including a transaction-begin block, all pending data and metadata updates, and a transaction-end block, to the log; wait for these writes to complete.\n  * 2.\n    **Checkpoint:**\n    Write the pending metadata and data updates to their final locations in the file system.\n\n\nIn our example, we would write\n   \n    TxB\n   \n   ,\n   \n    I[v2]\n   \n   ,\n   \n    B[v2]\n   \n   ,\n   \n    Db\n   \n   , and\n   \n    TxE\n   \n   to the journal first. When these writes complete, we would complete the update by checkpointing\n   \n    I[v2]\n   \n   ,\n   \n    B[v2]\n   \n   , and\n   \n    Db\n   \n   , to their final locations on disk.\n\n\nThings get a little trickier when a crash occurs during the writes to the journal. Here, we are trying to write the set of blocks in the transaction (e.g.,\n   \n    TxB\n   \n   ,\n   \n    I[v2]\n   \n   ,\n   \n    B[v2]\n   \n   ,\n   \n    Db\n   \n   ,\n   \n    TxE\n   \n   ) to disk. One simple way to do this would be to issue each one at a time, waiting for each to complete, and then issuing the next. However, this is slow. Ideally, we'd like to issue\n\n\n\n\n**ASIDE: FORCING WRITES TO DISK**\n\n\nTo enforce ordering between two disk writes, modern file systems have to take a few extra precautions. In olden times, forcing ordering between two writes,\n   \n    A\n   \n   and\n   \n    B\n   \n   , was easy: just issue the write of\n   \n    A\n   \n   to the disk, wait for the disk to interrupt the OS when the write is complete, and then issue the write of\n   \n    B\n   \n   .\n\n\nThings got slightly more complex due to the increased use of write caches within disks. With write buffering enabled (sometimes called\n   **immediate reporting**\n   ), a disk will inform the OS the write is complete when it simply has been placed in the disk’s memory cache, and has not yet reached disk. If the OS then issues a subsequent write, it is not guaranteed to reach the disk after previous writes; thus ordering between writes is not preserved. One solution is to disable write buffering. However, more modern systems take extra precautions and issue explicit\n   **write barriers**\n   ; such a barrier, when it completes, guarantees that all writes issued before the barrier will reach disk before any writes issued after the barrier.\n\n\nAll of this machinery requires a great deal of trust in the correct operation of the disk. Unfortunately, recent research shows that some disk manufacturers, in an effort to deliver “higher performing” disks, explicitly ignore write-barrier requests, thus making the disks seemingly run faster but at the risk of incorrect operation [C+13, R+11]. As Kahan said, the fast almost always beats out the slow, even if the fast is wrong.\n\n\nall five block writes at once, as this would turn five writes into a single sequential write and thus be faster. However, this is unsafe, for the following reason: given such a big write, the disk internally may perform scheduling and complete small pieces of the big write in any order. Thus, the disk internally may (1) write\n   \n    TxB\n   \n   ,\n   \n    I[v2]\n   \n   ,\n   \n    B[v2]\n   \n   , and\n   \n    TxE\n   \n   and only later (2) write\n   \n    Db\n   \n   . Unfortunately, if the disk loses power between (1) and (2), this is what ends up on disk:\n\n\n\nJournal | TxB\n     \n\n\n      id=1 | I[v2] | B[v2] | ?? | TxE\n     \n\n\n      id=1 | →\n\n\nWhy is this a problem? Well, the transaction looks like a valid transaction (it has a begin and an end with matching sequence numbers). Further, the file system can’t look at that fourth block and know it is wrong; after all, it is arbitrary user data. Thus, if the system now reboots and runs recovery, it will replay this transaction, and ignorantly copy the contents of the garbage block ‘??’ to the location where\n   \n    Db\n   \n   is supposed to live. This is bad for arbitrary user data in a file; it is much worse if it happens to a critical piece of file system, such as the superblock, which could render the file system unmountable.\n\n\n\n\n**ASIDE: OPTIMIZING LOG WRITES**\n\n\nYou may have noticed a particular inefficiency of writing to the log. Namely, the file system first has to write out the transaction-begin block and contents of the transaction; only after these writes complete can the file system send the transaction-end block to disk. The performance impact is clear, if you think about how a disk works: usually an extra rotation is incurred (think about why).\n\n\nOne of our former graduate students, Vijayan Prabhakaran, had a simple idea to fix this problem [P+05]. When writing a transaction to the journal, include a checksum of the contents of the journal in the begin and end blocks. Doing so enables the file system to write the entire transaction at once, without incurring a wait; if, during recovery, the file system sees a mismatch in the computed checksum versus the stored checksum in the transaction, it can conclude that a crash occurred during the write of the transaction and thus discard the file-system update. Thus, with a small tweak in the write protocol and recovery system, a file system can achieve faster common-case performance; on top of that, the system is slightly more reliable, as any reads from the journal are now protected by a checksum.\n\n\nThis simple fix was attractive enough to gain the notice of Linux file system developers, who then incorporated it into the next generation Linux file system, called (you guessed it!)\n   **Linux ext4**\n   . It now ships on millions of machines worldwide, including the Android handheld platform. Thus, every time you write to disk on many Linux-based systems, a little code developed at Wisconsin makes your system a little faster and more reliable.\n\n\nTo avoid this problem, the file system issues the transactional write in two steps. First, it writes all blocks except the\n   \n    Tx_E\n   \n   block to the journal, issuing these writes all at once. When these writes complete, the journal will look something like this (assuming our append workload again):\n\n\n\n\n![Diagram of a journal with four blocks: Tx_B (id=1), I[v2], B[v2], and Db. An arrow points to the right, indicating the end of the journal.](images/image_0116.jpeg)\n\n\nJournal | Tx_B\n      \n\n\n       id=1 | I[v2] | B[v2] | Db | →\n\n\nDiagram of a journal with four blocks: Tx_B (id=1), I[v2], B[v2], and Db. An arrow points to the right, indicating the end of the journal.\n\n\nWhen those writes complete, the file system issues the write of the\n   \n    Tx_E\n   \n   block, thus leaving the journal in this final, safe state:\n\n\n\n\n![Diagram of a journal with five blocks: Tx_B (id=1), I[v2], B[v2], Db, and Tx_E (id=1). An arrow points to the right, indicating the end of the journal.](images/image_0117.jpeg)\n\n\nJournal | Tx_B\n      \n\n\n       id=1 | I[v2] | B[v2] | Db | Tx_E\n      \n\n\n       id=1 | →\n\n\nDiagram of a journal with five blocks: Tx_B (id=1), I[v2], B[v2], Db, and Tx_E (id=1). An arrow points to the right, indicating the end of the journal.\n\n\nAn important aspect of this process is the atomicity guarantee provided by the disk. It turns out that the disk guarantees that any 512-byte\n\n\nwrite will either happen or not (and never be half-written); thus, to make sure the write of Tx\n   \n    E\n   \n   is atomic, one should make it a single 512-byte block. Thus, our current protocol to update the file system, with each of its three phases labeled:\n\n\n  * 1.\n    **Journal write:**\n    Write the contents of the transaction (including Tx\n    \n     B\n    \n    , metadata, and data) to the log; wait for these writes to complete.\n  * 2.\n    **Journal commit:**\n    Write the transaction commit block (containing Tx\n    \n     E\n    \n    ) to the log; wait for write to complete; transaction is said to be\n    **committed**\n    .\n  * 3.\n    **Checkpoint:**\n    Write the contents of the update (metadata and data) to their final on-disk locations.\n\n\n\n\n**Recovery**\n\n\nLet's now understand how a file system can use the contents of the journal to\n   **recover**\n   from a crash. A crash may happen at any time during this sequence of updates. If the crash happens before the transaction is written safely to the log (i.e., before Step 2 above completes), then our job is easy: the pending update is simply skipped. If the crash happens after the transaction has committed to the log, but before the checkpoint is complete, the file system can\n   **recover**\n   the update as follows. When the system boots, the file system recovery process will scan the log and look for transactions that have committed to the disk; these transactions are thus\n   **replayed**\n   (in order), with the file system again attempting to write out the blocks in the transaction to their final on-disk locations. This form of logging is one of the simplest forms there is, and is called\n   **redo logging**\n   . By recovering the committed transactions in the journal, the file system ensures that the on-disk structures are consistent, and thus can proceed by mounting the file system and readying itself for new requests.\n\n\nNote that it is fine for a crash to happen at any point during checkpointing, even after some of the updates to the final locations of the blocks have completed. In the worst case, some of these updates are simply performed again during recovery. Because recovery is a rare operation (only taking place after an unexpected system crash), a few redundant writes are nothing to worry about\n   \n    3\n   \n   .\n\n\n\n\n**Batching Log Updates**\n\n\nYou might have noticed that the basic protocol could add a lot of extra disk traffic. For example, imagine we create two files in a row, called\n   \n    file1\n   \n   and\n   \n    file2\n   \n   , in the same directory. To create one file, one has to update a number of on-disk structures, minimally including: the inode bitmap (to allocate a new inode), the newly-created inode of the file,\n\n\n3\n   \n   Unless you worry about everything, in which case we can't help you. Stop worrying so much, it is unhealthy! But now you're probably worried about over-worrying.\n\n\nthe data block of the parent directory containing the new directory entry, and the parent directory inode (which now has a new modification time). With journaling, we logically commit all of this information to the journal for each of our two file creations; because the files are in the same directory, and assuming they even have inodes within the same inode block, this means that if we're not careful, we'll end up writing these same blocks over and over.\n\n\nTo remedy this problem, some file systems do not commit each update to disk one at a time (e.g., Linux ext3); rather, one can buffer all updates into a global transaction. In our example above, when the two files are created, the file system just marks the in-memory inode bitmap, inodes of the files, directory data, and directory inode as dirty, and adds them to the list of blocks that form the current transaction. When it is finally time to write these blocks to disk (say, after a timeout of 5 seconds), this single global transaction is committed containing all of the updates described above. Thus, by buffering updates, a file system can avoid excessive write traffic to disk in many cases.\n\n\n\n\n**Making The Log Finite**\n\n\nWe thus have arrived at a basic protocol for updating file-system on-disk structures. The file system buffers updates in memory for some time; when it is finally time to write to disk, the file system first carefully writes out the details of the transaction to the journal (a.k.a. write-ahead log); after the transaction is complete, the file system checkpoints those blocks to their final locations on disk.\n\n\nHowever, the log is of a finite size. If we keep adding transactions to it (as in this figure), it will soon fill. What do you think happens then?\n\n\n\n\n![Diagram of a journal log structure. A horizontal bar represents the journal. To its left, the word 'Journal' is written vertically. The bar is divided into several rectangular cells. The first five cells are labeled 'Tx1', 'Tx2', 'Tx3', 'Tx4', and 'Tx5' respectively. After 'Tx5', there is an ellipsis '...' followed by a horizontal arrow pointing to the right, indicating that the log continues and is finite.](images/image_0118.jpeg)\n\n\nDiagram of a journal log structure. A horizontal bar represents the journal. To its left, the word 'Journal' is written vertically. The bar is divided into several rectangular cells. The first five cells are labeled 'Tx1', 'Tx2', 'Tx3', 'Tx4', and 'Tx5' respectively. After 'Tx5', there is an ellipsis '...' followed by a horizontal arrow pointing to the right, indicating that the log continues and is finite.\n\n\nTwo problems arise when the log becomes full. The first is simpler, but less critical: the larger the log, the longer recovery will take, as the recovery process must replay all the transactions within the log (in order) to recover. The second is more of an issue: when the log is full (or nearly full), no further transactions can be committed to the disk, thus making the file system “less than useful” (i.e., useless).\n\n\nTo address these problems, journaling file systems treat the log as a circular data structure, re-using it over and over; this is why the journal is sometimes referred to as a\n   **circular log**\n   . To do so, the file system must take action some time after a checkpoint. Specifically, once a transaction has been checkpointed, the file system should free the space it was occupying within the journal, allowing the log space to be reused. There are many ways to achieve this end; for example, you could simply mark the\n\n\noldest and newest non-checkpointed transactions in the log in a\n   **journal superblock**\n   ; all other space is free. Here is a graphical depiction:\n\n\n\n\n![Diagram of a journal structure showing a Journal Super block followed by transaction blocks Tx1, Tx2, Tx3, Tx4, Tx5, and an ellipsis, with an arrow indicating the direction of the log.](images/image_0119.jpeg)\n\n\nThe diagram illustrates a journal structure. It shows a horizontal sequence of blocks. The first block is labeled 'Journal Super' and is shaded gray. To its left, the word 'Journal' is written vertically. Following the 'Journal Super' block are several white blocks labeled 'Tx1', 'Tx2', 'Tx3', 'Tx4', 'Tx5', and an ellipsis '...'. An arrow at the end of the sequence points to the right, indicating the direction of the log.\n\n\nDiagram of a journal structure showing a Journal Super block followed by transaction blocks Tx1, Tx2, Tx3, Tx4, Tx5, and an ellipsis, with an arrow indicating the direction of the log.\n\n\nIn the journal superblock (not to be confused with the main file system superblock), the journaling system records enough information to know which transactions have not yet been checkpointed, and thus reduces recovery time as well as enables re-use of the log in a circular fashion. And thus we add another step to our basic protocol:\n\n\n  * 1.\n    **Journal write:**\n    Write the contents of the transaction (containing\n    \n     Tx_B\n    \n    and the contents of the update) to the log; wait for these writes to complete.\n  * 2.\n    **Journal commit:**\n    Write the transaction commit block (containing\n    \n     Tx_E\n    \n    ) to the log; wait for the write to complete; the transaction is now\n    **committed**\n    .\n  * 3.\n    **Checkpoint:**\n    Write the contents of the update to their final locations within the file system.\n  * 4.\n    **Free:**\n    Some time later, mark the transaction free in the journal by updating the journal superblock.\n\n\nThus we have our final data journaling protocol. But there is still a problem: we are writing each data block to the disk\n   *twice*\n   , which is a heavy cost to pay, especially for something as rare as a system crash. Can you figure out a way to retain consistency without writing data twice?\n\n\n\n\n**Metadata Journaling**\n\n\nAlthough recovery is now fast (scanning the journal and replaying a few transactions as opposed to scanning the entire disk), normal operation of the file system is slower than we might desire. In particular, for each write to disk, we are now also writing to the journal first, thus doubling write traffic; this doubling is especially painful during sequential write workloads, which now will proceed at half the peak write bandwidth of the drive. Further, between writes to the journal and writes to the main file system, there is a costly seek, which adds noticeable overhead for some workloads.\n\n\nBecause of the high cost of writing every data block to disk twice, people have tried a few different things in order to speed up performance. For example, the mode of journaling we described above is often called\n   **data journaling**\n   (as in Linux ext3), as it journals all user data (in addition to the metadata of the file system). A simpler (and more common) form of journaling is sometimes called\n   **ordered journaling**\n   (or just\n   **metadata**\n\n\n**journaling**\n   ), and it is nearly the same, except that user data is\n   *not*\n   written to the journal. Thus, when performing the same update as above, the following information would be written to the journal:\n\n\n\n\n![Diagram of a journal log structure. A horizontal line represents the journal. Below it, a sequence of four cells is shown: 'TxB' (shaded gray), 'I[v2]', 'B[v2]', and 'TxE' (shaded gray). An arrow points from the 'TxE' cell to the right, indicating the end of the log entry.](images/image_0120.jpeg)\n\n\nDiagram of a journal log structure. A horizontal line represents the journal. Below it, a sequence of four cells is shown: 'TxB' (shaded gray), 'I[v2]', 'B[v2]', and 'TxE' (shaded gray). An arrow points from the 'TxE' cell to the right, indicating the end of the log entry.\n\n\nThe data block Db, previously written to the log, would instead be written to the file system proper, avoiding the extra write; given that most I/O traffic to the disk is data, not writing data twice substantially reduces the I/O load of journaling. The modification does raise an interesting question, though: when should we write data blocks to disk?\n\n\nLet's again consider our example append of a file to understand the problem better. The update consists of three blocks: I[v2], B[v2], and Db. The first two are both metadata and will be logged and then checkpointed; the latter will only be written once to the file system. When should we write Db to disk? Does it matter?\n\n\nAs it turns out, the ordering of the data write does matter for metadata-only journaling. For example, what if we write Db to disk\n   *after*\n   the transaction (containing I[v2] and B[v2]) completes? Unfortunately, this approach has a problem: the file system is consistent but I[v2] may end up pointing to garbage data. Specifically, consider the case where I[v2] and B[v2] are written but Db did not make it to disk. The file system will then try to recover. Because Db is\n   *not*\n   in the log, the file system will replay writes to I[v2] and B[v2], and produce a consistent file system (from the perspective of file-system metadata). However, I[v2] will be pointing to garbage data, i.e., at whatever was in the slot where Db was headed.\n\n\nTo ensure this situation does not arise, some file systems (e.g., Linux ext3) write data blocks (of regular files) to the disk\n   *first*\n   , before related metadata is written to disk. Specifically, the protocol is as follows:\n\n\n  * 1.\n    **Data write:**\n    Write data to final location; wait for completion (the wait is optional; see below for details).\n  * 2.\n    **Journal metadata write:**\n    Write the begin block and metadata to the log; wait for writes to complete.\n  * 3.\n    **Journal commit:**\n    Write the transaction commit block (containing TxE) to the log; wait for the write to complete; the transaction (including data) is now\n    **committed**\n    .\n  * 4.\n    **Checkpoint metadata:**\n    Write the contents of the metadata update to their final locations within the file system.\n  * 5.\n    **Free:**\n    Later, mark the transaction free in journal superblock.\n\n\nBy forcing the data write first, a file system can guarantee that a pointer will never point to garbage. Indeed, this rule of “write the pointed-to object before the object that points to it” is at the core of crash consistency, and is exploited even further by other crash consistency schemes [GP94] (see below for details).\n\n\nIn most systems, metadata journaling (akin to ordered journaling of ext3) is more popular than full data journaling. For example, Windows NTFS and SG1's XFS both use some form of metadata journaling. Linux ext3 gives you the option of choosing either data, ordered, or unordered modes (in unordered mode, data can be written at any time). All of these modes keep metadata consistent; they vary in their semantics for data.\n\n\nFinally, note that forcing the data write to complete (Step 1) before issuing writes to the journal (Step 2) is not required for correctness, as indicated in the protocol above. Specifically, it would be fine to concurrently issue writes to data, the transaction-begin block, and journaled metadata; the only real requirement is that Steps 1 and 2 complete before the issuing of the journal commit block (Step 3).\n\n\n\n\n**Tricky Case: Block Reuse**\n\n\nThere are some interesting corner cases that make journaling more tricky, and thus are worth discussing. A number of them revolve around block reuse; as Stephen Tweedie (one of the main forces behind ext3) said:\n\n\n\"What's the hideous part of the entire system? ... It's deleting files. Everything to do with delete is hairy. Everything to do with delete... you have nightmares around what happens if blocks get deleted and then reallocated.\" [T00]\n\n\nThe particular example Tweedie gives is as follows. Suppose you are using some form of metadata journaling (and thus data blocks for files are\n   *not*\n   journaled). Let's say you have a directory called\n   \n    foo\n   \n   . The user adds an entry to\n   \n    foo\n   \n   (say by creating a file), and thus the contents of\n   \n    foo\n   \n   (because directories are considered metadata) are written to the log; assume the location of the\n   \n    foo\n   \n   directory data is block 1000. The log thus contains something like this:\n\n\n\n\n![](images/image_0121.jpeg)\n\n\nJournal | TxB\n      \n      id=1 | [foo]\n      \n      ptr:1000 | D[foo]\n      \n      [final addr:1000] | TxE\n      \n      id=1 | →\n\n\nAt this point, the user deletes everything in the directory and the directory itself, freeing up block 1000 for reuse. Finally, the user creates a new file (say\n   \n    bar\n   \n   ), which ends up reusing the same block (1000) that used to belong to\n   \n    foo\n   \n   . The inode of\n   \n    bar\n   \n   is committed to disk, as is its data; note, however, because metadata journaling is in use, only the inode of\n   \n    bar\n   \n   is committed to the journal; the newly-written data in block 1000 in the file\n   \n    bar\n   \n   is\n   *not*\n   journaled.\n\n\n\n\n![](images/image_0122.jpeg)\n\n\nJournal | TxB\n      \n      id=1 | [foo]\n      \n      ptr:1000 | D[foo]\n      \n      [final addr:1000] | TxE\n      \n      id=1 | TxB\n      \n      id=2 | [bar]\n      \n      ptr:1000 | TxE\n      \n      id=2 | →\n\n\n\n\n![](images/image_0123.jpeg)\n\n\nTxB | Journal Contents | TxE | File System\n(metadata) | (data) | Metadata | Data\nissue | issue | issue |  |  | \ncomplete | complete | complete |  |  | \n |  |  | issue |  | \n |  |  | complete |  | \n |  |  |  | issue | issue\n |  |  |  | complete | complete\n |  |  |  | complete |\n\n\nFigure 42.1: Data Journaling Timeline\n\n\nNow assume a crash occurs and all of this information is still in the log. During replay, the recovery process simply replays everything in the log, including the write of directory data in block 1000; the replay thus overwrites the user data of current file\n   \n    bar\n   \n   with old directory contents! Clearly this is not a correct recovery action, and certainly it will be a surprise to the user when reading the file\n   \n    bar\n   \n   .\n\n\nThere are a number of solutions to this problem. One could, for example, never reuse blocks until the delete of said blocks is checkpointed out of the journal. What Linux\n   \n    ext3\n   \n   does instead is to add a new type of record to the journal, known as a\n   **revoke**\n   record. In the case above, deleting the directory would cause a revoke record to be written to the journal. When replaying the journal, the system first scans for such revoke records; any such revoked data is never replayed, thus avoiding the problem mentioned above.\n\n\n\n\n**Wrapping Up Journaling: A Timeline**\n\n\nBefore ending our discussion of journaling, we summarize the protocols we have discussed with timelines depicting each of them. Figure 42.1 shows the protocol when journaling data and metadata, whereas Figure 42.2 shows the protocol when journaling only metadata.\n\n\nIn each figure, time increases in the downward direction, and each row in the figure shows the logical time that a write can be issued or might complete. For example, in the data journaling protocol (Figure 42.1), the writes of the transaction begin block (TxB) and the contents of the transaction can logically be issued at the same time, and thus can be completed in any order; however, the write to the transaction end block (TxE) must not be issued until said previous writes complete. Similarly, the checkpointing writes to data and metadata blocks cannot begin until the transaction end block has committed. Horizontal dashed lines show where write-ordering requirements must be obeyed.\n\n\nA similar timeline is shown for the metadata journaling protocol. Note\n\n\n\nTxB | Journal\n      \n      Contents\n      \n      (metadata) | TxE | File System\n |  |  | Metadata      Data\nissue | issue |  | issue\ncomplete | complete | issue | complete\n |  | complete | \n |  |  | issue\n |  |  | complete\n\n\nFigure 42.2: Metadata Journaling Timeline\n\n\nthat the data write can logically be issued at the same time as the writes to the transaction begin and the contents of the journal; however, it must be issued and complete before the transaction end has been issued.\n\n\nFinally, note that the time of completion marked for each write in the timelines is arbitrary. In a real system, completion time is determined by the I/O subsystem, which may reorder writes to improve performance. The only guarantees about ordering that we have are those that must be enforced for protocol correctness (and are shown via the horizontal dashed lines in the figures)."
        },
        {
          "name": "Solution #3: Other Approaches",
          "content": "We've thus far described two options in keeping file system metadata consistent: a lazy approach based on\n   \n    fsck\n   \n   , and a more active approach known as journaling. However, these are not the only two approaches. One such approach, known as Soft Updates [GP94], was introduced by Ganger and Patt. This approach carefully orders all writes to the file system to ensure that the on-disk structures are never left in an inconsistent state. For example, by writing a pointed-to data block to disk\n   *before*\n   the inode that points to it, we can ensure that the inode never points to garbage; similar rules can be derived for all the structures of the file system. Implementing Soft Updates can be a challenge, however; whereas the journaling layer described above can be implemented with relatively little knowledge of the exact file system structures, Soft Updates requires intricate knowledge of each file system data structure and thus adds a fair amount of complexity to the system.\n\n\nAnother approach is known as\n   **copy-on-write**\n   (yes,\n   **COW**\n   ), and is used in a number of popular file systems, including Sun's ZFS [B07]. This technique never overwrites files or directories in place; rather, it places new updates to previously unused locations on disk. After a number of updates are completed, COW file systems flip the root structure of the file system to include pointers to the newly updated structures. Doing so makes keeping the file system consistent straightforward. We'll be learn-\n\n\ning more about this technique when we discuss the log-structured file system (LFS) in a future chapter; LFS is an early example of a COW.\n\n\nAnother approach is one we just developed here at Wisconsin. In this technique, entitled\n   **backpointer-based consistency**\n   (or BBC), no ordering is enforced between writes. To achieve consistency, an additional\n   **back pointer**\n   is added to every block in the system; for example, each data block has a reference to the inode to which it belongs. When accessing a file, the file system can determine if the file is consistent by checking if the forward pointer (e.g., the address in the inode or direct block) points to a block that refers back to it. If so, everything must have safely reached disk and thus the file is consistent; if not, the file is inconsistent, and an error is returned. By adding back pointers to the file system, a new form of lazy crash consistency can be attained [C+12].\n\n\nFinally, we also have explored techniques to reduce the number of times a journal protocol has to wait for disk writes to complete. Entitled\n   **optimistic crash consistency**\n   [C+13], this new approach issues as many writes to disk as possible by using a generalized form of the\n   **transaction checksum**\n   [P+05], and includes a few other techniques to detect inconsistencies should they arise. For some workloads, these optimistic techniques can improve performance by an order of magnitude. However, to truly function well, a slightly different disk interface is required [C+13]."
        }
      ]
    },
    {
      "name": "Log-structured File Systems",
      "sections": [
        {
          "name": "Writing To Disk Sequentially",
          "content": "We thus have our first challenge: how do we transform all updates to file-system state into a series of sequential writes to disk? To understand this better, let’s use a simple example. Imagine we are writing a data block\n   \n    D\n   \n   to a file. Writing the data block to disk might result in the following on-disk layout, with\n   \n    D\n   \n   written at disk address\n   \n    A0\n   \n   :\n\n\n\n\n![Diagram showing a single data block D written at disk address A0.](images/image_0124.jpeg)\n\n\nThe diagram illustrates a single data block labeled 'D' within a rectangular frame. Below the frame, the label 'A0' is centered, indicating the disk address of the block.\n\n\nDiagram showing a single data block D written at disk address A0.\n\n\nHowever, when a user writes a data block, it is not only data that gets written to disk; there is also other\n   **metadata**\n   that needs to be updated. In this case, let's also write the\n   **inode**\n   (\n   \n    I\n   \n   ) of the file to disk, and have it point to the data block\n   \n    D\n   \n   . When written to disk, the data block and inode would look something like this (note that the inode looks as big as the data block, which generally isn't the case; in most systems, data blocks are 4 KB in size, whereas an inode is much smaller, around 128 bytes):\n\n\n\n\n![Diagram showing a data block D and an inode I on disk. The data block D is at address A0. The inode I is at address A0+1. The inode contains a pointer b[0] pointing to the data block D. A downward arrow points to the data block D.](images/image_0125.jpeg)\n\n\nThe diagram illustrates a disk layout with two adjacent blocks. The first block is labeled 'D' and the second block is labeled 'I'. Below the first block is the label 'A0'. Inside the second block, there is a label 'b[0]:A0'. A downward-pointing arrow originates from the top of block 'D' and points to its top edge, indicating a pointer from the inode to the data block.\n\n\nDiagram showing a data block D and an inode I on disk. The data block D is at address A0. The inode I is at address A0+1. The inode contains a pointer b[0] pointing to the data block D. A downward arrow points to the data block D.\n\n\nThis basic idea, of simply writing all updates (such as data blocks, inodes, etc.) to the disk sequentially, sits at the heart of LFS. If you understand this, you get the basic idea. But as with all complicated systems, the devil is in the details."
        },
        {
          "name": "Writing Sequentially And Effectively",
          "content": "Unfortunately, writing to disk sequentially is not (alone) enough to guarantee efficient writes. For example, imagine if we wrote a single block to address\n   \n    A\n   \n   , at time\n   \n    T\n   \n   . We then wait a little while, and write to the disk at address\n   \n    A + 1\n   \n   (the next block address in sequential order), but at time\n   \n    T + \\delta\n   \n   . In-between the first and second writes, unfortunately, the disk has rotated; when you issue the second write, it will thus wait for most of a rotation before being committed (specifically, if the rotation takes time\n   \n    T_{rotation}\n   \n   , the disk will wait\n   \n    T_{rotation} - \\delta\n   \n   before it can commit the second write to the disk surface). And thus you can hopefully see that simply writing to disk in sequential order is not enough to achieve peak performance; rather, you must issue a large number of\n   *contiguous*\n   writes (or one large write) to the drive in order to achieve good write performance.\n\n\nTo achieve this end, LFS uses an ancient technique known as\n   **write buffering**\n\n    1\n   \n   . Before writing to the disk, LFS keeps track of updates in memory; when it has received a sufficient number of updates, it writes them to disk all at once, thus ensuring efficient use of the disk.\n\n\nThe large chunk of updates LFS writes at one time is referred to by the name of a\n   **segment**\n   . Although this term is over-used in computer systems, here it just means a large-ish chunk which LFS uses to group writes. Thus, when writing to disk, LFS buffers updates in an in-memory\n\n\n1\n   \n   Indeed, it is hard to find a good citation for this idea, since it was likely invented by many and very early on in the history of computing. For a study of the benefits of write buffering, see Solworth and Orji [SO90]; to learn about its potential harms, see Mogul [M94].\n\n\nsegment, and then writes the segment all at once to the disk. As long as the segment is large enough, these writes will be efficient.\n\n\nHere is an example, in which LFS buffers two sets of updates into a small segment; actual segments are larger (a few MB). The first update is of four block writes to file\n   \n    j\n   \n   ; the second is one block being added to file\n   \n    k\n   \n   . LFS then commits the entire segment of seven blocks to disk at once. The resulting on-disk layout of these blocks is as follows:\n\n\n\n\n![Diagram showing the on-disk layout of blocks after LFS commits a segment. The layout is: A0, A1, A2, A3, Inode j, Inode k. Above these are blocks: D_{j,0}, D_{j,1}, D_{j,2}, D_{j,3}, and D_{k,0}. Above D_{j,3} are four blocks labeled b[0]:A0, b[1]:A1, b[2]:A2, b[3]:A3. Above D_{k,0} is one block labeled b[0]:A5. Arrows indicate the mapping from these blocks to their respective disk locations.](images/image_0126.jpeg)\n\n\nDiagram showing the on-disk layout of blocks after LFS commits a segment. The layout is: A0, A1, A2, A3, Inode j, Inode k. Above these are blocks: D_{j,0}, D_{j,1}, D_{j,2}, D_{j,3}, and D_{k,0}. Above D_{j,3} are four blocks labeled b[0]:A0, b[1]:A1, b[2]:A2, b[3]:A3. Above D_{k,0} is one block labeled b[0]:A5. Arrows indicate the mapping from these blocks to their respective disk locations."
        },
        {
          "name": "How Much To Buffer?",
          "content": "This raises the following question: how many updates should LFS buffer before writing to disk? The answer, of course, depends on the disk itself, specifically how high the positioning overhead is in comparison to the transfer rate; see the FFS chapter for a similar analysis.\n\n\nFor example, assume that positioning (i.e., rotation and seek overheads) before each write takes roughly\n   \n    T_{position}\n   \n   seconds. Assume further that the disk transfer rate is\n   \n    R_{peak}\n   \n   MB/s. How much should LFS buffer before writing when running on such a disk?\n\n\nThe way to think about this is that every time you write, you pay a fixed overhead of the positioning cost. Thus, how much do you have to write in order to\n   **amortize**\n   that cost? The more you write, the better (obviously), and the closer you get to achieving peak bandwidth.\n\n\nTo obtain a concrete answer, let's assume we are writing out\n   \n    D\n   \n   MB. The time to write out this chunk of data (\n   \n    T_{write}\n   \n   ) is the positioning time\n   \n    T_{position}\n   \n   plus the time to transfer\n   \n    D\n   \n   (\n   \n    \\frac{D}{R_{peak}}\n   \n   ), or:\n\n\nT_{write} = T_{position} + \\frac{D}{R_{peak}} \\quad (43.1)\n\n\nAnd thus the effective\n   *rate*\n   of writing (\n   \n    R_{effective}\n   \n   ), which is just the amount of data written divided by the total time to write it, is:\n\n\nR_{effective} = \\frac{D}{T_{write}} = \\frac{D}{T_{position} + \\frac{D}{R_{peak}}}. \\quad (43.2)\n\n\nWhat we're interested in is getting the effective rate (\n   \n    R_{effective}\n   \n   ) close to the peak rate. Specifically, we want the effective rate to be some fraction\n   \n    F\n   \n   of the peak rate, where\n   \n    0 < F < 1\n   \n   (a typical\n   \n    F\n   \n   might be 0.9, or 90% of the peak rate). In mathematical form, this means we want\n   \n    R_{effective} = F \\times R_{peak}\n   \n   .\n\n\nAt this point, we can solve for\n   \n    D\n   \n   :\n\n\nR_{\\text{effective}} = \\frac{D}{T_{\\text{position}} + \\frac{D}{R_{\\text{peak}}}} = F \\times R_{\\text{peak}} \\quad (43.3)\n\n\nD = F \\times R_{\\text{peak}} \\times (T_{\\text{position}} + \\frac{D}{R_{\\text{peak}}}) \\quad (43.4)\n\n\nD = (F \\times R_{\\text{peak}} \\times T_{\\text{position}}) + (F \\times R_{\\text{peak}} \\times \\frac{D}{R_{\\text{peak}}}) \\quad (43.5)\n\n\nD = \\frac{F}{1 - F} \\times R_{\\text{peak}} \\times T_{\\text{position}} \\quad (43.6)\n\n\nLet's do an example, with a disk with a positioning time of 10 milliseconds and peak transfer rate of 100 MB/s; assume we want an effective bandwidth of 90% of peak (\n   \n    F = 0.9\n   \n   ). In this case,\n   \n    D = \\frac{0.9}{0.1} \\times 100 \\text{ MB/s} \\times 0.01 \\text{ seconds} = 9 \\text{ MB}\n   \n   . Try some different values to see how much we need to buffer in order to approach peak bandwidth. How much is needed to reach 95% of peak? 99%?"
        },
        {
          "name": "Problem: Finding Inodes",
          "content": "To understand how we find an inode in LFS, let us briefly review how to find an inode in a typical UNIX file system. In a typical file system such as FFS, or even the old UNIX file system, finding inodes is easy, because they are organized in an array and placed on disk at fixed locations.\n\n\nFor example, the old UNIX file system keeps all inodes at a fixed portion of the disk. Thus, given an inode number and the start address, to find a particular inode, you can calculate its exact disk address simply by multiplying the inode number by the size of an inode, and adding that to the start address of the on-disk array; array-based indexing, given an inode number, is fast and straightforward.\n\n\nFinding an inode given an inode number in FFS is only slightly more complicated, because FFS splits up the inode table into chunks and places a group of inodes within each cylinder group. Thus, one must know how big each chunk of inodes is and the start addresses of each. After that, the calculations are similar and also easy.\n\n\nIn LFS, life is more difficult. Why? Well, we've managed to scatter the inodes all throughout the disk! Worse, we never overwrite in place, and thus the latest version of an inode (i.e., the one we want) keeps moving."
        },
        {
          "name": "Solution Through Indirection: The Inode Map",
          "content": "To remedy this, the designers of LFS introduced a\n   **level of indirection**\n   between inode numbers and the inodes through a data structure called the\n   **inode map (imap)**\n   . The imap is a structure that takes an inode number as input and produces the disk address of the most recent version of the\n\n\n\n\n**TIP: USE A LEVEL OF INDIRECTION**\n\n\nPeople often say that the solution to all problems in Computer Science is simply a\n   **level of indirection**\n   . This is clearly not true; it is just the solution to\n   *most*\n   problems (yes, this is still too strong of a comment, but you get the point). You certainly can think of every virtualization we have studied, e.g., virtual memory, or the notion of a file, as simply a level of indirection. And certainly the inode map in LFS is a virtualization of inode numbers. Hopefully you can see the great power of indirection in these examples, allowing us to freely move structures around (such as pages in the VM example, or inodes in LFS) without having to change every reference to them. Of course, indirection can have a downside too:\n   **extra overhead**\n   . So next time you have a problem, try solving it with indirection, but make sure to think about the overheads of doing so first. As Wheeler famously said, “All problems in computer science can be solved by another level of indirection, except of course for the problem of too many indirections.”\n\n\ninode. Thus, you can imagine it would often be implemented as a simple\n   *array*\n   , with 4 bytes (a disk pointer) per entry. Any time an inode is written to disk, the imap is updated with its new location.\n\n\nThe imap, unfortunately, needs to be kept persistent (i.e., written to disk); doing so allows LFS to keep track of the locations of inodes across crashes, and thus operate as desired. Thus, a question: where should the imap reside on disk?\n\n\nIt could live on a fixed part of the disk, of course. Unfortunately, as it gets updated frequently, this would then require updates to file structures to be followed by writes to the imap, and hence performance would suffer (i.e., there would be more disk seeks, between each update and the fixed location of the imap).\n\n\nInstead, LFS places chunks of the inode map right next to where it is writing all of the other new information. Thus, when appending a data block to a file\n   \n    k\n   \n   , LFS actually writes the new data block, its inode, and a piece of the inode map all together onto the disk, as follows:\n\n\n\n\n![Diagram illustrating the placement of data, inode, and inode map on disk. A horizontal line represents the disk. Below it, three blocks are shown: a data block 'D' at address A0, an inode block 'I[k]' at address A1, and a piece of the inode map 'imap' at address A1. The imap block contains two entries: 'b[0]:A0' and 'm[k]:A1'. Arrows show that the imap entry 'b[0]:A0' points to the data block 'D' at A0, and the imap entry 'm[k]:A1' points to the inode block 'I[k]' at A1.](images/image_0127.jpeg)\n\n\nDiagram illustrating the placement of data, inode, and inode map on disk. A horizontal line represents the disk. Below it, three blocks are shown: a data block 'D' at address A0, an inode block 'I[k]' at address A1, and a piece of the inode map 'imap' at address A1. The imap block contains two entries: 'b[0]:A0' and 'm[k]:A1'. Arrows show that the imap entry 'b[0]:A0' points to the data block 'D' at A0, and the imap entry 'm[k]:A1' points to the inode block 'I[k]' at A1.\n\n\nIn this picture, the piece of the imap array stored in the block marked\n   *imap*\n   tells LFS that the inode\n   \n    k\n   \n   is at disk address\n   \n    A1\n   \n   ; this inode, in turn, tells LFS that its data block\n   \n    D\n   \n   is at address\n   \n    A0\n   \n   ."
        },
        {
          "name": "Completing The Solution: The Checkpoint Region",
          "content": "The clever reader (that's you, right?) might have noticed a problem here. How do we find the inode map, now that pieces of it are also now spread across the disk? In the end, there is no magic: the file system must have\n   *some*\n   fixed and known location on disk to begin a file lookup.\n\n\nLFS has just such a fixed place on disk for this, known as the\n   **checkpoint region (CR)**\n   . The checkpoint region contains pointers to (i.e., addresses of) the latest pieces of the inode map, and thus the inode map pieces can be found by reading the CR first. Note the checkpoint region is only updated periodically (say every 30 seconds or so), and thus performance is not ill-affected. Thus, the overall structure of the on-disk layout contains a checkpoint region (which points to the latest pieces of the inode map); the inode map pieces each contain addresses of the inodes; the inodes point to files (and directories) just like typical UNIX file systems.\n\n\nHere is an example of the checkpoint region (note it is all the way at the beginning of the disk, at address 0), and a single imap chunk, inode, and data block. A real file system would of course have a much bigger CR (indeed, it would have two, as we'll come to understand later), many imap chunks, and of course many more inodes, data blocks, etc.\n\n\n\n\n![Diagram of the LFS on-disk layout showing the Checkpoint Region (CR) at address 0, followed by an inode map chunk (imap) at address A2, an inode (I[k]) at address A1, and a data block (D) at address A0. Arrows indicate that the CR points to the imap at A2, and the imap points to the inode at A1, which in turn points to the data block at A0.](images/image_0128.jpeg)\n\n\nThe diagram illustrates the LFS on-disk layout. It shows a horizontal bar representing the disk, divided into several segments. The first segment, starting at address 0, is labeled 'imap [k...k+N]: A2 CR'. The next segment is labeled 'D' and starts at address A0. The third segment is labeled 'I[k]' and starts at address A1. The fourth segment is labeled 'imap' and starts at address A2. Arrows indicate the following relationships: a double-headed arrow between the CR segment and the imap segment at A2; a single-headed arrow from the CR segment to the I[k] segment at A1; and a single-headed arrow from the imap segment at A2 to the I[k] segment at A1. The addresses 0, A0, A1, and A2 are marked below the corresponding segments.\n\n\nDiagram of the LFS on-disk layout showing the Checkpoint Region (CR) at address 0, followed by an inode map chunk (imap) at address A2, an inode (I[k]) at address A1, and a data block (D) at address A0. Arrows indicate that the CR points to the imap at A2, and the imap points to the inode at A1, which in turn points to the data block at A0."
        },
        {
          "name": "Reading A File From Disk: A Recap",
          "content": "To make sure you understand how LFS works, let us now walk through what must happen to read a file from disk. Assume we have nothing in memory to begin. The first on-disk data structure we must read is the checkpoint region. The checkpoint region contains pointers (i.e., disk addresses) to the entire inode map, and thus LFS then reads in the entire inode map and caches it in memory. After this point, when given an inode number of a file, LFS simply looks up the inode-number to inode-disk-address mapping in the imap, and reads in the most recent version of the inode. To read a block from the file, at this point, LFS proceeds exactly as a typical UNIX file system, by using direct pointers or indirect pointers or doubly-indirect pointers as need be. In the common case, LFS should perform the same number of I/Os as a typical file system when reading a file from disk; the entire imap is cached and thus the extra work LFS does during a read is to look up the inode's address in the imap."
        },
        {
          "name": "What About Directories?",
          "content": "Thus far, we’ve simplified our discussion a bit by only considering inodes and data blocks. However, to access a file in a file system (such as\n   \n    /home/remzi/foo\n   \n   , one of our favorite fake file names), some directories must be accessed too. So how does LFS store directory data?\n\n\nFortunately, directory structure is basically identical to classic UNIX file systems, in that a directory is just a collection of (name, inode number) mappings. For example, when creating a file on disk, LFS must both write a new inode, some data, as well as the directory data and its inode that refer to this file. Remember that LFS will do so sequentially on the disk (after buffering the updates for some time). Thus, creating a file\n   \n    foo\n   \n   in a directory would lead to the following new structures on disk:\n\n\n\n\n![Diagram illustrating the sequential creation of structures for a file 'foo' in a directory. It shows four blocks labeled A0, A1, A2, and A3. A0 contains D_k. A1 contains b[0]:A0, l[k], and a pointer to A0. A2 contains (foo, k), D_dir, b[0]:A2, l[dir], and a pointer to A2. A3 contains m[k]:A1, m[dir]:A3, and imap. Arrows show the flow from A0 to A1, A1 to A2, and A2 to A3.](images/image_0129.jpeg)\n\n\nThe diagram shows four blocks labeled A0, A1, A2, and A3. Block A0 contains\n    \n     D_k\n    \n    . Block A1 contains\n    \n     b[0]:A0\n    \n    ,\n    \n     l[k]\n    \n    , and a pointer to A0. Block A2 contains\n    \n     (foo, k)\n    \n    ,\n    \n     D_{dir}\n    \n    ,\n    \n     b[0]:A2\n    \n    ,\n    \n     l[dir]\n    \n    , and a pointer to A2. Block A3 contains\n    \n     m[k]:A1\n    \n    ,\n    \n     m[dir]:A3\n    \n    , and\n    \n     imap\n    \n    . Arrows indicate the flow of pointers: from A0 to A1, from A1 to A2, and from A2 to A3.\n\n\nDiagram illustrating the sequential creation of structures for a file 'foo' in a directory. It shows four blocks labeled A0, A1, A2, and A3. A0 contains D_k. A1 contains b[0]:A0, l[k], and a pointer to A0. A2 contains (foo, k), D_dir, b[0]:A2, l[dir], and a pointer to A2. A3 contains m[k]:A1, m[dir]:A3, and imap. Arrows show the flow from A0 to A1, A1 to A2, and A2 to A3.\n\n\nThe piece of the inode map contains the information for the location of both the directory file\n   *dir*\n   as well as the newly-created file\n   *f*\n   . Thus, when accessing file\n   \n    foo\n   \n   (with inode number\n   *k*\n   ), you would first look in the inode map (usually cached in memory) to find the location of the inode of directory\n   *dir*\n   (A3); you then read the directory inode, which gives you the location of the directory data (A2); reading this data block gives you the name-to-inode-number mapping of (\n   \n    foo\n   \n   ,\n   *k*\n   ). You then consult the inode map again to find the location of inode number\n   *k*\n   (A1), and finally read the desired data block at address A0.\n\n\nThere is one other serious problem in LFS that the inode map solves, known as the\n   **recursive update problem**\n   [Z+12]. The problem arises in any file system that never updates in place (such as LFS), but rather moves updates to new locations on the disk.\n\n\nSpecifically, whenever an inode is updated, its location on disk changes. If we hadn’t been careful, this would have also entailed an update to the directory that points to this file, which then would have mandated a change to the parent of that directory, and so on, all the way up the file system tree.\n\n\nLFS cleverly avoids this problem with the inode map. Even though the location of an inode may change, the change is never reflected in the directory itself; rather, the imap structure is updated while the directory holds the same name-to-inode-number mapping. Thus, through indirection, LFS avoids the recursive update problem."
        },
        {
          "name": "A New Problem: Garbage Collection",
          "content": "You may have noticed another problem with LFS; it repeatedly writes the latest version of a file (including its inode and data) to new locations on disk. This process, while keeping writes efficient, implies that LFS leaves old versions of file structures scattered throughout the disk. We (rather unceremoniously) call these old versions\n   **garbage**\n   .\n\n\nFor example, let's imagine the case where we have an existing file referred to by inode number\n   \n    k\n   \n   , which points to a single data block\n   \n    D_0\n   \n   . We now update that block, generating both a new inode and a new data block. The resulting on-disk layout of LFS would look something like this (note we omit the imap and other structures for simplicity; a new chunk of imap would also have to be written to disk to point to the new inode):\n\n\n\n\n![Diagram showing the on-disk layout of LFS after an update. It shows two versions of an inode and two versions of a data block. The left side shows the old inode (labeled A0) pointing to the old data block (labeled D0). The right side shows the new inode (labeled A4) pointing to the new data block (labeled D0). The old data block D0 is now marked as garbage.](images/image_0130.jpeg)\n\n\nThe diagram illustrates the disk layout for two versions of a file. On the left, an inode labeled\n    \n     A_0\n    \n    (garbage) points to a data block labeled\n    \n     D_0\n    \n    . On the right, a new inode labeled\n    \n     A_4\n    \n    points to a new data block labeled\n    \n     D_0\n    \n    . Both inodes contain a pointer\n    \n     b[0]:A_0\n    \n    and a file size\n    \n     l[k]\n    \n    . The old data block\n    \n     D_0\n    \n    is now marked as garbage.\n\n\nDiagram showing the on-disk layout of LFS after an update. It shows two versions of an inode and two versions of a data block. The left side shows the old inode (labeled A0) pointing to the old data block (labeled D0). The right side shows the new inode (labeled A4) pointing to the new data block (labeled D0). The old data block D0 is now marked as garbage.\n\n\nIn the diagram, you can see that both the inode and data block have two versions on disk, one old (the one on the left) and one current and thus\n   **live**\n   (the one on the right). By the simple act of (logically) updating a data block, a number of new structures must be persisted by LFS, thus leaving old versions of said blocks on the disk.\n\n\nAs another example, imagine we instead append a block to that original file\n   \n    k\n   \n   . In this case, a new version of the inode is generated, but the old data block is still pointed to by the inode. Thus, it is still live and very much part of the current file system:\n\n\n\n\n![Diagram showing the on-disk layout of LFS after an append operation. It shows two versions of an inode and two versions of data blocks. The left side shows the old inode (labeled A0) pointing to the old data block (labeled D0). The right side shows the new inode (labeled A4) pointing to two data blocks: the old data block D0 and the new data block D1. The old data block D0 is still live and part of the current file system.](images/image_0131.jpeg)\n\n\nThe diagram illustrates the disk layout for two versions of a file after an append operation. On the left, an inode labeled\n    \n     A_0\n    \n    (garbage) points to a data block labeled\n    \n     D_0\n    \n    . On the right, a new inode labeled\n    \n     A_4\n    \n    points to two data blocks:\n    \n     D_0\n    \n    and\n    \n     D_1\n    \n    . Both inodes contain a pointer\n    \n     b[0]:A_0\n    \n    and a file size\n    \n     l[k]\n    \n    . The old data block\n    \n     D_0\n    \n    is still live and part of the current file system.\n\n\nDiagram showing the on-disk layout of LFS after an append operation. It shows two versions of an inode and two versions of data blocks. The left side shows the old inode (labeled A0) pointing to the old data block (labeled D0). The right side shows the new inode (labeled A4) pointing to two data blocks: the old data block D0 and the new data block D1. The old data block D0 is still live and part of the current file system.\n\n\nSo what should we do with these older versions of inodes, data blocks, and so forth? One could keep those older versions around and allow users to restore old file versions (for example, when they accidentally overwrite or delete a file, it could be quite handy to do so); such a file system is known as a\n   **versioning file system**\n   because it keeps track of the different versions of a file.\n\n\nHowever, LFS instead keeps only the latest live version of a file; thus (in the background), LFS must periodically find these old dead versions of file data, inodes, and other structures, and\n   **clean**\n   them; cleaning should\n\n\nthus make blocks on disk free again for use in subsequent writes. Note that the process of cleaning is a form of\n   **garbage collection**\n   , a technique that arises in programming languages that automatically free unused memory for programs.\n\n\nEarlier we discussed segments as important as they are the mechanism that enables large writes to disk in LFS. As it turns out, they are also quite integral to effective cleaning. Imagine what would happen if the LFS cleaner simply went through and freed single data blocks, inodes, etc., during cleaning. The result: a file system with some number of free\n   **holes**\n   mixed between allocated space on disk. Write performance would drop considerably, as LFS would not be able to find a large contiguous region to write to disk sequentially and with high performance.\n\n\nInstead, the LFS cleaner works on a segment-by-segment basis, thus clearing up large chunks of space for subsequent writing. The basic cleaning process works as follows. Periodically, the LFS cleaner reads in a number of old (partially-used) segments, determines which blocks are live within these segments, and then write out a new set of segments with just the live blocks within them, freeing up the old ones for writing. Specifically, we expect the cleaner to read in\n   \n    M\n   \n   existing segments,\n   **compact**\n   their contents into\n   \n    N\n   \n   new segments (where\n   \n    N < M\n   \n   ), and then write the\n   \n    N\n   \n   segments to disk in new locations. The old\n   \n    M\n   \n   segments are then freed and can be used by the file system for subsequent writes.\n\n\nWe are now left with two problems, however. The first is mechanism: how can LFS tell which blocks within a segment are live, and which are dead? The second is policy: how often should the cleaner run, and which segments should it pick to clean?"
        },
        {
          "name": "Determining Block Liveness",
          "content": "We address the mechanism first. Given a data block\n   \n    D\n   \n   within an on-disk segment\n   \n    S\n   \n   , LFS must be able to determine whether\n   \n    D\n   \n   is live. To do so, LFS adds a little extra information to each segment that describes each block. Specifically, LFS includes, for each data block\n   \n    D\n   \n   , its inode number (which file it belongs to) and its offset (which block of the file this is). This information is recorded in a structure at the head of the segment known as the\n   **segment summary block**\n   .\n\n\nGiven this information, it is straightforward to determine whether a block is live or dead. For a block\n   \n    D\n   \n   located on disk at address\n   \n    A\n   \n   , look in the segment summary block and find its inode number\n   \n    N\n   \n   and offset\n   \n    T\n   \n   . Next, look in the imap to find where\n   \n    N\n   \n   lives and read\n   \n    N\n   \n   from disk (perhaps it is already in memory, which is even better). Finally, using the offset\n   \n    T\n   \n   , look in the inode (or some indirect block) to see where the inode thinks the\n   \n    T\n   \n   th block of this file is on disk. If it points exactly to disk address\n   \n    A\n   \n   , LFS can conclude that the block\n   \n    D\n   \n   is live. If it points anywhere else, LFS can conclude that\n   \n    D\n   \n   is not in use (i.e., it is dead) and thus know that this version is no longer needed. Here is a pseudocode summary:\n\n\n(N, T) = SegmentSummary[A];\ninode = Read(imap[N]);\nif (inode[T] == A)\n    // block D is alive\nelse\n    // block D is garbage\nHere is a diagram depicting the mechanism, in which the segment summary block (marked\n   *SS*\n   ) records that the data block at address\n   *A0*\n   is actually a part of file\n   *k*\n   at offset 0. By checking the imap for\n   *k*\n   , you can find the inode, and see that it does indeed point to that location.\n\n\n\n\n![Diagram showing the mechanism of Log-Structured File System (LFS) for determining block liveness. A segment summary block (SS) at address A0 contains the record 'A0: (k,0)'. This block points to a data block 'D' at address A0. Block 'D' points to an inode block 'I[k]' at address A1. Block 'I[k]' points to a mapping block 'imap' at address A1. The imap block contains the record 'b[0]:A0' and 'm[k]:A1'. Arrows indicate the flow of pointers from the summary block through the data block and inode block to the mapping block.](images/image_0132.jpeg)\n\n\nDiagram showing the mechanism of Log-Structured File System (LFS) for determining block liveness. A segment summary block (SS) at address A0 contains the record 'A0: (k,0)'. This block points to a data block 'D' at address A0. Block 'D' points to an inode block 'I[k]' at address A1. Block 'I[k]' points to a mapping block 'imap' at address A1. The imap block contains the record 'b[0]:A0' and 'm[k]:A1'. Arrows indicate the flow of pointers from the summary block through the data block and inode block to the mapping block.\n\n\nThere are some shortcuts LFS takes to make the process of determining liveness more efficient. For example, when a file is truncated or deleted, LFS increases its\n   **version number**\n   and records the new version number in the imap. By also recording the version number in the on-disk segment, LFS can short circuit the longer check described above simply by comparing the on-disk version number with a version number in the imap, thus avoiding extra reads.\n\n\n\n\n**43.11 A Policy Question: Which Blocks To Clean, And When?**\n\n\nOn top of the mechanism described above, LFS must include a set of policies to determine both when to clean and which blocks are worth cleaning. Determining when to clean is easier; either periodically, during idle time, or when you have to because the disk is full.\n\n\nDetermining which blocks to clean is more challenging, and has been the subject of many research papers. In the original LFS paper [RO91], the authors describe an approach which tries to segregate\n   *hot*\n   and\n   *cold*\n   segments. A hot segment is one in which the contents are being frequently over-written; thus, for such a segment, the best policy is to wait a long time before cleaning it, as more and more blocks are getting over-written (in new segments) and thus being freed for use. A cold segment, in contrast, may have a few dead blocks but the rest of its contents are relatively stable. Thus, the authors conclude that one should clean cold segments sooner and hot segments later, and develop a heuristic that does exactly that. However, as with most policies, this policy isn't perfect; later approaches show how to do better [MR+97]."
        },
        {
          "name": "Crash Recovery And The Log",
          "content": "One final problem: what happens if the system crashes while LFS is writing to disk? As you may recall in the previous chapter about journaling, crashes during updates are tricky for file systems, and thus something LFS must consider as well.\n\n\nDuring normal operation, LFS buffers writes in a segment, and then (when the segment is full, or when some amount of time has elapsed), writes the segment to disk. LFS organizes these writes in a\n   **log**\n   , i.e., the checkpoint region points to a head and tail segment, and each segment points to the next segment to be written. LFS also periodically updates the checkpoint region. Crashes could clearly happen during either of these operations (write to a segment, write to the CR). So how does LFS handle crashes during writes to these structures?\n\n\nLet's cover the second case first. To ensure that the CR update happens atomically, LFS actually keeps two CRs, one at either end of the disk, and writes to them alternately. LFS also implements a careful protocol when updating the CR with the latest pointers to the inode map and other information; specifically, it first writes out a header (with timestamp), then the body of the CR, and then finally one last block (also with a timestamp). If the system crashes during a CR update, LFS can detect this by seeing an inconsistent pair of timestamps. LFS will always choose to use the most recent CR that has consistent timestamps, and thus consistent update of the CR is achieved.\n\n\nLet's now address the first case. Because LFS writes the CR every 30 seconds or so, the last consistent snapshot of the file system may be quite old. Thus, upon reboot, LFS can easily recover by simply reading in the checkpoint region, the imap pieces it points to, and subsequent files and directories; however, the last many seconds of updates would be lost.\n\n\nTo improve upon this, LFS tries to rebuild many of those segments through a technique known as\n   **roll forward**\n   in the database community. The basic idea is to start with the last checkpoint region, find the end of the log (which is included in the CR), and then use that to read through the next segments and see if there are any valid updates within it. If there are, LFS updates the file system accordingly and thus recovers much of the data and metadata written since the last checkpoint. See Rosenblum's award-winning dissertation for details [R92]."
        }
      ]
    },
    {
      "name": "Flash-based SSDs",
      "sections": [
        {
          "name": "From Bits to Banks/Planes",
          "content": "As they say in ancient Greece, storing a single bit (or a few) does not a storage system make. Hence, flash chips are organized into\n   **banks**\n   or\n   **planes**\n   which consist of a large number of cells.\n\n\nA bank is accessed in two different sized units:\n   **blocks**\n   (sometimes called\n   **erase blocks**\n   ), which are typically of size 128 KB or 256 KB, and\n   **pages**\n   , which are a few KB in size (e.g., 4KB). Within each bank there are a large number of blocks; within each block, there are a large number of pages. When thinking about flash, you must remember this new terminology, which is different than the blocks we refer to in disks and RAID5 and the pages we refer to in virtual memory.\n\n\nFigure 44.1 shows an example of a flash plane with blocks and pages; there are three blocks, each containing four pages, in this simple example. We'll see below why we distinguish between blocks and pages; it turns out this distinction is critical for flash operations such as reading and writing, and even more so for the overall performance of the device. The most important (and weird) thing you will learn is that to write to a page within a block, you first have to erase the entire block; this tricky detail makes building a flash-based SSD an interesting and worthwhile challenge, and the subject of the second-half of the chapter.\n\n\n\nBlock: | 0 | 1 | 2\nPage: | 00 01 02 03 | 04 05 06 07 | 08 09 10 11\nContent: |  |  | \n\n\nFigure 44.1: A Simple Flash Chip: Pages Within Blocks"
        },
        {
          "name": "Basic Flash Operations",
          "content": "Given this flash organization, there are three low-level operations that a flash chip supports. The\n   **read**\n   command is used to read a page from the flash;\n   **erase**\n   and\n   **program**\n   are used in tandem to write. The details:\n\n\n  * •\n    **Read (a page):**\n    A client of the flash chip can read any page (e.g., 2KB or 4KB), simply by specifying the read command and appropriate page number to the device. This operation is typically quite fast, 10s of microseconds or so, regardless of location on the device, and (more or less) regardless of the location of the previous request (quite unlike a disk). Being able to access any location uniformly quickly means the device is a\n    **random access**\n    device.\n  * •\n    **Erase (a block):**\n    Before writing to a\n    *page*\n    within a flash, the nature of the device requires that you first\n    **erase**\n    the entire\n    *block*\n    the page lies within. Erase, importantly, destroys the contents of the block (by setting each bit to the value 1); therefore, you must be sure that any data you care about in the block has been copied elsewhere (to memory, or perhaps to another flash block)\n    *before*\n    executing the erase. The erase command is quite expensive, taking a few milliseconds to complete. Once finished, the entire block is reset and each page is ready to be programmed.\n  * •\n    **Program (a page):**\n    Once a block has been erased, the program command can be used to change some of the 1's within a page to 0's, and write the desired contents of a page to the flash. Programming a page is less expensive than erasing a block, but more costly than reading a page, usually taking around 100s of microseconds on modern flash chips.\n\n\nOne way to think about flash chips is that each page has a state associated with it. Pages start in an\n   **INVALID**\n   state. By erasing the block that a page resides within, you set the state of the page (and all pages within that block) to\n   **ERASED**\n   , which resets the content of each page in the block but also (importantly) makes them programmable. When you program a page, its state changes to\n   **VALID**\n   , meaning its contents have been set and can be read. Reads do not affect these states (although you should only read from pages that have been programmed). Once a page has been programmed, the only way to change its contents is to erase the entire block within which the page resides. Here is an example of states transition after various erase and program operations within a 4-page block:\n\n\n\n |  | iiii | Initial: pages in block are invalid (i)\nErase() | → | EEEE | State of pages in block set to erased (E)\nProgram(0) | → | VEEE | Program page 0; state set to valid (V)\nProgram(0) | → | error | Cannot re-program page after programming\nProgram(1) | → | VVEE | Program page 1\nErase() | → | EEEE | Contents erased; all pages programmable\n\n\n\n\n**A Detailed Example**\n\n\nBecause the process of writing (i.e., erasing and programming) is so unusual, let's go through a detailed example to make sure it makes sense. In this example, imagine we have the following four 8-bit pages, within a 4-page block (both unrealistically small sizes, but useful within this example); each page is\n   \n    VALID\n   \n   as each has been previously programmed.\n\n\n\nPage 0 | Page 1 | Page 2 | Page 3\n00011000 | 11001110 | 00000001 | 00111111\nVALID | VALID | VALID | VALID\n\n\nNow say we wish to write to page 0, filling it with new contents. To write any page, we must first erase the entire block. Let's assume we do so, thus leaving the block in this state:\n\n\n\nPage 0 | Page 1 | Page 2 | Page 3\n11111111 | 11111111 | 11111111 | 11111111\nERASED | ERASED | ERASED | ERASED\n\n\nGood news! We could now go ahead and program page 0, for example with the contents\n   \n    00000011\n   \n   , overwriting the old page 0 (contents\n   \n    00011000\n   \n   ) as desired. After doing so, our block looks like this:\n\n\n\nPage 0 | Page 1 | Page 2 | Page 3\n00000011 | 11111111 | 11111111 | 11111111\nVALID | ERASED | ERASED | ERASED\n\n\nAnd now the bad news: the previous contents of pages 1, 2, and 3 are all gone! Thus, before overwriting any page\n   *within*\n   a block, we must first move any data we care about to another location (e.g., memory, or elsewhere on the flash). The nature of erase will have a strong impact on how we design flash-based SSDs, as we'll soon learn about.\n\n\n\n\n**Summary**\n\n\nTo summarize, reading a page is easy: just read the page. Flash chips do this quite well, and quickly; in terms of performance, they offer the potential to greatly exceed the random read performance of modern disk drives, which are slow due to mechanical seek and rotation costs.\n\n\nWriting a page is trickier; the entire block must first be erased (taking care to first move any data we care about to another location), and then the desired page programmed. Not only is this expensive, but frequent repetitions of this program/erase cycle can lead to the biggest reliability problem flash chips have:\n   **wear out**\n   . When designing a storage system with flash, the performance and reliability of writing is a central focus. We'll soon learn more about how modern SSDs attack these issues, delivering excellent performance and reliability despite these limitations.\n\n\n\nDevice | Read\n      \n      (\n      \n       \\mu\n      \n      s) | Program\n      \n      (\n      \n       \\mu\n      \n      s) | Erase\n      \n      (\n      \n       \\mu\n      \n      s)\nSLC | 25 | 200-300 | 1500-2000\nMLC | 50 | 600-900 | ~3000\nTLC | ~75 | ~900-1350 | ~4500\n\n\nFigure 44.2: Raw Flash Performance Characteristics\n\n\n\n\n**44.4 Flash Performance And Reliability**\n\n\nBecause we're interested in building a storage device out of raw flash chips, it is worthwhile to understand their basic performance characteristics. Figure 44.2 presents a rough summary of some numbers found in the popular press [V12]. Therein, the author presents the basic operation latency of reads, programs, and erases across SLC, MLC, and TLC flash, which store 1, 2, and 3 bits of information per cell, respectively.\n\n\nAs we can see from the table, read latencies are quite good, taking just 10s of microseconds to complete. Program latency is higher and more variable, as low as 200 microseconds for SLC, but higher as you pack more bits into each cell; to get good write performance, you will have to make use of multiple flash chips in parallel. Finally, erases are quite expensive, taking a few milliseconds typically. Dealing with this cost is central to modern flash storage design.\n\n\nLet's now consider reliability of flash chips. Unlike mechanical disks, which can fail for a wide variety of reasons (including the gruesome and quite physical\n   **head crash**\n   , where the drive head actually makes contact with the recording surface), flash chips are pure silicon and in that sense have fewer reliability issues to worry about. The primary concern is\n   **wear out**\n   ; when a flash block is erased and programmed, it slowly accrues a little bit of extra charge. Over time, as that extra charge builds up, it becomes increasingly difficult to differentiate between a 0 and a 1. At the point where it becomes impossible, the block becomes unusable.\n\n\nThe typical lifetime of a block is currently not well known. Manufacturers rate MLC-based blocks as having a 10,000 P/E (Program/Erase) cycle lifetime; that is, each block can be erased and programmed 10,000 times before failing. SLC-based chips, because they store only a single bit per transistor, are rated with a longer lifetime, usually 100,000 P/E cycles. However, recent research has shown that lifetimes are much longer than expected [BD10].\n\n\nOne other reliability problem within flash chips is known as\n   **disturbance**\n   . When accessing a particular page within a flash, it is possible that some bits get flipped in neighboring pages; such bit flips are known as\n   **read disturbs**\n   or\n   **program disturbs**\n   , depending on whether the page is being read or programmed, respectively.\n\n\n**TIP: THE IMPORTANCE OF BACKWARDS COMPATIBILITY**\nBackwards compatibility is always a concern in layered systems. By defining a stable interface between two systems, one enables innovation on each side of the interface while ensuring continued interoperability. Such an approach has been quite successful in many domains: operating systems have relatively stable APIs for applications, disks provide the same block-based interface to file systems, and each layer in the IP networking stack provides a fixed unchanging interface to the layer above.\n\n\nNot surprisingly, there can be a downside to such rigidity, as interfaces defined in one generation may not be appropriate in the next. In some cases, it may be useful to think about redesigning the entire system entirely. An excellent example is found in the Sun ZFS file system [B07]; by reconsidering the interaction of file systems and RAID, the creators of ZFS envisioned (and then realized) a more effective integrated whole."
        },
        {
          "name": "From Raw Flash to Flash-Based SSDs",
          "content": "Given our basic understanding of flash chips, we now face our next task: how to turn a basic set of flash chips into something that looks like a typical storage device. The standard storage interface is a simple block-based one, where blocks (sectors) of size 512 bytes (or larger) can be read or written, given a block address. The task of the flash-based SSD is to provide that standard block interface atop the raw flash chips inside it.\n\n\nInternally, an SSD consists of some number of flash chips (for persistent storage). An SSD also contains some amount of volatile (i.e., non-persistent) memory (e.g., SRAM); such memory is useful for caching and buffering of data as well as for mapping tables, which we'll learn about below. Finally, an SSD contains control logic to orchestrate device operation. See Agrawal et. al for details [A+08]; a simplified block diagram is seen in Figure 44.3 (page 7).\n\n\nOne of the essential functions of this control logic is to satisfy client reads and writes, turning them into internal flash operations as need be. The\n   **flash translation layer**\n   , or\n   **FTL**\n   , provides exactly this functionality. The FTL takes read and write requests on\n   *logical blocks*\n   (that comprise the device interface) and turns them into low-level read, erase, and program commands on the underlying\n   *physical blocks*\n   and\n   *physical pages*\n   (that comprise the actual flash device). The FTL should accomplish this task with the goal of delivering excellent performance and high reliability.\n\n\nExcellent performance, as we'll see, can be realized through a combination of techniques. One key will be to utilize multiple flash chips in\n   **parallel**\n   ; although we won't discuss this technique much further, suffice it to say that all modern SSDs use multiple chips internally to obtain higher performance. Another performance goal will be to reduce\n   **write amplification**\n   , which is defined as the total write traffic (in bytes) issued to the flash chips by the FTL divided by the total write traffic (in bytes) is-\n\n\n\n\n![Figure 44.3: A Flash-based SSD: Logical Diagram. The diagram shows a block diagram of an SSD. On the left, an 'Interface Logic' block is connected to a 'Memory' block and a 'Flash Controller' block. The 'Flash Controller' is connected to a grid of six 'Flash' blocks arranged in two rows of three. The 'Memory' block is also connected to the 'Flash Controller'.](images/image_0133.jpeg)\n\n\nFigure 44.3: A Flash-based SSD: Logical Diagram. The diagram shows a block diagram of an SSD. On the left, an 'Interface Logic' block is connected to a 'Memory' block and a 'Flash Controller' block. The 'Flash Controller' is connected to a grid of six 'Flash' blocks arranged in two rows of three. The 'Memory' block is also connected to the 'Flash Controller'.\n\n\nFigure 44.3: A Flash-based SSD: Logical Diagram\n\n\nsued by the client to the SSD. As we'll see below, naive approaches to FTL construction will lead to high write amplification and low performance.\n\n\nHigh reliability will be achieved through the combination of a few different approaches. One main concern, as discussed above, is\n   **wear out**\n   . If a single block is erased and programmed too often, it will become unusable; as a result, the FTL should try to spread writes across the blocks of the flash as evenly as possible, ensuring that all of the blocks of the device wear out at roughly the same time; doing so is called\n   **wear leveling**\n   and is an essential part of any modern FTL.\n\n\nAnother reliability concern is program disturbance. To minimize such disturbance, FTLs will commonly program pages within an erased block\n   *in order*\n   , from low page to high page. This sequential-programming approach minimizes disturbance and is widely utilized.\n\n\n\n\n**44.6 FTL Organization: A Bad Approach**\n\n\nThe simplest organization of an FTL would be something we call\n   **di-rect mapped**\n   . In this approach, a read to logical page\n   \n    N\n   \n   is mapped directly to a read of physical page\n   \n    N\n   \n   . A write to logical page\n   \n    N\n   \n   is more complicated; the FTL first has to read in the entire block that page\n   \n    N\n   \n   is contained within; it then has to erase the block; finally, the FTL programs the old pages as well as the new one.\n\n\nAs you can probably guess, the direct-mapped FTL has many problems, both in terms of performance as well as reliability. The performance problems come on each write: the device has to read in the entire block (costly), erase it (quite costly), and then program it (costly). The end result is severe write amplification (proportional to the number of pages in a block) and as a result, terrible write performance, even slower than typical hard drives with their mechanical seeks and rotational delays.\n\n\nEven worse is the reliability of this approach. If file system metadata or user file data is repeatedly overwritten, the same block is erased and programmed, over and over, rapidly wearing it out and potentially losing data. The direct mapped approach simply gives too much control over wear out to the client workload; if the workload does not spread write load evenly across its logical blocks, the underlying physical blocks containing popular data will quickly wear out. For both reliability and performance reasons, a direct-mapped FTL is a bad idea."
        },
        {
          "name": "A Log-Structured FTL",
          "content": "For these reasons, most FTLs today are\n   **log structured**\n   , an idea useful in both storage devices (as we'll see now) and file systems above them (e.g., in\n   **log-structured file systems**\n   ). Upon a write to logical block\n   \n    N\n   \n   , the device appends the write to the next free spot in the currently-being-written-to block; we call this style of writing\n   **logging**\n   . To allow for subsequent reads of block\n   \n    N\n   \n   , the device keeps a\n   **mapping table**\n   (in its memory, and persistent, in some form, on the device); this table stores the physical address of each logical block in the system.\n\n\nLet's go through an example to make sure we understand how the basic log-based approach works. To the client, the device looks like a typical disk, in which it can read and write 512-byte sectors (or groups of sectors). For simplicity, assume that the client is reading or writing 4-KB sized chunks. Let us further assume that the SSD contains some large number of 16-KB sized blocks, each divided into four 4-KB pages; these parameters are unrealistic (flash blocks usually consist of more pages) but will serve our didactic purposes quite well.\n\n\nAssume the client issues the following sequence of operations:\n\n\n  * • Write(100) with contents\n    \n     a_1\n  * • Write(101) with contents\n    \n     a_2\n  * • Write(2000) with contents\n    \n     b_1\n  * • Write(2001) with contents\n    \n     b_2\n\n\nThese\n   **logical block addresses**\n   (e.g., 100) are used by the client of the SSD (e.g., a file system) to remember where information is located.\n\n\nInternally, the device must transform these block writes into the erase and program operations supported by the raw hardware, and somehow record, for each logical block address, which\n   **physical page**\n   of the SSD stores its data. Assume that all blocks of the SSD are currently not valid, and must be erased before any page can be programmed. Here we show the initial state of our SSD, with all pages marked\n   **INVALID**\n   (\n   \n    i\n   \n   ):\n\n\n\nBlock: | 0 | 1 | 2\nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11\nContent: |  |  | \nState: | i | i | i | i | i | i | i | i | i | i | i | i\n\n\nWhen the first write is received by the SSD (to logical block 100), the FTL decides to write it to physical block 0, which contains four physical pages: 0, 1, 2, and 3. Because the block is not erased, we cannot write to it yet; the device must first issue an erase command to block 0. Doing so leads to the following state:\n\n\n\nBlock: | 0 | 1 | 2\nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11\nContent: |  |  | \nState: | E | E | E | E | i | i | i | i | i | i | i | i\n\n\nBlock 0 is now ready to be programmed. Most SSDs will write pages in order (i.e., low to high), reducing reliability problems related to\n   **program disturbance**\n   . The SSD then directs the write of logical block 100 into physical page 0:\n\n\n\nBlock: | 0 | 1 | 2\nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11\nContent: | a1 |  |  |  |  |  |  |  |  |  |  | \nState: | V | E | E | E | i | i | i | i | i | i | i | i\n\n\nBut what if the client wants to\n   *read*\n   logical block 100? How can it find where it is? The SSD must transform a read issued to logical block 100 into a read of physical page 0. To accommodate such functionality, when the FTL writes logical block 100 to physical page 0, it records this fact in an\n   **in-memory mapping table**\n   . We will track the state of this mapping table in the diagrams as well:\n\n\n\nTable: 100 → 0 | Memory\nBlock: | 0 | 1 | 2 | Flash Chip\nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11 | \nContent: | a1 |  |  |  |  |  |  |  |  |  |  |  | \nState: | V | E | E | E | i | i | i | i | i | i | i | i | \n\n\nNow you can see what happens when the client writes to the SSD. The SSD finds a location for the write, usually just picking the next free page; it then programs that page with the block's contents, and records the logical-to-physical mapping in its mapping table. Subsequent reads simply use the table to\n   **translate**\n   the logical block address presented by the client into the physical page number required to read the data.\n\n\nLet's now examine the rest of the writes in our example write stream: 101, 2000, and 2001. After writing these blocks, the state of the device is:\n\n\n\nTable: 100 → 0    101 → 1    2000 → 2    2001 → 3 | Memory\nBlock: | 0 | 1 | 2 | Flash Chip\nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11 | \nContent: | a1 | a2 | b1 | b2 |  |  |  |  |  |  |  |  | \nState: | V | V | V | V | i | i | i | i | i | i | i | i | \n\n\nThe log-based approach by its nature improves performance (erases only being required once in a while, and the costly read-modify-write of the direct-mapped approach avoided altogether), and greatly enhances reliability. The FTL can now spread writes across all pages, performing what is called\n   **wear leveling**\n   and increasing the lifetime of the device; we'll discuss wear leveling further below.\n\n\n\n\n**ASIDE: FTL MAPPING INFORMATION PERSISTENCE**\n\n\nYou might be wondering: what happens if the device loses power? Does the in-memory mapping table disappear? Clearly, such information cannot truly be lost, because otherwise the device would not function as a persistent storage device. An SSD must have some means of recovering mapping information.\n\n\nThe simplest thing to do is to record some mapping information with each page, in what is called an\n   **out-of-band (OOB)**\n   area. When the device loses power and is restarted, it must reconstruct its mapping table by scanning the OOB areas and reconstructing the mapping table in memory. This basic approach has its problems; scanning a large SSD to find all necessary mapping information is slow. To overcome this limitation, some higher-end devices use more complex\n   **logging**\n   and\n   **checkpointing**\n   techniques to speed up recovery; learn more about logging by reading chapters on crash consistency and log-structured file systems [AD14a].\n\n\nUnfortunately, this basic approach to log structuring has some downsides. The first is that overwrites of logical blocks lead to something we call\n   **garbage**\n   , i.e., old versions of data around the drive and taking up space. The device has to periodically perform\n   **garbage collection (GC)**\n   to find said blocks and free space for future writes; excessive garbage collection drives up write amplification and lowers performance. The second is high cost of in-memory mapping tables; the larger the device, the more memory such tables need. We now discuss each in turn."
        },
        {
          "name": "Garbage Collection",
          "content": "The first cost of any log-structured approach such as this one is that garbage is created, and therefore\n   **garbage collection**\n   (i.e., dead-block reclamation) must be performed. Let's use our continued example to make sense of this. Recall that logical blocks 100, 101, 2000, and 2001 have been written to the device.\n\n\nNow, let's assume that blocks 100 and 101 are written to again, with contents\n   \n    c_1\n   \n   and\n   \n    c_2\n   \n   . The writes are written to the next free pages (in this case, physical pages 4 and 5), and the mapping table is updated accordingly. Note that the device must have first erased block 1 to make such programming possible:\n\n\n\n | Memory\nTable: | 100 | → 4 | 101 | → 5 | 2000 | → 2 | 2001 | → 3 | \nBlock: | 0 | 1 | 2 | \nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11 | \nContent: | a1 | a2 | b1 | b2 | c1 | c2 |  |  |  |  |  |  | \nState: | V | V | V | V | V | V | E | E | i | i | i | i | \n\n\nThe problem we have now should be obvious: physical pages 0 and 1, although marked\n   \n    VALID\n   \n   , have\n   **garbage**\n   in them, i.e., the old versions of blocks 100 and 101. Because of the log-structured nature of the device, overwrites create garbage blocks, which the device must reclaim to provide free space for new writes to take place.\n\n\nThe process of finding garbage blocks (also called\n   **dead blocks**\n   ) and reclaiming them for future use is called\n   **garbage collection**\n   , and it is an important component of any modern SSD. The basic process is simple: find a block that contains one or more garbage pages, read in the live (non-garbage) pages from that block, write out those live pages to the log, and (finally) reclaim the entire block for use in writing.\n\n\nLet's now illustrate with an example. The device decides it wants to reclaim any dead pages within block 0 above. Block 0 has two dead blocks (pages 0 and 1) and two live blocks (pages 2 and 3, which contain blocks 2000 and 2001, respectively). To do so, the device will:\n\n\n  * • Read live data (pages 2 and 3) from block 0\n  * • Write live data to end of the log\n  * • Erase block 0 (freeing it for later usage)\n\n\nFor the garbage collector to function, there must be enough information within each block to enable the SSD to determine whether each page is live or dead. One natural way to achieve this end is to store, at some location within each block, information about which logical blocks are stored within each page. The device can then use the mapping table to determine whether each page within the block holds live data or not.\n\n\nFrom our example above (before the garbage collection has taken place), block 0 held logical blocks 100, 101, 2000, 2001. By checking the mapping table (which, before garbage collection, contained\n   \n    100 \\to 4\n   \n   ,\n   \n    101 \\to 5\n   \n   ,\n   \n    2000 \\to 2\n   \n   ,\n   \n    2001 \\to 3\n   \n   ), the device can readily determine whether each of the pages within the SSD block holds live information. For example, pages 2 and 3 are clearly still pointed to by the map; pages 0 and 1 are not and therefore are candidates for garbage collection.\n\n\nWhen this garbage collection process is complete in our example, the state of the device is:\n\n\n\n | Table:    100 →4    101 →5    2000→6    2001→7 | Memory\nBlock: | 0 | 1 | 2 | Flash Chip\nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11\nContent: |  | c1 | c2 | b1 | b2 | \nState: | E | E | E | E | V | V | V | V | i | i | i | i\n\n\nAs you can see, garbage collection can be expensive, requiring reading and rewriting of live data. The ideal candidate for reclamation is a block that consists of only dead pages; in this case, the block can immediately be erased and used for new data, without expensive data migration.\n\n\n**ASIDE: A NEW STORAGE API KNOWN AS TRIM**\nWhen we think of hard drives, we usually just think of the most basic interface to read and write them: read and write (there is also usually some kind of\n   **cache flush**\n   command, ensuring that writes have actually been persisted, but sometimes we omit that for simplicity). With log-structured SSDs, and indeed, any device that keeps a flexible and changing mapping of logical-to-physical blocks, a new interface is useful, known as the\n   **trim**\n   operation.\n\n\nThe trim operation takes an address (and possibly a length) and simply informs the device that the block(s) specified by the address (and length) have been deleted; the device thus no longer has to track any information about the given address range. For a standard hard drive, trim isn't particularly useful, because the drive has a static mapping of block addresses to specific platter, track, and sector(s). For a log-structured SSD, however, it is highly useful to know that a block is no longer needed, as the SSD can then remove this information from the FTL and later reclaim the physical space during garbage collection.\n\n\nAlthough we sometimes think of interface and implementation as separate entities, in this case, we see that the implementation shapes the interface. With complex mappings, knowledge of which blocks are no longer needed makes for a more effective implementation.\n\n\nTo reduce GC costs, some SSDs\n   **overprovision**\n   the device [A+08]; by adding extra flash capacity, cleaning can be delayed and pushed to the\n   **background**\n   , perhaps done at a time when the device is less busy. Adding more capacity also increases internal bandwidth, which can be used for cleaning and thus not harm perceived bandwidth to the client. Many modern drives overprovision in this manner, one key to achieving excellent overall performance."
        },
        {
          "name": "Mapping Table Size",
          "content": "The second cost of log-structuring is the potential for extremely large mapping tables, with one entry for each 4-KB page of the device. With a large 1-TB SSD, for example, a single 4-byte entry per 4-KB page results in 1 GB of memory needed by the device, just for these mappings! Thus, this\n   **page-level**\n   FTL scheme is impractical.\n\n\n\n\n**Block-Based Mapping**\n\n\nOne approach to reduce the costs of mapping is to only keep a pointer per\n   *block*\n   of the device, instead of per page, reducing the amount of mapping information by a factor of\n   \n    \\frac{\\text{Size}_{\\text{block}}}{\\text{Size}_{\\text{page}}}\n   \n   . This\n   **block-level**\n   FTL is akin to having\n\n\nbigger page sizes in a virtual memory system; in that case, you use fewer bits for the VPN and have a larger offset in each virtual address.\n\n\nUnfortunately, using a block-based mapping inside a log-based FTL does not work very well for performance reasons. The biggest problem arises when a “small write” occurs (i.e., one that is less than the size of a physical block). In this case, the FTL must read a large amount of live data from the old block and copy it into a new one (along with the data from the small write). This data copying increases write amplification greatly and thus decreases performance.\n\n\nTo make this issue more clear, let’s look at an example. Assume the client previously wrote out logical blocks 2000, 2001, 2002, and 2003 (with contents, a, b, c, d), and that they are located within physical block 1 at physical pages 4, 5, 6, and 7. With per-page mappings, the translation table would have to record four mappings for these logical blocks: 2000→4, 2001→5, 2002→6, 2003→7.\n\n\nIf, instead, we use block-level mapping, the FTL only needs to record a single address translation for all of this data. The address mapping, however, is slightly different than our previous examples. Specifically, we think of the logical address space of the device as being chopped into chunks that are the size of the physical blocks within the flash. Thus, the logical block address consists of two portions: a chunk number and an offset. Because we are assuming four logical blocks fit within each physical block, the offset portion of the logical addresses requires 2 bits; the remaining (most significant) bits form the chunk number.\n\n\nLogical blocks 2000, 2001, 2002, and 2003 all have the same chunk number (500), and have different offsets (0, 1, 2, and 3, respectively). Thus, with a block-level mapping, the FTL records that chunk 500 maps to block 1 (starting at physical page 4), as shown in this diagram:\n\n\n\nTable: | 500 | → | 4 | Memory\nBlock: | 0 | 1 | 2 | Flash Chip\nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11 | \nContent: | ■ | a b c d | ■ | \nState: | i | i | i | i | V | V | V | V | i | i | i | i | \n\n\nIn a block-based FTL, reading is easy. First, the FTL extracts the chunk number from the logical block address presented by the client, by taking the topmost bits out of the address. Then, the FTL looks up the chunk-number to physical-page mapping in the table. Finally, the FTL computes the address of the desired flash page by\n   *adding*\n   the offset from the logical address to the physical address of the block.\n\n\nFor example, if the client issues a read to logical address 2002, the device extracts the logical chunk number (500), looks up the translation in the mapping table (finding 4), and adds the offset from the logical address (2) to the translation (4). The resulting physical-page address (6) is\n\n\nwhere the data is located; the FTL can then issue the read to that physical address and obtain the desired data (c).\n\n\nBut what if the client writes to logical block 2002 (with contents c')? In this case, the FTL must read in 2000, 2001, and 2003, and then write out all four logical blocks in a new location, updating the mapping table accordingly. Block 1 (where the data used to reside) can then be erased and reused, as shown here.\n\n\n\nTable: 500 → 8 | Memory\nBlock: | 0 | 1 | 2 | Flash Chip\nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11 | \nContent: |  |  | a | b | c' | d | \nState: | i | i | i | i | E | E | E | E | V | V | V | V | \n\n\nAs you can see from this example, while block level mappings greatly reduce the amount of memory needed for translations, they cause significant performance problems when writes are smaller than the physical block size of the device; as real physical blocks can be 256KB or larger, such writes are likely to happen quite often. Thus, a better solution is needed. Can you sense that this is the part of the chapter where we tell you what that solution is? Better yet, can you figure it out yourself, before reading on?\n\n\n\n\n**Hybrid Mapping**\n\n\nTo enable flexible writing but also reduce mapping costs, many modern FTLs employ a\n   **hybrid mapping**\n   technique. With this approach, the FTL keeps a few blocks erased and directs all writes to them; these are called\n   **log blocks**\n   . Because the FTL wants to be able to write any page to any location within the log block without all the copying required by a pure block-based mapping, it keeps\n   *per-page*\n   mappings for these log blocks.\n\n\nThe FTL thus logically has two types of mapping table in its memory: a small set of\n   *per-page*\n   mappings in what we'll call the\n   *log table*\n   , and a larger set of\n   *per-block*\n   mappings in the\n   *data table*\n   . When looking for a particular logical block, the FTL will first consult the log table; if the logical block's location is not found there, the FTL will then consult the data table to find its location and then access the requested data.\n\n\nThe key to the hybrid mapping strategy is keeping the number of log blocks small. To keep the number of log blocks small, the FTL has to periodically examine log blocks (which have a pointer per page) and\n   *switch*\n   them into blocks that can be pointed to by only a single block pointer. This switch is accomplished by one of three main techniques, based on the contents of the block [KK+02].\n\n\nFor example, let's say the FTL had previously written out logical pages 1000, 1001, 1002, and 1003, and placed them in physical block 2 (physical\n\n\npages 8, 9, 10, 11); assume the contents of the writes to 1000, 1001, 1002, and 1003 are a, b, c, and d, respectively.\n\n\n\nLog Table: |  |  |  |  |  |  |  |  |  |  | Memory\nData Table: | 250 | → | 8 |  |  |  |  |  |  |  | \nBlock: | 0 |  |  | 1 |  |  | 2 |  |  |  | \nPage: | 00 01 02 03 |  | 04 05 06 07 |  |  | 08 09 10 11 |  |  |  |  | Flash Chip\nContent: |  |  |  |  |  |  | a | b | c | d | \nState: | i | i | i | i | i | i | V | V | V | V | \n\n\nNow assume that the client overwrites each of these blocks (with data\n   \n    a', b', c',\n   \n   and\n   \n    d'\n   \n   ), in the exact same order, in one of the currently available log blocks, say physical block 0 (physical pages 0, 1, 2, and 3). In this case, the FTL will have the following state:\n\n\n\nLog Table: | 1000→0 | 1001→1 | 1002→2 | 1003→3 |  |  |  |  |  |  | Memory\nData Table: | 250 | → | 8 |  |  |  |  |  |  |  | \nBlock: | 0 |  |  | 1 |  |  | 2 |  |  |  | \nPage: | 00 01 02 03 |  | 04 05 06 07 |  |  | 08 09 10 11 |  |  |  |  | Flash Chip\nContent: | a' | b' | c' | d' |  |  | a | b | c | d | \nState: | V | V | V | V | i | i | i | V | V | V | \n\n\nBecause these blocks have been written exactly in the same manner as before, the FTL can perform what is known as a\n   **switch merge**\n   . In this case, the log block (0) now becomes the storage location for blocks 0, 1, 2, and 3, and is pointed to by a single block pointer; the old block (2) is now erased and used as a log block. In this best case, all the per-page pointers required replaced by a single block pointer.\n\n\n\nLog Table: |  |  |  |  |  |  |  |  |  |  | Memory\nData Table: | 250 | → | 0 |  |  |  |  |  |  |  | \nBlock: | 0 |  |  | 1 |  |  | 2 |  |  |  | \nPage: | 00 01 02 03 |  | 04 05 06 07 |  |  | 08 09 10 11 |  |  |  |  | Flash Chip\nContent: | a' | b' | c' | d' |  |  |  |  |  |  | \nState: | V | V | V | V | i | i | i | i | i | i | \n\n\nThis switch merge is the best case for a hybrid FTL. Unfortunately, sometimes the FTL is not so lucky. Imagine the case where we have the same initial conditions (logical blocks 1000 ... 1003 stored in physical block 2) but then the client overwrites logical blocks 1000 and 1001.\n\n\nWhat do you think happens in this case? Why is it more challenging to handle? (\n   *think before looking at the result on the next page*\n   )\n\n\n\nLog Table: | 1000 → 0 | 1001 → 1 |  |  |  |  |  |  |  |  |  | Memory\nData Table: | 250 → 8 |  |  |  |  |  |  |  |  |  |  | \nBlock: | 0 | 1 | 2 | Flash\nPage: | 00 | 01 | 02 | 03 | 04 | 05 | 06 | 07 | 08 | 09 | 10 | 11 | Chip\nContent: | a' | b' |  |  |  |  |  |  | a | b | c | d | \nState: | V | V | i | i | i | i | i | i | V | V | V | V | \n\n\nTo reunite the other pages of this physical block, and thus be able to refer to them by only a single block pointer, the FTL performs what is called a\n   **partial merge**\n   . In this operation, logical blocks 1002 and 1003 are read from physical block 2, and then appended to the log. The resulting state of the SSD is the same as the switch merge above; however, in this case, the FTL had to perform extra I/O to achieve its goals, thus increasing write amplification.\n\n\nThe final case encountered by the FTL known as a\n   **full merge**\n   , and requires even more work. In this case, the FTL must pull together pages from many other blocks to perform cleaning. For example, imagine that logical blocks 0, 4, 8, and 12 are written to log block\n   *A*\n   . To switch this log block into a block-mapped page, the FTL must first create a data block containing logical blocks 0, 1, 2, and 3, and thus the FTL must read 1, 2, and 3 from elsewhere and then write out 0, 1, 2, and 3 together. Next, the merge must do the same for logical block 4, finding 5, 6, and 7 and reconciling them into a single physical block. The same must be done for logical blocks 8 and 12, and then (finally), the log block\n   *A*\n   can be freed. Frequent full merges, as is not surprising, can seriously harm performance and thus should be avoided when at all possible [GY+09].\n\n\n\n\n**Page Mapping Plus Caching**\n\n\nGiven the complexity of the hybrid approach above, others have suggested simpler ways to reduce the memory load of page-mapped FTLs. Probably the simplest is just to cache only the active parts of the FTL in memory, thus reducing the amount of memory needed [GY+09].\n\n\nThis approach can work well. For example, if a given workload only accesses a small set of pages, the translations of those pages will be stored in the in-memory FTL, and performance will be excellent without high memory cost. Of course, the approach can also perform poorly. If memory cannot contain the\n   **working set**\n   of necessary translations, each access will minimally require an extra flash read to first bring in the missing mapping before being able to access the data itself. Even worse, to make room for the new mapping, the FTL might have to\n   **evict**\n   an old mapping, and if that mapping is\n   **dirty**\n   (i.e., not yet written to the flash persistently), an extra write will also be incurred. However, in many cases, the workload will display locality, and this caching approach will both reduce memory overheads and keep performance high."
        },
        {
          "name": "Wear Leveling",
          "content": "Finally, a related background activity that modern FTLs must implement is\n   **wear leveling**\n   , as introduced above. The basic idea is simple: because multiple erase/program cycles will wear out a flash block, the FTL should try its best to spread that work across all the blocks of the device evenly. In this manner, all blocks will wear out at roughly the same time, instead of a few “popular” blocks quickly becoming unusable.\n\n\nThe basic log-structuring approach does a good initial job of spreading out write load, and garbage collection helps as well. However, sometimes a block will be filled with long-lived data that does not get over-written; in this case, garbage collection will never reclaim the block, and thus it does not receive its fair share of the write load.\n\n\nTo remedy this problem, the FTL must periodically read all the live data out of such blocks and re-write it elsewhere, thus making the block available for writing again. This process of wear leveling increases the write amplification of the SSD, and thus decreases performance as extra I/O is required to ensure that all blocks wear at roughly the same rate. Many different algorithms exist in the literature [A+08, M+14]; read more if you are interested."
        },
        {
          "name": "SSD Performance And Cost",
          "content": "Before closing, let’s examine the performance and cost of modern SSDs, to better understand how they will likely be used in persistent storage systems. In both cases, we’ll compare to classic hard-disk drives (HDDs), and highlight the biggest differences between the two.\n\n\n\n\n**Performance**\n\n\nUnlike hard disk drives, flash-based SSDs have no mechanical components, and in fact are in many ways more similar to DRAM, in that they are “random access” devices. The biggest difference in performance, as compared to disk drives, is realized when performing random reads and writes; while a typical disk drive can only perform a few hundred random I/Os per second, SSDs can do much better. Here, we use some data from modern SSDs to see just how much better SSDs perform; we’re particularly interested in how well the FTLs hide the performance issues of the raw chips.\n\n\nTable 44.4 shows some performance data for three different SSDs and one top-of-the-line hard drive; the data was taken from a few different online sources [S13, T15]. The left two columns show random I/O performance, and the right two columns show sequential; the first three rows show data for three different SSDs (from Samsung, Seagate, and Intel), and the last row shows performance for a\n   **hard disk drive**\n   (or\n   **HDD**\n   ), in this case a Seagate high-end drive.\n\n\nWe can learn a few interesting facts from the table. First, and most dramatic, is the difference in random I/O performance between the SSDs\n\n\n\nDevice | Random | Sequential\nReads\n      \n      (MB/s) | Writes\n      \n      (MB/s) | Reads\n      \n      (MB/s) | Writes\n      \n      (MB/s)\nSamsung 840 Pro SSD | 103 | 287 | 421 | 384\nSeagate 600 SSD | 84 | 252 | 424 | 374\nIntel SSD 335 SSD | 39 | 222 | 344 | 354\nSeagate Savvio 15K.3 HDD | 2 | 2 | 223 | 223\n\n\nFigure 44.4: SSDs And Hard Drives: Performance Comparison\n\n\nand the lone hard drive. While the SSDs obtain tens or even hundreds of MB/s in random I/Os, this “high performance” hard drive has a peak of just a couple MB/s (in fact, we rounded up to get to 2 MB/s). Second, you can see that in terms of sequential performance, there is much less of a difference; while the SSDs perform better, a hard drive is still a good choice if sequential performance is all you need. Third, you can see that SSD random read performance is not as good as SSD random write performance. The reason for such unexpectedly good random-write performance is due to the log-structured design of many SSDs, which transforms random writes into sequential ones and improves performance. Finally, because SSDs exhibit some performance difference between sequential and random I/Os, many of the techniques in chapters about how to build file systems for hard drives are still applicable to SSDs [AD14b]; although the magnitude of difference between sequential and random I/Os is smaller, there is enough of a gap to carefully consider how to design file systems to reduce random I/Os.\n\n\n\n\n**Cost**\n\n\nAs we saw above, the performance of SSDs greatly outstrips modern hard drives, even when performing sequential I/O. So why haven’t SSDs completely replaced hard drives as the storage medium of choice? The answer is simple: cost, or more specifically, cost per unit of capacity. Currently [A15], an SSD costs something like $150 for a 250-GB drive; such an SSD costs 60 cents per GB. A typical hard drive costs roughly $50 for 1-TB of storage, which means it costs 5 cents per GB. There is still more than a 10× difference in cost between these two storage media.\n\n\nThese performance and cost differences dictate how large-scale storage systems are built. If performance is the main concern, SSDs are a terrific choice, particularly if random read performance is important. If, on the other hand, you are assembling a large data center and wish to store massive amounts of information, the large cost difference will drive you towards hard drives. Of course, a hybrid approach can make sense – some storage systems are being assembled with both SSDs and hard drives, using a smaller number of SSDs for more popular “hot” data and delivering high performance, while storing the rest of the “colder” (less used) data on hard drives to save on cost. As long as the price gap exists, hard drives are here to stay."
        }
      ]
    },
    {
      "name": "Data Integrity and Protection",
      "sections": [
        {
          "name": "Disk Failure Modes",
          "content": "As you learned in the chapter about RAID, disks are not perfect, and can fail (on occasion). In early RAID systems, the model of failure was quite simple: either the entire disk is working, or it fails completely, and the detection of such a failure is straightforward. This\n   **fail-stop**\n   model of disk failure makes building RAID relatively simple [S90].\n\n\nWhat you didn't learn is about all of the other types of failure modes modern disks exhibit. Specifically, as Bairavasundaram et al. studied in great detail [B+07, B+08], modern disks will occasionally seem to be mostly working but have trouble successfully accessing one or more blocks. Specifically, two types of single-block failures are common and worthy of consideration:\n   **latent sector errors (LSEs)**\n   and\n   **block corruption**\n   . We'll now discuss each in more detail.\n\n\n\n | Cheap | Costly\nLSEs | 9.40% | 1.40%\nCorruption | 0.50% | 0.05%\n\n\nFigure 45.1: Frequency Of LSEs And Block Corruption\n\n\nLSEs arise when a disk sector (or group of sectors) has been damaged in some way. For example, if the disk head touches the surface for some reason (a\n   **head crash**\n   , something which shouldn't happen during normal operation), it may damage the surface, making the bits unreadable. Cosmic rays can also flip bits, leading to incorrect contents. Fortunately, in-disk\n   **error correcting codes**\n   (ECC) are used by the drive to determine whether the on-disk bits in a block are good, and in some cases, to fix them; if they are not good, and the drive does not have enough information to fix the error, the disk will return an error when a request is issued to read them.\n\n\nThere are also cases where a disk block becomes\n   **corrupt**\n   in a way not detectable by the disk itself. For example, buggy disk firmware may write a block to the wrong location; in such a case, the disk ECC indicates the block contents are fine, but from the client's perspective the wrong block is returned when subsequently accessed. Similarly, a block may get corrupted when it is transferred from the host to the disk across a faulty bus; the resulting corrupt data is stored by the disk, but it is not what the client desires. These types of faults are particularly insidious because they are\n   **silent faults**\n   ; the disk gives no indication of the problem when returning the faulty data.\n\n\nPrabhakaran et al. describes this more modern view of disk failure as the\n   **fail-partial**\n   disk failure model [P+05]. In this view, disks can still fail in their entirety (as was the case in the traditional fail-stop model); however, disks can also seemingly be working and have one or more blocks become inaccessible (i.e., LSEs) or hold the wrong contents (i.e., corruption). Thus, when accessing a seemingly-working disk, once in a while it may either return an error when trying to read or write a given block (a non-silent partial fault), and once in a while it may simply return the wrong data (a silent partial fault).\n\n\nBoth of these types of faults are somewhat rare, but just how rare? Figure 45.1 summarizes some of the findings from the two Bairavasundaram studies [B+07,B+08].\n\n\nThe figure shows the percent of drives that exhibited at least one LSE or block corruption over the course of the study (about 3 years, over 1.5 million disk drives). The figure further sub-divides the results into “cheap” drives (usually SATA drives) and “costly” drives (usually SCSI or Fibre Channel). As you can see, while buying better drives reduces the frequency of both types of problem (by about an order of magnitude), they still happen often enough that you need to think carefully about how to handle them in your storage system.\n\n\nSome additional findings about LSEs are:\n\n\n  * • Costly drives with more than one LSE are as likely to develop additional errors as cheaper drives\n  * • For most drives, annual error rate increases in year two\n  * • The number of LSEs increase with disk size\n  * • Most disks with LSEs have less than 50\n  * • Disks with LSEs are more likely to develop additional LSEs\n  * • There exists a significant amount of spatial and temporal locality\n  * • Disk scrubbing is useful (most LSEs were found this way)\n\n\nSome findings about corruption:\n\n\n  * • Chance of corruption varies greatly across different drive models within the same drive class\n  * • Age effects are different across models\n  * • Workload and disk size have little impact on corruption\n  * • Most disks with corruption only have a few corruptions\n  * • Corruption is not independent within a disk or across disks in RAID\n  * • There exists spatial locality, and some temporal locality\n  * • There is a weak correlation with LSEs\n\n\nTo learn more about these failures, you should likely read the original papers [B+07,B+08]. But hopefully the main point should be clear: if you really wish to build a reliable storage system, you must include machinery to detect and recover from both LSEs and block corruption."
        },
        {
          "name": "Handling Latent Sector Errors",
          "content": "Given these two new modes of partial disk failure, we should now try to see what we can do about them. Let's first tackle the easier of the two, namely latent sector errors.\n\n\n\n\n**CRUX: HOW TO HANDLE LATENT SECTOR ERRORS**\n\n\nHow should a storage system handle latent sector errors? How much extra machinery is needed to handle this form of partial failure?\n\n\nAs it turns out, latent sector errors are rather straightforward to handle, as they are (by definition) easily detected. When a storage system tries to access a block, and the disk returns an error, the storage system should simply use whatever redundancy mechanism it has to return the correct data. In a mirrored RAID, for example, the system should access the alternate copy; in a RAID-4 or RAID-5 system based on parity, the system should reconstruct the block from the other blocks in the parity group. Thus, easily detected problems such as LSEs are readily recovered through standard redundancy mechanisms.\n\n\nThe growing prevalence of LSEs has influenced RAID designs over the years. One particularly interesting problem arises in RAID-4/5 systems when both full-disk faults and LSEs occur in tandem. Specifically, when an entire disk fails, the RAID tries to\n   **reconstruct**\n   the disk (say, onto a hot spare) by reading through all of the other disks in the parity group and recomputing the missing values. If, during reconstruction, an LSE is encountered on any one of the other disks, we have a problem: the reconstruction cannot successfully complete.\n\n\nTo combat this issue, some systems add an extra degree of redundancy. For example, NetApp's\n   **RAID-DP**\n   has the equivalent of two parity disks instead of one [C+04]. When an LSE is discovered during reconstruction, the extra parity helps to reconstruct the missing block. As always, there is a cost, in that maintaining two parity blocks for each stripe is more costly; however, the log-structured nature of the NetApp\n   **WAFL**\n   file system mitigates that cost in many cases [HLM94]. The remaining cost is space, in the form of an extra disk for the second parity block."
        },
        {
          "name": "Detecting Corruption: The Checksum",
          "content": "Let's now tackle the more challenging problem, that of silent failures via data corruption. How can we prevent users from getting bad data when corruption arises, and thus leads to disks returning bad data?\n\n\n\n\n**CRUX: HOW TO PRESERVE DATA INTEGRITY DESPITE CORRUPTION**\n\n\nGiven the silent nature of such failures, what can a storage system do to detect when corruption arises? What techniques are needed? How can one implement them efficiently?\n\n\nUnlike latent sector errors,\n   *detection*\n   of corruption is a key problem. How can a client tell that a block has gone bad? Once it is known that a particular block is bad,\n   *recovery*\n   is the same as before: you need to have some other copy of the block around (and hopefully, one that is not corrupt!). Thus, we focus here on detection techniques.\n\n\nThe primary mechanism used by modern storage systems to preserve data integrity is called the\n   **checksum**\n   . A checksum is simply the result of a function that takes a chunk of data (say a 4KB block) as input and computes a function over said data, producing a small summary of the contents of the data (say 4 or 8 bytes). This summary is referred to as the checksum. The goal of such a computation is to enable a system to detect if data has somehow been corrupted or altered by storing the checksum with the data and then confirming upon later access that the data's current checksum matches the original storage value.\n\n\n**TIP: THERE'S NO FREE LUNCH**\nThere's No Such Thing As A Free Lunch, or TINSTAAFL for short, is an old American idiom that implies that when you are seemingly getting something for free, in actuality you are likely paying some cost for it. It comes from the old days when diners would advertise a free lunch for customers, hoping to draw them in; only when you went in, did you realize that to acquire the “free” lunch, you had to purchase one or more alcoholic beverages. Of course, this may not actually be a problem, particularly if you are an aspiring alcoholic (or typical undergraduate student).\n\n\n**Common Checksum Functions**\nA number of different functions are used to compute checksums, and vary in strength (i.e., how good they are at protecting data integrity) and speed (i.e., how quickly can they be computed). A trade-off that is common in systems arises here: usually, the more protection you get, the costlier it is. There is no such thing as a free lunch.\n\n\nOne simple checksum function that some use is based on exclusive or (XOR). With XOR-based checksums, the checksum is computed by XOR'ing each chunk of the data block being checksummed, thus producing a single value that represents the XOR of the entire block.\n\n\nTo make this more concrete, imagine we are computing a 4-byte checksum over a block of 16 bytes (this block is of course too small to really be a disk sector or block, but it will serve for the example). The 16 data bytes, in hex, look like this:\n\n\n365e c4cd ba14 8a92 ecef 2c3a 40be f666\nIf we view them in binary, we get the following:\n\n\n0011 0110 0101 1110    1100 0100 1100 1101\n1011 1010 0001 0100    1000 1010 1001 0010\n1110 1100 1110 1111    0010 1100 0011 1010\n0100 0000 1011 1110    1111 0110 0110 0110\nBecause we've lined up the data in groups of 4 bytes per row, it is easy to see what the resulting checksum will be: perform an XOR over each column to get the final checksum value:\n\n\n0010 0000 0001 1011    1001 0100 0000 0011\nThe result, in hex, is 0x201b9403.\n\n\nXOR is a reasonable checksum but has its limitations. If, for example, two bits in the same position within each checksummed unit change, the checksum will not detect the corruption. For this reason, people have investigated other checksum functions.\n\n\nAnother basic checksum function is addition. This approach has the advantage of being fast; computing it just requires performing 2's-complement addition over each chunk of the data, ignoring overflow. It can detect many changes in data, but is not good if the data, for example, is shifted.\n\n\nA slightly more complex algorithm is known as the\n   **Fletcher checksum**\n   , named (as you might guess) for the inventor, John G. Fletcher [F82]. It is quite simple to compute and involves the computation of two check bytes,\n   \n    s_1\n   \n   and\n   \n    s_2\n   \n   . Specifically, assume a block\n   \n    D\n   \n   consists of bytes\n   \n    d_1 \\dots d_n\n   \n   ;\n   \n    s_1\n   \n   is defined as follows:\n   \n    s_1 = (s_1 + d_i) \\bmod 255\n   \n   (computed over all\n   \n    d_i\n   \n   );\n   \n    s_2\n   \n   in turn is:\n   \n    s_2 = (s_2 + s_1) \\bmod 255\n   \n   (again over all\n   \n    d_i\n   \n   ) [F04]. The Fletcher checksum is almost as strong as the CRC (see below), detecting all single-bit, double-bit errors, and many burst errors [F04].\n\n\nOne final commonly-used checksum is known as a\n   **cyclic redundancy check (CRC)**\n   . Assume you wish to compute the checksum over a data block\n   \n    D\n   \n   . All you do is treat\n   \n    D\n   \n   as if it is a large binary number (it is just a string of bits after all) and divide it by an agreed upon value (\n   \n    k\n   \n   ). The remainder of this division is the value of the CRC. As it turns out, one can implement this binary modulo operation rather efficiently, and hence the popularity of the CRC in networking as well. See elsewhere for more details [M13].\n\n\nWhatever the method used, it should be obvious that there is no perfect checksum: it is possible two data blocks with non-identical contents will have identical checksums, something referred to as a\n   **collision**\n   . This fact should be intuitive: after all, computing a checksum is taking something large (e.g., 4KB) and producing a summary that is much smaller (e.g., 4 or 8 bytes). In choosing a good checksum function, we are thus trying to find one that minimizes the chance of collisions while remaining easy to compute.\n\n\n\n\n**Checksum Layout**\n\n\nNow that you understand a bit about how to compute a checksum, let's next analyze how to use checksums in a storage system. The first question we must address is the layout of the checksum, i.e., how should checksums be stored on disk?\n\n\nThe most basic approach simply stores a checksum with each disk sector (or block). Given a data block\n   \n    D\n   \n   , let us call the checksum over that data\n   \n    C(D)\n   \n   . Thus, without checksums, the disk layout looks like this:\n\n\n\nD0 | D1 | D2 | D3 | D4 | D5 | D6\n\n\nWith checksums, the layout adds a single checksum for every block:\n\n\n\nC(D_0) | D_0 | C(D_1) | D_1 | C(D_2) | D_2 | C(D_3) | D_3 | C(D_4) | D_4\n\n\nBecause checksums are usually small (e.g., 8 bytes), and disks only can write in sector-sized chunks (512 bytes) or multiples thereof, one problem that arises is how to achieve the above layout. One solution employed by drive manufacturers is to format the drive with 520-byte sectors; an extra 8 bytes per sector can be used to store the checksum.\n\n\nIn disks that don't have such functionality, the file system must figure out a way to store the checksums packed into 512-byte blocks. One such possibility is as follows:\n\n\n\nC(D_0) | C(D_1) | C(D_2) | C(D_3) | C(D_4) | D_0 | D_1 | D_2 | D_3 | D_4\n\n\nIn this scheme, the\n   \n    n\n   \n   checksums are stored together in a sector, followed by\n   \n    n\n   \n   data blocks, followed by another checksum sector for the next\n   \n    n\n   \n   blocks, and so forth. This approach has the benefit of working on all disks, but can be less efficient; if the file system, for example, wants to overwrite block\n   \n    D_1\n   \n   , it has to read in the checksum sector containing\n   \n    C(D_1)\n   \n   , update\n   \n    C(D_1)\n   \n   in it, and then write out the checksum sector and new data block\n   \n    D_1\n   \n   (thus, one read and two writes). The earlier approach (of one checksum per sector) just performs a single write."
        },
        {
          "name": "Using Checksums",
          "content": "With a checksum layout decided upon, we can now proceed to actually understand how to\n   *use*\n   the checksums. When reading a block\n   \n    D\n   \n   , the client (i.e., file system or storage controller) also reads its checksum from disk\n   \n    C_s(D)\n   \n   , which we call the\n   **stored checksum**\n   (hence the subscript\n   \n    C_s\n   \n   ). The client then\n   *computes*\n   the checksum over the retrieved block\n   \n    D\n   \n   , which we call the\n   **computed checksum**\n\n    C_c(D)\n   \n   . At this point, the client compares the stored and computed checksums; if they are equal (i.e.,\n   \n    C_s(D) == C_c(D)\n   \n   ), the data has likely not been corrupted, and thus can be safely returned to the user. If they do\n   *not*\n   match (i.e.,\n   \n    C_s(D) \\neq C_c(D)\n   \n   ), this implies the data has changed since the time it was stored (since the stored checksum reflects the value of the data at that time). In this case, we have a corruption, which our checksum has helped us to detect.\n\n\nGiven a corruption, the natural question is what should we do about it? If the storage system has a redundant copy, the answer is easy: try to use it instead. If the storage system has no such copy, the likely answer is\n\n\nto return an error. In either case, realize that corruption detection is not a magic bullet; if there is no other way to get the non-corrupted data, you are simply out of luck.\n\n\n\n\n**45.5 A New Problem: Misdirected Writes**\n\n\nThe basic scheme described above works well in the general case of corrupted blocks. However, modern disks have a couple of unusual failure modes that require different solutions.\n\n\nThe first failure mode of interest is called a\n   **misdirected write**\n   . This arises in disk and RAID controllers which write the data to disk correctly, except in the\n   *wrong*\n   location. In a single-disk system, this means that the disk wrote block\n   \n    D_x\n   \n   not to address\n   \n    x\n   \n   (as desired) but rather to address\n   \n    y\n   \n   (thus “corrupting”\n   \n    D_y\n   \n   ); in addition, within a multi-disk system, the controller may also write\n   \n    D_{i,x}\n   \n   not to address\n   \n    x\n   \n   of disk\n   \n    i\n   \n   but rather to some other disk\n   \n    j\n   \n   . Thus our question:\n\n\n\n\n**CRUX: HOW TO HANDLE MISDIRECTED WRITES**\n\n\nHow should a storage system or disk controller detect misdirected writes? What additional features are required from the checksum?\n\n\nThe answer, not surprisingly, is simple: add a little more information to each checksum. In this case, adding a\n   **physical identifier (physical ID)**\n   is quite helpful. For example, if the stored information now contains the checksum\n   \n    C(D)\n   \n   and both the disk and sector numbers of the block, it is easy for the client to determine whether the correct information resides within a particular locale. Specifically, if the client is reading block 4 on disk 10 (\n   \n    D_{10,4}\n   \n   ), the stored information should include that disk number and sector offset, as shown below. If the information does not match, a misdirected write has taken place, and a corruption is now detected. Here is an example of what this added information would look like on a two-disk system. Note that this figure, like the others before it, is not to scale, as the checksums are usually small (e.g., 8 bytes) whereas the blocks are much larger (e.g., 4 KB or bigger):\n\n\n\nDisk 1 | C[D0] | C[D0]\n      \n      , disk=1, block=0 | D0 | C[D1] | C[D1]\n      \n      , disk=1, block=1 | D1 | C[D2] | C[D2]\n      \n      , disk=1, block=2 | D2\nDisk 0 | C[D0] | C[D0]\n      \n      , disk=0, block=0 | D0 | C[D1] | C[D1]\n      \n      , disk=0, block=1 | D1 | C[D2] | C[D2]\n      \n      , disk=0, block=2 | D2\n\n\nYou can see from the on-disk format that there is now a fair amount of redundancy on disk: for each block, the disk number is repeated within each block, and the offset of the block in question is also kept next to the block itself. The presence of redundant information should be no surprise, though; redundancy is the key to error detection (in this case) and recovery (in others). A little extra information, while not strictly needed with perfect disks, can go a long ways in helping detect problematic situations should they arise."
        },
        {
          "name": "One Last Problem: LostWrites",
          "content": "Unfortunately, misdirected writes are not the last problem we will address. Specifically, some modern storage devices also have an issue known as a\n   **lost write**\n   , which occurs when the device informs the upper layer that a write has completed but in fact it never is persisted; thus, what remains is the old contents of the block rather than the updated new contents.\n\n\nThe obvious question here is: do any of our checksumming strategies from above (e.g., basic checksums, or physical identity) help to detect lost writes? Unfortunately, the answer is no: the old block likely has a matching checksum, and the physical ID used above (disk number and block offset) will also be correct. Thus our final problem:\n\n\n\n\n**CRUX: HOW TO HANDLE LOST WRITES**\n\n\nHow should a storage system or disk controller detect lost writes? What additional features are required from the checksum?\n\n\nThere are a number of possible solutions that can help [K+08]. One classic approach [BS04] is to perform a\n   **write verify**\n   or\n   **read-after-write**\n   ; by immediately reading back the data after a write, a system can ensure that the data indeed reached the disk surface. This approach, however, is quite slow, doubling the number of I/Os needed to complete a write.\n\n\nSome systems add a checksum elsewhere in the system to detect lost writes. For example, Sun's\n   **Zettabyte File System (ZFS)**\n   includes a checksum in each file system inode and indirect block for every block included within a file. Thus, even if the write to a data block itself is lost, the checksum within the inode will not match the old data. Only if the writes to both the inode and the data are lost simultaneously will such a scheme fail, an unlikely (but unfortunately, possible!) situation."
        },
        {
          "name": "Scrubbing",
          "content": "Given all of this discussion, you might be wondering: when do these checksums actually get checked? Of course, some amount of checking\n\n\noccurs when data is accessed by applications, but most data is rarely accessed, and thus would remain unchecked. Unchecked data is problematic for a reliable storage system, as bit rot could eventually affect all copies of a particular piece of data.\n\n\nTo remedy this problem, many systems utilize\n   **disk scrubbing**\n   of various forms [K+08]. By periodically reading through\n   *every*\n   block of the system, and checking whether checksums are still valid, the disk system can reduce the chances that all copies of a certain data item become corrupted. Typical systems schedule scans on a nightly or weekly basis."
        },
        {
          "name": "Overheads Of Checksumming",
          "content": "Before closing, we now discuss some of the overheads of using checksums for data protection. There are two distinct kinds of overheads, as is common in computer systems: space and time.\n\n\nSpace overheads come in two forms. The first is on the disk (or other storage medium) itself; each stored checksum takes up room on the disk, which can no longer be used for user data. A typical ratio might be an 8-byte checksum per 4 KB data block, for a 0.19% on-disk space overhead.\n\n\nThe second type of space overhead comes in the memory of the system. When accessing data, there must now be room in memory for the checksums as well as the data itself. However, if the system simply checks the checksum and then discards it once done, this overhead is short-lived and not much of a concern. Only if checksums are kept in memory (for an added level of protection against memory corruption [Z+13]) will this small overhead be observable.\n\n\nWhile space overheads are small, the time overheads induced by checksumming can be quite noticeable. Minimally, the CPU must compute the checksum over each block, both when the data is stored (to determine the value of the stored checksum) and when it is accessed (to compute the checksum again and compare it against the stored checksum). One approach to reducing CPU overheads, employed by many systems that use checksums (including network stacks), is to combine data copying and checksumming into one streamlined activity; because the copy is needed anyhow (e.g., to copy the data from the kernel page cache into a user buffer), combined copying/checksumming can be quite effective.\n\n\nBeyond CPU overheads, some checksumming schemes can induce extra I/O overheads, particularly when checksums are stored distinctly from the data (thus requiring extra I/Os to access them), and for any extra I/O needed for background scrubbing. The former can be reduced by design; the latter can be tuned and thus its impact limited, perhaps by controlling when such scrubbing activity takes place. The middle of the night, when most (not all!) productive workers have gone to bed, may be a good time to perform such scrubbing activity and increase the robustness of the storage system."
        }
      ]
    },
    {
      "name": "Distributed Systems",
      "sections": [
        {
          "name": "Communication Basics",
          "content": "The central tenet of modern networking is that communication is fundamentally unreliable. Whether in the wide-area Internet, or a local-area high-speed network such as Infiniband, packets are regularly lost, corrupted, or otherwise do not reach their destination.\n\n\nThere are a multitude of causes for packet loss or corruption. Sometimes, during transmission, some bits get flipped due to electrical or other similar problems. Sometimes, an element in the system, such as a network link or packet router or even the remote host, are somehow damaged or otherwise not working correctly; network cables do accidentally get severed, at least sometimes.\n\n\nMore fundamental however is packet loss due to lack of buffering within a network switch, router, or endpoint. Specifically, even if we could guarantee that all links worked correctly, and that all the components in the system (switches, routers, end hosts) were up and running as expected, loss is still possible, for the following reason. Imagine a packet arrives at a router; for the packet to be processed, it must be placed in memory somewhere within the router. If many such packets arrive at\n\n\n// client code\nint main(int argc, char *argv[]) {\n    int sd = UDP_Open(20000);\n    struct sockaddr_in addrSnd, addrRcv;\n    int rc = UDP_FillSockAddr(&addrSnd, \"cs.wisc.edu\", 10000);\n    char message[**BUFFER_SIZE**];\n    sprintf(message, \"hello world\");\n    rc = UDP_Write(sd, &addrSnd, message, **BUFFER_SIZE**);\n    if (rc > 0)\n        int rc = UDP_Read(sd, &addrRcv, message, **BUFFER_SIZE**);\n    return 0;\n}\n\n// server code\nint main(int argc, char *argv[]) {\n    int sd = UDP_Open(10000);\n    assert(sd > -1);\n    while (1) {\n        struct sockaddr_in addr;\n        char message[**BUFFER_SIZE**];\n        int rc = UDP_Read(sd, &addr, message, **BUFFER_SIZE**);\n        if (rc > 0) {\n            char reply[**BUFFER_SIZE**];\n            sprintf(reply, \"goodbye world\");\n            rc = UDP_Write(sd, &addr, reply, **BUFFER_SIZE**);\n        }\n    }\n    return 0;\n}\nFigure 48.1:\n   **Example UDP Code (client.c, server.c)**\n\n\nonce, it is possible that the memory within the router cannot accommodate all of the packets. The only choice the router has at that point is to\n   **drop**\n   one or more of the packets. This same behavior occurs at end hosts as well; when you send a large number of messages to a single machine, the machine's resources can easily become overwhelmed, and thus packet loss again arises.\n\n\nThus, packet loss is fundamental in networking. The question thus becomes: how should we deal with it?"
        },
        {
          "name": "Unreliable Communication Layers",
          "content": "One simple way is this: we don't deal with it. Because some applications know how to deal with packet loss, it is sometimes useful to let them communicate with a basic unreliable messaging layer, an example of the\n   **end-to-end argument**\n   one often hears about (see the\n   **Aside**\n   at end of chapter). One excellent example of such an unreliable layer is found\n\n\nint UDP_Open(int port) {\n    int sd;\n    if ((sd = socket(AF_INET, SOCK_DGRAM, 0)) == -1)\n        return -1;\n    struct sockaddr_in myaddr;\n    bzero(&myaddr, sizeof(myaddr));\n    myaddr.sin_family      = AF_INET;\n    myaddr.sin_port        = htons(port);\n    myaddr.sin_addr.s_addr = INADDR_ANY;\n    if (bind(sd, (struct sockaddr *) &myaddr,\n             sizeof(myaddr)) == -1) {\n        close(sd);\n        return -1;\n    }\n    return sd;\n}\n\nint UDP_FillSockAddr(struct sockaddr_in *addr,\n                    char *hostname, int port) {\n    bzero(addr, sizeof(struct sockaddr_in));\n    addr->sin_family = AF_INET;    // host byte order\n    addr->sin_port   = htons(port); // network byte order\n    struct in_addr *in_addr;\n    struct hostent *host_entry;\n    if ((host_entry = gethostbyname(hostname)) == NULL)\n        return -1;\n    in_addr = (struct in_addr *) host_entry->h_addr;\n    addr->sin_addr = *in_addr;\n    return 0;\n}\n\nint UDP_Write(int sd, struct sockaddr_in *addr,\n              char *buffer, int n) {\n    int addr_len = sizeof(struct sockaddr_in);\n    return sendto(sd, buffer, n, 0, (struct sockaddr *)\n                  addr, addr_len);\n}\n\nint UDP_Read(int sd, struct sockaddr_in *addr,\n             char *buffer, int n) {\n    int len = sizeof(struct sockaddr_in);\n    return recvfrom(sd, buffer, n, 0, (struct sockaddr *)\n                    addr, (socklen_t *) &len);\n}\nFigure 48.2: A Simple UDP Library (udp.c)\n\n\n**TIP: USE CHECKSUMS FOR INTEGRITY**\nChecksums are a commonly-used method to detect corruption quickly and effectively in modern systems. A simple checksum is addition: just sum up the bytes of a chunk of data; of course, many other more sophisticated checksums have been created, including basic cyclic redundancy codes (CRCs), the Fletcher checksum, and many others [MK09].\n\n\nIn networking, checksums are used as follows. Before sending a message from one machine to another, compute a checksum over the bytes of the message. Then send both the message and the checksum to the destination. At the destination, the receiver computes a checksum over the incoming message as well; if this computed checksum matches the sent checksum, the receiver can feel some assurance that the data likely did not get corrupted during transmission.\n\n\nChecksums can be evaluated along a number of different axes. Effectiveness is one primary consideration: does a change in the data lead to a change in the checksum? The stronger the checksum, the harder it is for changes in the data to go unnoticed. Performance is the other important criterion: how costly is the checksum to compute? Unfortunately, effectiveness and performance are often at odds, meaning that checksums of high quality are often expensive to compute. Life, again, isn't perfect.\n\n\nin the\n   **UDP/IP**\n   networking stack available today on virtually all modern systems. To use UDP, a process uses the\n   **sockets**\n   API in order to create a\n   **communication endpoint**\n   ; processes on other machines (or on the same machine) send UDP\n   **datagrams**\n   to the original process (a datagram is a fixed-sized message up to some max size).\n\n\nFigures 48.1 and 48.2 show a simple client and server built on top of UDP/IP. The client can send a message to the server, which then responds with a reply. With this small amount of code, you have all you need to begin building distributed systems!\n\n\nUDP is a great example of an unreliable communication layer. If you use it, you will encounter situations where packets get lost (dropped) and thus do not reach their destination; the sender is never thus informed of the loss. However, that does not mean that UDP does not guard against any failures at all. For example, UDP includes a\n   **checksum**\n   to detect some forms of packet corruption.\n\n\nHowever, because many applications simply want to send data to a destination and not worry about packet loss, we need more. Specifically, we need reliable communication on top of an unreliable network."
        },
        {
          "name": "Reliable Communication Layers",
          "content": "To build a reliable communication layer, we need some new mechanisms and techniques to handle packet loss. Let us consider a simple\n\n\n\n\n![Diagram of Message Plus Acknowledgment protocol. A Sender sends a message to a Receiver. The Receiver sends an acknowledgment back to the Sender.](images/image_0134.jpeg)\n\n\ngraph LR\n    S[Sender] -- \"[send message]\" --> R[Receiver]\n    R -- \"[receive message]\" --> S\n    R -- \"[send ack]\" --> S\n    S -- \"[receive ack]\" --> R\n  \nDiagram of Message Plus Acknowledgment protocol. A Sender sends a message to a Receiver. The Receiver sends an acknowledgment back to the Sender.\n\n\n**Message Plus Acknowledgment**\n\n\n![Diagram of Message Plus Acknowledgment with a dropped request. The Sender sends a message to the Receiver, but the message is dropped (indicated by an X). The Sender then waits for an acknowledgment. When the timer expires, the Sender retries by sending the message again. The Receiver receives the second message and sends an acknowledgment back to the Sender.](images/image_0135.jpeg)\n\n\ngraph TD\n    S[Sender] -- \"[send message; keep copy; set timer]\" --> R[Receiver]\n    R --> X((X))\n    S -- \"... (waiting for ack)\" --> S\n    S -- \"... (timer goes off; set timer/retry)\" --> R\n    R -- \"[receive message]\" --> S\n    R -- \"[send ack]\" --> S\n    S -- \"[receive ack; delete copy/timer off]\" --> R\n  \nDiagram of Message Plus Acknowledgment with a dropped request. The Sender sends a message to the Receiver, but the message is dropped (indicated by an X). The Sender then waits for an acknowledgment. When the timer expires, the Sender retries by sending the message again. The Receiver receives the second message and sends an acknowledgment back to the Sender.\n\n\n**Message Plus Acknowledgment: Dropped Request**\nexample in which a client is sending a message to a server over an unreliable connection. The first question we must answer: how does the sender know that the receiver has actually received the message?\n\n\nThe technique that we will use is known as an\n   **acknowledgment**\n   , or\n   **ack**\n   for short. The idea is simple: the sender sends a message to the receiver; the receiver then sends a short message back to\n   *acknowledge*\n   its receipt. Figure 48.3 depicts the process.\n\n\nWhen the sender receives an acknowledgment of the message, it can then rest assured that the receiver did indeed receive the original message. However, what should the sender do if it does not receive an acknowledgment?\n\n\nTo handle this case, we need an additional mechanism, known as a\n   **timeout**\n   . When the sender sends a message, the sender now sets a timer to go off after some period of time. If, in that time, no acknowledgment has been received, the sender concludes that the message has been lost. The sender then simply performs a\n   **retry**\n   of the send, sending the same message again with hopes that this time, it will get through. For this approach to work, the sender must keep a copy of the message around, in case it needs to send it again. The combination of the timeout and the retry have led some to call the approach\n   **timeout/retry**\n   ; pretty clever crowd, those networking types, no? Figure 48.4 shows an example.\n\n\n\n\n![Sequence diagram illustrating Message Plus Acknowledgment with a dropped reply.](images/image_0136.jpeg)\n\n\nThe diagram shows the interaction between a Sender and a Receiver. \n  1. The Sender initiates with '[send message; keep copy; set timer]' and sends a message to the Receiver.\n  2. The Receiver performs '[receive message]' and then '[send ack]'.\n  3. The ack message is shown with a large 'X' over the arrow, indicating it is dropped.\n  4. The Sender is in a state of '... (waiting for ack) ...'.\n  5. The timer eventually 'goes off; set timer/retry', and the Sender sends a second message to the Receiver.\n  6. The Receiver again performs '[receive message]' and '[send ack]'.\n  7. The Receiver's ack message is received by the Sender, who then performs '[receive ack; delete copy/timer off]'.\n\n\nSequence diagram illustrating Message Plus Acknowledgment with a dropped reply.\n\n\nFigure 48.5:\n   **Message Plus Acknowledgment: Dropped Reply**\n\n\nUnfortunately, timeout/retry in this form is not quite enough. Figure 48.5 shows an example of packet loss which could lead to trouble. In this example, it is not the original message that gets lost, but the acknowledgment. From the perspective of the sender, the situation seems the same: no ack was received, and thus a timeout and retry are in order. But from the perspective of the receiver, it is quite different: now the same message has been received twice! While there may be cases where this is OK, in general it is not; imagine what would happen when you are downloading a file and extra packets are repeated inside the download. Thus, when we are aiming for a reliable message layer, we also usually want to guarantee that each message is received\n   **exactly once**\n   by the receiver.\n\n\nTo enable the receiver to detect duplicate message transmission, the sender has to identify each message in some unique way, and the receiver needs some way to track whether it has already seen each message before. When the receiver sees a duplicate transmission, it simply acks the message, but (critically) does\n   *not*\n   pass the message to the application that receives the data. Thus, the sender receives the ack but the message is not received twice, preserving the exactly-once semantics mentioned above.\n\n\nThere are myriad ways to detect duplicate messages. For example, the sender could generate a unique ID for each message; the receiver could track every ID it has ever seen. This approach could work, but it is prohibitively costly, requiring unbounded memory to track all IDs.\n\n\nA simpler approach, requiring little memory, solves this problem, and the mechanism is known as a\n   **sequence counter**\n   . With a sequence counter, the sender and receiver agree upon a start value (e.g., 1) for a counter that each side will maintain. Whenever a message is sent, the current value of the counter is sent along with the message; this counter value (\n   \n    N\n   \n   ) serves as an ID for the message. After the message is sent, the sender then increments the value (to\n   \n    N + 1\n   \n   ).\n\n\n**TIP: BE CAREFUL SETTING THE TIMEOUT VALUE**\nAs you can probably guess from the discussion, setting the timeout value correctly is an important aspect of using timeouts to retry message sends. If the timeout is too small, the sender will re-send messages needlessly, thus wasting CPU time on the sender and network resources. If the timeout is too large, the sender waits too long to re-send and thus perceived performance at the sender is reduced. The “right” value, from the perspective of a single client and server, is thus to wait just long enough to detect packet loss but no longer.\n\n\nHowever, there are often more than just a single client and server in a distributed system, as we will see in future chapters. In a scenario with many clients sending to a single server, packet loss at the server may be an indicator that the server is overloaded. If true, clients might retry in a different adaptive manner; for example, after the first timeout, a client might increase its timeout value to a higher amount, perhaps twice as high as the original value. Such an\n   **exponential back-off**\n   scheme, pioneered in the early Aloha network and adopted in early Ethernet [A70], avoids situations where resources are being overloaded by an excess of re-sends. Robust systems strive to avoid overload of this nature.\n\n\nThe receiver uses its counter value as the expected value for the ID of the incoming message from that sender. If the ID of a received message (\n   \n    N\n   \n   ) matches the receiver’s counter (also\n   \n    N\n   \n   ), it acks the message and passes it up to the application; in this case, the receiver concludes this is the first time this message has been received. The receiver then increments its counter (to\n   \n    N + 1\n   \n   ), and waits for the next message.\n\n\nIf the ack is lost, the sender will timeout and re-send message\n   \n    N\n   \n   . This time, the receiver’s counter is higher (\n   \n    N + 1\n   \n   ), and thus the receiver knows it has already received this message. Thus it acks the message but does\n   *not*\n   pass it up to the application. In this simple manner, sequence counters can be used to avoid duplicates.\n\n\nThe most commonly used reliable communication layer is known as\n   **TCP/IP**\n   , or just\n   **TCP**\n   for short. TCP has a great deal more sophistication than we describe above, including machinery to handle congestion in the network [VJ88], multiple outstanding requests, and hundreds of other small tweaks and optimizations. Read more about it if you’re curious; better yet, take a networking course and learn that material well."
        },
        {
          "name": "Communication Abstractions",
          "content": "Given a basic messaging layer, we now approach the next question in this chapter: what abstraction of communication should we use when building a distributed system?\n\n\nThe systems community developed a number of approaches over the years. One body of work took OS abstractions and extended them to\n\n\noperate in a distributed environment. For example,\n   **distributed shared memory (DSM)**\n   systems enable processes on different machines to share a large, virtual address space [LH89]. This abstraction turns a distributed computation into something that looks like a multi-threaded application; the only difference is that these threads run on different machines instead of different processors within the same machine.\n\n\nThe way most DSM systems work is through the virtual memory system of the OS. When a page is accessed on one machine, two things can happen. In the first (best) case, the page is already local on the machine, and thus the data is fetched quickly. In the second case, the page is currently on some other machine. A page fault occurs, and the page fault handler sends a message to some other machine to fetch the page, install it in the page table of the requesting process, and continue execution.\n\n\nThis approach is not widely in use today for a number of reasons. The largest problem for DSM is how it handles failure. Imagine, for example, if a machine fails; what happens to the pages on that machine? What if the data structures of the distributed computation are spread across the entire address space? In this case, parts of these data structures would suddenly become unavailable. Dealing with failure when parts of your address space go missing is hard; imagine a linked list where a “next” pointer points into a portion of the address space that is gone. Yikes!\n\n\nA further problem is performance. One usually assumes, when writing code, that access to memory is cheap. In DSM systems, some accesses are inexpensive, but others cause page faults and expensive fetches from remote machines. Thus, programmers of such DSM systems had to be very careful to organize computations such that almost no communication occurred at all, defeating much of the point of such an approach. Though much research was performed in this space, there was little practical impact; nobody builds reliable distributed systems using DSM today."
        },
        {
          "name": "Remote Procedure Call (RPC)",
          "content": "While OS abstractions turned out to be a poor choice for building distributed systems, programming language (PL) abstractions make much more sense. The most dominant abstraction is based on the idea of a\n   **remote procedure call**\n   , or\n   **RPC**\n   for short [BN84]\n   \n    1\n   \n   .\n\n\nRemote procedure call packages all have a simple goal: to make the process of executing code on a remote machine as simple and straightforward as calling a local function. Thus, to a client, a procedure call is made, and some time later, the results are returned. The server simply defines some routines that it wishes to export. The rest of the magic is handled by the RPC system, which in general has two pieces: a\n   **stub generator**\n   (sometimes called a\n   **protocol compiler**\n   ), and the\n   **run-time library**\n   . We’ll now take a look at each of these pieces in more detail.\n\n\n1\n   \n   In modern programming languages, we might instead say\n   **remote method invocation (RMI)**\n   , but who likes these languages anyhow, with all of their fancy objects?\n\n\n\n\n**Stub Generator**\n\n\nThe stub generator’s job is simple: to remove some of the pain of packing function arguments and results into messages by automating it. Numerous benefits arise: one avoids, by design, the simple mistakes that occur in writing such code by hand; further, a stub compiler can perhaps optimize such code and thus improve performance.\n\n\nThe input to such a compiler is simply the set of calls a server wishes to export to clients. Conceptually, it could be something as simple as this:\n\n\ninterface {\n    int func1(int arg1);\n    int func2(int arg1, int arg2);\n};\nThe stub generator takes an interface like this and generates a few different pieces of code. For the client, a\n   **client stub**\n   is generated, which contains each of the functions specified in the interface; a client program wishing to use this RPC service would link with this client stub and call into it in order to make RPCs.\n\n\nInternally, each of these functions in the client stub do all of the work needed to perform the remote procedure call. To the client, the code just appears as a function call (e.g., the client calls\n   \n    func1(x)\n   \n   ); internally, the code in the client stub for\n   \n    func1()\n   \n   does this:\n\n\n  * •\n    **Create a message buffer.**\n    A message buffer is usually just a contiguous array of bytes of some size.\n  * •\n    **Pack the needed information into the message buffer.**\n    This information includes some kind of identifier for the function to be called, as well as all of the arguments that the function needs (e.g., in our example above, one integer for\n    \n     func1\n    \n    ). The process of putting all of this information into a single contiguous buffer is sometimes referred to as the\n    **marshaling**\n    of arguments or the\n    **serialization**\n    of the message.\n  * •\n    **Send the message to the destination RPC server.**\n    The communication with the RPC server, and all of the details required to make it operate correctly, are handled by the RPC run-time library, described further below.\n  * •\n    **Wait for the reply.**\n    Because function calls are usually\n    **synchronous**\n    , the call will wait for its completion.\n  * •\n    **Unpack return code and other arguments.**\n    If the function just returns a single return code, this process is straightforward; however, more complex functions might return more complex results (e.g., a list), and thus the stub might need to unpack those as well. This step is also known as\n    **unmarshaling**\n    or\n    **deserialization**\n    .\n  * •\n    **Return to the caller.**\n    Finally, just return from the client stub back into the client code.\n\n\nFor the server, code is also generated. The steps taken on the server are as follows:\n\n\n  * •\n    **Unpack the message.**\n    This step, called\n    **unmarshaling**\n    or\n    **deserialization**\n    , takes the information out of the incoming message. The function identifier and arguments are extracted.\n  * •\n    **Call into the actual function.**\n    Finally! We have reached the point where the remote function is actually executed. The RPC runtime calls into the function specified by the ID and passes in the desired arguments.\n  * •\n    **Package the results.**\n    The return argument(s) are marshaled back into a single reply buffer.\n  * •\n    **Send the reply.**\n    The reply is finally sent to the caller.\n\n\nThere are a few other important issues to consider in a stub compiler. The first is complex arguments, i.e., how does one package and send a complex data structure? For example, when one calls the\n   \n    write()\n   \n   system call, one passes in three arguments: an integer file descriptor, a pointer to a buffer, and a size indicating how many bytes (starting at the pointer) are to be written. If an RPC package is passed a pointer, it needs to be able to figure out how to interpret that pointer, and perform the correct action. Usually this is accomplished through either well-known types (e.g., a\n   \n    buffer_t\n   \n   that is used to pass chunks of data given a size, which the RPC compiler understands), or by annotating the data structures with more information, enabling the compiler to know which bytes need to be serialized.\n\n\nAnother important issue is the organization of the server with regards to concurrency. A simple server just waits for requests in a simple loop, and handles each request one at a time. However, as you might have guessed, this can be grossly inefficient; if one RPC call blocks (e.g., on I/O), server resources are wasted. Thus, most servers are constructed in some sort of concurrent fashion. A common organization is a\n   **thread pool**\n   . In this organization, a finite set of threads are created when the server starts; when a message arrives, it is dispatched to one of these worker threads, which then does the work of the RPC call, eventually replying; during this time, a main thread keeps receiving other requests, and perhaps dispatching them to other workers. Such an organization enables concurrent execution within the server, thus increasing its utilization; the standard costs arise as well, mostly in programming complexity, as the RPC calls may now need to use locks and other synchronization primitives in order to ensure their correct operation.\n\n\n\n\n**Run-Time Library**\n\n\nThe run-time library handles much of the heavy lifting in an RPC system; most performance and reliability issues are handled herein. We'll now discuss some of the major challenges in building such a run-time layer.\n\n\nOne of the first challenges we must overcome is how to locate a remote service. This problem, of\n   **naming**\n   , is a common one in distributed systems, and in some sense goes beyond the scope of our current discussion. The simplest of approaches build on existing naming systems, e.g., hostnames and port numbers provided by current internet protocols. In such a system, the client must know the hostname or IP address of the machine running the desired RPC service, as well as the port number it is using (a port number is just a way of identifying a particular communication activity taking place on a machine, allowing multiple communication channels at once). The protocol suite must then provide a mechanism to route packets to a particular address from any other machine in the system. For a good discussion of naming, you'll have to look elsewhere, e.g., read about DNS and name resolution on the Internet, or better yet just read the excellent chapter in Saltzer and Kaashoek's book [SK09].\n\n\nOnce a client knows which server it should talk to for a particular remote service, the next question is which transport-level protocol should RPC be built upon. Specifically, should the RPC system use a reliable protocol such as TCP/IP, or be built upon an unreliable communication layer such as UDP/IP?\n\n\nNaively the choice would seem easy: clearly we would like for a request to be reliably delivered to the remote server, and clearly we would like to reliably receive a reply. Thus we should choose the reliable transport protocol such as TCP, right?\n\n\nUnfortunately, building RPC on top of a reliable communication layer can lead to a major inefficiency in performance. Recall from the discussion above how reliable communication layers work: with acknowledgments plus timeout/retry. Thus, when the client sends an RPC request to the server, the server responds with an acknowledgment so that the caller knows the request was received. Similarly, when the server sends the reply to the client, the client acks it so that the server knows it was received. By building a request/response protocol (such as RPC) on top of a reliable communication layer, two “extra” messages are sent.\n\n\nFor this reason, many RPC packages are built on top of unreliable communication layers, such as UDP. Doing so enables a more efficient RPC layer, but does add the responsibility of providing reliability to the RPC system. The RPC layer achieves the desired level of responsibility by using timeout/retry and acknowledgments much like we described above. By using some form of sequence numbering, the communication layer can guarantee that each RPC takes place exactly once (in the case of no failure), or at most once (in the case where failure arises).\n\n\n\n\n**Other Issues**\n\n\nThere are some other issues an RPC run-time must handle as well. For example, what happens when a remote call takes a long time to complete? Given our timeout machinery, a long-running remote call might appear as a failure to a client, thus triggering a retry, and thus the need for some care here. One solution is to use an explicit acknowledgment\n\n\n\n\n**Aside: THE END-TO-END ARGUMENT**\n\n\nThe\n   **end-to-end argument**\n   makes the case that the highest level in a system, i.e., usually the application at “the end”, is ultimately the only locale within a layered system where certain functionality can truly be implemented. In their landmark paper [SRC84], Saltzer et al. argue this through an excellent example: reliable file transfer between two machines. If you want to transfer a file from machine\n   \n    A\n   \n   to machine\n   \n    B\n   \n   , and make sure that the bytes that end up on\n   \n    B\n   \n   are exactly the same as those that began on\n   \n    A\n   \n   , you must have an “end-to-end” check of this; lower-level reliable machinery, e.g., in the network or disk, provides no such guarantee.\n\n\nThe contrast is an approach which tries to solve the reliable-file-transfer problem by adding reliability to lower layers of the system. For example, say we build a reliable communication protocol and use it to build our reliable file transfer. The communication protocol guarantees that every byte sent by a sender will be received in order by the receiver, say using timeout/ retry, acknowledgments, and sequence numbers. Unfortunately, using such a protocol does not a reliable file transfer make; imagine the bytes getting corrupted in sender memory before the communication even takes place, or something bad happening when the receiver writes the data to disk. In those cases, even though the bytes were delivered reliably across the network, our file transfer was ultimately not reliable. To build a reliable file transfer, one must include end-to-end checks of reliability, e.g., after the entire transfer is complete, read back the file on the receiver disk, compute a checksum, and compare that checksum to that of the file on the sender.\n\n\nThe corollary to this maxim is that sometimes having lower layers provide extra functionality can indeed improve system performance or otherwise optimize a system. Thus, you should not rule out having such machinery at a lower-level in a system; rather, you should carefully consider the utility of such machinery, given its eventual usage in an overall system or application.\n\n\n(from the receiver to sender) when the reply isn’t immediately generated; this lets the client know the server received the request. Then, after some time has passed, the client can periodically ask whether the server is still working on the request; if the server keeps saying “yes”, the client should be happy and continue to wait (after all, sometimes a procedure call can take a long time to finish executing).\n\n\nThe run-time must also handle procedure calls with large arguments, larger than what can fit into a single packet. Some lower-level network protocols provide such sender-side\n   **fragmentation**\n   (of larger packets into a set of smaller ones) and receiver-side\n   **reassembling**\n   (of smaller parts into one larger logical whole); if not, the RPC run-time may have to implement such functionality itself. See Birrell and Nelson’s paper for details [BN84].\n\n\nOne issue that many systems handle is that of\n   **byte ordering**\n   . As you may know, some machines store values in what is known as\n   **big endian**\n   ordering, whereas others use\n   **little endian**\n   ordering. Big endian stores bytes (say, of an integer) from most significant to least significant bits, much like Arabic numerals; little endian does the opposite. Both are equally valid ways of storing numeric information; the question here is how to communicate between machines of\n   *different*\n   endianness.\n\n\nRPC packages often handle this by providing a well-defined endianness within their message formats. In Sun's RPC package, the\n   **XDR (eXternal Data Representation)**\n   layer provides this functionality. If the machine sending or receiving a message matches the endianness of XDR, messages are just sent and received as expected. If, however, the machine communicating has a different endianness, each piece of information in the message must be converted. Thus, the difference in endianness can have a small performance cost.\n\n\nA final issue is whether to expose the asynchronous nature of communication to clients, thus enabling some performance optimizations. Specifically, typical RPCs are made\n   **synchronously**\n   , i.e., when a client issues the procedure call, it must wait for the procedure call to return before continuing. Because this wait can be long, and because the client may have other work it could be doing, some RPC packages enable you to invoke an RPC\n   **asynchronously**\n   . When an asynchronous RPC is issued, the RPC package sends the request and returns immediately; the client is then free to do other work, such as call other RPCs or other useful computation. The client at some point will want to see the results of the asynchronous RPC; it thus calls back into the RPC layer, telling it to wait for outstanding RPCs to complete, at which point return arguments can be accessed."
        }
      ]
    },
    {
      "name": "Sun’s Network File System (NFS)",
      "sections": [
        {
          "name": "A Basic Distributed File System",
          "content": "We now will study the architecture of a simplified distributed file system. A simple client/server distributed file system has more components than the file systems we have studied so far. On the client side, there are client applications which access files and directories through the\n   **client-side file system**\n   . A client application issues\n   **system calls**\n   to the client-side file system (such as\n   \n    open()\n   \n   ,\n   \n    read()\n   \n   ,\n   \n    write()\n   \n   ,\n   \n    close()\n   \n   ,\n   \n    mkdir()\n   \n   , etc.) in order to access files which are stored on the server. Thus, to client applications, the file system does not appear to be any different than a local (disk-based) file system, except perhaps for performance; in this way, distributed file systems provide\n   **transparent**\n   access to files, an obvious goal; after all, who would want to use a file system that required a different set of APIs or otherwise was a pain to use?\n\n\nThe role of the client-side file system is to execute the actions needed to service those system calls. For example, if the client issues a\n   \n    read()\n   \n   request, the client-side file system may send a message to the\n   **server-side file system**\n   (or, as it is commonly called, the\n   **file server**\n   ) to read a particular block; the file server will then read the block from disk (or its own in-memory cache), and send a message back to the client with the requested data. The client-side file system will then copy the data into the user buffer supplied to the\n   \n    read()\n   \n   system call and thus the request will complete. Note that a subsequent\n   \n    read()\n   \n   of the same block on the client may be\n   **cached**\n   in client memory or on the client's disk even; in the best such case, no network traffic need be generated.\n\n\n\n\n![Diagram of Distributed File System Architecture showing Client Application, Client-side File System, Networking Layer, File Server, and Disks.](images/image_0137.jpeg)\n\n\nThe diagram illustrates the architecture of a distributed file system. It shows two main components: a 'Client Application' on the left and a 'File Server' on the right. The 'Client Application' is connected to the 'Client-side File System', which is part of the 'Networking Layer'. The 'File Server' is also part of the 'Networking Layer'. A double-headed arrow indicates communication between the 'Client-side File System' and the 'File Server'. The 'File Server' is connected to 'Disks' via a double-headed arrow.\n\n\nDiagram of Distributed File System Architecture showing Client Application, Client-side File System, Networking Layer, File Server, and Disks.\n\n\nFigure 49.2: Distributed File System Architecture\n\n\nFrom this simple overview, you should get a sense that there are two important pieces of software in a client/server distributed file system: the client-side file system and the file server. Together their behavior determines the behavior of the distributed file system. Now it's time to study one particular system: Sun's Network File System (NFS).\n\n\n**ASIDE: WHY SERVERS CRASH**\nBefore getting into the details of the NFSv2 protocol, you might be wondering: why do servers crash? Well, as you might guess, there are plenty of reasons. Servers may simply suffer from a\n   **power outage**\n   (temporarily); only when power is restored can the machines be restarted. Servers are often comprised of hundreds of thousands or even millions of lines of code; thus, they have\n   **bugs**\n   (even good software has a few bugs per hundred or thousand lines of code), and thus they eventually will trigger a bug that will cause them to crash. They also have memory leaks; even a small memory leak will cause a system to run out of memory and crash. And, finally, in distributed systems, there is a network between the client and the server; if the network acts strangely (for example, if it becomes\n   **partitioned**\n   and clients and servers are working but cannot communicate), it may appear as if a remote machine has crashed, but in reality it is just not currently reachable through the network."
        },
        {
          "name": "On To NFS",
          "content": "One of the earliest and quite successful distributed systems was developed by Sun Microsystems, and is known as the Sun Network File System (or NFS) [S86]. In defining NFS, Sun took an unusual approach: instead of building a proprietary and closed system, Sun instead developed an\n   **open protocol**\n   which simply specified the exact message formats that clients and servers would use to communicate. Different groups could develop their own NFS servers and thus compete in an NFS marketplace while preserving interoperability. It worked: today there are many companies that sell NFS servers (including Oracle/Sun, NetApp [HLM94], EMC, IBM, and others), and the widespread success of NFS is likely attributed to this “open market” approach."
        },
        {
          "name": "Focus: Simple And Fast Server Crash Recovery",
          "content": "In this chapter, we will discuss the classic NFS protocol (version 2, a.k.a. NFSv2), which was the standard for many years; small changes were made in moving to NFSv3, and larger-scale protocol changes were made in moving to NFSv4. However, NFSv2 is both wonderful and frustrating and thus serves as our focus.\n\n\nIn NFSv2, the main goal in the design of the protocol was\n   *simple and fast server crash recovery*\n   . In a multiple-client, single-server environment, this goal makes a great deal of sense; any minute that the server is down (or unavailable) makes\n   *all*\n   the client machines (and their users) unhappy and unproductive. Thus, as the server goes, so goes the entire system.\n\n\n\n\n**49.4 Key To Fast Crash Recovery: Statelessness**\n\n\nThis simple goal is realized in NFSv2 by designing what we refer to as a\n   **stateless**\n   protocol. The server, by design, does not keep track of anything about what is happening at each client. For example, the server does not know which clients are caching which blocks, or which files are currently open at each client, or the current file pointer position for a file, etc. Simply put, the server does not track anything about what clients are doing; rather, the protocol is designed to deliver in each protocol request\n   *all the information*\n   that is needed in order to complete the request. If it doesn't now, this stateless approach will make more sense as we discuss the protocol in more detail below.\n\n\nFor an example of a\n   **stateful**\n   (not stateless) protocol, consider the\n   \n    open()\n   \n   system call. Given a pathname,\n   \n    open()\n   \n   returns a file descriptor (an integer). This descriptor is used on subsequent\n   \n    read()\n   \n   or\n   \n    write()\n   \n   requests to access various file blocks, as in this application code (note that proper error checking of the system calls is omitted for space reasons):\n\n\nchar buffer[MAX];\nint fd = open(\"foo\", O_RDONLY); // get descriptor \"fd\"\nread(fd, buffer, MAX);          // read MAX from foo via \"fd\"\nread(fd, buffer, MAX);          // read MAX again\n...\nread(fd, buffer, MAX);          // read MAX again\nclose(fd);                      // close file\nFigure 49.3: Client Code: Reading From A File\n\n\nNow imagine that the client-side file system opens the file by sending a protocol message to the server saying \"open the file 'foo' and give me back a descriptor\". The file server then opens the file locally on its side and sends the descriptor back to the client. On subsequent reads, the client application uses that descriptor to call the\n   \n    read()\n   \n   system call; the client-side file system then passes the descriptor in a message to the file server, saying \"read some bytes from the file that is referred to by the descriptor I am passing you here\".\n\n\nIn this example, the file descriptor is a piece of\n   **shared state**\n   between the client and the server (Ousterhout calls this\n   **distributed state**\n   [O91]). Shared state, as we hinted above, complicates crash recovery. Imagine the server crashes after the first read completes, but before the client has issued the second one. After the server is up and running again, the client then issues the second read. Unfortunately, the server has no idea to which file\n   \n    fd\n   \n   is referring; that information was ephemeral (i.e., in memory) and thus lost when the server crashed. To handle this situation, the client and server would have to engage in some kind of\n   **recovery protocol**\n   , where the client would make sure to keep enough information around in its memory to be able to tell the server what it needs to know (in this case, that file descriptor\n   \n    fd\n   \n   refers to file\n   \n    foo\n   \n   ).\n\n\nIt gets even worse when you consider the fact that a stateful server has to deal with client crashes. Imagine, for example, a client that opens a file and then crashes. The\n   \n    open()\n   \n   uses up a file descriptor on the server; how can the server know it is OK to close a given file? In normal operation, a client would eventually call\n   \n    close()\n   \n   and thus inform the server that the file should be closed. However, when a client crashes, the server never receives a\n   \n    close()\n   \n   , and thus has to notice the client has crashed in order to close the file.\n\n\nFor these reasons, the designers of NFS decided to pursue a stateless approach: each client operation contains all the information needed to complete the request. No fancy crash recovery is needed; the server just starts running again, and a client, at worst, might have to retry a request."
        },
        {
          "name": "The NFSv2 Protocol",
          "content": "We thus arrive at the NFSv2 protocol definition. Our problem statement is simple:\n\n\n\n\n**THE CRUX: HOW TO DEFINE A STATELESS FILE PROTOCOL**\n\n\nHow can we define the network protocol to enable stateless operation? Clearly, stateful calls like\n   \n    open()\n   \n   can't be a part of the discussion (as it would require the server to track open files); however, the client application will want to call\n   \n    open()\n   \n   ,\n   \n    read()\n   \n   ,\n   \n    write()\n   \n   ,\n   \n    close()\n   \n   and other standard API calls to access files and directories. Thus, as a refined question, how do we define the protocol to both be stateless\n   *and*\n   support the POSIX file system API?\n\n\nOne key to understanding the design of the NFS protocol is understanding the\n   **file handle**\n   . File handles are used to uniquely describe the file or directory a particular operation is going to operate upon; thus, many of the protocol requests include a file handle.\n\n\nYou can think of a file handle as having three important components: a\n   *volume identifier*\n   , an\n   *inode number*\n   , and a\n   *generation number*\n   ; together, these three items comprise a unique identifier for a file or directory that a client wishes to access. The volume identifier informs the server which file system the request refers to (an NFS server can export more than one file system); the inode number tells the server which file within that partition the request is accessing. Finally, the generation number is needed when reusing an inode number; by incrementing it whenever an inode number is reused, the server ensures that a client with an old file handle can't accidentally access the newly-allocated file.\n\n\nHere is a summary of some of the important pieces of the protocol; the full protocol is available elsewhere (see Callaghan's book for an excellent and detailed overview of NFS [C00]).\n\n\n\nNFSPROC_GETATTR | file handle\n     \n     returns: attributes\nNFSPROC_SETATTR | file handle, attributes\n     \n     returns: attributes\nNFSPROC_LOOKUP | directory file handle, name of file/dir to look up\n     \n     returns: file handle, attributes\nNFSPROC_READ | file handle, offset, count\n     \n     data, attributes\nNFSPROC_WRITE | file handle, offset, count, data\n     \n     attributes\nNFSPROC_CREATE | directory file handle, name of file, attributes\n     \n     file handle, attributes\nNFSPROC_REMOVE | directory file handle, name of file to be removed\n     \n     —\nNFSPROC_MKDIR | directory file handle, name of directory, attributes\n     \n     file handle, attributes\nNFSPROC_RMDIR | directory file handle, name of directory to be removed\n     \n     —\nNFSPROC_READDIR | directory handle, count of bytes to read, cookie\n     \n     returns: directory entries, cookie (to get more entries)\n\n\nFigure 49.4:\n   **The NFS Protocol: Examples**\n\n\nWe briefly highlight the important components of the protocol. First, the LOOKUP protocol message is used to obtain a file handle, which is then subsequently used to access file data. The client passes a directory file handle and name of a file to look up, and the handle to that file (or directory) plus its attributes are passed back to the client from the server.\n\n\nFor example, assume the client already has a directory file handle for the root directory of a file system (/) (indeed, this would be obtained through the NFS\n   **mount protocol**\n   , which is how clients and servers first are connected together; we do not discuss the mount protocol here for sake of brevity). If an application running on the client opens the file\n   \n    /foo.txt\n   \n   , the client-side file system sends a lookup request to the server, passing it the root file handle and the name\n   \n    foo.txt\n   \n   ; if successful, the file handle (and attributes) for\n   \n    foo.txt\n   \n   will be returned.\n\n\nIn case you are wondering, attributes are just the metadata that the file system tracks about each file, including fields such as file creation time, last modification time, size, ownership and permissions information, and so forth, i.e., the same type of information that you would get back if you called\n   \n    stat()\n   \n   on a file.\n\n\nOnce a file handle is available, the client can issue READ and WRITE protocol messages on a file to read or write the file, respectively. The READ protocol message requires the protocol to pass along the file handle of the file along with the offset within the file and number of bytes to read. The server then will be able to issue the read (after all, the handle tells the server which volume and which inode to read from, and the offset and count tells it which bytes of the file to read) and return the data (and up-\n\n\nto-date attributes) to the client (or an error if there was a failure). WRITE is handled similarly, except the data is passed from the client to the server, and just a success code (and up-to-date attributes) is returned.\n\n\nOne last interesting protocol message is the GETATTR request; given a file handle, it simply fetches the attributes for that file, including the last modified time of the file. We will see why this protocol request is important in NFSv2 below when we discuss caching (can you guess why?)."
        },
        {
          "name": "From Protocol To Distributed File System",
          "content": "Hopefully you are now getting some sense of how this protocol is turned into a file system across the client-side file system and the file server. The client-side file system tracks open files, and generally translates application requests into the relevant set of protocol messages. The server simply responds to protocol messages, each of which contains all of the information needed to complete the request.\n\n\nFor example, let us consider a simple application which reads a file. In the diagram (Figure 49.5), we show what system calls the application makes, and what the client-side file system and file server do in responding to such calls.\n\n\nA few comments about the figure. First, notice how the client tracks all relevant\n   **state**\n   for the file access, including the mapping of the integer file descriptor to an NFS file handle as well as the current file pointer. This enables the client to turn each read request (which you may have noticed do\n   *not*\n   specify the offset to read from explicitly) into a properly-formatted read protocol message which tells the server exactly which bytes from the file to read. Upon a successful read, the client updates the current file position; subsequent reads are issued with the same file handle but a different offset.\n\n\nSecond, you may notice where server interactions occur. When the file is opened for the first time, the client-side file system sends a LOOKUP request message. Indeed, if a long pathname must be traversed (e.g.,\n   \n    /home/remzi/foo.txt\n   \n   ), the client would send three LOOKUPs: one to look up\n   \n    home\n   \n   in the directory\n   \n    /\n   \n   , one to look up\n   \n    remzi\n   \n   in\n   \n    home\n   \n   , and finally one to look up\n   \n    foo.txt\n   \n   in\n   \n    remzi\n   \n   .\n\n\nThird, you may notice how each server request has all the information needed to complete the request in its entirety. This design point is critical to be able to gracefully recover from server failure, as we will now discuss in more detail; it ensures that the server does not need state to be able to respond to the request.\n\n\n\nClient | Server\nfd = open(\"/foo\", ...);\nSend LOOKUP (rootdir FH, \"foo\") | Receive LOOKUP request\nlook for \"foo\" in root dir\nreturn foo's FH + attributes\nReceive LOOKUP reply\nallocate file desc in open file table\nstore foo's FH in table\nstore current file position (0)\nreturn file descriptor to application | \nread(fd, buffer, MAX);\nIndex into open file table with fd\nget NFS file handle (FH)\nuse current file position as offset\nSend READ (FH, offset=0, count=MAX) | Receive READ request\nuse FH to get volume/inode num\nread inode from disk (or cache)\ncompute block location (using offset)\nread data from disk (or cache)\nreturn data to client\nReceive READ reply\nupdate file position (+bytes read)\nset current file position = MAX\nreturn data/error code to app | \nread(fd, buffer, MAX);\nSame except offset=MAX and set current file position = 2*MAX | \nread(fd, buffer, MAX);\nSame except offset=2*MAX and set current file position = 3*MAX | \nclose(fd);\nJust need to clean up local structures\nFree descriptor \"fd\" in open file table\n(No need to talk to server) | \n\n\nFigure 49.5:\n   **Reading A File: Client-side And File Server Actions**\n\n\n\n\n**TIP: IDEMPOTENCY IS POWERFUL**\n\n\n**Idempotency**\n   is a useful property when building reliable systems. When an operation can be issued more than once, it is much easier to handle failure of the operation; you can just retry it. If an operation is\n   *not*\n   idempotent, life becomes more difficult."
        },
        {
          "name": "Handling Server FailureWith Idempotent Operations",
          "content": "When a client sends a message to the server, it sometimes does not receive a reply. There are many possible reasons for this failure to respond. In some cases, the message may be dropped by the network; networks do lose messages, and thus either the request or the reply could be lost and thus the client would never receive a response.\n\n\nIt is also possible that the server has crashed, and thus is not currently responding to messages. After a bit, the server will be rebooted and start running again, but in the meanwhile all requests have been lost. In all of these cases, clients are left with a question: what should they do when the server does not reply in a timely manner?\n\n\nIn NFSv2, a client handles all of these failures in a single, uniform, and elegant way: it simply\n   *retries*\n   the request. Specifically, after sending the request, the client sets a timer to go off after a specified time period. If a reply is received before the timer goes off, the timer is canceled and all is well. If, however, the timer goes off\n   *before*\n   any reply is received, the client assumes the request has not been processed and resends it. If the server replies, all is well and the client has neatly handled the problem.\n\n\nThe ability of the client to simply retry the request (regardless of what caused the failure) is due to an important property of most NFS requests: they are\n   **idempotent**\n   . An operation is called idempotent when the effect of performing the operation multiple times is equivalent to the effect of performing the operation a single time. For example, if you store a value to a memory location three times, it is the same as doing so once; thus \"store value to memory\" is an idempotent operation. If, however, you increment a counter three times, it results in a different amount than doing so just once; thus, \"increment counter\" is not idempotent. More generally, any operation that just reads data is obviously idempotent; an operation that updates data must be more carefully considered to determine if it has this property.\n\n\nThe heart of the design of crash recovery in NFS is the idempotency of most common operations. LOOKUP and READ requests are trivially idempotent, as they only read information from the file server and do not update it. More interestingly, WRITE requests are also idempotent. If, for example, a WRITE fails, the client can simply retry it. The WRITE message contains the data, the count, and (importantly) the exact offset to write the data to. Thus, it can be repeated with the knowledge that the outcome of multiple writes is the same as the outcome of a single one.\n\n\n\n\n![](images/image_0138.jpeg)\n\n\n**Case 1: Request Lost**\n\n\nClient [send request] → X Server (no msg)\n\n\n-----\n\n\n**Case 2: Server Down**\n\n\nClient [send request] → X Server (down)\n\n\n-----\n\n\n**Case 3: Reply lost on way back from Server**\n\n\nClient [send request] → Server [recv request] [handle request] [send reply] X\n\n\nFigure 49.6:\n   **The Three Types Of Loss**\n\n\nIn this way, the client can handle all timeouts in a unified way. If a WRITE request was simply lost (Case 1 above), the client will retry it, the server will perform the write, and all will be well. The same will happen if the server happened to be down while the request was sent, but back up and running when the second request is sent, and again all works as desired (Case 2). Finally, the server may in fact receive the WRITE request, issue the write to its disk, and send a reply. This reply may get lost (Case 3), again causing the client to re-send the request. When the server receives the request again, it will simply do the exact same thing: write the data to disk and reply that it has done so. If the client this time receives the reply, all is again well, and thus the client has handled both message loss and server failure in a uniform manner. Neat!\n\n\nA small aside: some operations are hard to make idempotent. For example, when you try to make a directory that already exists, you are informed that the mkdir request has failed. Thus, in NFS, if the file server receives a MKDIR protocol message and executes it successfully but the reply is lost, the client may repeat it and encounter that failure when in fact the operation at first succeeded and then only failed on the retry. Thus, life is not perfect.\n\n\n**TIP: PERFECT IS THE ENEMY OF THE GOOD (VOLTAIRE'S LAW)**\nEven when you design a beautiful system, sometimes all the corner cases don't work out exactly as you might like. Take the\n   \n    mkdir\n   \n   example above; one could redesign\n   \n    mkdir\n   \n   to have different semantics, thus making it idempotent (think about how you might do so); however, why bother? The NFS design philosophy covers most of the important cases, and overall makes the system design clean and simple with regards to failure. Thus, accepting that life isn't perfect and still building the system is a sign of good engineering. Apparently, this wisdom is attributed to Voltaire, for saying \"... a wise Italian says that the best is the enemy of the good\" [V72], and thus we call it\n   **Voltaire's Law**\n   ."
        },
        {
          "name": "Improving Performance: Client-side Caching",
          "content": "Distributed file systems are good for a number of reasons, but sending all read and write requests across the network can lead to a big performance problem: the network generally isn't that fast, especially as compared to local memory or disk. Thus, another problem: how can we improve the performance of a distributed file system?\n\n\nThe answer, as you might guess from reading the big bold words in the sub-heading above, is client-side\n   **caching**\n   . The NFS client-side file system caches file data (and metadata) that it has read from the server in client memory. Thus, while the first access is expensive (i.e., it requires network communication), subsequent accesses are serviced quite quickly out of client memory.\n\n\nThe cache also serves as a temporary buffer for writes. When a client application first writes to a file, the client buffers the data in client memory (in the same cache as the data it read from the file server) before writing the data out to the server. Such\n   **write buffering**\n   is useful because it decouples application\n   \n    write()\n   \n   latency from actual write performance, i.e., the application's call to\n   \n    write()\n   \n   succeeds immediately (and just puts the data in the client-side file system's cache); only later does the data get written out to the file server.\n\n\nThus, NFS clients cache data and performance is usually great and we are done, right? Unfortunately, not quite. Adding caching into any sort of system with multiple client caches introduces a big and interesting challenge which we will refer to as the\n   **cache consistency problem**\n   ."
        },
        {
          "name": "The Cache Consistency Problem",
          "content": "The cache consistency problem is best illustrated with three clients and a single server. Imagine client C1 reads a file F, and keeps a copy of the file in its local cache. Now imagine a different client, C2, overwrites the file F, thus changing its contents; let's call the new version of the file F\n\n\n\n\n![Diagram illustrating the Cache Consistency Problem. Three clients (C1, C2, C3) are shown with their respective caches. Client C1 has cache: F[v1]. Client C2 has cache: F[v2]. Client C3 has cache: empty. Below them is a Server S with disk: F[v1] at first, F[v2] eventually.](images/image_0139.jpeg)\n\n\nThe diagram shows three clients (C1, C2, C3) and a server (Server S).\n    \n\n    - Client C1: cache: F[v1]\n    \n\n    - Client C2: cache: F[v2]\n    \n\n    - Client C3: cache: empty\n    \n\n    - Server S: disk: F[v1] at first, F[v2] eventually\n\n\nDiagram illustrating the Cache Consistency Problem. Three clients (C1, C2, C3) are shown with their respective caches. Client C1 has cache: F[v1]. Client C2 has cache: F[v2]. Client C3 has cache: empty. Below them is a Server S with disk: F[v1] at first, F[v2] eventually.\n\n\nFigure 49.7: The Cache Consistency Problem\n\n\n(version 2), or F[v2] and the old version F[v1] so we can keep the two distinct (but of course the file has the same name, just different contents). Finally, there is a third client, C3, which has not yet accessed the file F.\n\n\nYou can probably see the problem that is upcoming (Figure 49.7). In fact, there are two subproblems. The first subproblem is that the client C2 may buffer its writes in its cache for a time before propagating them to the server; in this case, while F[v2] sits in C2's memory, any access of F from another client (say C3) will fetch the old version of the file (F[v1]). Thus, by buffering writes at the client, other clients may get stale versions of the file, which may be undesirable; indeed, imagine the case where you log into machine C2, update F, and then log into C3 and try to read the file, only to get the old copy! Certainly this could be frustrating. Thus, let us call this aspect of the cache consistency problem\n   **update visibility**\n   ; when do updates from one client become visible at other clients?\n\n\nThe second subproblem of cache consistency is a\n   **stale cache**\n   ; in this case, C2 has finally flushed its writes to the file server, and thus the server has the latest version (F[v2]). However, C1 still has F[v1] in its cache; if a program running on C1 reads file F, it will get a stale version (F[v1]) and not the most recent copy (F[v2]), which is (often) undesirable.\n\n\nNFSv2 implementations solve these cache consistency problems in two ways. First, to address update visibility, clients implement what is sometimes called\n   **flush-on-close**\n   (a.k.a.,\n   **close-to-open**\n   ) consistency semantics; specifically, when a file is written to and subsequently closed by a client application, the client flushes all updates (i.e., dirty pages in the cache) to the server. With flush-on-close consistency, NFS ensures that a subsequent open from another node will see the latest file version.\n\n\nSecond, to address the stale-cache problem, NFSv2 clients first check to see whether a file has changed before using its cached contents. Specifically, before using a cached block, the client-side file system will issue a GETATTR request to the server to fetch the file's attributes. The attributes, importantly, include information as to when the file was last modified on the server; if the time-of-modification is more recent than the time that the file was fetched into the client cache, the client\n   **invalidates**\n   the file, thus removing it from the client cache and ensuring that subsequent reads will\n\n\ngo to the server and retrieve the latest version of the file. If, on the other hand, the client sees that it has the latest version of the file, it will go ahead and use the cached contents, thus increasing performance.\n\n\nWhen the original team at Sun implemented this solution to the stale-cache problem, they realized a new problem; suddenly, the NFS server was flooded with GETATTR requests. A good engineering principle to follow is to design for the\n   **common case**\n   , and to make it work well; here, although the common case was that a file was accessed only from a single client (perhaps repeatedly), the client always had to send GETATTR requests to the server to make sure no one else had changed the file. A client thus bombards the server, constantly asking “has anyone changed this file?”, when most of the time no one had.\n\n\nTo remedy this situation (somewhat), an\n   **attribute cache**\n   was added to each client. A client would still validate a file before accessing it, but most often would just look in the attribute cache to fetch the attributes. The attributes for a particular file were placed in the cache when the file was first accessed, and then would timeout after a certain amount of time (say 3 seconds). Thus, during those three seconds, all file accesses would determine that it was OK to use the cached file and thus do so with no network communication with the server."
        },
        {
          "name": "Assessing NFS Cache Consistency",
          "content": "A few final words about NFS cache consistency. The flush-on-close behavior was added to “make sense”, but introduced a certain performance problem. Specifically, if a temporary or short-lived file was created on a client and then soon deleted, it would still be forced to the server. A more ideal implementation might keep such short-lived files in memory until they are deleted and thus remove the server interaction entirely, perhaps increasing performance.\n\n\nMore importantly, the addition of an attribute cache into NFS made it very hard to understand or reason about exactly what version of a file one was getting. Sometimes you would get the latest version; sometimes you would get an old version simply because your attribute cache hadn't yet timed out and thus the client was happy to give you what was in client memory. Although this was fine most of the time, it would (and still does!) occasionally lead to odd behavior.\n\n\nAnd thus we have described the oddity that is NFS client caching. It serves as an interesting example where details of an implementation serve to define user-observable semantics, instead of the other way around."
        },
        {
          "name": "Implications On Server-SideWrite Buffering",
          "content": "Our focus so far has been on client caching, and that is where most of the interesting issues arise. However, NFS servers tend to be well-equipped machines with a lot of memory too, and thus they have caching\n\n\nconcerns as well. When data (and metadata) is read from disk, NFS servers will keep it in memory, and subsequent reads of said data (and metadata) will not go to disk, a potential (small) boost in performance.\n\n\nMore intriguing is the case of write buffering. An NFS server absolutely may\n   *not*\n   return success on a WRITE protocol request until the write has been forced to stable storage (e.g., to disk or some other persistent device). While the server can place a copy of the data in its memory, returning success to the client on a WRITE protocol request could result in incorrect behavior; can you figure out why?\n\n\nThe answer lies in our assumptions about how clients handle server failure. Imagine the following sequence of writes as issued by a client:\n\n\nwrite(fd, a_buffer, size); // fill 1st block with a's\nwrite(fd, b_buffer, size); // fill 2nd block with b's\nwrite(fd, c_buffer, size); // fill 3rd block with c's\nThese writes overwrite the three blocks of a file with a block of a's, then b's, and then c's. Thus, if the file initially looked like this:\n\n\nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\nzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\nWe might expect the final result after these writes to be like this, with the x's, y's, and z's, would be overwritten with a's, b's, and c's, respectively.\n\n\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\ncccccccccccccccccccccccccccccccccccccccccc\nNow let's assume for the sake of the example that these three client writes were issued to the server as three distinct WRITE protocol messages. Assume the first WRITE message is received by the server and issued to the disk, and the client informed of its success. Now assume the second write is just buffered in memory, and the server also reports it success to the client\n   *before*\n   forcing it to disk; unfortunately, the server crashes before writing it to disk. The server quickly restarts and receives the third write request, which also succeeds.\n\n\nThus, to the client, all the requests succeeded, but we are surprised that the file contents look like this:\n\n\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy <--- oops\ncccccccccccccccccccccccccccccccccccccccccc\nYikes! Because the server told the client that the second write was successful before committing it to disk, an old chunk is left in the file, which, depending on the application, might be catastrophic.\n\n\n\n\n**ASIDE: INNOVATION BREEDS INNOVATION**\n\n\nAs with many pioneering technologies, bringing NFS into the world also required other fundamental innovations to enable its success. Probably the most lasting is the\n   **Virtual File System (VFS)**\n   /\n   **Virtual Node (vnode)**\n   interface, introduced by Sun to allow different file systems to be readily plugged into the operating system [K86].\n\n\nThe VFS layer includes operations that are done to an entire file system, such as mounting and unmounting, getting file-system wide statistics, and forcing all dirty (not yet written) writes to disk. The vnode layer consists of all operations one can perform on a file, such as open, close, reads, writes, and so forth.\n\n\nTo build a new file system, one simply has to define these “methods”; the framework then handles the rest, connecting system calls to the particular file system implementation, performing generic functions common to all file systems (e.g., caching) in a centralized manner, and thus providing a way for multiple file system implementations to operate simultaneously within the same system.\n\n\nAlthough some of the details have changed, many modern systems have some form of a VFS/vnode layer, including Linux, BSD variants, macOS, and even Windows (in the form of the Installable File System). Even if NFS becomes less relevant to the world, some of the necessary foundations beneath it will live on.\n\n\nTo avoid this problem, NFS servers\n   *must*\n   commit each write to stable (persistent) storage before informing the client of success; doing so enables the client to detect server failure during a write, and thus retry until it finally succeeds. Doing so ensures we will never end up with file contents intermingled as in the above example.\n\n\nThe problem that this requirement gives rise to in NFS server implementation is that write performance, without great care, can be\n   *the*\n   major performance bottleneck. Indeed, some companies (e.g., Network Appliance) came into existence with the simple objective of building an NFS server that can perform writes quickly; one trick they use is to first put writes in a battery-backed memory, thus enabling to quickly reply to WRITE requests without fear of losing the data and without the cost of having to write to disk right away; the second trick is to use a file system design specifically designed to write to disk quickly when one finally needs to do so [HLM94, RO91]."
        }
      ]
    },
    {
      "name": "The Andrew File System (AFS)",
      "sections": [
        {
          "name": "AFS Version 1",
          "content": "We will discuss two versions of AFS [H+88, S+85]. The first version (which we will call AFSv1, but actually the original system was called the ITC distributed file system [S+85]) had some of the basic design in place, but didn't scale as desired, which led to a re-design and the final protocol (which we will call AFSv2, or just AFS) [H+88]. We now discuss the first version.\n\n\n1\n   \n   Though originally referred to as \"Carnegie-Mellon University\", CMU later dropped the hyphen, and thus was born the modern form, \"Carnegie Mellon University.\" As AFS derived from work in the early 80's, we refer to CMU in its original fully-hyphenated form. See\n   https://www.quora.com/When-did-Carnegie-Mellon-University-remove-the-hyphen-in-the-university-name\n   for more details, if you are into really boring minutiae.\n\n\n\nTestAuth | Test whether a file has changed\n     \n     (used to validate cached entries)\nGetFileStat | Get the stat info for a file\nFetch | Fetch the contents of file\nStore | Store this file on the server\nSetFileStat | Set the stat info for a file\nListDir | List the contents of a directory\n\n\nFigure 50.1: AFSv1 Protocol Highlights\n\n\nOne of the basic tenets of all versions of AFS is\n   **whole-file caching**\n   on the\n   **local disk**\n   of the client machine that is accessing a file. When you\n   \n    open()\n   \n   a file, the entire file (if it exists) is fetched from the server and stored in a file on your local disk. Subsequent application\n   \n    read()\n   \n   and\n   \n    write()\n   \n   operations are redirected to the local file system where the file is stored; thus, these operations require no network communication and are fast. Finally, upon\n   \n    close()\n   \n   , the file (if it has been modified) is flushed back to the server. Note the obvious contrasts with NFS, which caches\n   *blocks*\n   (not whole files), although NFS could of course cache every block of an entire file) and does so in client\n   *memory*\n   (not local disk).\n\n\nLet's get into the details a bit more. When a client application first calls\n   \n    open()\n   \n   , the AFS client-side code (which the AFS designers call\n   **Venus**\n   ) would send a Fetch protocol message to the server. The Fetch protocol message would pass the entire pathname of the desired file (for example,\n   \n    /home/remzi/notes.txt\n   \n   ) to the file server (the group of which they called\n   **Vice**\n   ), which would then traverse the pathname, find the desired file, and ship the entire file back to the client. The client-side code would then cache the file on the local disk of the client (by writing it to local disk). As we said above, subsequent\n   \n    read()\n   \n   and\n   \n    write()\n   \n   system calls are strictly\n   *local*\n   in AFS (no communication with the server occurs); they are just redirected to the local copy of the file. Because the\n   \n    read()\n   \n   and\n   \n    write()\n   \n   calls act just like calls to a local file system, once a block is accessed, it also may be cached in client memory. Thus, AFS also uses client memory to cache copies of blocks that it has in its local disk. Finally, when finished, the AFS client checks if the file has been modified (i.e., that it has been opened for writing); if so, it flushes the new version back to the server with a Store protocol message, sending the entire file and pathname to the server for permanent storage.\n\n\nThe next time the file is accessed, AFSv1 does so much more efficiently. Specifically, the client-side code first contacts the server (using the TestAuth protocol message) in order to determine whether the file has changed. If not, the client would use the locally-cached copy, thus improving performance by avoiding a network transfer. The figure above shows some of the protocol messages in AFSv1. Note that this early version of the protocol only cached file contents; directories, for example, were only kept at the server.\n\n\n**TIP: MEASURE THEN BUILD (PATTERSON'S LAW)**\nOne of our advisors, David Patterson (of RISC and RAID fame), used to always encourage us to measure a system and demonstrate a problem\n   *before*\n   building a new system to fix said problem. By using experimental evidence, rather than gut instinct, you can turn the process of system building into a more scientific endeavor. Doing so also has the fringe benefit of making you think about how exactly to measure the system before your improved version is developed. When you do finally get around to building the new system, two things are better as a result: first, you have evidence that shows you are solving a real problem; second, you now have a way to measure your new system in place, to show that it actually improves upon the state of the art. And thus we call this\n   **Patterson's Law**\n   ."
        },
        {
          "name": "Problems with Version 1",
          "content": "A few key problems with this first version of AFS motivated the designers to rethink their file system. To study the problems in detail, the designers of AFS spent a great deal of time measuring their existing prototype to find what was wrong. Such experimentation is a good thing, because\n   **measurement**\n   is the key to understanding how systems work and how to improve them; obtaining concrete, good data is thus a necessary part of systems construction. In their study, the authors found two main problems with AFSv1:\n\n\n  * •\n    **Path-traversal costs are too high:**\n    When performing a Fetch or Store protocol request, the client passes the entire pathname (e.g.,\n    \n     /home/remzi/notes.txt\n    \n    ) to the server. The server, in order to access the file, must perform a full pathname traversal, first looking in the root directory to find\n    \n     home\n    \n    , then in\n    \n     home\n    \n    to find\n    \n     remzi\n    \n    , and so forth, all the way down the path until finally the desired file is located. With many clients accessing the server at once, the designers of AFS found that the server was spending much of its CPU time simply walking down directory paths.\n  * •\n    **The client issues too many TestAuth protocol messages:**\n    Much like NFS and its overabundance of GETATTR protocol messages, AFSv1 generated a large amount of traffic to check whether a local file (or its stat information) was valid with the TestAuth protocol message. Thus, servers spent much of their time telling clients whether it was OK to use their cached copies of a file. Most of the time, the answer was that the file had not changed.\n\n\nThere were actually two other problems with AFSv1: load was not balanced across servers, and the server used a single distinct process per client thus inducing context switching and other overheads. The load\n\n\nimbalance problem was solved by introducing\n   **volumes**\n   , which an administrator could move across servers to balance load; the context-switch problem was solved in AFSv2 by building the server with threads instead of processes. However, for the sake of space, we focus here on the main two protocol problems above that limited the scale of the system."
        },
        {
          "name": "Improving the Protocol",
          "content": "The two problems above limited the scalability of AFS; the server CPU became the bottleneck of the system, and each server could only service 20 clients without becoming overloaded. Servers were receiving too many TestAuth messages, and when they received Fetch or Store messages, were spending too much time traversing the directory hierarchy. Thus, the AFS designers were faced with a problem:\n\n\n\n\n**THE CRUX: HOW TO DESIGN A SCALABLE FILE PROTOCOL**\n\n\nHow should one redesign the protocol to minimize the number of server interactions, i.e., how could they reduce the number of TestAuth messages? Further, how could they design the protocol to make these server interactions efficient? By attacking both of these issues, a new protocol would result in a much more scalable version AFS."
        },
        {
          "name": "AFS Version 2",
          "content": "AFSv2 introduced the notion of a\n   **callback**\n   to reduce the number of client/server interactions. A callback is simply a promise from the server to the client that the server will inform the client when a file that the client is caching has been modified. By adding this\n   **state**\n   to the system, the client no longer needs to contact the server to find out if a cached file is still valid. Rather, it assumes that the file is valid until the server tells it otherwise; notice the analogy to\n   **polling**\n   versus\n   **interrupts**\n   .\n\n\nAFSv2 also introduced the notion of a\n   **file identifier (FID)**\n   (similar to the NFS\n   **file handle**\n   ) instead of pathnames to specify which file a client was interested in. An FID in AFS consists of a volume identifier, a file identifier, and a “uniquifier” (to enable reuse of the volume and file IDs when a file is deleted). Thus, instead of sending whole pathnames to the server and letting the server walk the pathname to find the desired file, the client would walk the pathname, one piece at a time, caching the results and thus hopefully reducing the load on the server.\n\n\nFor example, if a client accessed the file\n   \n    /home/remzi/notes.txt\n   \n   , and\n   \n    home\n   \n   was the AFS directory mounted onto\n   \n    /\n   \n   (i.e.,\n   \n    /\n   \n   was the local root directory, but\n   \n    home\n   \n   and its children were in AFS), the client would first Fetch the directory contents of\n   \n    home\n   \n   , put them in the local-disk cache, and set up a callback on\n   \n    home\n   \n   . Then, the client would Fetch the directory\n\n\n\nClient (C\n      \n       1\n      \n      ) | Server\nfd = open(\"/home/remzi/notes.txt\", ...);\nSend Fetch (home FID, \"remzi\") | Receive Fetch request\nlook for remzi in home dir\nsetup callback(C1) on remzi\nreturn remzi's content/FID\nReceive Fetch reply\nwrite remzi to local disk cache\nrecord callback status of remzi\nSend Fetch (remzi FID, \"notes.txt\") | Receive Fetch request\nlook for notes.txt in remzi dir\nsetup callback(C1) on notes.txt\nreturn notes.txt's content/FID\nReceive Fetch reply\nwrite notes.txt to local disk cache\nrecord callback status of notes.txt\nlocal open() of cached notes.txt\nreturn file descriptor to application | \nread(fd, buffer, MAX);\nperform local read() on cached copy | \nclose(fd);\ndo local close() on cached copy\nif file has changed, flush to server | \nfd = open(\"/home/remzi/notes.txt\", ...);\nForEach dir (home, remzi)\n  if (callback(dir) == VALID)\n    use local copy for lookup(dir)\n  else\n    Fetch (as above)\n  if (callback(notes.txt) == VALID)\n    open local cached copy\n    return file descriptor to it\n  else\n    Fetch (as above) then open and return fd | \n\n\nFigure 50.2:\n   **Reading A File: Client-side And File Server Actions**\n\n\nremzi, put it in the local-disk cache, and set up a callback on remzi. Finally, the client would Fetch\n   \n    notes.txt\n   \n   , cache this regular file in the local disk, set up a callback, and finally return a file descriptor to the calling application. See Figure 50.2 for a summary.\n\n\nThe key difference, however, from NFS, is that with each fetch of a directory or file, the AFS client would establish a callback with the server,\n\n\n\n\n**ASIDE: CACHE CONSISTENCY IS NOT A PANACEA**\n\n\nWhen discussing distributed file systems, much is made of the cache consistency the file systems provide. However, this baseline consistency does not solve all problems with regards to file access from multiple clients. For example, if you are building a code repository, with multiple clients performing check-ins and check-outs of code, you can't simply rely on the underlying file system to do all of the work for you; rather, you have to use explicit\n   **file-level locking**\n   in order to ensure that the “right” thing happens when such concurrent accesses take place. Indeed, any application that truly cares about concurrent updates will add extra machinery to handle conflicts. The baseline consistency described in this chapter and the previous one are useful primarily for casual usage, i.e., when a user logs into a different client, they expect some reasonable version of their files to show up there. Expecting more from these protocols is setting yourself up for failure, disappointment, and tear-filled frustration.\n\n\nthus ensuring that the server would notify the client of a change in its cached state. The benefit is obvious: although the\n   *first*\n   access to\n   \n    /home/remzi/notes.txt\n   \n   generates many client-server messages (as described above), it also establishes callbacks for all the directories as well as the file\n   \n    notes.txt\n   \n   , and thus subsequent accesses are entirely local and require no server interaction at all. Thus, in the common case where a file is cached at the client, AFS behaves nearly identically to a local disk-based file system. If one accesses a file more than once, the second access should be just as fast as accessing a file locally."
        },
        {
          "name": "Cache Consistency",
          "content": "When we discussed NFS, there were two aspects of cache consistency we considered:\n   **update visibility**\n   and\n   **cache staleness**\n   . With update visibility, the question is: when will the server be updated with a new version of a file? With cache staleness, the question is: once the server has a new version, how long before clients see the new version instead of an older cached copy?\n\n\nBecause of callbacks and whole-file caching, the cache consistency provided by AFS is easy to describe and understand. There are two important cases to consider: consistency between processes on\n   *different*\n   machines, and consistency between processes on the\n   *same*\n   machine.\n\n\nBetween different machines, AFS makes updates visible at the server and invalidates cached copies at the exact same time, which is when the updated file is closed. A client opens a file, and then writes to it (perhaps repeatedly). When it is finally closed, the new file is flushed to the server (and thus visible). At this point, the server then “breaks” callbacks for any clients with cached copies; the break is accomplished by contacting each client and informing it that the callback it has on the file is no longer\n\n\n\nP\n      \n       1 | Client\n      \n       1 | P\n      \n       3 | Client\n      \n       2 | Server\n      \n      Disk | Comments\nP\n      \n       2 | Cache | Cache | Cache\nopen(F) | - | - | - | - | - | - | File created\nwrite(A) | A | - | - | - | - | A | \nclose() | A | - | - | - | - | A | \nopen(F) | A | - | - | - | - | A | \nread() → A | A | - | - | - | - | A | \nclose() | A | - | - | - | - | A | \nopen(F) | A | - | - | - | - | A | \nwrite(B) | B | - | - | - | - | A | Local processes\n      \n      see writes immediately\nopen(F) | B | - | - | - | - | A | \nread() → B | B | - | - | - | - | A | \nclose() | B | - | - | - | - | A | \n | B | - | open(F) | A | A | A | Remote processes\n      \n      do not see writes...\n | B | - | read() → A | A | A | A | \n | B | - | close() | A | A | A | \nclose() | B | ✓ |  |  |  | B | ... until close()\n      \n      has taken place\n | B | - | open(F) | B | B | B | \n | B | - | read() → B | B | B | B | \n | B | - | close() | B | B | B | \nopen(F) | B | - | open(F) | B | B | B | \nwrite(D) | D | - |  |  |  | B | \n | D | - | write(C) | C | C | B | \n | D | - | close() | C | C | C | \nclose() | D | ✓ |  |  |  | D | Unfortunately for P\n      \n       3\n      \n\n      the last writer wins\n | D | - | open(F) | D | D | D | \n | D | - | read() → D | D | D | D | \n | D | - | close() | D | D | D | \n\n\nFigure 50.3: Cache Consistency Timeline\n\n\nvalid. This step ensures that clients will no longer read stale copies of the file; subsequent opens on those clients will require a re-fetch of the new version of the file from the server (and will also serve to reestablish a callback on the new version of the file).\n\n\nAFS makes an exception to this simple model between processes on the same machine. In this case, writes to a file are immediately visible to other local processes (i.e., a process does not have to wait until a file is closed to see its latest updates). This makes using a single machine behave exactly as you would expect, as this behavior is based upon typical UNIX semantics. Only when switching to a different machine would you be able to detect the more general AFS consistency mechanism.\n\n\nThere is one interesting cross-machine case that is worthy of further discussion. Specifically, in the rare case that processes on different machines are modifying a file at the same time, AFS naturally employs what is known as a\n   **last writer wins**\n   approach (which perhaps should be called\n   **last closer wins**\n   ). Specifically, whichever client calls\n   \n    close()\n   \n   last will update the entire file on the server last and thus will be the “winning”\n\n\nfile, i.e., the file that remains on the server for others to see. The result is a file that was generated in its entirety either by one client or the other. Note the difference from a block-based protocol like NFS: in NFS, writes of individual blocks may be flushed out to the server as each client is updating the file, and thus the final file on the server could end up as a mix of updates from both clients. In many cases, such a mixed file output would not make much sense, i.e., imagine a JPEG image getting modified by two clients in pieces; the resulting mix of writes would not likely constitute a valid JPEG.\n\n\nA timeline showing a few of these different scenarios can be seen in Figure 50.3. The columns show the behavior of two processes (\n   \n    P_1\n   \n   and\n   \n    P_2\n   \n   ) on\n   \n    \\text{Client}_1\n   \n   and its cache state, one process (\n   \n    P_3\n   \n   ) on\n   \n    \\text{Client}_2\n   \n   and its cache state, and the server (Server), all operating on a single file called, imaginatively,\n   \n    F\n   \n   . For the server, the figure simply shows the contents of the file after the operation on the left has completed. Read through it and see if you can understand why each read returns the results that it does. A commentary field on the right will help you if you get stuck."
        },
        {
          "name": "Crash Recovery",
          "content": "From the description above, you might sense that crash recovery is more involved than with NFS. You would be right. For example, imagine there is a short period of time where a server (\n   \n    S\n   \n   ) is not able to contact a client (\n   \n    C_1\n   \n   ), for example, while the client\n   \n    C_1\n   \n   is rebooting. While\n   \n    C_1\n   \n   is not available,\n   \n    S\n   \n   may have tried to send it one or more callback recall messages; for example, imagine\n   \n    C_1\n   \n   had file\n   \n    F\n   \n   cached on its local disk, and then\n   \n    C_2\n   \n   (another client) updated\n   \n    F\n   \n   , thus causing\n   \n    S\n   \n   to send messages to all clients caching the file to remove it from their local caches. Because\n   \n    C_1\n   \n   may miss those critical messages when it is rebooting, upon rejoining the system,\n   \n    C_1\n   \n   should treat all of its cache contents as suspect. Thus, upon the next access to file\n   \n    F\n   \n   ,\n   \n    C_1\n   \n   should first ask the server (with a\n   \n    TestAuth\n   \n   protocol message) whether its cached copy of file\n   \n    F\n   \n   is still valid; if so,\n   \n    C_1\n   \n   can use it; if not,\n   \n    C_1\n   \n   should fetch the newer version from the server.\n\n\nServer recovery after a crash is also more complicated. The problem that arises is that callbacks are kept in memory; thus, when a server reboots, it has no idea which client machine has which files. Thus, upon server restart, each client of the server must realize that the server has crashed and treat all of their cache contents as suspect, and (as above) reestablish the validity of a file before using it. Thus, a server crash is a big event, as one must ensure that each client is aware of the crash in a timely manner, or risk a client accessing a stale file. There are many ways to implement such recovery; for example, by having the server send a message (saying “don’t trust your cache contents!”) to each client when it is up and running again, or by having clients check that the server is alive periodically (with a\n   **heartbeat**\n   message, as it is called). As you can see, there is a cost to building a more scalable and sensible caching model; with NFS, clients hardly noticed a server crash.\n\n\n\nWorkload | NFS | AFS | AFS/NFS\n1. Small file, sequential read | N_s \\cdot L_{net} | N_s \\cdot L_{net} | 1\n2. Small file, sequential re-read | N_s \\cdot L_{mem} | N_s \\cdot L_{mem} | 1\n3. Medium file, sequential read | N_m \\cdot L_{net} | N_m \\cdot L_{net} | 1\n4. Medium file, sequential re-read | N_m \\cdot L_{mem} | N_m \\cdot L_{mem} | 1\n5. Large file, sequential read | N_L \\cdot L_{net} | N_L \\cdot L_{net} | 1\n6. Large file, sequential re-read | N_L \\cdot L_{net} | N_L \\cdot L_{disk} | \\frac{L_{disk}}{L_{net}}\n7. Large file, single read | L_{net} | N_L \\cdot L_{net} | \\frac{N_L}{L_{net}}\n8. Small file, sequential write | N_s \\cdot L_{net} | N_s \\cdot L_{net} | 1\n9. Large file, sequential write | N_L \\cdot L_{net} | N_L \\cdot L_{net} | 1\n10. Large file, sequential overwrite | N_L \\cdot L_{net} | 2 \\cdot N_L \\cdot L_{net} | 2\n11. Large file, single write | L_{net} | 2 \\cdot N_L \\cdot L_{net} | 2 \\cdot N_L\n\n\nFigure 50.4: Comparison: AFS vs. NFS"
        },
        {
          "name": "Scale And Performance Of AFSv2",
          "content": "With the new protocol in place, AFSv2 was measured and found to be much more scalable than the original version. Indeed, each server could support about 50 clients (instead of just 20). A further benefit was that client-side performance often came quite close to local performance, because in the common case, all file accesses were local; file reads usually went to the local disk cache (and potentially, local memory). Only when a client created a new file or wrote to an existing one was there need to send a Store message to the server and thus update the file with new contents.\n\n\nLet us also gain some perspective on AFS performance by comparing common file-system access scenarios with NFS. Figure 50.4 (page 9) shows the results of our qualitative comparison.\n\n\nIn the figure, we examine typical read and write patterns analytically, for files of different sizes. Small files have\n   \n    N_s\n   \n   blocks in them; medium files have\n   \n    N_m\n   \n   blocks; large files have\n   \n    N_L\n   \n   blocks. We assume that small and medium files fit into the memory of a client; large files fit on a local disk but not in client memory.\n\n\nWe also assume, for the sake of analysis, that an access across the network to the remote server for a file block takes\n   \n    L_{net}\n   \n   time units. Access to local memory takes\n   \n    L_{mem}\n   \n   , and access to local disk takes\n   \n    L_{disk}\n   \n   . The general assumption is that\n   \n    L_{net} > L_{disk} > L_{mem}\n   \n   .\n\n\nFinally, we assume that the first access to a file does not hit in any caches. Subsequent file accesses (i.e., “re-reads”) we assume will hit in caches, if the relevant cache has enough capacity to hold the file.\n\n\nThe columns of the figure show the time a particular operation (e.g., a small file sequential read) roughly takes on either NFS or AFS. The right-most column displays the ratio of AFS to NFS.\n\n\nWe make the following observations. First, in many cases, the performance of each system is roughly equivalent. For example, when first reading a file (e.g., Workloads 1, 3, 5), the time to fetch the file from the re-\n\n\nremote server dominates, and is similar on both systems. You might think AFS would be slower in this case, as it has to write the file to local disk; however, those writes are buffered by the local (client-side) file system cache and thus said costs are likely hidden. Similarly, you might think that AFS reads from the local cached copy would be slower, again because AFS stores the cached copy on disk. However, AFS again benefits here from local file system caching; reads on AFS would likely hit in the client-side memory cache, and performance would be similar to NFS.\n\n\nSecond, an interesting difference arises during a large-file sequential re-read (Workload 6). Because AFS has a large local disk cache, it will access the file from there when the file is accessed again. NFS, in contrast, only can cache blocks in client memory; as a result, if a large file (i.e., a file bigger than local memory) is re-read, the NFS client will have to re-fetch the entire file from the remote server. Thus, AFS is faster than NFS in this case by a factor of\n   \n    \\frac{L_{net}}{L_{disk}}\n   \n   , assuming that remote access is indeed slower than local disk. We also note that NFS in this case increases server load, which has an impact on scale as well.\n\n\nThird, we note that sequential writes (of new files) should perform similarly on both systems (Workloads 8, 9). AFS, in this case, will write the file to the local cached copy; when the file is closed, the AFS client will force the writes to the server, as per the protocol. NFS will buffer writes in client memory, perhaps forcing some blocks to the server due to client-side memory pressure, but definitely writing them to the server when the file is closed, to preserve NFS flush-on-close consistency. You might think AFS would be slower here, because it writes all data to local disk. However, realize that it is writing to a local file system; those writes are first committed to the page cache, and only later (in the background) to disk, and thus AFS reaps the benefits of the client-side OS memory caching infrastructure to improve performance.\n\n\nFourth, we note that AFS performs worse on a sequential file overwrite (Workload 10). Thus far, we have assumed that the workloads that write are also creating a new file; in this case, the file exists, and is then over-written. Overwrite can be a particularly bad case for AFS, because the client first fetches the old file in its entirety, only to subsequently overwrite it. NFS, in contrast, will simply overwrite blocks and thus avoid the initial (useless) read\n   \n    2\n   \n   .\n\n\nFinally, workloads that access a small subset of data within large files perform much better on NFS than AFS (Workloads 7, 11). In these cases, the AFS protocol fetches the entire file when the file is opened; unfortunately, only a small read or write is performed. Even worse, if the file is modified, the entire file is written back to the server, doubling the per-\n\n\n2\n   \n   We assume here that NFS writes are block-sized and block-aligned; if they were not, the NFS client would also have to read the block first. We also assume the file was\n   *not*\n   opened with the O.TRUNC flag; if it had been, the initial open in AFS would not fetch the soon to be truncated file's contents.\n\n\n\n\n**ASIDE: THE IMPORTANCE OF WORKLOAD**\n\n\nOne challenge of evaluating any system is the choice of\n   **workload**\n   . Because computer systems are used in so many different ways, there are a large variety of workloads to choose from. How should the storage system designer decide which workloads are important, in order to make reasonable design decisions?\n\n\nThe designers of AFS, given their experience in measuring how file systems were used, made certain workload assumptions; in particular, they assumed that most files were not frequently shared, and accessed sequentially in their entirety. Given those assumptions, the AFS design makes perfect sense.\n\n\nHowever, these assumptions are not always correct. For example, imagine an application that appends information, periodically, to a log. These little log writes, which add small amounts of data to an existing large file, are quite problematic for AFS. Many other difficult workloads exist as well, e.g., random updates in a transaction database.\n\n\nOne place to get some information about what types of workloads are common are through various research studies that have been performed. See any of these studies for good examples of workload analysis [B+91, H+11, R+00, V99], including the AFS retrospective [H+88].\n\n\nformance impact. NFS, as a block-based protocol, performs I/O that is proportional to the size of the read or write.\n\n\nOverall, we see that NFS and AFS make different assumptions and not surprisingly realize different performance outcomes as a result. Whether these differences matter is, as always, a question of workload."
        },
        {
          "name": "AFS: Other Improvements",
          "content": "Like we saw with the introduction of Berkeley FFS (which added symbolic links and a number of other features), the designers of AFS took the opportunity when building their system to add a number of features that made the system easier to use and manage. For example, AFS provides a true global namespace to clients, thus ensuring that all files were named the same way on all client machines. NFS, in contrast, allows each client to mount NFS servers in any way that they please, and thus only by convention (and great administrative effort) would files be named similarly across clients.\n\n\nAFS also takes security seriously, and incorporates mechanisms to authenticate users and ensure that a set of files could be kept private if a user so desired. NFS, in contrast, had quite primitive support for security for many years.\n\n\nAFS also includes facilities for flexible user-managed access control. Thus, when using AFS, a user has a great deal of control over who exactly\n\n\ncan access which files. NFS, like most UNIX file systems, has much less support for this type of sharing.\n\n\nFinally, as mentioned before, AFS adds tools to enable simpler management of servers for the administrators of the system. In thinking about system management, AFS was light years ahead of the field."
        }
      ]
    },
    {
      "name": "Introduction to Operating System Security",
      "sections": [
        {
          "name": "Introduction",
          "content": "Security of computing systems is a vital topic whose importance only keeps increasing. Much money has been lost and many people's lives have been harmed when computer security has failed. Attacks on computer systems are so common as to be inevitable in almost any scenario where you perform computing. Generally, all elements of a computer system can be subject to attack, and flaws in any of them can give an attacker an opportunity to do something you want to prevent. But operating systems are particularly important from a security perspective. Why?\n\n\nTo begin with, pretty much everything runs on top of an operating system. As a rule, if the software you are running on top of, whether it be an operating system, a piece of middleware, or something else, is insecure, what's above it is going to also be insecure. It's like building a house on sand. You may build a nice solid structure, but a flood can still wash away the base underneath your home, totally destroying it despite the care you took in its construction. Similarly, your application might perhaps have no security flaws of its own, but if the attacker can misuse the software underneath you to steal your information, crash your program, or otherwise cause you harm, your own efforts to secure your code might be for naught.\n\n\nThis point is especially important for operating systems. You might not care about the security of a particular web server or database system if you don't run that software, and you might not care about the security of some middleware platform that you don't use, but everyone runs an operating system, and there are relatively few choices of which to run. Thus, security flaws in an operating system, especially a widely used one, have an immense impact on many users and many pieces of software.\n\n\nAnother reason that operating system security is so important is that ultimately all of our software relies on proper behavior of the underlying hardware: the processor, the memory, and the peripheral devices. What has ultimate control of those hardware resources? The operating system.\n\n\nThinking about what you have already studied concerning memory management, scheduling, file systems, synchronization, and so forth, what would happen with each of these components of your operating system if an adversary could force it to behave in some arbitrarily bad way? If you understand what you’ve learned so far, you should find this prospect deeply disturbing\n   \n    1\n   \n   . Our computing lives depend on our operating systems behaving as they have been defined to behave, and particularly on them not behaving in ways that benefit our adversaries, rather than us.\n\n\nThe task of securing an operating system is not an easy one, since modern operating systems are large and complex. Your experience in writing code should have already pointed out to you that the more code you’ve got, and the more complex the algorithms are, the more likely your code is to contain flaws. Failures in software security generally arise from these kinds of flaws. Large, complex programs are likely to be harder to secure than small, simple programs. Not many other programs are as large and complex as a modern operating system.\n\n\nAnother challenge in securing operating systems is that they are, for the most part, meant to support multiple processes simultaneously. As you’ve learned, there are many mechanisms in an operating system meant to segregate processes from each other, and to protect shared pieces of hardware from being used in ways that interfere with other processes. If every process could be trusted to do anything it wants with any hardware resource and any piece of data on the machine without harming any other process, securing the system would be a lot easier. However, we typically don’t trust everything equally. When you download and run a script from a web site you haven’t visited before, do you really want it to be able to wipe every file from your disk, kill all your other processes, and start using your network interface to send spam email to other machines? Probably not, but if you are the owner of your computer, you have the right to do all those things, if that’s what you want to do. And unless the operating system is careful, any process it runs, including the one running that script you downloaded, can do anything you can do.\n\n\nConsider the issue of operating system security from a different perspective. One role of an operating system is to provide useful abstractions for application programs to build on. These applications must rely on the OS implementations of the abstractions to work as they are defined. Often, one part of the definition of such abstractions is their security behavior. For example, we expect that the operating system’s file system will enforce the access restrictions it is supposed to enforce. Applications can then build on this expectation to achieve the security goals they require, such as counting on the file system access guarantees to ensure that a file they have specified as unwritable does not get altered. If the applications cannot rely on proper implementation of security guarantees for OS abstractions, then they cannot use these abstractions to achieve their own security goals. At the minimum, that implies a great deal more work on\n\n\n1\n   \n   If you don’t understand it, you have a lot of re-reading to do. A lot.\n\n\nthe part of the application developers, since they will need to take extra measures to achieve their desired security goals. Taking into account our earlier discussion, they will often be unable to achieve these goals if the abstractions they must rely on (such as virtual memory or a well-defined scheduling policy) cannot be trusted.\n\n\nObviously, operating system security is vital, yet hard to achieve. So what do we do to secure our operating system? Addressing that question has been a challenge for generations of computer scientists, and there is as yet no complete answer. But there are some important principles and tools we can use to secure operating systems. These are generally built into any general-purpose operating system you are likely to work with, and they alter what can be done with that system and how you go about doing it. So you might not think you're interested in security, but you need to understand what your OS does to secure itself to also understand how to get the system to do what you want.\n\n\n\n\n**CRUX: HOW TO SECURE OS RESOURCES**\n\n\nIn the face of multiple possibly concurrent and interacting processes running on the same machine, how can we ensure that the resources each process is permitted to access are exactly those it should access, in exactly the ways we desire? What primitives are needed from the OS? What mechanisms should be provided by the hardware? How can we use them to solve the problems of security?"
        },
        {
          "name": "What Are We Protecting?",
          "content": "We aren't likely to achieve good protection unless we have a fairly comprehensive view of what we're trying to protect when we say our operating system should be secure. Fortunately, that question is easy to answer for an operating system, at least at the high level: everything. That answer isn't very comforting, but it is best to have a realistic understanding of the broad implications of operating system security.\n\n\nA typical commodity operating system has complete control of all (or almost all) hardware on the machine and is able to do literally anything the hardware permits. That means it can control the processor, read and write all registers, examine any main memory location, and perform any operation one of its peripherals supports. As a result, among the things the OS can do are:\n\n\n  * • examine or alter any process's memory\n  * • read, write, delete or corrupt any file on any writeable persistent storage medium, including hard disks and flash drives\n  * • change the scheduling or even halt execution of any process\n  * • send any message to anywhere, including altered versions of those a process wished to send\n  * • enable or disable any peripheral device\n\n\n\n\n**ASIDE: SECURITY ENCLAVES**\n\n\nA little bit back, we said the operating system controls “almost all” the hardware on the machine. That kind of caveat should have gotten you asking, “well, what parts of the hardware doesn’t it control?” Originally, it really was all the hardware. But starting in the 1990s, hardware developer began to see a need to keep some hardware isolated, to a degree, from the operating system. The first such hardware was primarily intended to protect the boot process of the operating system.\n   **TPM**\n   , or\n   **Trusted Platform Module**\n   , provided assurance that you were booting the version of the operating system you intended to, protecting you from attacks that tried to boot compromised versions of the system. More recently, more general hardware elements have tried to control what can be done on the machine, typically with some particularly important data, often data that is related to cryptography. Such hardware elements are called security enclaves, since they are meant to allow only safe use of this data, even by the most powerful, trusted code in the system – the operating system itself. They are often used to support operations in a cloud computing environment, where multiple operating systems might be running under virtual machines sharing the same physical hardware.\n\n\nThis turns out to be a harder trick than anyone expected. Security tricks usually are. Security enclaves often prove not to provide quite as much isolation as their designers hoped. But the attacks on them tend to be sophisticated and difficult, and usually require the ability to run privileged code on the system already. So even if they don’t achieve their full goals, they do put an extra protective barrier against compromised operating system code.\n\n\n  * • give any process access to any other process’s resources\n  * • arbitrarily take away any resource a process controls\n  * • respond to any system call with a maximally harmful lie\n\n\nIn essence, processes are at the mercy of the operating system. It is nearly impossible for a process to ‘protect’ any part of itself from a malicious operating system. We typically assume our operating system is not actually malicious\n   \n    2\n   \n   , but a flaw that allows a malicious process to cause the operating system to misbehave is nearly as bad, since it could potentially allow that process to gain any of the powers of the operating system itself. This point should make you think very seriously about the importance of designing secure operating systems and, more commonly, applying security patches to any operating system you are running. Security flaws in your operating system can completely compromise everything about the machine the system runs on, so preventing them and patching any that are found is vitally important.\n\n\n2\n   \n   If you suspect your operating system is malicious, it’s time to get a new operating system."
        },
        {
          "name": "Security Goals and Policies",
          "content": "What do we mean when we say we want an operating system, or any system, to be secure? That's a rather vague statement. What we really mean is that there are things we would like to happen in the system and things we don't want to happen, and we'd like a high degree of assurance that we get what we want. As in most other aspects of life, we usually end up paying for what we get, so it's worthwhile to think about exactly what security properties and effects we actually need and then pay only for those, not for other things we don't need. What this boils down to is that we want to specify the goals we have for the security-relevant behavior of our system and choose defense approaches likely to achieve those goals at a reasonable cost.\n\n\nResearchers in security have thought about this issue in broad terms for a long time. At a high conceptual level, they have defined three big security-related goals that are common to many systems, including operating systems. They are:\n\n\n  * •\n    **Confidentiality**\n    – If some piece of information is supposed to be hidden from others, don't allow them to find it out. For example, you don't want someone to learn what your credit card number is – you want that number kept confidential.\n  * •\n    **Integrity**\n    – If some piece of information or component of a system is supposed to be in a particular state, don't allow an adversary to change it. For example, if you've placed an online order for delivery of one pepperoni pizza, you don't want a malicious prankster to change your order to 1000 anchovy pizzas. One important aspect of integrity is authenticity. It's often important to be sure not only that information has not changed, but that it was created by a particular party and not by an adversary.\n  * •\n    **Availability**\n    – If some information or service is supposed to be available for your own or others' use, make sure an attacker cannot prevent its use. For example, if your business is having a big sale, you don't want your competitors to be able to block off the streets around your store, preventing your customers from reaching you.\n\n\nAn important extra dimension of all three of these goals is that we want controlled sharing in our systems. We share our secrets with some people and not with others. We allow some people to change our enterprise's databases, but not just anyone. Some systems need to be made available to a particular set of preferred users (such as those who have paid to play your on-line game) and not to others (who have not). Who's doing the asking matters a lot, in computers as in everyday life.\n\n\nAnother important aspect of security for computer systems is we often want to be sure that when someone told us something, they cannot later deny that they did so. This aspect is often called\n   **non-repudiation**\n   . The\n\n\nharder and more expensive it is for someone to repudiate their actions, the easier it is to hold them to account for those actions, and thus the less likely people are to perform malicious actions. After all, they might well get caught and will have trouble denying they did it.\n\n\nThese are big, general goals. For a real system, you need to drill down to more detailed, specific goals. In a typical operating system, for example, we might have a confidentiality goal stating that a process's memory space cannot be arbitrarily read by another process. We might have an integrity goal stating that if a user writes a record to a particular file, another user who should not be able to write that file can't change the record. We might have an availability goal stating that one process running on the system cannot hog the CPU and prevent other processes from getting their share of the CPU. If you think back on what you've learned about the process abstraction, memory management, scheduling, file systems, IPC, and other topics from this class, you should be able to think of some other obvious confidentiality, integrity, and availability goals we are likely to want in our operating systems.\n\n\nFor any particular system, even goals at this level are not sufficiently specific. The integrity goal alluded to above, where a user's file should not be overwritten by another user not permitted to do so, gives you a hint about the extra specificity we need in our security goals for a particular system. Maybe there is some user who should be able to overwrite the file, as might be the case when two people are collaborating on writing a report. But that doesn't mean an unrelated third user should be able to write that file, if he is not collaborating on the report stored there. We need to be able to specify such detail in our security goals. Operating systems are written to be used by many different people with many different needs, and operating system security should reflect that generality. What we want in security mechanisms for operating systems is flexibility in describing our detailed security goals.\n\n\nUltimately, of course, the operating system software must do its best to enforce those flexible security goals, which implies we'll need to encode those goals in forms that software can understand. We typically must convert our vague understandings of our security goals into highly specific\n   **security policies**\n   . For example, in the case of the file described above, we might want to specify a policy like 'users A and B may write to file X, but no other user can write it.' With that degree of specificity, backed by carefully designed and implemented mechanisms, we can hope to achieve our security goals.\n\n\nNote an important implication for operating system security: in many cases, an operating system will have the mechanisms necessary to implement a desired security policy with a high degree of assurance in its proper application, but only if someone tells the operating system precisely what that policy is. With some important exceptions (like maintaining a process's address space private unless specifically directed otherwise), the operating system merely supplies general mechanisms that can implement many specific policies. Without intelligent design of poli-\n\n\n\n\n**ASIDE: SECURITY VS. FAULT TOLERANCE**\n\n\nWhen discussing the process abstraction, we talked about how virtualization protected a process from actions of other processes. For instance, we did not want our process’s memory to be accidentally overwritten by another process, so our virtualization mechanisms had to prevent such behavior. Then we were talking primarily about flaws or mistakes in processes. Is this actually any different than worrying about malicious behavior, which is more commonly the context in which we discuss security? Have we already solved all our problems by virtualizing our resources?\n\n\nYes and no. (Isn’t that a helpful phrase?) Yes, if we perfectly virtualized everything and allowed no interactions between anything, we very likely would have solved most problems of malice. However, most virtualization mechanisms are not totally bulletproof. They work well when no one tries to subvert them, but may not be perfect against all possible forms of misbehavior. Second, and perhaps more important, we don’t really want to totally isolate processes from each other. Processes share some OS resources by default (such as file systems) and can optionally choose to share others. These intentional relaxations of virtualization are not problematic when used properly, but the possibilities of legitimate sharing they open are also potential channels for malicious attacks. Finally, the OS does not always have complete control of the hardware...\n\n\ncies and careful application of the mechanisms, however, what the operating system\n   *should*\n   or\n   *could*\n   do may not be what your operating system\n   *will*\n   do."
        },
        {
          "name": "Designing Secure Systems",
          "content": "Few of you will ever build your own operating system, nor even make serious changes to any existing operating system, but we expect many of you will build large software systems of some kind. Experience of many computer scientists with system design has shown that there are certain design principles that are helpful in building systems with security requirements. These principles were originally laid out by Jerome Saltzer and Michael Schroeder in an influential paper [SS75], though some of them come from earlier observations by others. While neither the original authors nor later commentators would claim that following them will guarantee that your system is secure, paying attention to them has proven to lead to more secure systems, while you ignore them at your own peril. We’ll discuss them briefly here. If you are actually building a large software system, it would be worth your while to look up this paper (or more detailed commentaries on it) and study the concepts carefully.\n\n\n  * 1.\n    **Economy of mechanism**\n    – This basically means keep your system as small and simple as possible. Simple systems have fewer bugs and it’s easier to understand their behavior. If you don’t understand your system’s behavior, you’re not likely to know if it achieves its security goals.\n  * 2.\n    **Fail-safe defaults**\n    – Default to security, not insecurity. If policies can be set to determine the behavior of a system, have the default for those policies be more secure, not less.\n  * 3.\n    **Complete mediation**\n    – This is a security term meaning that you should check if an action to be performed meets security policies every single time the action is taken\n    \n     3\n    \n    .\n  * 4.\n    **Open design**\n    – Assume your adversary knows every detail of your design. If the system can achieve its security goals anyway, you’re in good shape. This principle does not necessarily mean that you actually tell everyone all the details, but base your security on the assumption that the attacker has learned everything. He often has, in practice.\n  * 5.\n    **Separation of privilege**\n    – Require separate parties or credentials to perform critical actions. For example, two-factor authentication, where you use both a password and possession of a piece of hardware to determine identity, is more secure than using either one of those methods alone.\n  * 6.\n    **Least privilege**\n    – Give a user or a process the minimum privileges required to perform the actions you wish to allow. The more privileges you give to a party, the greater the danger that they will abuse those privileges. Even if you are confident that the party is not malicious, if they make a mistake, an adversary can leverage their error to use their superfluous privileges in harmful ways.\n  * 7.\n    **Least common mechanism**\n    – For different users or processes, use separate data structures or mechanisms to handle them. For example, each process gets its own page table in a virtual memory system, ensuring that one process cannot access another’s pages.\n  * 8.\n    **Acceptability**\n    – A critical property not dear to the hearts of many programmers. If your users won’t use it, your system is worthless. Far too many promising secure systems have been abandoned because they asked too much of their users.\n\n\n3\n   \n   This particular principle is often ignored in many systems, in favor of lower overhead or usability. An overriding characteristic of all engineering design is that you often must balance conflicting goals, as we saw earlier in the course, such as in the scheduling chapters. We’ll say more about that in the context of security later.\n\n\nThese are not the only useful pieces of advice on designing secure systems out there. There is also lots of good material on taking the next step, converting a good design into code that achieves the security you intended, and other material on how to evaluate whether the system you have built does indeed meet those goals. These issues are beyond the scope of this course, but are extremely important when the time comes for you to build large, complex systems. For discussion of approaches to secure programming, you might start with Seacord [SE13], if you are working in C. If you are working in another language, you should seek out a similar text specific to that language, since many secure coding problem are related to details of the language. For a comprehensive treatment on how to evaluate if your system is secure, start with Dowd et al.’s work [D+07]."
        },
        {
          "name": "The Basics of OS Security",
          "content": "In a typical operating system, then, we have some set of security goals, centered around various aspects of confidentiality, integrity, and availability. Some of these goals tend to be built in to the operating system model, while others are controlled by the owners or users of the system. The built-in goals are those that are extremely common, or must be ensured to make the more specific goals achievable. Most of these built-in goals relate to controlling process access to pieces of the hardware. That’s because the hardware is shared by all the processes on a system, and unless the sharing is carefully controlled, one process can interfere with the security goals of another process. Other built-in goals relate to services that the operating system offers, such as file systems, memory management, and interprocess communications. If these services are not carefully controlled, processes can subvert the system’s security goals.\n\n\nClearly, a lot of system security is going to be related to process handling. If the operating system can maintain a clean separation of processes that can only be broken with the operating system’s help, then neither shared hardware nor operating system services can be used to subvert our security goals. That requirement implies that the operating system needs to be careful about allowing use of hardware and of its services. In many cases, the operating system has good opportunities to apply such caution. For example, the operating system controls virtual memory, which in turn completely controls which physical memory addresses each process can access. Hardware support prevents a process from even naming a physical memory address that is not mapped into its virtual memory space. (The software folks among us should remember to regularly thank the hardware folks for all the great stuff they’ve given us to work with.)\n\n\nSystem calls offer the operating system another opportunity to provide protection. In most operating systems, processes access system services by making an explicit system call, as was discussed in earlier chap-\n\n\n**TIP: BE CAREFUL OF THE WEAKEST LINK**\nIt's worthwhile to remember that the people attacking your systems share many characteristics with you. In particular, they're probably pretty smart and they probably are kind of lazy, in the positive sense that they don't do work that they don't need to do. That implies that attackers tend to go for the easiest possible way to overcome your system's security. They're not going to search for a zero-day buffer overflow if you've chosen \"password\" as your password to access the system.\n\n\nThe practical implication for you is that you should spend most of the time you devote to securing your system to identifying and strengthening your weakest link. Your weakest link is the least protected part of your system, the one that's easiest to attack, the one you can't hide away or augment with some external security system. Often, a running system's weakest link is actually its human users, not its software. You will have a hard time changing the behavior of people, but you can design the software bearing in mind that attackers may try to fool the legitimate users into misusing it. Remember that principle of least privilege? If an attacker can fool a user who has complete privileges into misusing the system, it will be a lot worse than fooling a user who can only damage his own assets.\n\n\nGenerally, thinking about security is a bit different than thinking about many other system design issues. It's more adversarial. If you want to learn more about good ways to think about security of the systems you build, check out Schneier's book \"Secrets and Lies\" [SC00].\n\n\nters. As you have learned, system calls switch the execution mode from the processor's user mode to its supervisor mode, invoking an appropriate piece of operating system code as they do so. That code can determine which process made the system call and what service the process requested. Earlier, we only talked about how this could allow the operating system to call the proper piece of system code to perform the service, and to keep track of who to return control to when the service had been completed. But the same mechanism gives the operating system the opportunity to check if the requested service should be allowed under the system's security policy. Since access to peripheral devices is through device drivers, which are usually also accessed via system call, the same mechanism can ensure proper application of security policies for hardware access.\n\n\nWhen a process performs a system call, then, the operating system will use the process identifier in the process control block or similar structure to determine the identity of the process. The OS can then use\n   **access control mechanisms**\n   to decide if the identified process is\n   **authorized**\n   to perform the requested action. If so, the OS either performs the action itself on behalf of the process or arranges for the process to perform it without\n\n\nfurther system intervention. If the process is not authorized, the OS can simply generate an error code for the system call and return control to the process, if the scheduling algorithm permits."
        }
      ]
    },
    {
      "name": "Authentication",
      "sections": [
        {
          "name": "Introduction",
          "content": "Given that we need to deal with a wide range of security goals and security policies that are meant to achieve those goals, what do we need from our operating system? Operating systems provide services for processes, and some of those services have security implications. Clearly, the operating system needs to be careful in such cases to do the right thing, security-wise. But the reason operating system services are allowed at all is that sometimes they need to be done, so any service that the operating system might be able to perform probably should be performed – under the right circumstances.\n\n\nContext will be everything in operating system decisions on whether to perform some service or to refuse to do so because it will compromise security goals. Perhaps the most important element of that context is who's doing the asking. In the real world, if your significant other asks you to pick up a gallon of milk at the store on the way home, you'll probably do so, while if a stranger on the street asks the same thing, you probably won't. In an operating system context, if the system administrator asks the operating system to install a new program, it probably should, while if a script downloaded from a random web page asks to install a new program, the operating system should take more care before performing the installation. In computer security discussions, we often refer to the party asking for something as the\n   **principal**\n   . Principals are security-meaningful entities that can request access to resources, such as human users, groups of users, or complex software systems.\n\n\nSo knowing who is requesting an operating system service is crucial in meeting your security goals. How does the operating system know that? Let's work a bit backwards here to figure it out.\n\n\nOperating system services are most commonly requested by system calls made by particular processes, which trap from user code into the operating system. The operating system then takes control and performs some service in response to the system call. Associated with the calling process is the OS-controlled data structure that describes the process, so\n\n\nthe operating system can check that data structure to determine the identity of the process. Based on that identity, the operating system now has the opportunity to make a policy-based decision on whether to perform the requested operation. In computer security discussions, the process or other active computing entity performing the request on behalf of a principal is often called its\n   **agent**\n   .\n\n\nThe request is for access to some particular resource, which we frequently refer to as the\n   **object**\n   of the access request\n   \n    1\n   \n   . Either the operating system has already determined this agent process can access the object or it hasn't. If it has determined that the process is permitted access, the OS can remember that decision and it's merely a matter of keeping track, presumably in some per-process data structure like the PCB, of that fact. For example, as we discovered when investigating virtualization of memory, per-process data structures like page tables show which pages and page frames can be accessed by a process at any given time. Any form of data created and managed by the operating system that keeps track of such access decisions for future reference is often called a\n   **credential**\n   .\n\n\nIf the operating system has not already produced a credential showing that an agent process can access a particular object, however, it needs information about the identity of the process's principal to determine if its request should be granted. Different operating systems have used different types of identity for principals. For instance, most operating systems have a notion of a user identity, where the user is, typically, some human being. (The concept of a user has been expanded over the years to increase its power, as we'll see later.) So perhaps all processes run by a particular person will have the same identity associated with them. Another common type of identity is a group of users. In a manufacturing company, you might want to give all your salespersons access to your inventory information, so they can determine how many widgets and whizz-bangs you have in the warehouse, while it wouldn't be necessary for your human resources personnel to have access to that information\n   \n    2\n   \n   . Yet another form of identity is the program that the process is running. Recall that a process is a running version of a program. In some systems (such as the Android Operating System), you can grant certain privileges to particular programs. Whenever they run, they can use these privileges, but other programs cannot.\n\n\nRegardless of the kind of identity we use to make our security decisions, we must have some way of attaching that identity to a particular process. Clearly, this attachment is a crucial security issue. If you\n\n\n1\n   \n   Another computer science overloading of the word \"object.\" Here, it does not refer to \"object oriented,\" but to the more general concept of a specific resource with boundaries and behaviors, such as a file or an IPC channel.\n\n\n2\n   \n   Remember the principle of least privilege from the previous chapter? Here's an example of using it. A rogue human services employee won't be able to order your warehouse emptied of pop-doodles if you haven't given such employees the right to do so. As you read through the security chapters of this book, keep your eyes out for other applications of the security principles we discussed earlier.\n\n\nmisidentify a programmer employee process as an accounting department employee process, you could end up with an empty bank account. (Not to mention needing to hire a new programmer.) Or if you fail to identify your company president correctly when he or she is trying to give an important presentation to investors, you may find yourself out of a job once the company determines that you're the one who derailed the next round of startup capital, because the system didn't allow the president to access the presentation that would have bowled over some potential investors.\n\n\nOn the other hand, since everything except the operating system's own activities are performed by some process, if we can get this right for processes, we can be pretty sure we will have the opportunity to check our policy on every important action. But we need to bear in mind one other important characteristic of operating systems' usual approach to authentication: once a principal has been authenticated, systems will almost always rely on that authentication decision for at least the lifetime of the process. This characteristic puts a high premium on getting it right. Mistakes won't be readily corrected. Which leads to the crux:\n\n\n\n\n**CRUX: HOW TO SECURELY IDENTIFY PROCESSES**\n\n\nFor systems that support processes belonging to multiple principals, how can we be sure that each process has the correct identity attached? As new processes are created, how can we be sure the new process has the correct identity? How can we be sure that malicious entities cannot improperly change the identity of a process?"
        },
        {
          "name": "Attaching Identities To Processes",
          "content": "Where do processes come from? Usually they are created by other processes. One simple way to attach an identity to a new process, then, is to copy the identity of the process that created it. The child inherits the parent's identity. Mechanically, when the operating system services a call from old process A to create new process B (\n   \n    fork\n   \n   , for example), it consults A's process control block to determine A's identity, creates a new process control block for B, and copies in A's identity. Simple, no?\n\n\nThat's all well and good if all processes always have the same identity. We can create a primal process when our operating system boots, perhaps assigning it some special system identity not assigned to any human user. All other processes are its descendants and all of them inherit that single identity. But if there really is only one identity, we're not going to be able to implement any policy that differentiates the privileges of one process versus another.\n\n\nWe must arrange that some processes have different identities and use those differences to manage our security policies. Consider a multi-user system. We can assign identities to processes based on which human user they belong to. If our security policies are primarily about some people\n\n\nbeing allowed to do some things and others not being allowed to, we now have an idea of how we can go about making our decisions.\n\n\nIf processes have a security-relevant identity, like a user ID, we're going to have to set the proper user ID for a new process. In most systems, a user has a process that he or she works with ordinarily: the shell process in command line systems, the window manager process in window-oriented system – you had figured out that both of these had to be processes themselves, right? So when you type a command into a shell or double click on an icon to start a process in a windowing system, you are asking the operating system to start a new process under your identity.\n\n\nGreat! But we do have another issue to deal with. How did that shell or window manager get your identity attached to itself? Here's where a little operating system privilege comes in handy. When a user first starts interacting with a system, the operating system can start a process up for that user. Since the operating system can fiddle with its own data structures, like the process control block, it can set the new process's ownership to the user who just joined the system.\n\n\nAgain, well and good, but how did the operating system determine the user's identity so it could set process ownership properly? You probably can guess the answer - the user logged in, implying that the user provided identity information to the OS proving who the user was. We've now identified a new requirement for the operating system: it must be able to query identity from human users and verify that they are who they claim to be, so we can attach reliable identities to processes, so we can use those identities to implement our security policies. One thing tends to lead to another in operating systems.\n\n\nSo how does the OS do that? As should be clear, we're building a towering security structure with unforeseeable implications based on the OS making the right decision here, so it's important. What are our options?"
        },
        {
          "name": "How To Authenticate Users?",
          "content": "*So this human being walks up to a computer...*\n\n\nAssuming we leave aside the possibilities for jokes, what can be done to allow the system to determine who this person is, with reasonable accuracy? First, if the person is not an authorized user of the system at all, we should totally reject this attempt to sneak in. Second, if he or she is an authorized user, we need to determine, which one?\n\n\nClassically, authenticating the identity of human beings has worked in one of three ways:\n\n\n  * • Authentication based on what you know\n  * • Authentication based on what you have\n  * • Authentication based on what you are\n\n\nWhen we say \"classically\" here, we mean \"classically\" in the, well, classical sense. Classically as in going back to the ancient Greeks and\n\n\nRomans. For example, Polybius, writing in the second century B.C., describes how the Roman army used “watchwords” to distinguish friends from foes [P-46], an example of authentication based on what you know. A Roman architect named Celer wrote a letter of recommendation (which still survives) for one of his slaves to be given to an imperial procurator at some time in the 2nd century AD [C100] – authentication based on what the slave had. Even further back, in (literally) Biblical times, the Gileadites required refugees after a battle to say the word “shibboleth,” since the enemies they sought (the Ephraimites) could not properly pronounce that word [JB-500]. This was a form of authentication by what you are: a native speaker of the Gileadites’ dialect or of the Ephraimite dialect.\n\n\nHaving established the antiquity of these methods of authentication, let’s leap past several centuries of history to the Computer Era to discuss how we use them in the context of computer authentication."
        },
        {
          "name": "Authentication By What You Know",
          "content": "Authentication by what you know is most commonly performed by using passwords. Passwords have a long (and largely inglorious) history in computer security, going back at least to the CTSS system at MIT in the early 1960s [MT79]. A password is a secret known only to the party to be authenticated. By divulging the secret to the computer’s operating system when attempting to log in, the party proves their identity. (You should be wondering about whether that implies that the system must also know the password, and what further implications that might have. We’ll get to that.) The effectiveness of this form of authentication depends, obviously, on several factors. We’re assuming other people don’t know the party’s password. If they do, the system gets fooled. We’re assuming that no one else can guess it, either. And, of course, that the party in question must know (and remember) it.\n\n\nLet’s deal with the problem of other people knowing a password first. Leaving aside guessing, how could they know it? Someone who already knows it might let it slip, so the fewer parties who have to know it, the fewer parties we have to worry about. The person we’re trying to authenticate has to know it, of course, since we’re authenticating this person based on the person knowing it. We really don’t want anyone else to be able to authenticate as that person to our system, so we’d prefer no third parties know the password. Thinking broadly about what a “third party” means here, that also implies the user shouldn’t write the password down on a slip of paper, since anyone who steals the paper now knows the password. But there’s one more party who would seem to need to know the password: our system itself. That suggests another possible vulnerability, since the system’s copy of our password might leak out\n   \n    3\n   \n   .\n\n\n3\n   \n   “Might” is too weak a word. The first known incident of such stored passwords leaking is from 1962 [MT79]; such leaks happen to this day with depressing regularity and much larger scope. [KA16] discusses a leak of over 100 million passwords stored in usable form.\n\n\n**TIP: AVOID STORING SECRETS**\nStoring secrets like plaintext passwords or cryptographic keys is a hazardous business, since the secrets usually leak out. Protect your system by not storing them if you don't need to. If you do need to, store them in a hashed form using a strong cryptographic hash. If you can't do that, encrypt them with a secure cipher. (Perhaps you're complaining to yourself that we haven't told you about those yet. Be patient.) Store them in as few places, with as few copies, as possible. Don't forget temporary editor files, backups, logs, and the like, since the secrets may be there, too. Remember that anything you embed into an executable you give to others will not remain secret, so it's particularly dangerous to store secrets in executables. In some cases, even secrets only kept in the heap of an executing program have been divulged, so avoid storing and keeping secrets even in running programs.\n\n\nInterestingly enough, though, our system does not actually need to know the password. Think carefully about what the system is doing when it checks the password the user provides. It's checking to see if the user knows it, not what that password actually is. So if the user provides us the password, but we don't know the password, how on earth could our system do that?\n\n\nYou already know the answer, or at least you'll slap your forehead and say \"I should have thought of that\" once you hear it. Store a\n   **hash**\n   of the password, not the password itself. When the user provides you with what he or she claims to be the password, hash the claim and compare it to the stored hashed value. If it matches, you believe he or she knows the password. If it doesn't, you don't. Simple, no? And now your system doesn't need to store the actual password. That means if you're not too careful with how you store the authentication information, you haven't actually lost the passwords, just their hashes. By their nature, you can't reverse hashing algorithms, so the adversary can't use the stolen hash to obtain the password. If the attacker provides the hash, instead of the password, the hash itself gets hashed by the system, and a hash of a hash won't match the hash.\n\n\nThere is a little more to it than that. The benefit we're getting by storing a hash of the password is that if the stored copy is leaked to an attacker, the attacker doesn't know the passwords themselves. But it's not quite enough just to store something different from the password. We also want to ensure that whatever we store offers an attacker no help in guessing what the password is. If an attacker steals the hashed password, he or she should not be able to analyze the hash to get any clues about the password itself. There is a special class of hashing algorithms called\n   **cryptographic hashes**\n   that make it infeasible to use the hash to figure out what the password is, other than by actually passing a guess at the password through the hashing algorithm. One unfortunate characteris-\n\n\ntic of cryptographic hashes is that they're hard to design, so even smart people shouldn't try. They use ones created by experts. That's what modern systems should do with password hashing: use a cryptographic hash that has been thoroughly studied and has no known flaws. At any given time, which cryptographic hashing algorithms meet those requirements may vary. At the time of this writing, SHA-3 [B+09] is the US standard for cryptographic hash algorithms, and is a good choice.\n\n\nLet's move on to the other problem: guessing. Can an attacker who wants to pose as a user simply guess the password? Consider the simplest possible password: a single bit, valued 0 or 1. If your password is a single bit long, then an attacker can try guessing \"0\" and have a 50/50 chances of being right. Even if wrong, if a second guess is allowed, the attacker now knows that the password is \"1\" and will correctly guess that.\n\n\nObviously, a one bit password is too easy to guess. How about an 8 bit password? Now there are 256 possible passwords you could choose. If the attacker guesses 256 times, sooner or later the guess will be right, taking 128 guesses (on average). Better than only having to guess twice, but still not good enough. It should be clear to you, at this point, that the length of the password is critical in being resistant to guessing. The longer the password, the harder to guess.\n\n\nBut there's another important factor, since we normally expect human beings to type in their passwords from keyboards or something similar. And given that we've already ruled out writing the password down somewhere as insecure, the person has to remember it. Early uses of passwords addressed this issue by restricting passwords to letters of the alphabet. While this made them easier to type and remember, it also cut down heavily on the number of bit patterns an attacker needed to guess to find someone's password, since all of the bit patterns that did not represent alphabetic characters would not appear in passwords. Over time, password systems have tended to expand the possible characters in a password, including upper and lower case letters, numbers, and special characters. The more possibilities, the harder to guess.\n\n\nSo we want long passwords composed of many different types of characters. But attackers know that people don't choose random strings of these types of characters as their passwords. They often choose names or familiar words, because those are easy to remember. Attackers trying to guess passwords will thus try lists of names and words before trying random strings of characters. This form of password guessing is called a\n   **dictionary attack**\n   , and it can be highly effective. The dictionary here isn't Webster's (or even the Oxford English Dictionary), but rather is a specialized list of words, names, meaningful strings of numbers (like \"123456\"), and other character patterns people tend to use for passwords, ordered by the probability that they will be chosen as the password. A good dictionary attack can figure out 90% of the passwords for a typical site [G13].\n\n\nIf you're smart in setting up your system, an attacker really should not be able to run a dictionary attack on a login process remotely. With any care at all, the attacker will not guess a user's password in the first five or\n\n\n\n\n**ASIDE: PASSWORD VAULTS**\n\n\nOne way you can avoid the problem of choosing passwords is to use what's called a password vault or key chain. This is an encrypted file kept on your computer that stores passwords. It's encrypted with a password of its own. To get passwords out of the vault, you must provide the password for the vault, reducing the problem of remembering a different password for every site to remembering one password. Also, it ensures that attackers can only use your passwords if they not only have the special password that opens the vault, but they have access to the vault itself. Of course, the benefits of securely storing passwords this way are limited to the strength of the passwords stored in the vault, since guessing and dictionary attacks will still work. Some password vaults will generate strong passwords for you – not very memorable ones, but that doesn't matter, since it's the vault that needs to remember it, not you. You can also find password vaults that store your passwords in the cloud. If you provide them with cleartext versions of your password to store them, however, you are sharing a password with another entity that doesn't really need to know it, thus taking a risk that perhaps you shouldn't take. If the cloud stores only your encrypted passwords, the risk is much lower.\n\n\nsix guesses (alas, sometimes no care is taken and the attacker will), and there's no good reason your system should allow a remote user to make 15,000 guesses at an account's password without getting it right. So by either shutting off access to an account when too many wrong guesses are made at its password, or (better) by drastically slowing down the process of password checking after a few wrong guesses (which makes a long dictionary attack take an infeasible amount of time), you can protect the account against such attacks.\n\n\nBut what if the attacker stole your password file? Since we assume you've been paying attention, it contains hashes of passwords, not passwords itself. But we also assume you paid attention when we told you to use a widely known cryptographic hash, and if you know it, so does the person who stole your password file. If the attacker obtained your hashed passwords, the hashing algorithm, a dictionary, and some compute power, the attacker can crank away at guessing your passwords at their leisure. Worse, if everyone used the same cryptographic hashing algorithm (which, in practice, they probably will), the attacker only needs to run each possible password through the hash once and store the results (essentially, the dictionary has been translated into hashed form). So when the attacker steals your password file, he or she would just need to do string comparisons to your hashed passwords and the newly created dictionary of hashed passwords, which is much faster.\n\n\nThere's a simple fix: before hashing a new password and storing it in your password file, generate a big random number (say 32 or 64 bits) and concatenate it to the password. Hash the result and store that. You also need to store that random number, since when the user tries to log\n\n\nin and provides the correct password, you'll need to take what the user provided, concatenate the stored random number, and run that through the hashing algorithm. Otherwise, the password hashed by itself won't match what you stored. You typically store the random number (which is called a\n   **salt**\n   ) in the password file right next to the hashed password. This concept was introduced in Robert Morris and Ken Thompson's early paper on password security [MT79].\n\n\nWhy does this help? The attacker can no longer create one translation of passwords in the dictionary to their hashes. What is needed is one translation for every possible salt, since the password files that were stolen are likely to have a different salt for every password. If the salt is 32 bits, that's\n   \n    2^{32}\n   \n   different translations for each word in the dictionary, which makes the approach of pre-computing the translations infeasible. Instead, for each entry in the stolen password file, the dictionary attack must freshly hash each guess with the password's salt. The attack is still feasible if you have chosen passwords badly, but it's not nearly as cheap. Any good system that uses passwords and cares about security stores cryptographically hashed and salted passwords. If yours doesn't, you're putting your users at risk.\n\n\nThere are other troubling issues for the use of passwords, but many of those are not particular to the OS, so we won't fling further mud at them here. Suffice it to say that there is a widely held belief in the computer security community that passwords are a technology of the past, and are no longer sufficiently secure for today's environments. At best, they can serve as one of several authentication mechanisms used in concert. This idea is called\n   **multi-factor authentication**\n   , with\n   **two-factor authentication**\n   being the version that gets the most publicity. You're perhaps already familiar with the concept: to get money out of an ATM, you need to know your personal identification number (PIN). That's essentially a password. But you also need to provide further evidence of your identity..."
        },
        {
          "name": "Authentication by What You Have",
          "content": "Most of us have probably been in some situation where we had an identity card that we needed to show to get us into somewhere. At least, we've probably all attended some event where admission depended on having a ticket for the event. Those are both examples of authentication based on what you have, an ID card or a ticket, in these cases.\n\n\nWhen authenticating yourself to an operating system, things are a bit different. In special cases, like the ATM mentioned above, the device (which has, after all, a computer inside – you knew that, right?) has special hardware to read our ATM card. That hardware allows it to determine that, yes, we have that card, thus providing the further proof to go along with your PIN. Most desktop computers, laptops, tablets, smart phones, and the like do not have that special hardware. So how can they tell what we have?\n\n\n**ASIDE: LINUX LOGIN PROCEDURES**\nLinux, in the tradition of earlier Unix systems, authenticates users based on passwords and then ties that identity to an initial process associated with the newly logged in user, much as described above. Here we will provide a more detailed step-by-step description of what actually goes on when a user steps up to a keyboard and tries to log in to a Unix system, as a solid example of how a real operating system handles this vital security issue.\n\n\n  * 1. A special login process running under a privileged system identity displays a prompt asking for the user to type in his or her identity, in the form of a generally short user name. The user types in a user name and hits carriage return. The name is echoed to the terminal.\n  * 2. The login process prompts for the user's password. The user types in the password, which is not echoed.\n  * 3. The login process looks up the name the user provided in the password file. If it is not found, the login process rejects the login attempt. If it is found, the login process determines the internal user identifier (a unique user ID number), the group (another unique ID number) that the user belongs to, the initial command shell that should be provided to this user once login is complete, and the home directory that shell should be started in. Also, the login process finds the salt and the salted, hashed version of the correct password for this user, which are permanently stored in a secure place in the system.\n  * 4. The login process combines the salt for the user's password and the password provided by the user and performs the hash on the combination. It compares the result to the stored version obtained in the previous step. If they do not match, the login process rejects the login attempt.\n  * 5. If they do match, fork a process. Set the user and group of the forked process to the values determined earlier, which the privileged identity of the login process is permitted to do. Change directory to the user's home directory and exec the shell process associated with this user (both the directory name and the type of shell were determined in step 3).\n\n\nThere are some other details associated with ensuring that we can log in another user on the same terminal after this one logs out that we don't go into here.\n\n\nNote that in steps 3 and 4, login can fail either because the user name is not present in the system or because the password does not match the user name. Linux and most other systems do not indicate which condition failed, if one of them did. This choice prevents attackers from learning the names of legitimate users of the system just by typing in guesses, since they cannot know if they guessed a non-existent name or guessed the wrong password for a legitimate user name. Not providing useful information to non-authenticated users is generally a good security idea that has applicability in other types of systems.\n\n\nThink a bit about why Linux's login procedure chooses to echo the typed user name when it doesn't echo the password. Is there no security disadvantage to echoing the user name, is it absolutely necessary to echo the user name, or is it a tradeoff of security for convenience? Why not echo the password?\n\n\nIf we have something that plugs into one of the ports on a computer, such as a hardware token that uses USB, then, with suitable software support, the operating system can tell whether the user trying to log in has the proper device or not. Some security tokens (sometimes called\n   **dongles**\n   , an unfortunate choice of name) are designed to work that way.\n\n\nIn other cases, since we're trying to authenticate a human user anyway, we make use of the person's capabilities to transfer information from whatever it is he or she has to the system where the authentication is required. For example, some smart tokens display a number or character string on a tiny built-in screen. The human user types the information read off that screen into the computer's keyboard. The operating system does not get direct proof that the user has the device, but if only someone with access to the device could know what information was supposed to be typed in, the evidence is nearly as good.\n\n\nThese kinds of devices rely on frequent changes of whatever information the device passes (directly or indirectly) to the operating system, perhaps every few seconds, perhaps every time the user tries to authenticate himself or herself. Why? Well, if it doesn't, anyone who can learn the static information from the device no longer needs the device to pose as the user. The authentication mechanism has been converted from \"something you have\" to \"something you know,\" and its security now depends on how hard it is for an attacker to learn that secret.\n\n\nOne weak point for all forms of authentication based on what you have is, what if you don't have it? What if you left your smartphone on your dresser bureau this morning? What if your dongle slipped out of your pocket on your commute to work? What if a subtle pickpocket brushed up against you at the coffee shop and made off with your secret authentication device? You now have a two-fold problem. First, you don't have the magic item you need to authenticate yourself to the operating system. You can whine at your computer all you want, but it won't care. It will continue to insist that you produce the magic item you lost. Second, someone else has your magic item, and possibly they can pretend to be you, fooling the operating system that was relying on authentication by what you have. Note that the multi-factor authentication we mentioned earlier can save your bacon here, too. If the thief stole your security token, but doesn't know your password, the thief will still have to guess that before they can pose as you\n   \n    4\n   \n   .\n\n\nIf you study system security in practice for very long, you'll find that there's a significant gap between what academics (like me) tell you is safe and what happens in the real world. Part of this gap is because the real world needs to deal with real issues, like user convenience. Part of it is because security academics have a tendency to denigrate anything where they can think of a way to subvert it, even if that way is not itself particularly practical. One example in the realm of authentication mechanisms\n\n\n4\n   \n   Assuming, of course, you haven't written the password with a Sharpie onto the back of the smart card the thief stole. Well, it seemed like a good idea at the time...\n\n\nbased on what you have is authenticating a user to a system by sending a text message to the user's cell phone. The user then types a message into the computer. Thinking about this in theory, it sounds very weak. In addition to the danger of losing the phone, security experts like to think about exotic attacks where the text message is misdirected to the attacker's phone, allowing the attacker to provide the secret information from the text message to the computer.\n\n\nIn practice, people usually have their phone with them and take reasonable care not to lose it. If they do lose it, they notice that quickly and take equally quick action to fix their problem. So there is likely to be a relatively small window of time between when your phone is lost and when systems learn that they can't authenticate you using that phone. Also in practice, redirecting text messages sent to cell phones is possible, but far from trivial. The effort involved is likely to outweigh any benefit the attacker would get from fooling the authentication system, at least in the vast majority of cases. So a mechanism that causes security purists to avert their gazes in horror in actual use provides quite reasonable security\n   \n    5\n   \n   . Keep this lesson in mind. Even if it isn't on the test\n   \n    6\n   \n   , it may come in handy some time in your later career."
        },
        {
          "name": "Authentication by What You Are",
          "content": "If you don't like methods like passwords and you don't like having to hand out smart cards or security tokens to your users, there is another option. Human beings (who are what we're talking about authenticating here) are unique creatures with physical characteristics that differ from all others, sometimes in subtle ways, sometimes in obvious ones. In addition to properties of the human body (from DNA at the base up to the appearance of our face at the top), there are characteristics of human behavior that are unique, or at least not shared by very many others. This observation suggests that if our operating system can only accurately measure these properties or characteristics, it can distinguish one person from another, solving our authentication problem.\n\n\nThis approach is very attractive to many people, most especially to those who have never tried to make it work. Going from the basic observation to a working, reliable authentication system is far from easy. But it can be made to work, to much the same extent as the other authentication mechanisms. We can use it, but it won't be perfect, and has its own set of problems and challenges.\n\n\n5\n   \n   However, in 2016 the United States National Institute of Standards and Technology issued draft guidance deprecating the use of this technique for two-factor authentication, at least in some circumstances. Here's another security lesson: what works today might not work tomorrow.\n\n\n6\n   \n   We don't know about you, but every time the word \"test\" or \"quiz\" or \"exam\" comes up, our heart skips a beat or two. Too many years of being a student will do this to a person.\n\n\nRemember that we're talking about a computer program (either the OS itself or some separate program it invokes for the purpose) measuring a human characteristic and determining if it belongs to a particular person. Think about what that entails. What if we plan to use facial recognition with the camera on a smart phone to authenticate the owner of the phone? If we decide it's the right person, we allow whoever we took the picture of to use the phone. If not, we give them the raspberry (in the cyber sense) and keep them out.\n\n\nYou should have identified a few challenges here. First, the camera is going to take a picture of someone who is, presumably, holding the phone. Maybe it's the owner, maybe it isn't. That's the point of taking the picture. If it isn't, we should assume whoever it is would like to fool us into thinking that they are the actual owner. What if it's someone who looks a lot like the right user, but isn't? What if the person is wearing a mask? What if the person holds up a photo of the right user, instead of their own face? What if the lighting is dim, or the person isn't fully facing the camera? Alternately, what if it is the right user and the person is not facing the camera, or the lighting is dim, or something else has changed about the person's look? (e.g., hairstyle)\n\n\nComputer programs don't recognize faces the way people do. They do what programs always do with data: they convert it to zeros and ones and process it using some algorithm. So that \"photo\" you took is actually a collection of numbers, indicating shadow and light, shades of color, contrasts, and the like. OK, now what? Time to decide if it's the right person's photo or not! How?\n\n\nIf it were a password, we could have stored the right password (or, better, a hash of the right password) and done a comparison of what got typed in (or its hash) to what we stored. If it's a perfect match, authenticate. Otherwise, don't. Can we do the same with this collection of zeros and ones that represent the picture we just took? Can we have a picture of the right user stored permanently in some file (also in the form of zeros and ones) and compare the data from the camera to that file?\n\n\nProbably not in the same way we compared the passwords. Consider one of those factors we just mentioned above: lighting. If the picture we stored in the file was taken under bright lights and the picture coming out of the camera was taken under dim lights, the two sets of zeros and ones are most certainly not going to match. In fact, it's quite unlikely that two pictures of the same person, taken a second apart under identical conditions, would be represented by exactly the same set of bits. So clearly we can't do a comparison based on bit-for-bit equivalence.\n\n\nInstead, we need to compare based on a higher-level analysis of the two photos, the stored one of the right user and the just-taken one of the person who claims to be that user. Generally this will involve extracting higher-level features from the photos and comparing those. We might, for example, try to calculate the length of the nose, or determine the color of the eyes, or make some kind of model of the shape of the mouth. Then we would compare the same feature set from the two photos.\n\n\n\n\n![A graph showing the relationship between False Positive Rate and False Negative Rate. The y-axis is labeled 'Errors' and the x-axis is labeled 'Sensitivity'. A blue curve represents the False Positive Rate, which decreases as sensitivity increases. An orange curve represents the False Negative Rate, which increases as sensitivity increases. The two curves intersect at a point marked with a black circle, representing the crossover error rate.](images/image_0140.jpeg)\n\n\nThe figure is a line graph with 'Errors' on the vertical axis and 'Sensitivity' on the horizontal axis. A blue curve, labeled 'False Positive Rate', starts at a high error rate on the y-axis when sensitivity is low and curves downwards towards the x-axis as sensitivity increases. An orange curve, labeled 'False Negative Rate', starts at a low error rate on the y-axis when sensitivity is low and curves upwards towards the y-axis as sensitivity increases. The two curves intersect at a single point, which is highlighted with a black circle.\n\n\nA graph showing the relationship between False Positive Rate and False Negative Rate. The y-axis is labeled 'Errors' and the x-axis is labeled 'Sensitivity'. A blue curve represents the False Positive Rate, which decreases as sensitivity increases. An orange curve represents the False Negative Rate, which increases as sensitivity increases. The two curves intersect at a point marked with a black circle, representing the crossover error rate.\n\n\nFigure 54.1:\n   **Crossover Error Rate**\n\n\nEven here, though, an exact match is not too likely. The lighting, for example, might slightly alter the perceived eye color. So we'll need to allow some sloppiness in our comparison. If the feature match is “close enough,” we authenticate. If not, we don't. We will look for close matches, not perfect matches, which brings the nose of the camel of tolerances into our authentication tent. If we are intolerant of all but the closest matches, on some days we will fail to match the real user's picture to the stored version. That's called a\n   **false negative**\n   , since we incorrectly decided not to authenticate. If we are too tolerant of differences in measured versus stored data, we will authenticate a user whom is not who they claim to be. That's a\n   **false positive**\n   , since we incorrectly decided to authenticate.\n\n\nThe nature of biometrics is that any implementation will have a characteristic false positive and false negative rate. Both are bad, so you'd like both to be low. For any given implementation of some biometric authentication technique, you can typically tune it to achieve some false positive rate, or tune it to achieve some false negative rate. But you usually can't minimize both. As the false positive rate goes down, the false negative rate goes up, and vice versa. The\n   **sensitivity**\n   describes how close the match must be.\n\n\nFigure 54.1 shows the typical relationship between these error rates. Note the circle at the point where the two curves cross. That point represents the crossover error rate, a common metric for describing the accuracy of a biometric. It represents an equal tradeoff between the two kinds of errors. It's not always the case that one tunes a biometric system to hit the crossover error rate, since you might care more about one kind of error than the other. For example, a smart phone that frequently locks its legitimate user out because it doesn't like today's fingerprint reading is not going to be popular, while the chances of a thief who stole the phone having a similar fingerprint are low. Perhaps low false negatives matter\n\n\nmore here. On the other hand, if you're opening a bank vault with a retinal scan, requiring the bank manager to occasionally provide a second scan isn't too bad, while allowing a robber to open the vault with a bogus fake eye would be a disaster. Low false positives might be better here.\n\n\nLeaving aside the issues of reliability of authentication using biometrics, another big issue for using human characteristics to authenticate is that many of the techniques for measuring them require special hardware not likely to be present on most machines. Many computers (including smart phones, tablets, and laptops) are likely to have cameras, but embedded devices and server machines probably don't. Relatively few machines have fingerprint readers, and even fewer are able to measure more exotic biometrics. While a few biometric techniques (such as measuring typing patterns) require relatively common hardware that is likely to be present on many machines anyway, there aren't many such techniques. Even if a special hardware device is available, the convenience of using them for this purpose can be limiting.\n\n\nOne further issue you want to think about when considering using biometric authentication is whether there is any physical gap between where the biometric quantity is measured and where it is checked. In particular, checking biometric readings provided by an untrusted machine across the network is hazardous. What comes in across the network is simply a pattern of bits spread across one or more messages, whether it represents a piece of a web page, a phoneme in a VoIP conversation, or part of a scanned fingerprint. Bits are bits, and anyone can create any bit pattern they want. If a remote adversary knows what the bit pattern representing your fingerprint looks like, they may not need your finger, or even a fingerprint scanner, to create it and feed it to your machine. When the hardware performing the scanning is physically attached to your machine, there is less opportunity to slip in a spurious bit pattern that didn't come from the device. When the hardware is on the other side of the world on a machine you have no control over, there is a lot more opportunity. The point here is to be careful with biometric authentication information provided to you remotely.\n\n\nIn all, it sort of sounds like biometrics are pretty terrible for authentication, but that's the wrong lesson. For that matter, previous sections probably made it sound like all methods of authentication are terrible. Certainly none of them are perfect, but your task as a system designer is not to find the perfect authentication mechanism, but to use mechanisms that are well suited to your system and its environment. A good fingerprint reader built in to a smart phone might do its job quite well. A long, unguessable password can provide a decent amount of security. Well-designed smart cards can make it nearly impossible to authenticate yourself without having them in your hand. And where each type of mechanism fails, you can perhaps correct for that failure by using a second or third authentication mechanism that doesn't fail in the same cases."
        },
        {
          "name": "Authenticating Non-Humans",
          "content": "No, we're not talking about aliens or extra-dimensional beings, or even your cat. If you think broadly about how computers are used today, you'll see that there are many circumstances in which no human user is associated with a process that's running. Consider a web server. There really isn't some human user logged in whose identity should be attached to the web server. Or think about embedded devices, such as a smart light bulb. Nobody logs in to a light bulb, but there is certainly code running there, and quite likely it is process-oriented code.\n\n\nMechanically, the operating system need not have a problem with the identities of such processes. Simply set up a user called\n   \n    webserver\n   \n   or\n   \n    lightbulb\n   \n   on the system in question and attach the identity of that \"user\" to the processes that are associated with running the web server or turning the light bulb on and off. But that does lead to the question of how you make sure that only real web server processes are tagged with that identity. We wouldn't want some arbitrary user on the web server machine creating processes that appear to belong to the server, rather than to that user.\n\n\nOne approach is to use passwords for these non-human users, as well. Simply assign a password to the web server user. When does it get used? When it's needed, which is when you want to create a process belonging to the web server, but you don't already have one in existence. The system administrator could log in as the web server user, creating a command shell and using it to generate the actual processes the server needs to do its business. As usual, the processes created by this shell process would inherit their parent's identity,\n   \n    webserver\n   \n   , in this case. More commonly, we skip the go-between (here, the login) and provide some mechanism whereby the privileged user is permitted to create processes that belongs not to that user, but to some other user such as\n   \n    webserver\n   \n   . Alternately, we can provide a mechanism that allows a process to change its ownership, so the web server processes would start off under some other user's identity (such as the system administrator's) and change their ownership to\n   \n    webserver\n   \n   . Yet another approach is to allow a temporary change of process identity, while still remembering the original identity. (We'll say more about this last approach in a future chapter.) Obviously, any of these approaches require strong controls, since they allow one user to create processes belonging to another user.\n\n\nAs mentioned above, passwords are the most common authentication method used to determine if a process can be assigned to one of these non-human users. Sometimes no authentication of the non-human user is required at all, though. Instead, certain other users (like trusted system administrators) are given the right to assign new identities to the processes they create, without providing any further authentication information than their own. In Linux and other Unix systems, the\n   \n    sudo\n   \n   command offers this capability. For example, if you type the following:\n\n\nsudo -u webserver apache2\n\n\n**ASIDE: OTHER AUTHENTICATION POSSIBILITIES**\n\n\nUsually, what you know, what you have, and what you are cover the useful authentication possibilities, but sometimes there are other options. Consider going into the Department of Motor Vehicles to apply for a driver's license. You probably go up to a counter and talk to some employee behind that counter, perhaps giving the person a bunch of personal information, maybe even money to cover a fee for the license. Why on earth did you believe that person was actually a DMV employee who was able to get you a legitimate driver's license? You probably didn't know the person; you weren't shown an official ID card; the person didn't recite the secret DMV mantra that proved he or she was an initiate of that agency. You believed it because the person was standing behind a particular counter, which is the counter DMV employees stand behind. You authenticated the person based on location.\n\n\nOnce in a while, that approach can be handy in computer systems, most frequently in mobile or pervasive computing. If you're tempted to use it, think carefully about how you're obtaining the evidence that the subject really is in a particular place. It's actually fairly tricky.\n\n\nWhat else? Perhaps you can sometimes authenticate based on what someone does. If you're looking for personally characteristic behavior, like their typing pattern or delays between commands, that's a type of biometric. (Google introduced multi-factor authentication of this kind in its Android phones, for example.) But you might be less interested in authenticating exactly who they are versus authenticating that they belong to the set of Well Behaved Users. Many web sites, for example, care less about who their visitors are and more about whether they use the web site properly. In this case, you might authenticate their membership in the set by their ongoing interactions with your system.\n\n\nThis would indicate that the\n   \n    apache2\n   \n   program should be started under the identity of\n   \n    webserver\n   \n   , rather than under the identity of whoever ran the\n   \n    sudo\n   \n   command. This command might require the user running it to provide their own authentication credentials (for extra certainty that it really is the privileged user asking for it, and not some random visitor accessing the computer during the privileged user's coffee break), but would not require authentication information associated with\n   \n    webserver\n   \n   . Any sub-processes created by\n   \n    apache2\n   \n   would, of course, inherit the identity of\n   \n    webserver\n   \n   . We'll say more about\n   \n    sudo\n   \n   in the chapter on access control.\n\n\nOne final identity issue we alluded to earlier is that sometimes we wish to identify not just individual users, but groups of users who share common characteristics, usually security-related characteristics. For example, we might have four or five system administrators, any one of whom is allowed to start up the web server. Instead of associating the\n\n\nprivilege with each one individually, it's advantageous to create a system-meaningful group of users with that privilege. We would then indicate that the four or five administrators are members of that group. This kind of group is another example of a security-relevant principal, since we will make our decisions on the basis of group membership, rather than individual identity. When one of the system administrators wished to do something requiring group membership, we would check that he or she was a member. We can either associate a group membership with each process, or use the process's individual identity information as an index into a list of groups that people belong to. The latter is more flexible, since it allows us to put each user into an arbitrary number of groups.\n\n\nMost modern operating systems, including Linux and Windows, support these kinds of groups, since they provide ease and flexibility in dealing with application of security policies. They handle group membership and group privileges in manners largely analogous to those for individuals. For example, a child process will usually have the same group-related privileges as its parent. When working with such systems, it's important to remember that group membership provides a second path by which a user can obtain access to a resource, which has its benefits and its dangers."
        }
      ]
    },
    {
      "name": "Access Control",
      "sections": [
        {
          "name": "Introduction",
          "content": "So we know what our security goals are, we have at least a general sense of the security policies we'd like to enforce, and we have some evidence about who is requesting various system services that might (or might not) violate our policies. Now we need to take that information and turn it into something actionable, something that a piece of software can perform for us.\n\n\nThere are two important steps here:\n\n\n  * 1. Figure out if the request fits within our security policy.\n  * 2. If it does, perform the operation. If not, make sure it isn't done.\n\n\nThe first step is generally referred to as\n   **access control**\n   . We will determine which system resources or services can be accessed by which parties in which ways under which circumstances. Basically, it boils down to another of those binary decisions that fit so well into our computing paradigms: yes or no. But how to make that decision? To make the problem more concrete, consider this case. User X wishes to read and write file\n   \n    /var/foo\n   \n   . Under the covers, this case probably implies that a process being run under the identity of User X issued a system call such as:\n\n\nopen(\"/var/foo\", O_RDWR)\nNote here that we're not talking about the Linux\n   \n    open()\n   \n   call, which is a specific implementation that handles access control a specific way. We're talking about the general idea of how you might be able to control access to a file open system call. Hence the different font, to remind you.\n\n\nHow should the system handle this request from the process, making sure that the file is not opened if the security policy to be enforced forbids it, but equally making sure that the file is opened if the policy allows it? We know that the system call will trap to the operating system, giving it the opportunity to do something to make this decision. Mechanically speaking, what should that \"something\" be?\n\n\n**THE CRUX OF THE PROBLEM:\n    \n\n    HOW TO DETERMINE IF AN ACCESS REQUEST SHOULD BE GRANTED?**\n\n\nHow can the operating system decide if a particular request made by a particular process belonging to a particular user at some given moment should or should not be granted? What information will be used to make this decision? How can we set this information to encode the security policies we want to enforce for our system?"
        },
        {
          "name": "Important Aspects Of The Access Control Problem",
          "content": "As usual, the system will run some kind of algorithm to make this decision. It will take certain inputs and produce a binary output, a yes-or-no decision on granting access. At the high level, access control is usually spoken of in terms of\n   **subjects**\n   ,\n   **objects**\n   , and\n   **access**\n   . A subject is the entity that wants to perform the access, perhaps a user or a process. An object is the thing the subject wants to access, perhaps a file or a device. Access is some particular mode of dealing with the object, such as reading it or writing it. So an access control decision is about whether a particular subject is allowed to perform a particular mode of access on a particular object. We sometimes refer to the process of determining if a particular subject is allowed to perform a particular form of access on a particular\n   \n    1\n   \n   object as\n   **authorization**\n   .\n\n\nOne relevant issue is when will access control decisions be made? The system must run whatever algorithm it uses every time it makes such a decision. The code that implements this algorithm is called a\n   **reference monitor**\n   , and there is an obvious incentive to make sure it is implemented both correctly and efficiently. If it's not correct, you make the wrong access decisions – obviously bad. Its efficiency is important because it will inject some overhead whenever it is used. Perhaps we wish to minimize these overheads by not checking access control on every possible opportunity. On the other hand, remember that principle of complete mediation we introduced a couple of chapters back? That principle said we should check security conditions every time someone asked for something.\n\n\nClearly, we'll need to balance costs against security benefits. But if we can find some beneficial special cases where we can achieve low cost without compromising security, we can possibly manage to avoid trading off one for the other, at least in those cases.\n\n\nOne way to do so is to give subjects objects that belong only to them. If the object is inherently theirs, by its very nature and unchangeably so, the system can let the subject (a process, in the operating system case) ac-\n\n\n1\n   \n   Wow. You know how hard it is to get so many instances of the word “particular” to line up like this? It's a column of particulars! But, perhaps, not particularly interesting.\n\n\ncess it freely. Virtualization allows us to create virtual objects of this kind. Virtual memory is an excellent example. A process is allowed to access its virtual memory freely\n   \n    2\n   \n   , with no special operating system access control check at the moment the process tries to use it. A good thing, too, since otherwise we would need to run our access control algorithm on every process memory reference, which would lead to a ridiculously slow system. We can play similar virtualization tricks with peripheral devices. If a process is given access to some virtual device, which is actually backed up by a real physical device controlled by the OS, and if no other process is allowed to use that device, the operating system need not check for access control every time the process wants to use it. For example, a process might be granted control of a GPU based on an initial access control decision, after which the process can write to the GPU's memory or issue instructions directly to it without further intervention by the OS.\n\n\nOf course, as discussed earlier, virtualization is mostly an operating-system provided illusion. Processes share memory, devices, and other computing resources. What appears to be theirs alone is actually shared, with the operating system running around behind the scenes to keep the illusion going, sometimes assisted by special hardware. That means the operating system, without the direct knowledge and participation of the applications using the virtualized resource, still has to make sure that only proper forms of access to it are allowed. So merely relying on virtualization to ensure proper access just pushes the problem down to protecting the virtualization functionality of the OS. Even if we leave that issue aside, sooner or later we have to move past cheap special cases and deal with the general problem. Subject X wants to read and write object /tmp/foo. Maybe it's allowable, maybe it isn't. Now what?\n\n\nComputer scientists have come up with two basic approaches to solving this question, relying on different data structures and different methods of making the decision. One is called\n   **access control lists**\n   and the other is called\n   **capabilities**\n   . It's actually a little inaccurate to claim that computer scientists came up with these approaches, since they've been in use in non-computer contexts for millennia. Let's look at them in a more general perspective before we consider operating system implementations.\n\n\nLet's say we want to start an exclusive nightclub (called, perhaps, Chez Andrea\n   \n    3\n   \n   ) restricted to only the best operating system researchers and developers. We don't want to let any of those database or programming language people slip in, so we'll need to make sure only our approved customers get through the door. How might we do that? One\n\n\n2\n   \n   Almost. Remember the bits in the page table that determine whether a particular page can be read, written, or executed? But it's not the operating system doing the runtime check here, it's the virtual memory hardware.\n\n\n3\n   \n   The authors Arpaci-Dusseau would like to note that author Reiher is in charge of these name choices for the security chapters, and did not strong-arm him into using their names throughout this and other examples. We now return you to your regular reading...\n\n\nway would be to hire a massive intimidating bouncer who has a list of all the approved members. When someone wants to enter the club, they would prove their identity to the bouncer, and the bouncer would see if they were on the list. If it was Linus Torvalds or Barbara Liskov, the bouncer would let them in, but would keep out the hoi polloi networking folks who had failed to distinguish themselves in operating systems.\n\n\nAnother approach would be to put a really great lock on the door of the club and hand out keys to that lock to all of our OS buddies. If Jerome Saltzer wanted to get in to Chez Andrea, he'd merely pull out his key and unlock the door. If some computer architects with no OS chops wanted to get in, they wouldn't have a key and thus would be stuck outside. Compared to the other approach, we'd save on the salary of the bouncer, though we would have to pay for the locks and keys\n   \n    4\n   \n   . As new luminaries in the OS field emerge who we want to admit, we'll need new keys for them, and once in a while we may make a mistake and hand out a key to someone who doesn't deserve it, or a member might lose a key, in which case we need to make sure that key no longer opens the club door.\n\n\nThe same ideas can be used in computer systems. Early computer scientists decided to call the approach that's kind of like locks and keys a\n   **capability-based system**\n   , while the approach based on the bouncer and the list of those to admit was called an\n   **access control list system**\n   . Capabilities are thus like keys, or tickets to a movie, or tokens that let you ride a subway. Access control lists are thus like, well, lists. How does this work in an operating system? If you're using capabilities, when a process belonging to user\n   *X*\n   wants to read and write file\n   \n    /tmp/foo\n   \n   , it hands a capability specific to that file to the system. (And precisely what, you may ask, is a capability in this context? Good question! We'll get to that.) If you're using access control lists (ACLs, for short), the system looks up user\n   *X*\n   on an ACL associated with\n   \n    /tmp/foo\n   \n   , only allowing the access if the user is on the list. In either case, the check can be made at the moment the access (an\n   \n    open()\n   \n   call, in our example) is requested. The check is made after trapping to the operating system, but before the access is actually permitted, with an early exit and error code returned if the access control check fails.\n\n\nAt a high level, these two options may not sound very different, but when you start thinking about the algorithm you'll need to run and the data structures required to support that algorithm, you'll quickly see that there are major differences. Let's walk through each in turn.\n\n\n4\n   \n   Note that for both access control lists and capabilities, we are assuming we've already authenticated the person trying to enter the club. If some nobody wearing a Linus Torvalds or Barbara Liskov mask gets past our bouncer, or if we aren't careful to determine that it really is Jerome Saltzer before handing a random person the key, we're not going to keep the riffraff out. Abandoning the cute analogy, absolutely the same issue applies in real computer systems, which is why the previous chapter discussed authentication in detail."
        },
        {
          "name": "Using ACLs For Access Control",
          "content": "What if, in the tradition of old British clubs, Chez Andrea gives each member his own private room, in addition to access to the library, the dining room, the billiard parlor, and other shared spaces? In this case, we need to ensure not just that only members get into the club at all, but that Ken Thompson (known to be a bit of a scamp [T84]) can't slip into Whitfield Diffie's room and short-sheet his bed. We could have one big access control list that specifies allowable access to every room, but that would get unmanageable. Instead, why not have one ACL for each room in the club?\n\n\nWe do the same thing with files in a typical OS that relies on ACLs for access control. Each file has its own access control list, resulting in simpler, shorter lists and quicker access control checks. So our\n   \n    open()\n   \n   call in an ACL system will examine a list for\n   \n    /tmp/foo\n   \n   , not an ACL encoding all accesses for every file in the system.\n\n\nWhen this\n   \n    open()\n   \n   call traps to the operating system, the OS consults the running process's PCB to determine who owns the process. That data structure indicates that user X owns the process. The system then must get hold of the access control list for\n   \n    /tmp/foo\n   \n   . This ACL is more file metadata, akin to the things we discussed in the chapter titled \"Files and Directories.\" So it's likely to be stored with or near the rest of the metadata for this file. Somehow, we obtain that list from persistent storage. We now look up X on the list. Either X is there or isn't. If not, no access for X. If yes, we'll typically go a step further to determine if the ACL entry for X allows the type of access being requested. In our example, X wanted to open\n   \n    /tmp/foo\n   \n   for read and write. Perhaps the ACL allows X to open that file for read, but not for write. In that case, the system will deny the access and return an error to the process.\n\n\nIn principle, this isn't too complicated, but remember the devil being in the details? He's still there. Consider some of those details. For example, where exactly is the ACL persistently stored? It really does need to be persistent for most resources, since the ACLs effectively encode our chosen security policy, which is probably not changing very often. So it's somewhere on the flash drive or disk. Unless it's cached, we'll need to read it off that device every time someone tries to open the file. In most file systems, as was discussed in the sections on persistence, you already need to perform several device reads to actually obtain any information from a file. Are we going to require another read to also get the ACL for the file? If so, where on the device do we put the ACL to ensure that it's quick to access? It would be best if it was close to, or even part of, something we're already reading, which suggests a few possible locations: the file's directory entry, the file's inode, or perhaps the first data block of the file. At the minimum, we want to have the ACL close to one of those locations, and it might be better if it was actually in one of them, such as the inode.\n\n\nThat leads to another vexing detail: how big is this list? If we do the\n\n\nobvious thing and create a list of actual user IDs and access modes, in principle the list could be of arbitrary size, up to the number of users known to the system. For some systems, that could be thousands of entries. But typically files belong to one user and are often available only to that user and perhaps a couple friends. So we wouldn't want to reserve enough space in every ACL for every possible user to be listed, since most users wouldn't appear in most ACLs. With some exceptions, of course: a lot of files should be available in some mode (perhaps read or execute) to all users. After all, commonly used executables (like\n   \n    ls\n   \n   and\n   \n    mv\n   \n   ) are stored in files, and we'll be applying access control to them, just like any other file. Our users will share the same font files, configuration files for networking, and so forth. We have to allow all users to access these files or they won't be able to do much of anything on the system.\n\n\nSo the obvious implementation would reserve a big per-file list that would be totally filled for some files and nearly empty for others. That's clearly wasteful. For the totally filled lists, there's another worrying detail: every time we want to check access in the list, we'll need to search it. Modern computers can search a list of a thousand entries rather quickly, but if we need to perform such searches all the time, we'll add a lot of undesirable overhead to our system. We could solve the problem with variable-sized access control lists, only allocating the space required for each list. Spend a few moments thinking about how you would fit that kind of metadata into the types of file systems we've studied, and the implications for performance.\n\n\nFortunately, in most circumstances we can benefit from a bit of legacy handed down to us from the original Bell Labs Unix system. Back in those primeval days when computer science giants roamed the Earth (or at least certain parts of New Jersey), persistent storage was in short supply and pretty expensive. There was simply no way they could afford to store large ACLs for each file. In fact, when they worked it out, they figured they could afford about nine bits for each file's ACL. Nine bits don't go far, but fortunately those early Unix designers had plenty of cleverness to make up for their lack of hardware. They thought about their problem and figured out that there were effectively three modes of access they cared about (read, write, and execute, for most files), and they could handle most security policies with only three entries on each access control list. Of course, if they were going to use one bit per access mode per entry, they would have already used up their nine bits, leaving no bits to specify who the entry pertained to. So they cleverly partitioned the entries on their access control list into three groups. One is the owner of the file, whose identity they had already stored in the inode. One is the members of a particular group or users; this group ID was also stored in the inode. The final one is everybody else, i.e., everybody who wasn't the owner or a member of his group. No need to use any bits to store that, since it was just the complement of the user and group.\n\n\nThis solution not only solved the problem of the amount of storage eaten up by ACLs, but also solved the problem of the cost of accessing\n\n\nand checking them. You already needed to access a file’s inode to do almost anything with it, so if the ACL was embedded in the inode, there would be no extra seeks and reads to obtain it. And instead of a search of an arbitrary sized list, a little simple logic on a few bits would provide the answer to the access control question. And that logic is still providing the answer in most systems that use Posix-compliant file systems to this very day. Of course, the approach has limitations, since it cannot express complex access modes and sharing relationships. For that reason, some modern systems (such as Windows) allow extensions that permit the use of more general ACLs, but many rely on the tried-and-true Unix-style nine-bit ACLs\n   \n    5\n   \n   .\n\n\nThere are some good features of ACLs and some limiting features. Good points first. First, what if you want to figure out who is allowed to access a resource? If you’re using ACLs, that’s an easy question to answer, since you can simply look at the ACL itself. Second, if you want to change the set of subjects who can access an object, you merely need to change the ACL, since nothing else can give the user access. Third, since the ACL is typically kept either with or near the file itself, if you can get to the file, you can get to all relevant access control information. This is particularly important in distributed systems, but it also has good performance implications for all systems, as long as your design keeps the ACL near the file or its inode.\n\n\nNow for the less desirable features. First, ACLs require you to solve problems we mentioned earlier: having to store the access control information somewhere near the file and dealing with potentially expensive searches of long lists. We described some practical solutions that work pretty well in most systems, but these solutions limit what ACLs can do. Second, what if you want to figure out the entire set of resources some principal (a process or a user) is permitted to access? You’ll need to check every single ACL in the system, since that principal might be on any of them. Third, in a distributed environment, you need to have a common view of identity across all the machines for ACLs to be effective. If a user on\n   \n    cs.ucla.edu\n   \n   wants to access a file stored on\n   \n    cs.wisconsin.edu\n   \n   , the Wisconsin machine is going to check some identity provided by UCLA against an access control list stored at Wisconsin. Does user\n   \n    remzi\n   \n   at UCLA actually refer to the same principal as user\n   \n    remzi\n   \n   at Wisconsin? If not, you may allow a remote user to access something he shouldn’t. But trying to maintain a consistent name space of users across multiple different computing domains is challenging.\n\n\n5\n   \n   The history is a bit more complicated than this. The CTSS system offered a more limited form of condensed ACL than Unix did [C+63], and the Multics system included the concept of groups in a more general access control list consisting of character string names of users and groups [S74]. Thus, the Unix approach was a cross-breeding of these even earlier systems.\n\n\n\n\n**ASIDE: NAME SPACES**\n\n\nWe just encountered one of the interesting and difficult problems in distributed systems: what do names mean on different machines? This\n   **name space**\n   problem is relatively easy on a single computer. If the name chosen for a new thing is already in use, don't allow it to be assigned. So when a particular name is issued on that system by any user or process, it means the same thing.\n   \n    /etc/password\n   \n   is the same file for you and for all the other users on your computer.\n\n\nBut what about distributed systems composed of multiple computers? If you want the same guarantee about unique names understood by all, you need to make sure someone on a machine at UCLA does not create a name already being used at the University of Wisconsin. How to do that? Different answers have different pluses and minuses. One approach is not to bother and to understand that the namespaces are different – that's what we do with process IDs, for example. Another approach is to require an authority to approve name selection – that's more or less how AFS handles file name creation. Another approach is to hand out portions of the name space to each participant and allow them to assign any name from that portion, but not any other name – that's how the World Wide Web and the IPv4 address space handle the issue. None of these answers are universally right or wrong. Design your name space for your needs, but understand the implications."
        },
        {
          "name": "Using Capabilities For Access Control",
          "content": "Access control lists are not your only option for controlling access in computer systems. Almost, but not quite. You can also use capabilities, the option that's more like keys or tickets. Chez Andrea could give keys to its members to allow admission. Different rooms could have different keys, preventing the more mischievous members from leaving little surprises in other members' rooms. Each member would carry around a set of keys that would admit him or her to the particular areas of the club she should have access to. Like ACLs, capabilities have a long history of use in computer systems, with Dennis and van Horn [DV64] being perhaps the earliest example. Wulf et al. [W+74] describe the Hydra Operating System, which used capabilities as a fundamental control mechanism. Levy [L84] gives a book-length summary of the use of capabilities in early hardware and software systems. In capability systems, a running process has some set of capabilities that specify its access permissions. If you're using a pure capability system, there is no ACL anywhere, and this set is the entire encoding of the access permissions for this process. That's not how Linux or Windows work, but other operating systems, such as Hydra, examined this approach to handling access control.\n\n\nHow would we perform that\n   \n    open()\n   \n   call in this kind of pure capabil-\n\n\nity system? When the call is made, either your application would provide a capability permitting your process to open the file in question as a parameter, or the operating system would find the capability for you. In either case, the operating system would check that the capability does or does not allow you to perform a read/write open on file\n   \n    /tmp/foo\n   \n   . If it does, the OS opens it for you. If not, back comes an error to your process, chiding it for trying to open a file it does not have a capability for. (Remember, we're not talking about Linux here. Linux uses ACLs, not capabilities, to determine if an\n   \n    open()\n   \n   call should be allowed.)\n\n\nThere are some obvious questions here. What, precisely, is a capability? Clearly we're not talking about metal keys or paper tickets. Also, how does the OS check the validity of capability? And where do capabilities come from, in the first place? Just like all other information in a computer, capabilities are bunches of bits. They are data. Given that there are probably lots of resources to protect, and capabilities must be specific to a resource, capabilities are likely to be fairly long, and perhaps fairly complex. But, ultimately, they're just bits. Anything composed of a bunch of bits has certain properties we must bear in mind. For example, anyone can create any bunch of bits they want. There are no proprietary or reserved bit patterns that processes cannot create. Also, if a process has one copy of a particular set of bits, it's trivial to create more copies of it. The first characteristic implies that it's possible for anyone at all to create any capability they want. The second characteristic implies that once someone has a working capability, they can make as many copies of it as they want, and can potentially store them anywhere they want, including on an entirely different machine.\n\n\nThat doesn't sound so good from a security perspective. If a process needs a capability with a particular bit pattern to open\n   \n    /tmp/foo\n   \n   for read and write, maybe it can just generate that bit pattern and successfully give itself the desired access to the file. That's not what we're looking for in an access control mechanism. We want capabilities to be unforgeable. Even if we can get around that problem, the ability to copy a capability would suggest we can't take access permission away, once granted, since the process might have copies of the capability stashed away elsewhere\n   \n    6\n   \n   . Further, perhaps the process can grant access to another process merely by using IPC to transfer a copy of the capability to that other process.\n\n\nWe typically deal with these issues when using capabilities for access control by never letting a process get its metaphorical hands on any capability. The operating system controls and maintains capabilities, storing them somewhere in its protected memory space. Processes can perform various operations on capabilities, but only with the mediation of the operating system. If, for example, process A wishes to give process B read/write access to file\n   \n    /tmp/foo\n   \n   using capabilities, A can't merely\n\n\n6\n   \n   This ability is commonly called\n   **revocation**\n   . Revocation is easy with ACLs, since you just go to the ACL and change it. Depending on implementation, it can be easy or hard for capabilities.\n\n\nsend B the appropriate bit pattern. Instead, A must make a system call requesting the operating system to give the appropriate capability to B. That gives the OS a chance to decide whether its security policy permits B to access\n   \n    /tmp/foo\n   \n   and deny the capability transfer if it does not.\n\n\nSo if we want to rely on capabilities for access control, the operating system will need to maintain its own protected capability list for each process. That's simple enough, since the OS already has a per-process protected data structure, the PCB. Slap a pointer to the capability list (stored in kernel memory) into the process' PCB and you're all set. Now when the process attempts to open\n   \n    /tmp/foo\n   \n   for read/write, the call traps to the OS, the OS consults the capability list for that process to see if there is a relevant capability for the operation on the list and proceeds accordingly.\n\n\nIn a general system, keeping an on-line capability list of literally everything some principal is permitted to access would incur high overheads. If we used capabilities for file-based access control, a user might have thousands of capabilities, one for each file the user was allowed to access in any way. Generally, if one is using capabilities, the system persistently stores the capabilities somewhere safe, and imports them as needed. So a capability list attached to a process is not necessarily very long, but there is an issue of deciding which capabilities of the immense set users have at their discretion to give to each process they run.\n\n\nThere is another option. Capabilities need not be stored in the operating system. Instead, they can be cryptographically protected. If capabilities are relatively long and are created with strong cryptography, they cannot be guessed in a practical way and can be left in the user's hands. Cryptographic capabilities make most sense in a distributed system, so we'll talk about them in the chapter on distributed system security.\n\n\nThere are good and bad points about capabilities, just as there were for access control lists. With capabilities, it's easy to determine which system resources a given principal can access. Just look through the principal's capability list. Revoking access merely requires removing the capability from the list, which is easy enough if the OS has exclusive access to the capability (but much more difficult if it does not). If you have the capability readily available in memory, it can be quite cheap to check it, particularly since the capability can itself contain a pointer to the data or software associated with the resource it protects. Perhaps merely having such a pointer is the system's core implementation of capabilities.\n\n\nOn the other hand, determining the entire set of principals who can access a resource becomes more expensive. Any principal might have a capability for the resource, so you must check all principals' capability lists to tell. Simple methods for making capability lists short and manageable have not been as well developed as the Unix method of providing short ACLs. Also, the system must be able to create, store, and retrieve capabilities in a way that overcomes the forgery problem, which can be challenging.\n\n\nOne neat aspect of capabilities is that they offer a good way to create processes with limited privileges. With access control lists, a process in-\n\n\ninherits the identity of its parent process, also inheriting all of the privileges of that principal. It's hard to give the process just a subset of the parent's privileges. Either you need to create a new principal with those limited privileges, change a bunch of access control lists, and set the new process's identity to that new principal, or you need some extension to your access control model that doesn't behave quite the way access control lists ordinarily do. With capabilities, it's easy. If the parent has capabilities for X, Y, and Z, but only wants the child process to have the X and Y capabilities, when the child is created, the parent transfers X and Y, not Z.\n\n\nIn practice, user-visible access control mechanisms tend to use access control lists, not capabilities, for a number of reasons. However, under the covers operating systems make extensive use of capabilities. For example, in a typical Linux system, that\n   \n    open()\n   \n   call we were discussing uses ACLs for access control. However, assuming the Linux\n   \n    open()\n   \n   was successful, as long as the process keeps the file open, the ACL is not examined on subsequent reads and writes. Instead, Linux creates a data structure that amounts to a capability indicating that the process has read and write privileges for that file. This structure is attached to the process's PCB. On each read or write operation, the OS can simply consult this data structure to determine if reading and writing are allowed, without having to find the file's access control list. If the file is closed, this capability-like structure is deleted from the PCB and the process can no longer access the file without performing another\n   \n    open()\n   \n   which goes back to the ACL. Similar techniques can be used to control access to hardware devices and IPC channels, especially since UNIX-like systems treat these resources as if they were files. This combined use of ACLs and capabilities allows the system to avoid some of the problems associated with each mechanism. The cost of checking an access control list on every operation is saved because this form of capability is easy to check, being merely the presence or absence of a pointer in an operating system data structure. The cost of managing capabilities for all accessible objects is avoided because the capability is only set up after a successful ACL check. If the object is never accessed by a process, the ACL is never checked and no capability is required. Since any given process typically opens only a tiny fraction of all the files it is permitted to open, the scaling issue doesn't usually arise."
        },
        {
          "name": "Mandatory And Discretionary Access Control",
          "content": "Who gets to decide what the access control on a computer resource should be? For most people, the answer seems obvious: whoever owns the resource. In the case of a user's file, the user should determine access control settings. In the case of a system resource, the system administrator, or perhaps the owner of the computer, should determine them. However, for some systems and some security policies, that's not the right answer. In particular, the parties who care most about information security sometimes want tighter controls than that.\n\n\nThe military is the most obvious example. We've all heard of Top Secret information, and probably all understand that even if you are allowed to see Top Secret information, you're not supposed to let other people see it, too. And that's true even if the information in question is in a file that you created yourself, such as a report that contains statistics or quotations from some other Top Secret document. In these cases, the simple answer of the creator controlling access permissions isn't right. Whoever is in overall charge of information security in the organization needs to make those decisions, which implies that principal has the power to set the access controls for information created by and belonging to other users, and that those users can't override his decisions. The more common case is called\n   **discretionary access control**\n   . Whether almost anyone or almost no one is given access to a resource is at the discretion of the owning user. The more restrictive case is called\n   **mandatory access control**\n   . At least some elements of the access control decisions in such systems are mandated by an authority, who can override the desires of the owner of the information. The choice of discretionary or mandatory access control is orthogonal to whether you use ACLs or capabilities, and is often independent of other aspects of the access control mechanism, such as how access information is stored and handled. A mandatory access control system can also include discretionary elements, which allow further restriction (but not loosening) of mandatory controls.\n\n\nMany people will never work with a system running mandatory access controls, so we won't go further into how they work, beyond observing that clearly the operating system is going to be involved in enforcing them. Should you ever need to work in an environment where mandatory access control is important, you can be sure you will hear about it. You should learn more about it at that point, since when someone cares enough to use mandatory access control mechanisms, they also care enough to punish users who don't follow the rules. Loscocco [L01] describes a special version of Linux that incorporates mandatory access control. This is a good paper to start with if you want to learn more about the characteristics of such systems."
        },
        {
          "name": "Practicalities Of Access Control Mechanisms",
          "content": "Most systems expose either a simple or more powerful access control list mechanism to their users, and most of them use discretionary access control. However, given that a modern computer can easily have hundreds of thousands, or even millions of files, having human users individually set access control permissions on them is infeasible. Generally, the system allows each user to establish a default access permission that is used for every file he creates. If one uses the Linux\n   \n    open()\n   \n   call to create a file, one can specify which access permissions to initially assign to that file. Access permissions on newly created files in Unix/Linux systems can be further controlled by the\n   \n    umask()\n   \n   call, which applies to all new file creations by the process that performed it.\n\n\n\n\n**ASIDE: THE ANDROID ACCESS CONTROL MODEL**\n\n\nThe Android system is one of the leading software platforms for today's mobile computing devices, especially smart phones. These devices pose different access control challenges than classic server computers, or even personal desktop computers or laptops. Their functionality is based on the use of many relatively small independent applications, commonly called apps, that are downloaded, installed, and run on a device belonging to only a single user. Thus, there is no issue of protecting multiple users on one machine from each other. If one used a standard access control model, these apps would run under that user's identity. But apps are developed by many entities, and some may be malicious. Further, most apps have no legitimate need for most of the resources on the device. If they are granted too many privileges, a malicious app can access the phone owner's contacts, make phone calls, or buy things over the network, among many other undesirable behaviors. The principle of least privilege implies that we should not give apps the full privileges belonging to owner, but they must have some privileges if they are to do anything interesting.\n\n\nAndroid runs on top of a version of Linux, and an application's access limitations are achieved in part by generating a new user ID for each installed app. The app runs under that ID and its accesses can be controlled on that basis. However, the Android middleware offers additional facilities for controlling access. Application developers define accesses required by their app. When a user considers installing an app on their device, they are shown what permissions it requires. The user can either grant the app those permissions, not install the app, or limit its permissions, though the latter choice may also limit app utility. Also, the developer specifies ways in which other apps can communicate with the new app. The data structure used to encode this access information is called a\n   **permission label**\n   . An app's permission labels (both what it can access and what it provides to others) are set at app design time, and encoded into a particular Android system at the moment the app is installed on that machine.\n\n\nPermission labels are thus like capabilities, since possession of them by the app allows the app to do something, while lacking a label prevents the app from doing that thing. An app's set of permission labels is set statically at install time. The user can subsequently change those permissions, although limiting them may damage app functionality. Permission labels are a form of mandatory access control. The Android security model is discussed in detail by Enck et al. [E+09].\n\n\nThe Android security approach is interesting, but not perfect. In particular, users are not always aware of the implications of granting an application access to something, and, faced with the choice of granting the access or not being able to effectively use the app, they will often grant it. This behavior can be problematic, if the app is malicious.\n\n\nIf desired, the owner can alter that initial ACL, but experience shows that users rarely do. This tendency demonstrates the importance of properly chosen defaults. Here, as in many other places in an operating system, a theoretically changeable or tunable setting will, in practice, be used unaltered by almost everyone almost always.\n\n\nHowever, while many will never touch access controls on their resources, for an important set of users and systems these controls are of vital importance to achieve their security goals. Even if you mostly rely on defaults, many software installation packages use some degree of care in setting access controls on executables and configuration files they create. Generally, you should exercise caution in fiddling around with access controls in your system. If you don't know what you're doing, you might expose sensitive information or allow attackers to alter critical system settings. If you tighten existing access controls, you might suddenly cause a bunch of daemon programs running in the background to stop working.\n\n\nOne practical issue that many large institutions discovered when trying to use standard access control methods to implement their security policies is that people performing different roles within the organization require different privileges. For example, in a hospital, all doctors might have a set of privileges not given to all pharmacists, who themselves have privileges not given to the doctors. Organizing access control on the basis of such roles and then assigning particular users to the roles they are allowed to perform makes implementation of many security policies easier. This approach is particularly valuable if certain users are permitted to switch roles depending on the task they are currently performing, since then one need not worry about setting or changing the individual's access permissions on the fly, but simply switch their role from one to another. Usually they will hold the role's permission only as long as they maintain that role. Once they exit the particular role (perhaps to enter a different role with different privileges), they lose the privileges of the role they exit.\n\n\nThis observation led to the development of\n   **Role-Based Access Control**\n   , or\n   **RBAC**\n   . The core ideas had been around for some time before they were more formally laid out in a research paper by Ferraiolo and Kuhn [FK92]. Now RBAC is in common use in many organizations, particularly large ones. Large organizations face more serious management challenges than small ones, so approaches like RBAC that allow groups of users to be dealt with in one operation can significantly ease the management task. For example, if a company determines that all programmers should be granted access to a new library that has been developed, but accountants should not, RBAC would achieve this effect with a single operation that assigns the necessary privilege to the\n   *Programmer*\n   role. If a programmer is promoted to a management position for which access to the library is unnecessary, the company can merely remove the\n   *Programmer*\n   role from the set of roles the manager could take on.\n\n\nSuch restrictions do not necessarily imply that you suspect your accountants of being dishonest and prone to selling your secret library code to competitors\n   \n    7\n   \n   . Remember the principle of least privilege: when you give someone access to something, you are relying not just on their honesty, but on their caution. If accountants can't access the library at all,\n\n\n7\n   \n   Dishonest accountants are generally good to avoid, so you probably did your best to hire honest ones, after all. Unless you're Bernie Madoff [W20], perhaps...\n\n\nthen neither malice nor carelessness on their part can lead to an accountant's privileges leaking your library code. Least privilege is not just a theoretically good idea, but a vital part of building secure systems in the real world.\n\n\nRBAC sounds a bit like using groups in access control lists, and there is some similarity, but RBAC systems are a good deal more powerful than mere group access permissions; RBAC systems allow a particular user to take on multiple disjoint roles. Perhaps our programmer was promoted to a management position, but still needs access to the library, for example when another team member's code needs to be tested. An RBAC system would allow our programmer to switch between the role of manager and programmer, temporarily leaving behind rights associated with the manager and gaining rights associated with the programmer role. When the manager tested someone else's new code, the manager would have permission to access the library, but would\n   *not*\n   have permission to access team member performance reviews. Thus, if a sneaky programmer slipped malicious code into the library (e.g., that tried to read other team members' performance reviews, or learn their salaries), the manager running that code would not unintentionally leak that information; using the proper role at the proper time prevents it.\n\n\nThese systems often require a new authentication step to take on an RBAC role, and usually taking on Role A requires relinquishing privileges associated with one's previous role, say Role B. The manager's switch to the code testing role would result in temporarily relinquishing privileges to examine the performance reviews. On completing the testing, the manager would switch back to the role allowing access to the reviews, losing privilege to access the library. RBAC systems may also offer finer granularity than merely being able to read or write a file. A particular role (\n   *Salesperson*\n   , for instance) might be permitted to add a purchase record for a particular product to a file, but would not be permitted to add a re-stocking record for the same product to the same file, since salespeople don't do re-stocking. This degree of control is sometimes called\n   **type enforcement**\n   . It associates detailed access rules to particular objects using what is commonly called a\n   **security context**\n   for that object. How exactly this is done has implications for performance, storage of the security context information, and authentication.\n\n\nOne can build a very minimal RBAC system under Linux and similar OSes using ACLs and groups. These systems have a feature in their access control mechanism called\n   **privilege escalation**\n   . Privilege escalation allows careful extension of privileges, typically by allowing a particular program to run with a set of privileges beyond those of the user who invokes them. In Unix and Linux systems, this feature is called\n   **setuid**\n   , and it allows a program to run with privileges associated with a different user, generally a user who has privileges not normally available to the user who runs the program. However, those privileges are only granted during the run of that program and are lost when the program exits. A carefully written\n   \n    setuid\n   \n   program will only perform a limited set of oper-\n\n\n**TIP: PRIVILEGE ESCALATION CONSIDERED DANGEROUS**\nWe just finished talking about how we could use privilege escalation to temporarily change what one of our users can do, and how this offers us new security options. But there's a dangerous side to privilege escalation. An attacker who breaks into your system frequently compromises a program running under an identity with very limited privileges. Perhaps all it's supposed to be able to do is work with a few simple informational files and provide remote users with their content, and maybe run standard utilities on those files. It might not even have write access to its files. You might think that this type of compromise has done little harm to the system, since the attacker cannot use the access to do very much.\n\n\nThis is where the danger of privilege escalation comes into play. Attackers who have gained any kind of a foothold on a system will then look around for ways to escalate their privileges. Even a fairly unprivileged application can do a lot of things that an outsider cannot directly do, so attackers look for flaws in the code or configuration that the compromised application can access. Such attempts to escalate privilege are usually an attacker's first order of business upon successful compromise of a system.\n\n\nIn many systems, there is a special user, often called the\n   **superuser**\n   or\n   **root**\n   user. This user has a lot more privilege than any other user on the system, since its purpose is to allow for the most vital and far-reaching system administration changes on that system. The paramount goal of an attacker with a foothold on your system is to use privilege escalation to become the root user. An attacker who can do that will effectively have total control of your system. Such an attacker can look at any file, alter any program, change any configuration, and perhaps even install a different operating system. This danger should point out how critical it is to be careful in allowing any path that permits privilege escalation up to superuser privilege.\n\n\nations using those privileges, ensuring that privileges cannot be abused\n   \n    8\n   \n   . One could create a simple RBAC system by defining an artificial user for each role and associating desired privileges with that user. Programs using those privileges could be designated as\n   \n    setuid\n   \n   to that user.\n\n\nThe Linux\n   \n    sudo\n   \n   command, which we encountered in the authentication chapter, offers this kind of functionality, allowing some designated users to run certain programs under another identity. For example,\n   \n\n\n    sudo -u Programmer install newprogram\n\n\nwould run this install command under the identity of user\n   \n    Programmer\n   \n   , rather than the identity of the user who ran the command, assuming that user was on a system-maintained list of users allowed to take on the identity\n   \n    Programmer\n   \n   . Secure use of this approach requires careful configura-\n\n\n8\n   \n   Unfortunately, not all programs run with the\n   \n    setuid\n   \n   feature are carefully written, which has led to many security problems over the years. Perhaps true for all security features, alas?\n\n\ntion of system files controlling who is allowed to execute which programs under which identities. Usually the\n   \n    sudo\n   \n   command requires a new authentication step, as with other RBAC systems.\n\n\nFor more advanced purposes, RBAC systems typically support finer granularity and more careful tracking of role assignment than\n   \n    setuid\n   \n   and\n   \n    sudo\n   \n   operations allow. Such an RBAC system might be part of the operating system or might be some form of add-on to the system, or perhaps a programming environment. Often, if you're using RBAC, you also run some degree of mandatory access control. If not, in the example of\n   \n    sudo\n   \n   above, the user running under the\n   \n    Programmer\n   \n   identity could run a command to change the access permissions on files, making the\n   \n    install\n   \n   command available to non-programmers. With mandatory access control, a user could take on the role of\n   \n    Programmer\n   \n   to do the installation, but could not use that role to allow salespeople or accountants to perform the installation."
        }
      ]
    },
    {
      "name": "Protecting Information With Cryptography",
      "sections": [
        {
          "name": "Introduction",
          "content": "In previous chapters, we've discussed clarifying your security goals, determining your security policies, using authentication mechanisms to identify principals, and using access control mechanisms to enforce policies concerning which principals can access which computer resources in which ways. While we identified a number of shortcomings and problems inherent in all of these elements of securing your system, if we regard those topics as covered, what's left for the operating system to worry about, from a security perspective? Why isn't that everything?\n\n\nThere are a number of reasons why we need more. Of particular importance: not everything is controlled by the operating system. But perhaps you respond, you told me the operating system is all-powerful! Not really. It has substantial control over a limited domain – the hardware on which it runs, using the interfaces of which it is given control. It has no real control over what happens on other machines, nor what happens if one of its pieces of hardware is accessed via some mechanism outside the operating system's control.\n\n\nBut how can we expect the operating system to protect something when the system does not itself control access to that resource? The answer is to prepare the resource for trouble in advance. In essence, we assume that we are going to lose the data, or that an opponent will try to alter it improperly. And we take steps to ensure that such actions don't cause us problems. The key observation is that if an opponent cannot understand the data in the form it is obtained, our secrets are safe. Further, if the attacker cannot understand it, it probably can't be altered, at least not in a controllable way. If the attacker doesn't know what the data means, how can it be changed into something the attacker prefers?\n\n\nThe core technology we'll use is\n   **cryptography**\n   , a set of techniques to convert data from one form to another, in controlled ways with expected outcomes. We will convert the data from its ordinary form into another form using cryptography. If we do it right, the opponent will not be able to determine what the original data was by examining the protected form.\n\n\nOf course, if we ever want to use it again ourselves, we must be able to reverse that transformation and return the data to its ordinary form. That must be hard for the opponent to do, as well. If we can get to that point, we can also provide some protection for the data from alteration, or, more precisely, prevent opponents from altering the data to suit their desires, and even know when opponents have tampered with our data. All through the joys of cryptography!\n\n\nBut using cryptography properly is not easy, and many uses of cryptography are computationally expensive. So we need to be selective about where and when we use cryptography, and careful in how we implement it and integrate it into our systems. Well chosen uses that are properly performed will tremendously increase security. Poorly chosen uses that are badly implemented won't help at all, and may even hurt.\n\n\n\n\n**THE CRUX OF THE PROBLEM:**\n\n\n\n\n**HOW TO PROTECT INFORMATION OUTSIDE THE OS'S DOMAIN**\n\n\nHow can we use cryptography to ensure that, even if others gain access to critical data outside the control of the operating system, they will be unable to either use or alter it? What cryptographic technologies are available to assist in this problem? How do we properly use those technologies? What are the limitations on what we can do with them?"
        },
        {
          "name": "Cryptography",
          "content": "Many books have been written about cryptography, but we're only going to spend a chapter on it. We'll still be able to say useful things about it because, fortunately, there are important and complex issues of cryptography that we can mostly ignore. That's because we aren't going to become cryptographers ourselves. We're merely going to be users of the technology, relying on experts in that esoteric field to provide us with tools that we can use without having full understanding of their workings\n   \n    1\n   \n   . That sounds kind of questionable, but you are already doing just that. Relatively few of us really understand the deep details of how our computer hardware works, yet we are able to make successful use of it, because we have good interfaces and know that smart people have taken great care in building the hardware for us. Similarly, cryptography provides us with strong interfaces, well-defined behaviors, and better than usual assurance that there is a lot of brain power behind the tools we use.\n\n\nThat said, cryptography is no magic wand, and there is a lot you need to understand merely to use it correctly. That, particularly in the context of operating system use, is what we're going to concentrate on here.\n\n\n1\n   \n   If you'd like to learn more about the fascinating history of cryptography, check out Kahn [K96]. If more technical detail is your desire, Schneier [S96] is a good start.\n\n\nThe basic idea behind cryptography is to take a piece of data and use an algorithm (often called a\n   **cipher**\n   ), usually augmented with a second piece of information (which is called a\n   **key**\n   ), to convert the data into a different form. The new form should look nothing like the old one, but, typically, we want to be able to run another algorithm, again augmented with a second piece of information, to convert the data back to its original form.\n\n\nLet's formalize that just a little bit. We start with data\n   \n    P\n   \n   (which we usually call the\n   **plaintext**\n   ), a key\n   \n    K\n   \n   , and an encryption algorithm\n   \n    E()\n   \n   . We end up with\n   \n    C\n   \n   , the altered form of\n   \n    P\n   \n   , which we usually call the\n   **ciphertext**\n   :\n\n\nC = E(P, K) \\quad (56.1)\n\n\nFor example, we might take the plaintext \"Transfer $100 to my savings account\" and convert it into ciphertext \"Sqzmrredq #099 sn lx rzuhmfr zbbntms.\" This example actually uses a pretty poor encryption algorithm called a Caesar cipher. Spend a minute or two studying the plaintext and ciphertext and see if you can figure out what the encryption algorithm was in this case.\n\n\nThe reverse transformation takes\n   \n    C\n   \n   , which we just produced, a decryption algorithm\n   \n    D()\n   \n   , and the key\n   \n    K\n   \n   :\n\n\nP = D(C, K) \\quad (56.2)\n\n\nSo we can decrypt \"Sqzmrredq #099 sn lx rzuhmfr zbbntms\" back into \"Transfer $100 to my savings account.\" If you figured out how we encrypted the data in the first place, it should be easy to figure out how to decrypt it.\n\n\nWe use cryptography for a lot of things, but when discussing it generally, it's common to talk about messages being sent and received. In such discussions, the plaintext\n   \n    P\n   \n   is the message we want to send and the ciphertext\n   \n    C\n   \n   is the protected version of that message that we send out into the cold, cruel world.\n\n\nFor the encryption process to be useful, it must be deterministic, so the first transformation always converts a particular\n   \n    P\n   \n   using a particular\n   \n    K\n   \n   to a particular\n   \n    C\n   \n   , and the second transformation always converts a particular\n   \n    C\n   \n   using a particular\n   \n    K\n   \n   to the original\n   \n    P\n   \n   . In many cases,\n   \n    E()\n   \n   and\n   \n    D()\n   \n   are actually the same algorithm, but that is not required. Also, it should be very hard to figure out\n   \n    P\n   \n   from\n   \n    C\n   \n   without knowing\n   \n    K\n   \n   . Impossible would be nice, but we'll usually settle for computationally infeasible. If we have that property, we can show\n   \n    C\n   \n   to the most hostile, smartest opponent in the world and they still won't be able to learn what\n   \n    P\n   \n   is.\n\n\nProvided, of course, that ...\n\n\nThis is where cleanly theoretical papers and messy reality start to collide. We only get that pleasant assurance of secrecy if the opponent does not know both\n   \n    D()\n   \n   and our key\n   \n    K\n   \n   . If they are known, the opponent will apply\n   \n    D()\n   \n   and\n   \n    K\n   \n   to\n   \n    C\n   \n   and extract the same information\n   \n    P\n   \n   that we can.\n\n\nIt turns out that we usually can't keep\n   \n    E()\n   \n   and\n   \n    D()\n   \n   secret. Since we're not trying to be cryptographers, we won't get into the why of the matter, but it is extremely hard to design good ciphers. If the cipher has weaknesses, then an opponent can extract the plaintext\n   \n    P\n   \n   even without\n   \n    K\n   \n   . So we need to have a really good cipher, which is hard to come by. Most of us don't have a world-class cryptographer at our fingertips to design a new one, so we have to rely on one of a relatively small number of known strong ciphers. AES, a standard cipher that was carefully designed and thoroughly studied, is one good example that you should think about using.\n\n\nIt sounds like we've thrown away half our protection, since now the cryptography's benefit relies entirely on the secrecy of the key. Precisely. Let's say that again in all caps, since it's so important that you really need to remember it:\n   **THE CRYPTOGRAPHY'S BENEFIT RELIES ENTIRELY ON THE SECRECY OF THE KEY**\n   . It probably wouldn't hurt for you to re-read that statement a few dozen times, since the landscape is littered with insecure systems that did not take that lesson to heart.\n\n\nThe good news is that if you're using a strong cipher and are careful about maintaining key secrecy, your cryptography is strong. You don't need to worry about anything else. The bad news is that maintaining key secrecy in practical systems for real uses of cryptography isn't easy. We'll talk more about that later.\n\n\nFor the moment, revel in the protection we have achieved, and rejoice to learn that we've gotten more than secrecy from our proper use of cryptography! Consider the properties of the transformations we've performed. If our opponent gets access to our encrypted data, it can't be understood. But what if the opponent can alter it? What's being altered is the encrypted form, i.e., making some changes in\n   \n    C\n   \n   to convert it to, say,\n   \n    C'\n   \n   . What will happen when we try to decrypt\n   \n    C\n   \n   ? Well, it won't decrypt to\n   \n    P\n   \n   . It will decrypt to something else, say\n   \n    P'\n   \n   . For a good cipher of the type you should be using, it will be difficult to determine what a piece of ciphertext\n   \n    C'\n   \n   will decrypt to, unless you know\n   \n    K\n   \n   . That means it will be hard to predict which ciphertext you need to have to decrypt to a particular plaintext. Which in turn means that the attacker will have no idea what the altered ciphertext\n   \n    C'\n   \n   will decrypt to.\n\n\nOut of all possible bit patterns it could decrypt to, the chances are good that\n   \n    P'\n   \n   will turn out to be garbage, when considered in the context of what we expected to see: ASCII text, a proper PDF file, or whatever. If we're careful, we can detect that\n   \n    P'\n   \n   isn't what we started with, which would tell us that our opponent tampered with our encrypted data. If we want to be really sure, we can perform a hashing function on the plaintext and include the hash with the message or encrypted file. If the plaintext we get out doesn't produce the same hash, we will have a strong indication that something is amiss.\n\n\nSo we can use cryptography to help us protect the integrity of our data, as well.\n\n\n**TIP: DEVELOPING YOUR OWN CIPHERS: DON'T DO IT**\nDon't.\n\n\nIt's tempting to leave it at that, since it's really important that you follow this guidance. But you may not believe it, so we'll expand a little. The world's best cryptographers often produce flawed ciphers. Are you one of the world's best cryptographers? If you aren't, and the top experts often fail to build strong ciphers, what makes you think you'll do better, or even as well?\n\n\nWe know what you'll say next: \"but the cipher I wrote is so strong that I can't even break it myself.\" Well, pretty much anyone who puts their mind to it can create a cipher they can't break themselves. But remember those world-class cryptographers we talked about? How did they get to be world class? By careful study of the underpinnings of cryptography and by breaking other people's ciphers. They're very good at it, and if it's worth their trouble, they will break yours. They might ignore it if you just go around bragging about your wonderful cipher (since they hear that all the time), but if you actually use it for something important, you will unfortunately draw their attention. Following which your secrets will be revealed, following which you will look foolish for designing your own cipher instead of using something standard like AES, which is easier to do, anyway.\n\n\nSo, don't.\n\n\nWait, there's more! What if someone hands you a piece of data that has been encrypted with a key\n   \n    K\n   \n   that is known only to you and your buddy Remzi? You know you didn't create it, so if it decrypts properly using key\n   \n    K\n   \n   , you know that Remzi must have created it. After all, he's the only other person who knew key\n   \n    K\n   \n   , so only he could have performed the encryption. Voila, we have used cryptography for authentication! Unfortunately, cryptography will not clean your room, do your homework for you, or make thousands of julienne fries in seconds, but it's a mighty fine tool, anyway.\n\n\nThe form of cryptography we just described is often called\n   **symmetric cryptography**\n   , because the same key is used to encrypt and decrypt the data. For a long time, everyone believed that was the only form of cryptography possible. It turns out everyone was wrong."
        },
        {
          "name": "Public Key Cryptography",
          "content": "When we discussed using cryptography for authentication, you might have noticed a little problem. In order to verify the authenticity of a piece of encrypted information, you need to know the key used to encrypt it. If we only care about using cryptography for authentication, that's inconvenient. It means that we need to communicate the key we're using for\n\n\nthat purpose to whoever might need to authenticate us. What if we're Microsoft, and we want to authenticate ourselves to every user who has purchased our software? We can't use just one key to do this, because we'd need to send that key to hundreds of millions of users and, once they had that key, they could pretend to be Microsoft by using it to encrypt information. Alternately, Microsoft could generate a different key for each of those hundreds of millions of users, but that would require secretly delivering a unique key to hundreds of millions of users, not to mention keeping track of all those keys. Bummer.\n\n\nFortunately, our good friends, the cryptographic wizards, came up with a solution. What if we use two different keys for cryptography, one to encrypt and one to decrypt? Our encryption operation becomes\n\n\nC = E(P, K_{\\text{encrypt}}) \\quad (56.3)\n\n\nAnd our decryption operation becomes\n\n\nP = D(C, K_{\\text{decrypt}}) \\quad (56.4)\n\n\nLife has just become a lot easier for Microsoft. They can tell everyone their decryption key\n   \n    K_{\\text{decrypt}}\n   \n   , but keep their encryption key\n   \n    K_{\\text{encrypt}}\n   \n   secret. They can now authenticate their data by encrypting it with their secret key, while their hundreds of millions of users can check the authenticity using the key Microsoft made public. For example, Microsoft could encrypt an update to their operating system with\n   \n    K_{\\text{encrypt}}\n   \n   and send it out to all their users. Each user could decrypt it with\n   \n    K_{\\text{decrypt}}\n   \n   . If it decrypted into a properly formatted software update, the user could be sure it was created by Microsoft. Since no one else knows that private key, no one else could have created the update.\n\n\nSounds like magic, but it isn't. It's actually mathematics coming to our rescue, as it so frequently does. We won't get into the details here, but you have to admit it's pretty neat. This form of cryptography is called\n   **public key cryptography**\n   , since one of the two keys can be widely known to the entire public, while still achieving desirable results. The key everyone knows is called the\n   **public key**\n   , and the key that only the owner knows is called the\n   **private key**\n   . Public key cryptography (often abbreviated as\n   **PK**\n   ) has a complicated invention history, which, while interesting, is not really germane to our discussion. Check out a paper by a pioneer in the field, Whitfield Diffie, for details [D88].\n\n\nPublic key cryptography avoids one hard issue that faced earlier forms of cryptography: securely distributing a secret key. Here, the private key is created by one party and kept secret by him. It's never distributed to anyone else. The public key must be distributed, but generally we don't care if some third party learns this key, since they can't use it to sign messages. Distributing a public key is an easier problem than distributing a secret key, though, alas, it's harder than it sounds. We'll get to that.\n\n\nPublic key cryptography is actually even neater, since it works the other way around. You can use the decryption key\n   \n    K_{\\text{decrypt}}\n   \n   to encrypt, in which case you need the encryption key\n   \n    K_{\\text{encrypt}}\n   \n   to decrypt. We still\n\n\nexpect the encryption key to be kept secret and the decryption key to be publicly known, so doing things in this order no longer allows authentication. Anyone could encrypt with\n   \n    K_{decrypt}\n   \n   , after all. But only the owner of the key can decrypt such messages using\n   \n    K_{encrypt}\n   \n   . So that allows anyone to send an encrypted message to someone who has a private key, provided you know their public key. Thus, PK allows authentication if you encrypt with the private key and secret communication if you encrypt with the public key.\n\n\nWhat if you want both, as you very well might? You'll need two different key pairs to do that. Let's say Alice wants to use PK to communicate secretly with her pal Bob, and also wants to be sure Bob can authenticate her messages. Let's also say Alice and Bob each have their own PK pair. Each of them knows his or her own private key and the other party's public key. If Alice encrypts her message with her own private key, she'll authenticate the message, since Bob can use her public key to decrypt and will know that only Alice could have created that message. But everyone knows Alice's public key, so there would be no secrecy achieved. However, if Alice takes the authenticated message and encrypts it a second time, this time with Bob's public key, she will achieve secrecy as well. Only Bob knows the matching private key, so only Bob can read the message. Of course, Bob will need to decrypt twice, once with his private key and then a second time with Alice's public key.\n\n\nSounds expensive. It's actually worse than you think, since it turns out that public key cryptography has a shortcoming: it's much more computationally expensive than traditional cryptography that relies on a single shared key. Public key cryptography can take hundreds of times longer to perform than standard symmetric cryptography. As a result, we really can't afford to use public key cryptography for everything. We need to pick and choose our spots, using it to achieve the things it's good at.\n\n\nThere's another important issue. We rather blithely said that Alice knows Bob's public key and Bob knows Alice's. How did we achieve this blissful state of affairs? Originally, only Alice knew her public key and only Bob knew his public key. We're going to need to do something to get that knowledge out to the rest of the world if we want to benefit from the magic of public key cryptography. And we'd better be careful about it, since Bob is going to assume that messages encrypted with the public key he thinks belongs to Alice\n   *were*\n   actually created by Alice. What if some evil genius, called, perhaps, Eve, manages to convince Bob that Eve's public key actually belongs to Alice? If that happens, messages created by Eve would be misidentified by Bob as originating from Alice, subverting our entire goal of authenticating the messages. We'd better make sure Eve can't fool Bob about which public key belongs to Alice.\n\n\nThis leads down a long and shadowy road to the arcane realm of key distribution infrastructures. You will be happier if you don't try to travel that road yourself, since even the most well prepared pioneers who have hazarded it often come to grief. We'll discuss how, in practice, we distribute public keys in a chapter on distributed system security. For the\n\n\nmoment, bear in mind that the beautiful magic of public key cryptography rests on the grubby and uncertain foundation of key distribution.\n\n\nOne more thing about PK cryptography:\n   **THE CRYPTOGRAPHY'S BENEFIT RELIES ENTIRELY ON THE SECRECY OF THE KEY**\n   . (Bet you've heard that before.) In this case, the private key. But the secrecy of that private key is every bit as important to the overall benefit of public key cryptography as the secrecy of the single shared key in the case of symmetric cryptography. Never divulge private keys. Never share private keys. Take great care in your use of private keys and in how you store them. If you lose a private key, everything you used it for is at risk, and whoever gets hold of it can pose as you and read your secret messages. That wouldn't be very good, would it?"
        },
        {
          "name": "Cryptographic Hashes",
          "content": "As we discussed earlier, we can protect data integrity by using cryptography, since alterations to encrypted data will not decrypt properly. We can reduce the costs of that integrity check by hashing the data and encrypting just the hash, instead of encrypting the entire thing. However, if we want to be really careful, we can't use just any hash function, since hash functions, by their very nature, have\n   **hash collisions**\n   , where two different bit patterns hash to the same thing. If an attacker can change the bit pattern we intended to send to some other bit pattern that hashes to the same thing, we would lose our integrity property.\n\n\nSo to be particularly careful, we can use a\n   **cryptographic hash**\n   to ensure integrity. Cryptographic hashes are a special category of hash functions with several important properties:\n\n\n  * • It is computationally infeasible to find two inputs that will produce the same hash value.\n  * • Any change to an input will result in an unpredictable change to the resulting hash value.\n  * • It is computationally infeasible to infer any properties of the input based only on the hash value.\n\n\nBased on these properties, if we only care about data integrity, rather than secrecy, we can take the cryptographic hash of a piece of data, encrypt only that hash, and send both the encrypted hash and the unencrypted data to our partner. If an opponent fiddles with the data in transit, when we decrypt the hash and repeat the hashing operation on the data, we'll see a mismatch and detect the tampering\n   \n    2\n   \n   .\n\n\n2\n   \n   Why do we need to encrypt the cryptographic hash? Well, anyone, including our opponent, can run a cryptographic hashing algorithm on anything, including an altered version of the message. If we don't encrypt the hash, the attacker will change the message, compute a new hash, replace both the original message and the original hash with these versions, and send the result. If the hash we sent is encrypted, though, the attacker can't know what the encrypted version of the altered hash should be.\n\n\nTo formalize it a bit, to perform a cryptographic hash we take a plaintext\n   \n    P\n   \n   and a hashing algorithm\n   \n    H()\n   \n   . Note that there is not necessarily any key involved. Here's what happens:\n\n\nS = H(P) \\quad (56.5)\n\n\nSince cryptographic hashes are a subclass of hashes in general, we normally expect\n   \n    S\n   \n   to be shorter than\n   \n    P\n   \n   , perhaps a lot shorter. That implies there will be collisions, situations in which two different plaintexts\n   \n    P\n   \n   and\n   \n    P'\n   \n   both hash to\n   \n    S\n   \n   . However, the properties of cryptographic hashes outlined above will make it difficult for an adversary to make use of collisions. Even if you know both\n   \n    S\n   \n   and\n   \n    P\n   \n   , it should be hard to find any other plaintext\n   \n    P'\n   \n   that hashes to\n   \n    S\n   \n\n    3\n   \n   . It won't be hard to figure out what\n   \n    S'\n   \n   should be for an altered value of plaintext\n   \n    P'\n   \n   , since you can simply apply the cryptographic hashing algorithm directly to\n   \n    P'\n   \n   . But even a slightly altered version of\n   \n    P\n   \n   , such as a\n   \n    P'\n   \n   differing only in one bit, should produce a hash\n   \n    S'\n   \n   that differs from\n   \n    S\n   \n   in completely unpredictable ways.\n\n\nCryptographic hashes can be used for other purposes than ensuring integrity of encrypted data, as well. They are the class of hashes of choice for storing salted hashed passwords, for example, as discussed in the chapter on authentication. They can be used to determine if a stored file has been altered, a function provided by well-known security software like Tripwire. They can also be used to force a process to perform a certain amount of work before submitting a request, an approach called \"proof of work.\" The submitter is required to submit a request that hashes to a certain value using some specified cryptographic hash, which, because of the properties of such hashes, requires them to try a lot of request formats before finding one that hashes to the required value. Since each hash operation takes some time, submitting a proper request will require a predictable amount of work. This use of hashes, in varying forms, occurs in several applications, including spam prevention and blockchains.\n\n\nLike other cryptographic algorithms, you're well advised to use standard algorithms for cryptographic hashing. For example, the SHA-3 algorithm is commonly regarded as a good choice. However, there is a history of cryptographic hashing algorithms becoming obsolete, so if you are designing a system that uses one, it's wise to first check to see what current recommendations are for choices of such an algorithm."
        },
        {
          "name": "Cracking Cryptography",
          "content": "Chances are that you've heard about people cracking cryptography. It's a popular theme in film and television. How worried should you be about that?\n\n\n3\n   \n   Every so often, a well known cryptographic hashing function is \"broken\" in the sense that someone figures out how to create a\n   \n    P'\n   \n   that uses the function to produce the same hash as\n   \n    P\n   \n   . That happened to a hashing function known as SHA-1 in 2017, rendering that function unsafe and unusable for integrity purposes [G17].\n\n\nWell, if you didn't take our earlier advice and went ahead and built your own cipher, you should be very worried. Worried enough that you should stop reading this, rip out your own cipher from your system, and replace it with a well-known respected standard. Go ahead, we'll still be here when you get back.\n\n\nWhat if you did use one of those standards? In that case, you're probably OK. If you use a modern standard, with a few unimportant exceptions, there are no known ways to read data encrypted with these algorithms without obtaining the key. Which isn't to say your system is secure, but probably no one will break into it by cracking the cryptographic algorithm.\n\n\nHow will they do it, then? Probably by exploiting software flaws in your system having nothing to do with the cryptography, but there's some chance they will crack it by obtaining your keys or exploiting some other flaw in your management of cryptography. How? Software flaws in how you create and use your keys are a common problem. In distributed environments, flaws in the methods used to share keys are also a common weakness that can be exploited. Peter Gutmann produced a nice survey of the sorts of problems improper management of cryptography frequently causes [G02]. Examples include distributing secret keys in software shared by many people, incorrectly transmitting plaintext versions of keys across a network, and choosing keys from a seriously reduced set of possible choices, rather than the larger theoretically possible set. More recently, the Heartbleed attack demonstrated a way to obtain keys being used in OpenSSL sessions from the memory of a remote computer, which allowed an attacker to decrypt the entire session, despite no flaws in either the cipher itself or its implementation, nor in its key selection procedures. This flaw allowed attackers to read the traffic of something between 1/4 and 1/2 of all sites using HTTPS, the cryptographically protected version of HTTP [D+14].\n\n\nOne way attackers deal with cryptography is by guessing the key. Doing so doesn't actually crack the cryptography at all. Cryptographic algorithms are designed to prevent people who don't know the key from obtaining the secrets. If you know the key, it's not supposed to make decryption hard.\n\n\nSo an attacker could try simply guessing each possible key and trying it. That's called a brute force attack, and it's why you should use long keys. For example, AES keys are at least 128 bits. Assuming you generate your AES key at random, an attacker will need to make\n   \n    2^{127}\n   \n   guesses at your key, on average, before he gets it right. That's a lot of guesses and will take a lot of time. Of course, if a software flaw causes your system to select one out of thirty two possible AES keys, instead of one out of\n   \n    2^{128}\n   \n   , a brute force attack may become trivial. Key selection is a big deal for cryptography.\n\n\nFor example, the original 802.11 wireless networking standard included no cryptographic protection of data being streamed through the air. The\n\n\n\n\n**TIP: SELECTING KEYS**\n\n\nOne important aspect of key secrecy is selecting a good one to begin with. For public key cryptography, you need to run an algorithm to select one of the few possible pairs of keys you will use. But for symmetric cryptography, you are free to select any of the possible keys. How should you choose?\n\n\nRandomly. If you use any deterministic method to select your key, your opponent's problem of finding out your key has just been converted into a problem of figuring out your method. Worse, since you'll probably generate many keys over the course of time, once he knows your method, he'll get all of them. If you use random chance to generate keys, though, figuring out one of them won't help your opponent figure out any of your other keys. This highly desirable property in a cryptographic system is called\n   **perfect forward secrecy**\n   .\n\n\nUnfortunately, true randomness is hard to come by. The best source for operating system purposes is to examine hardware processes that are believed to be random in nature, like low order bits of the times required for pieces of hardware to perform operations, and convert the results into random numbers. That's called\n   **gathering entropy**\n   . In Linux, this is done for you automatically, and you can use the gathered entropy by reading\n   \n    /dev/random\n   \n   . Windows has a similar entropy-gathering feature. Use these to generate your keys. They're not perfect, but they're good enough for many purposes.\n\n\nfirst attempt to add such protection was called WEP (Wired Equivalent Protocol, a rather optimistic name). WEP was constrained by the need to fit into the existing standard, but the method it used to generate and distribute symmetric keys was seriously flawed. Merely by listening in on wireless traffic on an 802.11 network, an attacker could determine the key being used in as little as a minute. There are widely available tools that allow anyone to do so\n   \n    4\n   \n   .\n\n\nAs another example, an early implementation of the Netscape web browser generated cryptographic keys using some easily guessable values as seeds to a random number generator, such as the time of day and the ID of the process requesting the key. Researchers discovered they could guess the keys produced in around 30 seconds [GW96].\n\n\nYou might have heard that PK systems use much longer keys, 2K or 4K bits. Sounds much safer, no? Shouldn't that at least make them stronger against brute force attacks? However, you can't select keys for this type of\n\n\n4\n   \n   WEP got replaced by WPA. Unfortunately, WPA proved to have its own weaknesses, so it was replaced by WPA2. Unfortunately, WPA2 proved to have its own weaknesses, so it is being replaced by WPA3, as of 2018. The sad fate of providing cryptography for wireless networks should serve as a lesson to any of you tempted to underestimate the difficulties in getting this stuff right.\n\n\ncryptosystem at random. Only a relatively few pairs of public and private keys are possible. That's because the public and private keys must be related to each other for the system to work. The relationship is usually mathematical, and usually intended to be mathematically hard to derive, so knowing the public key should not make it easy to learn the private key. However, with the public key in hand, one can use the mathematical properties of the system to derive the private key eventually. That's why PK systems use such big keys – to make sure \"eventually\" is a very long time.\n\n\nBut that only matters if you keep the private key secret. By now, we hope this sounds obvious, but many makers of embedded devices use PK to provide encryption for those devices, and include a private key in the device's software. All too often, the same private key is used for all devices of a particular model. Such shared private keys invariably become, well, public. In September 2016, one study found 4.5 million embedded devices relying on these private keys that were no longer so private [V16]. Anyone could pose as any of these devices for any purpose, and could read any information sent to them using PK. In essence, the cryptography performed by these devices was little more than window dressing and did not increase the security of the devices by any appreciable amount.\n\n\nTo summarize, cracking cryptography is usually about learning the key. Or, as you might have guessed:\n   **THE CRYPTOGRAPHY'S BENEFIT RELIES ENTIRELY ON THE SECRECY OF THE KEY.**"
        },
        {
          "name": "Cryptography And Operating Systems",
          "content": "Cryptography is fascinating, but lots of things are fascinating\n   \n    5\n   \n   , while having no bearing on operating systems. Why did we bother spending half a chapter on cryptography? Because we can use it to protect operating systems.\n\n\nBut not just anywhere and for all purposes. We've pounded into your head that key secrecy is vital for effective use of cryptography. That should make it clear that any time the key can't be kept secret, you can't effectively use cryptography. Casting your mind back to the first chapter on security, remember that the operating system has control of and access to all resources on a computer. Which implies that if you have encrypted information on the computer, and you have the necessary key to decrypt it on the same computer, the operating system on that machine can decrypt the data, whether that was the effect you wanted or not\n   \n    6\n   \n   .\n\n\n5\n   \n   For example, the late piano sonatas of Beethoven. One movement of his last Sonata, Opus 111, even sounds like jazz, while being written in the 1820s!\n\n\n6\n   \n   But remember our discussion of security enclaves in an earlier chapter, hardware that does not allow the operating system full access to information that the enclave protects. Think for a moment what the implications of that are for cryptography on a computer using such an enclave, and what new possibilities it offers.\n\n\nEither you trust your operating system or you don't. If you don't, life is going to be unpleasant anyway, but one implication is that the untrusted operating system, having access at one time to your secret key, can copy it and re-use it whenever it wants to. If, on the other hand, you trust your operating system, you don't need to hide your data from it, so cryptography isn't necessary in this case. This observation has relevance to any situation in which you provide your data to something you don't trust. For instance, if you don't trust your cloud computing facility with your data, you won't improve the situation by giving them your data in plaintext and asking them to encrypt it. They've seen the plaintext and can keep a copy of the key.\n\n\nIf you're sure your operating system is trustworthy right now, but are concerned it might not be later, you can encrypt something now and make sure the key is not stored on the machine. Of course, if you're wrong about the current security of the operating system, or if you ever decrypt the data on the machine after the OS goes rogue, your cryptography will not protect you, since that ever-so-vital secrecy of the key will be compromised.\n\n\nOne can argue that not all compromises of an operating system are permanent. Many are, but some only give an attacker temporary access to system resources, or perhaps access to only a few particular resources. In such cases, if the encrypted data is not stored in plaintext and the decryption key is not available at the time or in the place the attacker can access, encrypting that data may still provide benefit. The tricky issue here is that you can't know ahead of time whether successful attacks on your system will only occur at particular times, for particular durations, or on particular elements of the system. So if you take this approach, you want to minimize all your exposure: decrypt infrequently, dispose of plaintext data quickly and carefully, and don't keep a plaintext version of the key in the system except when performing the cryptographic operations. Such minimization can be difficult to achieve.\n\n\nIf cryptography won't protect us completely against a dishonest operating system, what OS uses for cryptography are there? We saw a specialized example in the chapter on authentication. Some cryptographic operations are one-way: they can encrypt, but never decrypt. We can use these to securely store passwords in encrypted form, even if the OS is compromised, since the encrypted passwords can't be decrypted\n   \n    7\n   \n   .\n\n\nWhat else? In a distributed environment, if we encrypt data on one machine and then send it across the network, all the intermediate components won't be part of our machine, and thus won't have access to the key. The data will be protected in transit. Of course, our partner on the\n\n\n7\n   \n   But if the legitimate user ever provides the correct password to a compromised OS, all bets are off, alas. The compromised OS will copy the password provided by the user and hand it off to whatever villain is working behind the scenes, before it runs the password through the one-way cryptographic hashing algorithm.\n\n\nfinal destination machine will need the key if he or she is to use the data. As we promised before, we'll get to that issue in another chapter.\n\n\nAnything else? Well, what if someone can get access to some of our hardware without going through our operating system? If the data stored on that hardware is encrypted, and the key isn't on that hardware itself, the cryptography will protect the data. This form of encryption is sometimes called\n   **at-rest data encryption**\n   , to distinguish it from encrypting data we're sending between machines. It's useful and important, so let's examine it in more detail."
        },
        {
          "name": "At-Rest Data Encryption",
          "content": "As we saw in the chapters on persistence, data can be stored on a disk drive, flash drive, or other medium. If it's sensitive data, we might want some of our desirable security properties, such as secrecy or integrity, to be applied to it. One technique to achieve these goals for this data is to store it in encrypted form, rather than in plaintext. Of course, encrypted data cannot be used in most computations, so if the machine where it is stored needs to perform a general computation on the data, it must first be decrypted\n   \n    8\n   \n   . If the purpose is merely to preserve a safe copy of the data, rather than to use it, decryption may not be necessary, but that is not the common case.\n\n\nThe data can be encrypted in different ways, using different ciphers (DES, AES, Blowfish), at different granularities (records, data blocks, individual files, entire file systems), by different system components (applications, libraries, file systems, device drivers). One common general use of at-rest data encryption is called\n   **full disk encryption**\n   . This usually means that the entire contents (or almost the entire contents) of the storage device are encrypted. Despite the name, full-disk encryption can actually be used on many kinds of persistent storage media, not just hard disk drives. Full disk encryption is usually provided either in hardware (built into the storage device) or by system software (a device driver or some element of a file system). In either case, the operating system plays a role in the protection provided. Windows BitLocker and Apple's FileVault are examples of software-based full disk encryption.\n\n\nGenerally, at boot time either the decryption key or information usable to obtain that key (such as a passphrase – like a password, but possibly multiple words) is requested from the user. If the right information is provided, the key or keys necessary to perform the decryption become available (either to the hardware or the operating system). As data is placed on the device, it is encrypted. As data moves off the device, it is\n\n\n8\n   \n   There's one possible exception worth mentioning. Those cryptographic wizards have created a form of cryptography called\n   **homomorphic cryptography**\n   , which allows you to perform operations on the encrypted form of the data without decrypting it. For example, you could add one to an encrypted integer without decrypting it first. When you decrypted the result, sure enough, one would have been added to the original number. Homomorphic ciphers have been developed, but high computational and storage costs render them impractical for most purposes, as of the writing of this chapter. Perhaps that will change, with time.\n\n\ndecrypted. The data remains decrypted as long as it is stored anywhere in the machine's memory, including in shared buffers or user address space. When new data is to be sent to the device, it is first encrypted. The data is never placed on the storage device in decrypted form. After the initial request to obtain the decryption key is performed, encryption and decryption are totally transparent to users and applications. They never see the data in encrypted form and are not asked for the key again, until the machine reboots.\n\n\nCryptography is a computationally expensive operation, particularly if performed in software. There will be overhead associated with performing software-based full disk encryption. Reports of the amount of overhead vary, but a few percent extra latency for disk-heavy operations is common. For operations making less use of the disk, the overhead may be imperceptible. For hardware-based full disk encryption, the rated speed of the disk drive will be achieved, which may or may not be slower than a similar model not using full disk encryption.\n\n\nWhat does this form of encryption protect against?\n\n\n  * • It offers no extra protection against users trying to access data they should not be allowed to see. Either the standard access control mechanisms that the operating system provides work (and such users can't get to the data because they lack access permissions) or they don't (in which case such users will be given equal use of the decryption key as anyone else).\n  * • It does not protect against flaws in applications that divulge data. Such flaws will permit attackers to pose as the user, so if the user can access the unencrypted data, so can the attacker. For example, it offers little protection against buffer overflows or SQL injections.\n  * • It does not protect against dishonest privileged users on the system, such as a system administrator. Administrator's privileges may allow the admin to pose as the user who owns the data or to install system components that provide access to the user's data; thus, the admin could access decrypted copies of the data on request.\n  * • It does not protect against security flaws in the OS itself. Once the key is provided, it is available (directly in memory, or indirectly by asking the hardware to use it) to the operating system, whether that OS is trustworthy and secure or compromised and insecure.\n\n\nSo what benefit does this form of encryption provide? Consider this situation. If a hardware device storing data is physically moved from one machine to another, the OS on the other machine is not obligated to honor the access control information stored on the device. In fact, it need not even use the same file system to access that device. For example, it can treat the device as merely a source of raw data blocks, rather than an organized file system. So any access control information associated with files on the device might be ignored by the new operating system.\n\n\nHowever, if the data on the device is encrypted via full disk encryption, the new machine will usually be unable to obtain the encryption\n\n\nkey. It can access the raw blocks, but they are encrypted and cannot be decrypted without the key. This benefit would be useful if the hardware in question was stolen and moved to another machine, for example. This situation is a very real possibility for mobile devices, which are frequently lost or stolen. Disk drives are sometimes resold, and data belonging to the former owner (including quite sensitive data) has been found on them by the re-purchaser. These are important cases where full disk encryption provides real benefits.\n\n\nFor other forms of encryption of data at rest, the system must still address the issues of how much is encrypted, how to obtain the key, and when to encrypt and decrypt the data, with different types of protection resulting depending on how these questions are addressed. Generally, such situations require that some software ensures that the unencrypted form of the data is no longer stored anywhere, including caches, and that the cryptographic key is not available to those who might try to illicitly access the data. There are relatively few circumstances where such protection is of value, but there are a few common examples:\n\n\n  * • Archiving data that might need to be copied and must be preserved, but need not be used. In this case, the data can be encrypted at the time of its creation, and perhaps never decrypted, or only decrypted under special circumstances under the control of the data's owner. If the machine was uncompromised when the data was first encrypted and the key is not permanently stored on the system, the encrypted data is fairly safe. Note, however, that if the key is lost, you will never be able to decrypt the archived data.\n  * • Storing sensitive data in a cloud computing facility, a variant of the previous example. If one does not completely trust the cloud computing provider (or one is uncertain of how careful that provider is – remember, when you trust another computing element, you're trusting not only its honesty, but also its carefulness and correctness), encrypting the data before sending it to the cloud facility is wise. Many cloud backup products include this capability. In this case, the cryptography and key use occur before moving the data to the untrusted system, or after it is recovered from that system.\n  * • User-level encryption performed through an application. For example, a user might choose to encrypt an email message, with any stored version of it being in encrypted form. In this case, the cryptography will be performed by the application, and the user will do something to make a cryptographic key available to the application. Ideally, that application will ensure that the unencrypted form of the data and the key used to encrypt it are no longer readily available after encryption is completed. Remember, however, that while the key exists, the operating system can obtain access to it without your application knowing.\n\n\nOne important special case for encrypting selected data at rest is a\n   **password vault**\n   (also known as a\n   **key ring**\n   ), which we discussed in the\n\n\nauthentication chapter. Typical users interact with many remote sites that require them to provide passwords (authentication based on “what you know”, remember?) The best security is achieved if one uses a different password for each site, but doing so places a burden on the human user, who generally has a hard time remembering many passwords. A solution is to encrypt all the different passwords and store them on the machine, indexed by the site they are used for. When one of the passwords is required, it is decrypted and provided to the site that requires it.\n\n\nFor password vaults and all such special cases, the system must have some way of obtaining the required key whenever data needs to be encrypted or decrypted. If an attacker can obtain the key, the cryptography becomes useless, so safe storage of the key becomes critical. Typically, if the key is stored in unencrypted form anywhere on the computer in question, the encrypted data is at risk, so well designed encryption systems tend not to do so. For example, in the case of password vaults, the key used to decrypt the passwords is not stored in the machine’s stable storage. It is obtained by asking the user for it when required, or asking for a passphrase used to derive the key. The key is then used to decrypt the needed password. Maximum security would suggest destroying the key as soon as this decryption was performed (remember the principle of least privilege?), but doing so would imply that the user would have to re-enter the key each time a password was needed (remember the principle of acceptability?). A compromise between usability and security is reached, in most cases, by remembering the key after first entry for a significant period of time, but only keeping it in RAM. When the user logs out, or the system shuts down, or the application that handles the password vault (such as a web browser) exits, the key is “forgotten.” This approach is reminiscent of single sign-on systems, where a user is asked for a password when the system is first accessed, but is not required to re-authenticate again until logging out. It has the same disadvantages as those systems, such as permitting an unattended terminal to be used by unauthorized parties to use someone else’s access permissions. Both have the tremendous advantage that they don’t annoy their users so much that they are abandoned in favor of systems offering no security whatsoever."
        },
        {
          "name": "Cryptographic Capabilities",
          "content": "Remember from our chapter on access control that capabilities had the problem that we could not leave them in users’ hands, since then users could forge them and grant themselves access to anything they wanted. Cryptography can be used to create unforgeable capabilities. A trusted entity could use cryptography to create a sufficiently long and securely encrypted data structure that indicated that the possessor was allowed to have access to a particular resource. This data structure could then be given to a user, who would present it to the owner of the matching resource to obtain access. The system that actually controlled the resource must be able to check the validity of the data structure before granting access, but would not need to maintain an access control list.\n\n\nSuch cryptographic capabilities could be created either with symmetric or public key cryptography. With symmetric cryptography, both the creator of the capability and the system checking it would need to share the same key. This option is most feasible when both of those entities are the same system, since otherwise it requires moving keys around between the machines that need to use the keys, possibly at high speed and scale, depending on the use scenario. One might wonder why the single machine would bother creating a cryptographic capability to allow access, rather than simply remembering that the user had passed an access check, but there are several possible reasons. For example, if the machine controlling the resource worked with vast numbers of users, keeping track of the access status for each of them would be costly and complex, particularly in a distributed environment where the system needed to worry about failures and delays. Or if the system wished to give transferable rights to the access, as it might if the principal might move from machine to machine, it would be more feasible to allow the capability to move with the principal and be used from any location. Symmetric cryptographic capabilities also make sense when all of the machines creating and checking them are inherently trusted and key distribution is not problematic.\n\n\nIf public key cryptography is used to create the capabilities, then the creator and the resource controller need not be co-located and the trust relationships need not be as strong. The creator of the capability needs one key (typically the secret key) and the controller of the resource needs the other. If the content of the capability is not itself secret, then a true public key can be used, with no concern over who knows it. If secrecy (or at least some degree of obscurity) is required, what would otherwise be a public key can be distributed only to the limited set of entities that would need to check the capabilities\n   \n    9\n   \n   . A resource manager could create a set of credentials (indicating which principal was allowed to use what resources, in what ways, for what period of time) and then encrypt them with a private key. Any one else can validate those credentials by decrypting them with the manager's public key. As long as only the resource manager knows the private key, no one can forge capabilities.\n\n\nAs suggested above, such cryptographic capabilities can hold a good deal of information, including expiration times, identity of the party who was given the capability, and much else. Since strong cryptography will ensure integrity of all such information, the capability can be relied upon. This feature allows the creator of the capability to prevent arbitrary copying and sharing of the capability, at least to a certain extent. For example, a cryptographic capability used in a network context can be tied to a particular IP address, and would only be regarded as valid if the message carrying it came from that address.\n\n\n9\n   \n   Remember, however, that if you are embedding a key in a piece of widely distributed software, you can count on that key becoming public knowledge. So even if you believe the matching key is secret, not public, it is unwise to rely too heavily on that belief.\n\n\nMany different encryption schemes can be used. The important point is that the encrypted capabilities must be long enough that it is computationally infeasible to find a valid capability by brute force enumeration or random guessing (e.g., the number of invalid bit patterns is\n   \n    10^{15}\n   \n   times larger than the number of valid bit patterns).\n\n\nWe'll say a bit more about cryptographic capabilities in the chapter on distributed system security."
        }
      ]
    },
    {
      "name": "Distributed System Security",
      "sections": [
        {
          "name": "Introduction",
          "content": "An operating system can only control its own machine's resources. Thus, operating systems will have challenges in providing security in distributed systems, where more than one machine must cooperate. There are two large problems:\n\n\n  * • The other machines in the distributed system might not properly implement the security policies you want, or they might be adversaries impersonating trusted partners. We cannot control remote systems, but we still have to be able to trust validity of the credentials and capabilities they give us.\n  * • Machines in a distributed system communicate across a network that none of them fully control and that, generally, cannot be trusted. Adversaries often have equal access to that network and can forge, copy, replay, alter, destroy, and delay our messages, and generally interfere with our attempts to use the network.\n\n\nAs suggested earlier, cryptography will be the major tool we use here, but we also said cryptography was hard to get right. That makes it sound like the perfect place to use carefully designed standard tools, rather than to expect everyone to build their own. That's precisely correct. As such:\n\n\n\n\n**THE CRUX: HOW TO PROTECT DISTRIBUTED SYSTEM OPERATIONS**\n\n\nHow can we secure a system spanning more than one machine? What tools are available to help us protect such systems? How do we use them properly? What are the areas in using the tools that require us to be careful and thoughtful?"
        },
        {
          "name": "The Role of Authentication",
          "content": "How can we handle our uncertainty about whether our partners in a distributed system are going to enforce our security policies? In most cases, we can't do much. At best, we can try to arrange to agree on policies and hope everyone follows through on those agreements. There are some special cases where we can get high-quality evidence that our partners have behaved properly, but that's not easy, in general. For example, how can we know that they are using full disk encryption, or that they have carefully wiped an encryption key we are finished using, or that they have set access controls on the local copies of their files properly? They can say they did, but how can we\n   *know*\n   ?\n\n\nGenerally, we can't. But you're used to that. In the real world, your friends and relatives know some secrets about you, and they might have keys to get into your home, and if you loan them your car you're fairly sure you'll get it back. That's not so much because you have perfect mechanisms to prevent those trusted parties from behaving badly, but because you are pretty sure they won't. If you're wrong, perhaps you can detect that they haven't behaved well and take compensating actions (like changing your locks or calling the police to report your car stolen). We'll need to rely on similar approaches in distributed computer systems. We will simply have to trust that some parties will behave well. In some cases, we can detect when they don't and adjust our trust in the parties accordingly, and maybe take other compensating actions.\n\n\nOf course, in the cyber world, our actions are at a distance over a network, and all we see are bits going out and coming in on the network. For a trust-based solution to work, we have to be quite sure that the bits we send out can be verified by our buddies as truly coming from us, and we have to be sure that the bits coming in really were created by them. That's a job for authentication. As suggested in the earlier authentication chapter, when working over a network, we need to authenticate based on a bundle of bits. Most commonly, we use a form of authentication based on what you know. Now, think back to the earlier chapters. What might someone running on a remote operating system know that no one else knows? How about a password? How about a private key?\n\n\nMost of our distributed system authentication will rely on one of these two elements. Either you require the remote machine to provide you with a password, or you require it to provide evidence using a private key stored only on that machine\n   \n    1\n   \n   . In each case, you need to know something to check the authentication: either the password (or, better, a cryptographic hash of the password plus a salt) or the public key.\n\n\n1\n   \n   We occasionally use other methods, such as smart cards or remote biometric readers. They are less common in today's systems, though. If you understand how we use passwords and public key cryptography for distributed system authentication, you can probably figure out how to make proper use of these other techniques, too. If you don't, you'll be better off figuring out the common techniques before moving to the less common ones.\n\n\nWhen is each appropriate? Passwords tend to be useful if there are a vast number of parties who need to authenticate themselves to one party. Public keys tend to be useful if there's one party who needs to authenticate himself to a vast number of parties. Why? With a password, the authentication provides evidence that somebody knows a password. If you want to know exactly who that is (which is usually important), only the party authenticating and the party checking can know it. With a public key, many parties can know the key, but only one party who knows the matching private key can authenticate himself. So we tend to use both mechanisms, but for different cases. When a web site authenticates itself to a user, it's done with PK cryptography. By distributing one single public key (to vast numbers of users), the web site can be authenticated by all its users. The web site need not bother keeping separate authentication information to authenticate itself to each user. When that user authenticates itself to the web site, it's done with a password. Each user must be separately authenticated to the web site, so we require a unique piece of identifying information for that user, preferably something that's easy for a person to use. Setting up and distributing public keys is hard, while setting up individual passwords is relatively easy.\n\n\nHow, practically, do we use each of these authentication mechanisms in a distributed system? If we want a remote partner to authenticate itself via passwords, we will require it to provide us with that password, which we will check. We'll need to encrypt the transport of the password across the network if we do that; otherwise anyone eavesdropping on the network (which is easy for many wireless networks) will readily learn passwords sent unencrypted. Encrypting the password will require that we already have either a shared symmetric key or our partner's public key. Let's concentrate now on how we get that public key, either to use it directly or set up the cryptography to protect the password in transit.\n\n\nWe'll spend the rest of the chapter on securing the network connection, but please don't forget that even if you secure the network perfectly, you still face the major security challenge of the uncontrolled site you're interacting with on the other side of the network. If your compromised partner attacks you, it will offer little consolation that the attack was authenticated and encrypted."
        },
        {
          "name": "Public Key Authentication For Distributed Systems",
          "content": "The public key doesn't need to be secret, but we need to be sure it really belongs to our partner. If we have a face-to-face meeting, our partner can directly give us a public key in some form or another, in which case we can be pretty sure it's the right one. That's limiting, though, since we often interact with partners whom we never see face to face. For that matter, whose \"face\" belongs to Amazon\n   \n    2\n   \n   or Google?\n\n\n2\n   \n   How successful would Amazon be if Jeff Bezos had to make an in-person visit to every customer to deliver them Amazon's public key? Answer: Not as successful.\n\n\nFortunately, we can use the fact that secrecy isn't required to simply create a bunch of bits containing the public key. Anyone who gets a copy of the bits has the key. But how do they know for sure whose key it is? What if some other trusted party known to everyone who needs to authenticate our partner used their own public key to cryptographically sign that bunch of bits, verifying that they do indeed belong to our partner? If we could check that signature, we could then be sure that bunch of bits really does represent our partner's public key, at least to the extent that we trust that third party who did the signature.\n\n\nThis technique is how we actually authenticate web sites and many other entities on the Internet. Every time you browse the web or perform any other web-based activity, you use it. The signed bundle of bits is called a\n   **certificate**\n   . Essentially, it contains information about the party that owns the public key, the public key itself, and other information, such as an expiration date. The entire set of information, including the public key, is run through a cryptographic hash, and the result is encrypted with the trusted third party's private key, digitally signing the certificate. If you obtain a copy of the certificate, and can check the signature, you can learn someone else's public key, even if you have never met or had any direct interaction with them. In certain ways, it's a beautiful technology that empowers the whole Internet.\n\n\nLet's briefly go through an example, to solidify the concepts. Let's say Frobazz Inc. wants to obtain a certificate for its public key, which is\n   \n    KF\n   \n   . Frobazz Inc. pays big bucks to Acmesign Co., a widely trusted company whose business it is to sell certificates, to obtain a certificate signed by AcmeSign. Such companies are commonly called\n   **Certificate Authorities**\n   , or\n   **CAs**\n   , since they create authoritative certificates trusted by many parties. Acmesign checks up on Frobazz Inc. to ensure that the people asking for the certificate actually are legitimate representatives of Frobazz. Acmesign then makes very, very sure that the public key it's about to embed in a certificate actually is the one that Frobazz wants to use. Assuming it is, Acmesign runs a cryptographic hashing algorithm (perhaps SHA-3 which, unlike SHA-1, has not been cracked, as of 2020) on Frobazz's name, public key\n   \n    KF\n   \n   , and other information, producing hash\n   \n    HF\n   \n   . Acmesign then encrypts\n   \n    HF\n   \n   with its own private key,\n   \n    PA\n   \n   , producing digital signature\n   \n    SF\n   \n   . Finally, Acmesign combines all the information used to produce\n   \n    HF\n   \n   , plus Acmesign's own identity and the signature\n   \n    SF\n   \n   , into the certificate\n   \n    CF\n   \n   , which it hands over to Frobazz, presumably in exchange for money. Remember,\n   \n    CF\n   \n   is just some bits.\n\n\nNow Frobazz Inc. wants to authenticate itself over the Internet to one of its customers. If the customer already has Frobazz's public key, we can use public key authentication mechanisms directly. If the customer does not have the public key, Frobazz sends\n   \n    CF\n   \n   to the customer. The customer examines the certificate, sees that it was generated by Acmesign using, say, SHA-3, and runs the same information that Acmesign hashed (all of which is in the certificate itself) through SHA-3, producing\n   \n    HF'\n   \n   . Then the customer uses Acmesign's public key to decrypt\n   \n    SF\n   \n   (also in the\n\n\ncertificate), obtaining\n   \n    HF\n   \n   . If all is well,\n   \n    HF\n   \n   equals\n   \n    HF'\n   \n   , and now the customer knows that the public key in the certificate is indeed Frobazz's. Public key-based authentication can proceed\n   \n    3\n   \n   . If the two hashes aren't exactly the same, the customer knows that something fishy is going on and will not accept the certificate.\n\n\nThere are some wonderful properties about this approach to learning public keys. First, note that the signing authority (Acmesign, in our example) did not need to participate in the process of the customer checking the certificate. In fact, Frobazz didn't really, either. The customer can get the certificate from literally anywhere and obtain the same degree of assurance of its validity. Second, it only needs to be done once per customer. After obtaining the certificate and checking it, the customer has the public key that is needed. From that point onward, the customer can simply store it and use it. If, for whatever reason, it gets lost, the customer can either extract it again from the certificate (if that has been saved), or go through the process of obtaining the certificate again. Third, the customer had no need to trust the party claiming to be Frobazz until that identity had been proven by checking the certificate. The customer can proceed with caution until the certificate checks out.\n\n\nAssuming you've been paying attention for the last few chapters, you should be saying to yourself, \"now, wait a minute, isn't there a chicken-and-egg problem here?\" We'll learn Frobazz's public key by getting a certificate for it. The certificate will be signed by Acmesign. We'll check the signature by knowing Acmesign's public key. But where did we get Acmesign's key? We really hope you did have that head-scratching moment and asked yourself that question, because if you did, you understand the true nature of the Internet authentication problem. Ultimately, we've got to bootstrap it. You've got to somehow or other obtain a public key for somebody that you trust. Once you do, if it's the right public key for the right kind of party, you can then obtain a lot of other public keys. But without something to start from, you can't do much of anything.\n\n\nWhere do you get that primal public key? Most commonly, it comes in a piece of software you obtain and install. The one you use most often is probably your browser, which typically comes with the public keys for several hundred trusted authorities\n   \n    4\n   \n   . Whenever you go to a new web site that cares about security, it provides you with a certificate containing that site's public key, and signed by one of those trusted authorities pre-configured into your browser. You use the pre-configured public key of that authority to verify that the certificate is indeed proper, after which you know the public key of that web site. From that point onward, you can use the web site's public key to authenticate it. There are some se-\n\n\n3\n   \n   And, indeed, must, since all this business with checking the certificate merely told the customer what Frobazz's public key was. It did nothing to assure the customer that whoever sent the certificate actually was Frobazz or knew Frobazz's private key.\n\n\n4\n   \n   You do know of several hundred companies out there that you trust with everything you do on the web, don't you? Well, know of them or not, you effectively trust them to that extent.\n\n\nrious caveats here (and some interesting approaches to addressing those caveats), but let's put those aside for the moment.\n\n\nAnyone can create a certificate, not just those trusted CAs, either by getting one from someone whose business it is to issue certificates or simply by creating one from scratch, following a certificate standard (X.509 is the most commonly used certificate standard [I12]). The necessary requirement: the party being authenticated and the parties performing the authentication must all trust whoever created the certificate. If they don't trust that party, why would they believe the certificate is correct?\n\n\nIf you are building your own distributed system, you can create your own certificates from a machine you (and other participants in the system) trust and can handle the bootstrapping issue by carefully hand-installing the certificate signing machine's public key wherever it needs to be. There are a number of existing software packages for creating certificates, and, as usual with critical cryptographic software, you're better off using an existing, trusted implementation rather than coding up one of your own. One example you might want to look at is PGP (available in both supported commercial versions and compatible but less supported free versions) [P16], but there are others. If you are working with a fixed number of machines and you can distribute the public key by hand in some reasonable way, you can dispense entirely with certificates. Remember, the only point of a PK certificate is to distribute the public key, so if your public keys are already where they need to be, you don't need certificates.\n\n\nOK, one way or another you've obtained the public key you need to authenticate some remote machine. Now what? Well, anything they send you encrypted with their private key will only decrypt with their public key, so anything that decrypts properly with the public key must have come from them, right? Yes, it must have come from them at some point, but it's possible for an adversary to have made a copy of a legitimate message the site sent at some point in the past and then send it again it at some future date. Depending on exactly what's going on, that could cause trouble, since you may take actions based on that message that the legitimate site did not ask for. So usually we take measures to ensure that we're not being subjected to a\n   **replay attack**\n   . Such measures generally involve ensuring that each encrypted message contains unique information not in any other message. This feature is built in properly to standard cryptographic protocols, so if you follow our advice and use one of those, you will get protection from such replay attacks. If you insist on building your own cryptography, you'll need to learn a good deal more about this issue and will have to apply that knowledge very carefully. Also, public key cryptography is expensive. We want to stop using it as soon as possible, but we also want to continue to get authentication guarantees. We'll see how to do that when we discuss SSL and TLS."
        },
        {
          "name": "Password Authentication For Distributed Systems",
          "content": "The other common option to authenticate in distributed systems is to use a password. As noted above, that will work best in situations where only two parties need to deal with any particular password: the party being authenticated and the authenticating party. They make sense when an individual user is authenticating himself to a site that hosts many users, such as when you log in to Amazon. They don't make sense when that site is trying to authenticate itself to an individual user, such as when a web site claiming to be Amazon wants to do business with you. Public key authentication works better there.\n\n\nHow do we properly handle password authentication over the network, when it is a reasonable choice? The password is usually associated with a particular user ID, so the user provides that ID and password to the site requiring authentication. That typically happens over a network, and typically we cannot guarantee that networks provide confidentiality. If our password is divulged to someone else, they'll be able to pose as us, so we must add confidentiality to this cross-network authentication, generally by encrypting at least the password itself (though encrypting everything involved is better). So a typical interchange with Alice trying to authenticate herself to Frobazz Inc.'s web site would involve the site requesting a user ID and password and Alice providing both, but encrypting them before sending them over the network.\n\n\nThe obvious question you should ask is, encrypting them with what key? Well, if Frobazz authenticated itself to Alice using PK, as discussed above, Alice can encrypt her user ID and password with Frobazz's public key. Frobazz Inc., having the matching private key, will be able to check them, but nobody else can read them. In actuality, there are various reasons why this alone would not suffice, including replay attacks, as mentioned above. But we can and do use Frobazz's private key to set up cryptography that will protect Alice's password in transit. We'll discuss the details in the section on SSL/TLS.\n\n\nWe discussed issues of password choice and management in the chapter on authentication, and those all apply in the networking context. Otherwise, there's not that much more to say about how we'll use passwords, other than to note that after the remote site has verified the password, what does it actually know? That the site or user who sent the password knows it, and, to the strength of the password, that site or user is who it claims to be. But what about future messages that come in, supposedly from that site? Remember, anyone can create any message they want, so if all we do is verify that the remote site sent us the right password, all we know is that particular message is authentic. We don't want to have to include the password on every message we send, just as we don't want to use PK to encrypt every message we send. We will use both authentication techniques to establish initial authenticity, then use something else to tie that initial authenticity to subsequent interactions. Let's move right along to SSL/TLS to talk about how we do that."
        },
        {
          "name": "SSL/TLS",
          "content": "We saw in an earlier chapter that a standard method of communicating between processes in modern systems is the socket. That's equally true when the processes are on different machines. So a natural way to add cryptographic protection to communications crossing unprotected networks is to add cryptographic features to sockets. That's precisely what\n   **SSL**\n   (the\n   **Secure Socket Layer**\n   ) was designed to do, many years ago. Unfortunately, SSL did not get it quite right. That's because it's pretty darn hard to get it right, not because the people who designed and built it were careless. They learned from their mistakes and created a new version of encrypted sockets called\n   **Transport Layer Security (TLS)**\n\n    5\n   \n   . You will frequently hear people talk about using SSL. They are usually treating it as a shorthand for SSL/TLS. SSL, formally, is insecure and should never be used for anything. Use TLS. The only exception is that some very old devices might run software that doesn't support TLS. In that case, it's better to use SSL than nothing. We'll adopt the same shorthand as others from here on, since it's ubiquitous.\n\n\nThe concept behind SSL is simple: move encrypted data through an ordinary socket. You set up a socket, set up a special structure to perform whatever cryptography you want, and hook the output of that structure to the input of the socket. You reverse the process on the other end. What's simple in concept is rather laborious in execution, with a number of steps required to achieve the desired result. There are further complications due to the general nature of SSL. The technology is designed to support a variety of cryptographic operations and many different ciphers, as well as multiple methods to perform key exchange and authentication between the sender and receiver.\n\n\nThe process of adding SSL to your program is intricate, requiring the use of particular libraries and a sequence of calls into those libraries to set up a correct SSL connection. We will not go through those operations step by step here, but you will need to learn about them to make proper use of SSL. Their purpose is, for the most part, to allow a wide range of generality both in the cryptographic options SSL supports and the ways you use those options in your program. For example, these setup calls would allow you to create one set of SSL connections using AES-128 and another using AES-256, if that's what you needed to do.\n\n\nOne common requirement for setting up an SSL connection that we will go through in a bit more detail is how to securely distribute whatever cryptographic key you will use for the connection you are setting up. Best cryptographic practice calls for you to use a brand new key to encrypt the bulk of your data for each connection you set up. You will use\n\n\n5\n   \n   Actually, even the first couple of versions of TLS didn't get it quite right. As of 2020, the current version of TLS is 1.3, and that's probably what you should use. TLS 1.3 closed some vulnerabilities that TLS 1.2 is subject to. The history of required changes to SSL/TLS should further reinforce the lesson of how hard it is to use cryptography properly, which in turn should motivate you to foreswear ever trying to roll your own crypto.\n\n\npublic/private keys for authentication many times, but as we discussed earlier, you need to use symmetric cryptography to encrypt the data once you have authenticated your partner, and you want a fresh key for that. Even if you are running multiple simultaneous SSL connections with the same partner, you want a different symmetric key for each connection.\n\n\nSo what do you need to do to set up a new SSL connection? We won't go through all of the gory details, but, in essence, SSL needs to bootstrap a secure connection based (usually) on asymmetric cryptography when no usable symmetric key exists. (You'll hear \"usually\" and \"normally\" and \"by default\" a lot in SSL discussions, because of SSL's ability to support a very wide range of options, most of which are ordinarily not what you want to do.) The very first step is to start a negotiation between the client and the server. Each party might only be able to handle particular ciphers, secure hashes, key distribution strategies, or authentication schemes, based on what version of SSL they have installed, how it's configured, and how the programs that set up the SSL connection on each side were written. In the most common cases, the negotiation will end in both sides finding some acceptable set of ciphers and techniques that hit a balance between security and performance. For example, they might use RSA with 2048 bit keys for asymmetric cryptography, some form of a Diffie-Hellman key exchange mechanism (see the Aside on this mechanism) to establish a new symmetric key, SHA-3 to generate secure hashes for integrity, and AES with 256 bit keys for bulk encryption. A modern installation of SSL might support 50 or more different combinations of these options.\n\n\nIn some cases, it may be important for you to specify which of these many combinations are acceptable for your system, but often most of them will do, in which case you can let SSL figure out which to use for each connection without worrying about it yourself. The negotiation will happen invisibly and SSL will get on with its main business: authenticating at least the server (optionally the client), creating and distributing a new symmetric key, and running the communication through the chosen cipher using that key.\n\n\nWe can use Diffie-Hellman key exchange to create the key (and SSL frequently does), but we need to be sure who we are sharing that key with. SSL offers a number of possibilities for doing so. The most common method is for the client to obtain a certificate containing the server's public key (typically by having the server send it to the client) and to use the public key in that certificate to verify the authenticity of the server's messages. It is possible for the client to obtain the certificate through some other means, though less common. Note that having the server send the certificate is every bit as secure (or insecure) as having the client obtain the certificate through other means. Certificate security is not based on the method used to transport it, but on the cryptography embedded in the certificate.\n\n\nWith the certificate in hand (however the client got it), the Diffie-Hellman key exchange can now proceed in an authenticated fashion. The server\n\n\n\n\n**ASIDE: DIFFIE-HELLMAN KEY EXCHANGE**\n\n\nWhat if you want to share a secret key between two parties, but they can only communicate over an insecure channel, where eavesdroppers can hear anything they say? You might think this is an impossible problem to solve, but you'd be wrong. Two extremely smart cryptographers named Whitfield Diffie and Martin Hellman solved this problem years ago, and their solution is in common use. It's called\n   **Diffie-Hellman key exchange**\n   .\n\n\nHere's how it works. Let's say Alice and Bob want to share a secret key, but currently don't share anything, other than the ability to send each other messages. First, they agree on two numbers,\n   \n    n\n   \n   (a large prime number) and\n   \n    g\n   \n   (which is primitive mod\n   \n    n\n   \n   ). They can use the insecure channel to do this, since\n   \n    n\n   \n   and\n   \n    g\n   \n   don't need to be secret. Alice chooses a large random integer, say\n   \n    x\n   \n   , calculates\n   \n    X = g^x \\bmod n\n   \n   , and sends\n   \n    X\n   \n   to Bob. Bob independently chooses a large random integer, say\n   \n    y\n   \n   , calculates\n   \n    Y = g^y \\bmod n\n   \n   , and sends\n   \n    Y\n   \n   to Alice. The eavesdroppers can hear\n   \n    X\n   \n   and\n   \n    Y\n   \n   , but since Alice and Bob didn't send\n   \n    x\n   \n   or\n   \n    y\n   \n   , the eavesdroppers don't know those values. It's important that Alice and Bob keep\n   \n    x\n   \n   and\n   \n    y\n   \n   secret.\n\n\nAlice now computes\n   \n    k = Y^x \\bmod n\n   \n   , and Bob computes\n   \n    k = X^y \\bmod n\n   \n   . Alice and Bob get the same value\n   \n    k\n   \n   from these computations. Why? Well,\n   \n    Y^x \\bmod n = (g^y \\bmod n)^x \\bmod n\n   \n   , which in turn equals\n   \n    g^{yx} \\bmod n\n   \n   .\n   \n    X^y \\bmod n = (g^x \\bmod n)^y \\bmod n = g^{xy} \\bmod n\n   \n   , which is the same thing Alice got. Nothing magic there, that's just how exponentiation and modulus arithmetic work. Ah, the glory of mathematics! So\n   \n    k\n   \n   is the same in both calculations and is known to both Alice and Bob.\n\n\nWhat about those eavesdroppers? They know\n   \n    g\n   \n   ,\n   \n    n\n   \n   ,\n   \n    X\n   \n   , and\n   \n    Y\n   \n   , but not\n   \n    x\n   \n   or\n   \n    y\n   \n   . They can compute\n   \n    k' = XY \\bmod n\n   \n   , but that is not equal to the\n   \n    k\n   \n   Alice and Bob calculated. They do have approaches to derive\n   \n    x\n   \n   or\n   \n    y\n   \n   , which would give them enough information to obtain\n   \n    k\n   \n   , but those approaches require them either to perform a calculation for every possible value of\n   \n    n\n   \n   (which is why you want\n   \n    n\n   \n   to be very large) or to compute a discrete logarithm. Computing a discrete logarithm is a solvable problem, but it's computationally infeasible for large numbers. So if the prime\n   \n    n\n   \n   is large (and meets other properties), the eavesdroppers are out of luck. How large? 600 digit primes should be good enough.\n\n\nNeat, no? But there is a fly in the ointment, when one considers using Diffie-Hellman over a network. It ensures that you securely share a key with someone, but gives you no assurance of who you're sharing the key with. Maybe Alice is sharing the key with Bob, as she thinks and hopes, but maybe she's sharing it with Mallory, who posed as Bob and injected his own\n   \n    Y\n   \n   . Since we usually care who we're in secure communication with, we typically augment Diffie-Hellman with an authentication mechanism to provide the assurance of our partner's identity.\n\n\nwill sign its Diffie-Hellman messages with its private key, which will allow the client to determine that its partner in this key exchange is the correct server. Typically, the client does not provide (or even have) its own certificate, so it cannot sign its Diffie-Hellman messages. This implies that when SSL's Diffie-Hellman key exchange completes, typically the client is pretty sure who the server is, but the server has no clue about the client's identity. (Again, this need not be the case for all uses of SSL. SSL includes connection creation options where both parties know each other's public key and the key exchange is authenticated on both sides. Those options are simply not the most commonly used ones, and particularly are not the ones typically used to secure web browsing.)\n\n\nRecalling our discussion earlier in this chapter, it actually isn't a problem for the server to be unsure about the client's identity at this point, in many cases. As we stated earlier, the client will probably want to use a password to authenticate itself, not a public key extracted from a certificate. As long as the server doesn't permit the client to do anything requiring trust before the server obtains and checks the client's password, the server probably doesn't care who the client is, anyway. Many servers offer some services to anonymous clients (such as providing them with publicly available information), so as long as they can get a password from the client before proceeding to more sensitive subjects, there is no security problem. The server can ask the client for a user ID and password later, at any point after the SSL connection is established. Since creating the SSL connection sets up a symmetric key, the exchange of ID and password can be protected with that key.\n\n\nA final word about SSL/TLS: it's a protocol, not a software package. There are multiple different software packages that implement this protocol. Ideally, if they all implement the protocol properly, they all interact correctly. However, they use different code to implement the protocol. As a result, software flaws in one implementation of SSL/TLS might not be present in other implementations. For example, the Heartbleed attack was based on implementation details of OpenSSL [H14], but was not present in other implementations, such as the version of SSL/TLS found in Microsoft's Windows operating system. It is also possible that the current protocol definition of SSL/TLS contains protocol flaws that would be present in any compliant implementation. If you hear of a security problem involving SSL, determine whether it is a protocol flaw or an implementation flaw before taking further action. If it's an implementation flaw, and you use a different implementation, you might not need to take any action in response."
        },
        {
          "name": "Other Authentication Approaches",
          "content": "While passwords and public keys are the most common ways to authenticate a remote user or machines, there are other options. One such option is used all the time. After you have authenticated yourself to a web site by providing a password, as we described above, the web site\n\n\nwill continue to assume that the authentication is valid. It won't ask for your password every time you click a link or perform some other interaction with it. (And a good thing, too. Imagine how much of a pain it would be if you had to provide your password every time you wanted to do anything.) If your session is encrypted at this point, it could regard your proper use of the cryptography as a form of authentication; but you might even be able to quit your web browser, start it up again, navigate back to that web site, and still be treated as an authenticated user, without a new request for your password. At that point, you're no longer using the same cryptography you used before, since you would have established a new session and set up a new cryptographic key. How did your partner authenticate that you were the one receiving the new key?\n\n\nIn such cases, the site you are working with has chosen to make a security tradeoff. It verified your identity at some time in the past using your password and then relies on another method to authenticate you in the future. A common method is to use\n   **web cookies**\n   . Web cookies are pieces of data that a web site sends to a client with the intention that the client stores that data and send it back again whenever the client next communicates with the server. Web cookies are built into most browsers and are handled invisibly, without any user intervention. With proper use of cryptography, a server that has verified the password of a client can create a web cookie that securely stores the client's identity. When the client communicates with the server again, the web browser automatically includes the cookie in the request, which allows the server to verify the client's identity without asking for a password again\n   \n    6\n   \n   .\n\n\nIf you spend a few minutes thinking about this authentication approach, you might come up with some possible security problems associated with it. The people designing this technology have dealt with some of these problems, like preventing an eavesdropper from simply using a cookie that was copied as it went across the network. However, there are other security problems (like someone other than the legitimate user using the computer that was running the web browser and storing the cookie) that can't be solved with these kinds of cookies, but could have been solved if you required the user to provide the password every time. When you build your own system, you will need to think about these sorts of security tradeoffs yourself. Is it better to make life simpler for your user by not asking for a password except when absolutely necessary, or is it better to provide your user with improved security by frequently requiring proof of identity? The point isn't that there is one correct an-\n\n\n6\n   \n   You might remember from the chapter on access control that we promised to discuss protecting capabilities in a network context using cryptography. That, in essence, is what these web cookies are. After a user authenticates itself with another mechanism, the remote system creates a cryptographic capability for that user that no one else could create, generally using a key known only to that system. That capability/cookie can now be passed back to the other party and used for future authorization operations. The same basic approach is used in a lot of other distributed systems.\n\n\nswer to this question, but that you need to think about such questions in the design of your system.\n\n\nThere are other authentication options. One example is what is called a\n   **challenge/response protocol**\n   . The remote machine sends you a challenge, typically in the form of a number. To authenticate yourself, you must perform some operation on the challenge that produces a response. This should be an operation that only the authentic party can perform, so it probably relies on the use of a secret that party knows, but no one else does. The secret is applied to the challenge, producing the response, which is sent to the server. The server must be able to verify that the proper response has been provided. A different challenge is sent every time, requiring a different response, so attackers gain no advantage by listening to and copying down old challenges and responses. Thus, the challenges and responses need not be encrypted. Challenge/response systems usually perform some kind of cryptographic operation, perhaps a hashing operation, on the challenge plus the secret to produce the response. Such operations are better performed by machines than people, so either your computer calculates the response for you or you have a special hardware token that takes care of it. Either way, a challenge/response system requires pre-arrangement between the challenging machine and the machine trying to authenticate itself. The hardware token or data secret must have been set up and distributed before the challenge is issued.\n\n\nAnother authentication option is to use an authentication server. In essence, you talk to a server that you trust and that trusts you. The party you wish to authenticate to must also trust the server. The authentication server vouches for your identity in some secure form, usually involving cryptography. The party who needs to authenticate you is able to check the secure information provided by the authentication server and thus determine that the server verified your identity. Since the party you wish to communicate with trusts the authentication server, it now trusts that you are who you claim to be. In a vague sense, certificates and CAs are an offline version of such authentication servers. There are more active online versions that involve network interactions of various sorts between the two machines wishing to communicate and one or more authentication servers. Online versions are more responsive to changes in security conditions than offline versions like CAs. An old certificate that should not be honored is hard to get rid of, but an online authentication server can invalidate authentication for a compromised party instantly and apply the changes immediately. The details of such systems can be quite complex, so we will not discuss them in depth. Kerberos is one example of such an online authentication server [NT94]."
        },
        {
          "name": "Some Higher Level Tools",
          "content": "In some cases, we can achieve desirable security effects by working at a higher level.\n   **HTTPS**\n   (the cryptographically protected version of the HTTP protocol) and\n   **SSH**\n   (a competitor to SSL most often used to set up secure sessions with remote computers) are two good examples.\n\n\n\n\n**HTTPS**\n\n\nHTTP, the protocol that supports the World Wide Web, does not have its own security features. Nowadays, though, much sensitive and valuable information is moved over the web, so sending it all unprotected over the network is clearly a bad idea. Rather than come up with a fresh implementation of security for HTTP, however, HTTPS takes the existing HTTP definition and connects it to SSL/TLS. SSL takes care of establishing a secure connection, including authenticating the web server using the certificate approach discussed earlier and establishing a new symmetric encryption key known only to the client and server. Once the SSL connection is established, all subsequent interactions between the client and server use the secured connection. To a large extent, HTTPS is simply HTTP passed through an SSL connection.\n\n\nThat does not devalue the importance of HTTPS, however. In fact, it is a useful object lesson. Rather than spend years in development and face the possibility of the same kinds of security flaws that other developers of security protocols inevitably find, HTTPS makes direct use of a high quality transport security tool, thus replacing an insecure transport with a highly secure transport at very little development cost.\n\n\nHTTPS obviously depends heavily on authentication, since we want to be sure we aren't communicating with malicious web sites. HTTPS uses certificates for that purpose. Since HTTPS is intended primarily for use in web browsers, the certificates in question are gathered and managed by the browser. Modern browsers come configured with the public keys of many certificate signing authorities (CAs, as we mentioned earlier). Certificates for web sites are checked against these signing authorities to determine if the certificate is real or bogus. Remember, however, what a certificate actually tells you, assuming it checks out: that at some moment in time the signing authority thought it was a good idea to vouch that a particular public key belongs to a particular party. There is no implication that the party is good or evil, that the matching private key is still secret, or even that the certificate signing authority itself is secure and uncompromised, either when it created the certificate or at the moment you check it. There have been real world problems with web certificates for all these reasons. Remember also that HTTPS only vouches for authenticity. An authenticated web site using HTTPS can still launch an attack on your client. An authenticated attack, true, but that won't be much consolation if it succeeds.\n\n\nNot all web browsers always supported HTTPS, typically because they didn't have SSL installed or configured. In those cases, a web site using HTTPS only would not be able to interact with the client, since the client couldn't set up its end of the SSL socket. The standard solution for web servers was to fall back on HTTP when a client claimed it was unable to use HTTPS. When the server did so, no security would be applied, just as if the server wasn't running HTTPS at all. As ability to support HTTPS in browsers and client machines has become more common, there has been\n\n\na push towards servers insisting on HTTPS, and refusing to talk to clients who can't or won't speak HTTPS. This approach is called HSTS (HTTP Strict Transport Security). HSTS is an option for a web site. If the web site decides it will support HSTS, all interactions with it will be cryptographically secured for any client. Clients who can't or won't accept HTTPS will not be allowed to interact with such a web site. HSTS is used by a number of major web sites, including Google's\n   \n    google.com\n   \n   domain, but is far from ubiquitous as of 2020.\n\n\nWhile HTTPS is primarily intended to help secure web browsing, it is sometimes used to secure other kinds of communications. Some developers have leveraged HTTP for purposes rather different than standard web browsing, and, for them, using HTTPS to secure their communications is both natural and cheap. However, you can only use HTTPS to secure your system if you commit to using HTTP as your application protocol, and HTTP was intended primarily to support a human-based activity. HTTP messages, for example, are typically encoded in ASCII and include substantial headers designed to support web browsing needs. You may be able to achieve far greater efficiency of your application by using SSL, rather than HTTPS. Or you can use SSH.\n\n\n\n\n**SSH**\n\n\nSSH stands for\n   **Secure Shell**\n   which accurately describes the original purpose of the program. SSH is available on Linux and other Unix systems, and to some extent on Windows systems. SSH was envisioned as a secure remote shell, but it has been developed into a more general tool for allowing secure interactions between computers. Most commonly this shell is used for command line interfaces, but SSH can support many other forms of secure remote interactions. For example, it can be used to protect remote X Windows sessions. Generally, TCP ports can be forwarded through SSH, providing a powerful method to protect interactions between remote systems.\n\n\nSSH addresses many of the same problems seen by SSL, often in similar ways. Remote users must be authenticated, shared encryption keys must be established, integrity must be checked, and so on. SSH typically relies on public key cryptography and certificates to authenticate remote servers. Clients frequently do not have their own certificates and private keys, in which case providing a user ID and password is permitted. SSH supports other options for authentication not based on certificates or passwords, such as the use of authentication servers (such as Kerberos). Various ciphers (both for authentication and for symmetric encryption) are supported, and some form of negotiation is required between the client and the server to choose a suitable set.\n\n\nA typical use of SSH provides a good example of a common general kind of network security vulnerability called a\n   **man-in-the-middle**\n   attack. This kind of attack occurs when two parties think they are communicating directly, but actually are communicating through a malicious third\n\n\nparty without knowing it. That third party sees all of the messages passed between them, and can alter such messages or inject new messages without their knowledge\n   \n    7\n   \n   .\n\n\nWell-designed network security tools are immune to man-in-the-middle attacks of many types, but even a good tool like SSH can sometimes be subject to them. If you use SSH much, you might have encountered an example yourself. When you first use SSH to log into a remote machine you've never logged into before, you probably don't have the public key associated with that remote machine. How do you get it? Often, not through a certificate or any other secure means, but simply by asking the remote site to send it to you. Then you have its public key and away you go, securely authenticating that machine and setting up encrypted communications. But what if there's a man in the middle when you first attempt to log into the remote machine? In that case, when the remote machine sends you its public key, the man in the middle can discard the message containing the correct public key and substitute one containing his own public key. Now you think you have the public key for the remote server, but you actually have the public key of the man in the middle. That means the man in the middle can pose as the remote server and you'll never be the wiser. The folks who designed SSH were well aware of this problem, and if you ever do use SSH this way, you will pop a message warning you of the danger and asking if you want to go ahead despite the risk. Folk wisdom suggests that everyone always says \"yes, go ahead\" when they get this message, including network security professionals. For that matter, folk wisdom suggests that all messages warning a user of the possibility of insecure actions are always ignored, which should suggest to you just how much security benefit will arise from adding such confirmation messages to your system.\n\n\nSSH is not built on SSL, but is a separate implementation. As a result, the two approaches each have their own bugs, features, and uses. A security flaw found in SSH will not necessarily have any impact on SSL, and vice versa."
        }
      ]
    }
  ]
}